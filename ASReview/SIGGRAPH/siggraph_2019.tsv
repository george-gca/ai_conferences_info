title	abstract	url	authors
 displays	A key challenge of volumetric displays is presenting a 3D scene as if naturally existed in the physical space. However, the displayable scenes are limited because current volumetric displays do not have a substantial depth reconstruction capability to show scenes with significant depth. In this talk, we propose a dynamic depth compression method that modifies the 3D geometries of presented scenes while considering changes to the spectator's view point such that entire scenes are fitted within a smaller depth range while maintaining the perceptual quality. Extensive depth compression induces a feeling of unnaturalness in viewers, but the results of an evaluation experiment using a volumetric display simulator indicated that a depth of just 10 cm was needed to show scenes that originally had about 50 m without an unacceptable feeling of unnaturalness. We applied our method to a real volumetric display and validated our findings through an additional user study. The results suggest that our method works well as a virtual extender of a volumetric display's depth reconstruction capability, enabling hundreds of times larger depth reconstruction than that of current volumetric displays.	https://dl.acm.org/authorize?N689308	Yamato Miyashita, Yasuhito Sawahata, Miwa Katayama, Kazuteru Komine
 migration	Traditional snapshot hyperspectral imaging systems include various optical elements: a dispersive optical element (prism), a coded aperture, several relay lenses, and an imaging lens, resulting in an impractically large form factor. We seek an alternative, minimal form factor of snapshot spectral imaging based on recent advances in diffractive optical technology. We thereupon present a compact, diffraction-based snapshot hyperspectral imaging method, using only a novel diffractive optical element (DOE) in front of a conventional, bare image sensor. Our diffractive imaging method replaces the common optical elements in hyperspectral imaging with a single optical element. To this end, we tackle two main challenges: First, the traditional diffractive lenses are not suitable for color imaging under incoherent illumination due to severe chromatic aberration because the size of the point spread function (PSF) changes depending on the wavelength. By leveraging this wavelength-dependent property alternatively for hyperspectral imaging, we introduce a novel DOE design that generates an anisotropic shape of the spectrally-varying PSF. The PSF size remains virtually unchanged, but instead the PSF shape rotates as the wavelength of light changes. Second, since there is no dispersive element and no coded aperture mask, the ill-posedness of spectral reconstruction increases significantly. Thus, we propose an end-to-end network solution based on the unrolled architecture of an optimization procedure with a spatial-spectral prior, specifically designed for deconvolution-based spectral reconstruction. Finally, we demonstrate hyperspectral imaging with a fabricated DOE attached to a conventional DSLR sensor. Results show that our method compares well with other state-of-the-art hyperspectral imaging methods in terms of spectral accuracy and spatial resolution, while our compact, diffraction-based spectral imaging method uses only a single optical element on a bare image sensor.	https://dl.acm.org/authorize?N688162	Daniel S. Jeon, Seung-Hwan Baek, Shinyoung Yi, Qiang Fu, Xiong Dun, Wolfgang Heidrich, Min H. Kim
"""Birth of Planet Earth"" fulldome excerpt: photosynthesis in a chromatophore"	Scientific visualization of the energy harvesting process of an early photosynthetic organelle called a chromatophore.	https://dl.acm.org/authorize?N689238	Donna Cox
"""Reality vs illusion"" real-time ray tracing"	"We will discuss how we achieved the goals for the production of Unity's ""Reality vs illusion"" demo, as well as give an overview of using real-time ray tracing technology in a games production pipeline. Our system is built on the insights learned by extending the engine architecture to support realtime ray tracing APIs and incorporating the power of realtime ray tracing to increase realism for resulting renders at interactive 30 fps rendering on high-end consumer hardware. Rendered in real-time, this demo spotlights one real car and another powered by Unity's rendering technology. A real-world car (2019 BMW 8 Series Coupe) was filmed, and then the scene was recreated using the exact camera / lighting conditions in CG. Then we transition the shots from a real-world car to the ray traced car. This shows off some of the effect that we will cover - global reflections, multi-layer transparency with refraction, area lights, shadows, ambient occlusion and more. We will share state-of-the art techniques developed for achieving high-visual quality in real-time rendering with our hybrid ray tracing / rasterization render pipeline, built on top of Unity's high definition rendering pipeline. The presentation will demonstrate the technology developed to extend the original rasterization-based pipeline to provide higher-fidelity rendering through the efficient usage of real-time ray tracing, for example, by rendering primary ray visibility for higher-fidelity materials including multi-layer smooth transparency, describe advanced approaches for shadowed textured realtime area lights, support of dynamic indirect diffuse and specular lighting as well as other global effects, such as ambient occlusion, reflections, and others, taking advantage of ray tracing algorithms and touch on runtime performance, including runtime BVH update."	https://dl.acm.org/authorize?N689123	Natalie Burke, Arisa Scott, Natalya Tatarchuk, Sebastien Lagarde
"""Space Explorers: Life in Orbit"": filming VR in microgravity"	In December 2018, TIME and Felix & Paul Studios launched virtual reality cameras---built to operate in microgravity---to the International Space Station. Since then, filming has documented astronauts from several countries in their daring missions more than 250 miles above Earth, capturing life in space as viewers have never truly seen before, and culminating in the first-ever spacewalk in cinematic virtual reality. Join Felix & Paul Studios, along with collaborators from NASA and the ISS National Lab, as they share insights from one of the most ambitious VR projects ever undertaken. In this production session, we will discuss the background of how this partnership came to be, before diving into the technical challenges of capturing cinematic virtual reality on the ISS. How do you direct a scene in such a tight and constrained place, especially while down on Earth? How can you transfer terabytes of data from the cameras to Mission Control? And finally, what does it take to build and operate cameras that can capture a spacewalk? The team will explore the variety of challenges inherent in such a groundbreaking project, from building a camera that can capture an EVA (extra-vehicular activity) in the extreme environment of space, to tracking and crafting months of astronaut footage into a cohesive episodic narrative. Finally, the team will share never-before-seen early footage from the project.	https://dl.acm.org/authorize?N689205	Sebastian Sylwan, Michael Interbartolo, Liz Warren, Felix Lajeunesse
1Inch VR	Upon discovering the mysterious crystal, the size of the protagonist became smaller. Later, he is chased by the insects, and he starts the adventure...	https://dl.acm.org/authorize?N689367	Seok Nam Koong, Yong Hwan Kim
"2D animation in the VR clouds: the making of disney's ""a kite's tale"""	"The experimental animated virtual reality short ""a kite's tale"" required cgi and hand-drawn characters to interact in a highly art-directed environment made of spectacular clouds. In this talk we'll examine the workflows developed to create the short, with particular emphasis on the integration of hand-drawn animation and performant real-time cloud rendering."	https://dl.acm.org/authorize?N689389	Bruce Wright, Michael Anderson, Angela McBride, Henrik Falt, Daniel Peixe, Tony DeRosa
2nd Step	"""2nd Step"" is a soaring VR journey through space --- accentuated with the beautiful music of Debussy and Ravel --- in which the viewer gets an intense feeling of being in the middle of alien worlds while visiting some of the most thrilling settings of current and future space missions."	https://dl.acm.org/authorize?N689366	Joerg Courtial
360-degree transparent holographic screen display	"We propose a technique for creating a feeling of reality to 2D images. We have succeeded in fabricating a holographic screen with higher transparency and luminance compared to one based on conventional technology. With the combination of a 360-degree transparent holographic screen display and sensing technology using multiple high-speed cameras, the observer gets the feeling that an object is ""actually there"". Fusion of the background and the image increases the feeling of ""floating"" in the image by using a holographic screen, and the multiple highspeed cameras can make the motion parallax image according to the position of the observer in real time. Therefore, the image seems to be located at the center of the cylinder."	https://dl.acm.org/authorize?N688249	Tomoharu Nakamura, Tomoya Yano, Kohki Watanabe, Yui Ishii, Hideki Ono, Ippei Tambata, Nobuki Furue, Yuji Nakahata
3D aerial display with micro mirror array plate and reversed depth integral photography	We propose a new aerial imaging display in which autostereoscopic objects with horizontal and vertical parallax appear as if they are floating in the air. This system operates by displaying an integral photography image in which depth is reversed beforehand and by observing the image from the other side of a micro mirror array.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338556	Nahomi Maki, Toshiaki Yamanouchi, Kazuhisa Yanaka
3D printing for mixed reality hands-on museum exhibit interaction	This work presents a combination of 3D printing with mixed reality to use the results in the context of museum exhibitions or for cultural heritage. While now priceless artefacts are encased in glass, kept safe and out of reach of the visitors, we present a new pipeline which would allow visitors hands-on interaction with realistic 3D printed replicas of the artefacts which are then digitally augmented to have the genuine artefacts' appearances.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338609	Laura Mann, Oleg Fryazinov
A Kite's Tale	"Walt Disney Animation Studios presents ""a kite's tale,"" an experimental animated short combining classic hand-drawn animation with virtual reality. A kite with the spirit of a playful puppy tangles with a pompous dragon-kite in a short that brings the audience into a breathtaking world."	https://dl.acm.org/authorize?N689365	Bruce Wright
A Monte Carlo framework for rendering speckle statistics in scattering media	We present a Monte Carlo rendering framework for the physically-accurate simulation of speckle patterns arising from volumetric scattering of coherent waves. These noise-like patterns are characterized by strong statistical properties, such as the so-called memory effect. These properties are at the core of imaging techniques for applications as diverse as tissue imaging, motion tracking, and non-line-of-sight imaging. Our rendering framework can replicate these properties computationally, in a way that is orders of magnitude more efficient than alternatives based on directly solving the wave equations. At the core of our framework is a path-space formulation for the covariance of speckle patterns arising from a scattering volume, which we derive from first principles. We use this formulation to develop two Monte Carlo rendering algorithms, for computing speckle covariance as well as directly speckle fields. While approaches based on wave equation solvers require knowing the microscopic position of wavelength-sized scatterers, our approach takes as input only bulk parameters describing the statistical distribution of these scatterers inside a volume. We validate the accuracy of our framework by comparing against speckle patterns simulated using wave equation solvers, use it to simulate memory effect observations that were previously only possible through lab measurements, and demonstrate its applicability for computational imaging tasks.	https://dl.acm.org/authorize?N688084	Chen Bar, Marina Alterman, Ioannis Gkioulekas, Anat Levin
A compact retinal scan near-eye display	A compact full color laser beam scanning (LBS)-based Augmented Reality (AR) near-eye display has been developed. By the unique relay optical system adopting a novel holographic grating to compensate the color dispersion of holographic image combiner in front of the eye, we have achieved high resolution (1280x720p), large field of view (47degree diagonal), high transparency (over 85%) and hand-held miniaturization. The prototype implemented only two connectors of USB Type-C and HDMI Type-D to supply power and video signal of HDMI interface respectively from small control box using two cables.	https://dl.acm.org/authorize?N688240	Katsuyuki Akutsu, Susumu Seino, Yusuke Ogawa, Kenji Ohki, Atsushi Takahashi, Daisuke Ueda, Ryo Ogawa, Teppei Imamura, Akira Yoshikaie
A data-driven compression method for transient rendering	Monte Carlo methods for transient rendering have become a powerful instrument to generate reliable data in transient imaging applications, either for benchmarking, analysis, or as a source for data-driven approaches. However, due to the increased dimensionality of time-resolved renders, storage and data bandwidth are significant limiting constraints, where a single time-resolved render of a scene can take several hundreds of megabytes. In this work we propose a learning-based approach that makes use of deep encoder-decoder architectures to learn lower-dimensional feature vectors of time-resolved pixels. We demonstrate how our method is capable of compressing transient renders up to a factor of 32, and recover the full transient profile making use of a decoder. Additionally, we show how our learned features significantly mitigate variance on the recovered signal, addressing one of the pathological problems in transient rendering.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338582	Yun Liang, Mingqin Chen, Zesheng Huang, Diego Gutierrez, Adolfo Muñoz, Julio Marco
A dataset for benchmarking time-resolved non-line-of-sight imaging	Time-resolved imaging has made it possible to by exploiting information from diffuse light bounces. While there have been successive improvements in the field since its conception, so far it has only been proven to work in very simple and controlled scenarios. We present a public dataset of synthetic time-resolved Non-Line-of-Sight (NLOS) scenes with varied complexity aimed at benchmarking reconstructions. It includes scenes that are common in the real world but remain a challenge for NLOS reconstruction methods due to the ambiguous nature of higher-order diffuse bounces naturally occurring in them. With over 300 reconstructible scenes, the dataset contains an order of magnitude more scenes than what is available currently. The final objective of the dataset it to boost NLOS research to take it closer to its real-world applications.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338583	Miguel Galindo, Julio Marco, Matthew O'Toole, Gordon Wetzstein, Diego Gutierrez, Adrian Jarabo
A deep dive into universal scene description and hydra	Physics-based animation has emerged as a core area of computer graphics finding widespread application in the film and video game industries as well as in areas such as virtual surgery, virtual reality, and training simulations. This course introduces students and practitioners to fundamental concepts in physics-based animation, placing an emphasis on breadth of coverage and providing a foundation for pursuing more advanced topics and current research in the area. The course focuses on imparting practical knowledge and intuitive understanding rather than providing detailed derivations of the underlying mathematics. The course is suitable for someone with no background in physics-based animation---the only prerequisites are basic calculus, linear algebra, and introductory physics. We begin with a simple, and complete, example of a mass-spring system, introducing the principles behind physics-based animation: mathematical modeling and numerical integration. From there, we systematically present the mathematical models commonly used in physics-based animation beginning with Newton's laws of motion and conservation of mass, momentum, and energy. We then describe the underlying physical and mathematical models for animating rigid bodies, soft bodies, and fluids. Then we describe how these continuous models are discretized in space and time, covering Lagrangian and Eulerian formulations, spatial discretizations and interpolation, and explicit and implicit time integration. In the final section, we discuss commonly used constraint formulations and solution methods.	https://dl.acm.org/authorize?N689269	George Elkoura, Sebastian Grassia, Sunya Boonyatera, Alex Mohr, Pol Jeremias-Vila, Matt Kuruc
A design for optical cloaking display	In the graphics research context, optical cloaking that hides an object by relaying a light field is also a kind of display. Despite much interest in the cloaking, large-size cloaking has not been achieved without some limitations and/or assumptions. To solve this problem, a computational design method is proposed for an optical cloaking display via viewpoint transformation. The method uses two kinds of passive optical elements that transfer a point into a plane symmetric point. In the experiments, a novel passive display was developed that optically cloaks the object and transmits a light field properly. Experimental results for the multiviewpoint scene that was captured are presented.	https://dl.acm.org/authorize?N688241	Takahito Aoto, Yuta Itoh, Kazuki Otao, Kazuki Takazawa, Yoichi Ochiai
A formal process to design visual archetypes based on character taxonomies	"While there are many professional examples of successful character designs, there seems to be little academic formalization in standardizing a process to achieve consistent visual results. In this work, we present such a formal process to construct visual designs for character archetypes that are given by ""verbal descriptions"". This process is based on visual semiotics that are used for creating clear meaning behind design choices while still retaining a sense of aesthetic through principles of artistic design. Using this process, we have developed a set of encyclopedic references for a wide variety of psychology and literary archetypes to demonstrate the power of this approach. We also used this process successfully in a visual storytelling class."	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338579	Angela Wang, Anthony Dalton Eason, Ergun Akleman
A learned shape-adaptive subsurface scattering model	In this paper, we introduce BiMocq , an unconditionally stable, pure Eulerianbased advection scheme to efficiently preserve the advection accuracy of all physical quantities for long-term fluid simulations. Our approach is built upon the method of characteristic mapping (MCM). Instead of the costly evaluation of the temporal characteristic integral, we evolve the mapping function itself by solving an advection equation for the mappings. Dual mesh characteristics (DMC) method is adopted to more accurately update the mapping. Furthermore, to avoid visual artifacts like instant blur and temporal inconsistency introduced by re-initialization, we introduce multi-level mapping and back and forth error compensation. We conduct comprehensive 2D and 3D benchmark experiments to compare against alternative advection schemes. In particular, for the vortical flow and level set experiments, our method outperforms almost all state-of-art hybrid schemes, including FLIP, PolyPic and Particle-Level-Set, at the cost of only two Semi-Lagrangian advections. Additionally, our method does not rely on the particle-grid transfer operations, leading to a highly parallelizable pipeline. As a result, more than 45× performance acceleration can be achieved via even a straightforward porting of the code from CPU to GPU.	https://dl.acm.org/authorize?N688173	Ziyin Qu, Xinxin Zhang, Ming Gao, Chenfanfu Jiang, Baoquan Chen
A look into five years of locomotion in virtual reality	Survios, a virtual reality (VR) game developer dedicated to building active, immersive experiences that push the limits of VR innovation, developed a range of proprietary locomotion systems in VR to solve for simulator sickness and immersive gameplay.	https://dl.acm.org/authorize?N689374	Alex Silkin
A low-discrepancy sampler that distributes monte carlo errors as a blue noise in screen space	An image-space hierarchy is introduced to reduce artifacts from prematurely stopping in adaptive sampling in a Monte Carlo ray tracing context, while maintaining good performance and fitting into an existing sampling architecture.	https://dl.acm.org/authorize?N689332	Keith Jeffery
A method for rectifying inclination of panoramic images	We propose a method for transforming inclined panoramic images into upright posture as if they were captured up straight. The method rectifies images automatically using neither additional information nor external instructions, while it enables panoramic images of both indoor and outdoor scenes to stand upright robustly. It helps unskilled people to make high quality panoramic images used for widespread applications including city navigation and real estate easier.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338558	Naoki Kawai
A null-scattering path integral formulation of light transport	Unbiased rendering of general, heterogeneous participating media currently requires using null-collision approaches for estimating transmittance and generating free-flight distances. A long-standing limitation of these approaches, however, is that the corresponding path pdfs cannot be computed due to the black-box nature of the null-collision rejection sampling process. These techniques therefore cannot be combined with other sampling techniques via multiple importance sampling (MIS), which significantly limits their robustness and generality. Recently, Galtier et al. [2013] showed how to derive these algorithms directly from the radiative transfer equation (RTE). We build off this generalized RTE to derive a formulation of null scattering, which reveals the sampling pdfs and allows us to devise new, express existing, and combine complementary unbiased techniques via MIS. We demonstrate the practicality of our theory by combining, for the first time, several path sampling techniques in spatially and spectrally varying media, generalizing and outperforming the prior state of the art.	https://dl.acm.org/authorize?N688099	Bailey Miller, Iliyan Georgiev, Wojciech Jarosz
A portal for managing reviews and beyond	In order to meet the rapidly evolving needs of the VFX industry, studios need to be able to adapt and upgrade quickly. However, the infrastructure stack at most companies is complex, usually set up over a number of years with significant customizations and proprietary software. Maintaining this stack requires dedicated teams. Upgrades can take months and are usually fraught with risk. The engineering team at MPC drastically reduced this time to deployment from months to a few days by using cloud native solutions. Built on a foundation of microservices, the infrastructure stack provides an asset management system, storage, sync and compute capabilities. Within the first year it was deployed across two sites in different timezones, supporting up to 200 artists. Thus proving the ability for VFX studios to scale rapidly.	https://dl.acm.org/authorize?N689312	Natasha Kelkar
A practical guide to thin film and drips simulation	The environments of are immersed in waterfalls, from lush Nordic islands to mysterious, submerged kingdoms of lakes and crystals. Because of the variety and complexity of these sets (about 4000 waterfalls had to be created), a traditional approach would have consumed too many artistic and rendering resources. We decided to develop a collection of tools named with a few goals in mind: streamlining the creative process by minimizing the time spent on simulations, iterating faster thanks to real-time feedback, and managing the large quantity of generated data. Using this system, artists were able to create all the water elements of an environment set, from misty calderas to foamy, aerated ponds.	https://dl.acm.org/authorize?N689346	Baptiste Van Opstal, Youxi Woo, Amaury Aubel
A procedural approach to creating second empire houses	In this work, we present a procedural approach to capture a variety of appearances of American Second Empire houses. To develop this procedural approach, we have identified the set of rules and similarities of Second Empire houses. Our procedural approach, therefore, captures the style differences of Second Empire houses with a relatively few numbers of parameters. Using our interface, we are able to generate virtual houses in a wide variety of styles of American Second Empire architecture. We have also developed a method to break up these virtual models into slices in order to efficiently and economically 3D print them. Using this approach we have created miniatures of two landmark buildings: the Hamilton-Turner Inn in Savannah and the Enoch Pratt House in Baltimore. Note that the virtual models still provide more details because of the limited resolution of 3D printing processes.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338549	Madison Kramer, Ergun Akleman
A ragdoll-less approach to physical animations of characters in vehicles	We introduce a sampler that generates per-pixel samples achieving high visual quality thanks to two key properties related to the Monte Carlo errors that it produces. First, the sequence of each pixel is an Owen-scrambled Sobol sequence that has state-of-the-art convergence properties. The Monte Carlo errors have thus low magnitudes. Second, these errors are distributed as a blue noise in screen space. This makes them visually even more acceptable. Our sampler is lightweight and fast. We implement it with a small texture and two xor operations.	https://dl.acm.org/authorize?N689331	Eric Heitz, Laurent Belcour, V. Ostromoukhov, David Coeurjolly, Jean-Claude Iehl
A scalable real-time many-shadowed-light rendering system	In this paper, we present a new shadow rendering system, with a number of novel design to support large amount of shadowed lights in a large virtual environment, with real-time performance on mainstream GPUs.	https://dl.acm.org/authorize?N689393	Bo Li
A similarity measure for material appearance	We present a compact and efficient representation of spectra for accurate rendering using more than three dimensions. While tristimulus color spaces are sufficient for color display, a spectral renderer has to simulate light transport per wavelength. Consequently, emission spectra and surface albedos need to be known at each wavelength. It is practical to store dense samples for emission spectra but for albedo textures, the memory requirements of this approach are unreasonable. Prior works that approximate dense spectra from tristimulus data introduce strong errors under illuminants with sharp peaks and in indirect illumination. We represent spectra by an arbitrary number of Fourier coefficients. However, we do not use a common truncated Fourier series because its ringing could lead to albedos below zero or above one. Instead, we present a novel approach for reconstruction of bounded densities based on the theory of moments. The core of our technique is our bounded maximum entropy spectral estimate. It uses an efficient closed form to compute a smooth signal between zero and one that matches the given Fourier coefficients exactly. Still, a ground truth that localizes all of its mass around a few wavelengths can be reconstructed adequately. Therefore, our representation covers the full gamut of valid reflectances. The resulting textures are compact because each coefficient can be stored in 10 bits. For compatibility with existing tristimulus assets, we implement a mapping from tristimulus color spaces to three Fourier coefficients. Using three coefficients, our technique gives state of the art results without some of the drawbacks of related work. With four to eight coefficients, our representation is superior to all existing representations. Our focus is on offline rendering but we also demonstrate that the technique is fast enough for real-time rendering.	https://dl.acm.org/authorize?N688181	Christoph Peters, Sebastian Merzbach, Johannes Hanika, Carsten Dachsbacher
A stretch-sensing soft glove for interactive hand pose estimation	We present a stretch-sensing soft glove to interactively capture full hand poses with high accuracy and without requiring an external optical setup. Our device can be fabricated with simple tools available in most fabrication labs. The pose is reconstructed from a capacitive sensor array embedded in the glove. We propose a data representation that allows deep neural networks to exploit the spatial layout of the sensor itself. The network is trained only once, using an inexpensive off-the-shelf hand pose reconstruction system to gather the training data. The per-user calibration is then performed on-the-fly using only the glove.	https://dl.acm.org/authorize?N688242	Oliver Glauser, Shihao Wu, Daniele Panozzo, Otmar Hilliges, Olga Sorkine-Hornung
A symmetric objective function for ICP	The Iterative Closest Point (ICP) algorithm, commonly used for alignment of 3D models, has previously been defined using either a point-to-point or point-to-plane objective. Alternatively, researchers have proposed computationally-expensive methods that directly minimize the distance function between surfaces. We introduce a new symmetrized objective function that achieves the simplicity and computational efficiency of point-to-plane optimization, while yielding improved convergence speed and a wider convergence basin. In addition, we present a linearization of the objective that is exact in the case of exact correspondences. We experimentally demonstrate the improved speed and convergence basin of the symmetric objective, on both smooth models and challenging cases involving noise and partial overlap.	https://dl.acm.org/authorize?N688030	Szymon Rusinkiewicz
A transparent display with per-pixel color and opacity control	We propose a new display system that composites matted foreground animated graphics and video, with per-pixel controllable emitted color and transparency, over real-world dynamic objects seen through a transparent display. Multiple users can participate simultaneously without any glasses, trackers, or additional devices. The current prototype is deployed as a desktop-monitor-sized transparent display box assembled from commodity hardware components with the addition of a high-frame-rate controllable diffuser.	https://dl.acm.org/authorize?N688243	TJ Rhodes, Gavin Miller, Qi Sun, Daichi Ito, Li-Yi Wei
Accurate markerless jaw tracking for facial performance capture	We present the first method to accurately track the invisible jaw based solely on the visible skin surface, without the need for any markers or augmentation of the actor. As such, the method can readily be integrated with off-the-shelf facial performance capture systems. The core idea is to learn a non-linear mapping from the skin deformation to the underlying jaw motion on a dataset where ground-truth jaw poses have been acquired, and then to retarget the mapping to new subjects. Solving for the jaw pose plays a central role in visual effects pipelines, since accurate jaw motion is required when retargeting to fantasy characters and for physical simulation. Currently, this task is performed mostly manually to achieve the desired level of accuracy, and the presented method has the potential to fully automate this labour intense and error prone process.	https://dl.acm.org/authorize?N688005	Gaspard Zoss, Thabo Beeler, Markus Gross, Derek Bradley
Active textile tailoring	Active Textile Tailoring is a new process for creating smart textiles in which its fibers change shape and structure in response to heat. This adaptive textile can create a new type of sizing customization or aesthetic patterning for the preference of individual customers. This system was developed in collaboration with MIT, Ministry of Supply, Hills Inc. and Iowa State University with support from the federal non-profit Advanced Functional Fabrics of America (AFFOA).	https://dl.acm.org/authorize?N688244	Lavender Tessmer, Carmel Dunlap, Bjorn Sparrman, Schendy Kernizan, Jared Laucks, Skylar Tibbits
Adaptive environments with parallel reality	It is challenging to provide signage for the individual needs of each person in crowded, public spaces. Having too many signs leads to a cluttered environment, while having too few signs can fail to provide needed information to individuals. Personal display devices, such as smart phones and AR glasses can provide adaptive content, but they require each user to have a compatible device with them, turned on, and running appropriate software. PARALLEL REALITY™ displays are a new type of shared, public display that can simultaneously target personalized content to each viewer, without special glasses. In this way, adaptation becomes a feature of a venue that is available to all without encumbrances. Examples include adaptation based on language needs, visual acuity and relative location of the display.	https://dl.acm.org/authorize?N689307	Paul H. Dietz, Matt Lathrop
Advances in real-time rendering in games, part 1	"The Advanced Visualization Lab (AVL) is part of the the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign. The AVL is led by Professor Donna Cox, who coined the term ""Renaissance Team"", with the belief that bringing together specialists of diverse backgrounds creates a team that is greater than the sum of its parts, and members of the AVL team reflect that in our interdisciplinarity. We specialize in creating high-quality cinematic scientific visualizations of supercomputer simulations for public outreach."	https://dl.acm.org/citation.cfm?id=3335035	Natalya Tatarchuk
Advances in real-time rendering in games, part 2	Manufacturing Today	https://dl.acm.org/citation.cfm?id=3335036	Natalya Tatarchuk
AffectiveHMD: facial expression recognition in head mounted display using embedded photo reflective sensors	We propose a facial expression mapping technology between virtual avatars and Head-Mounted Display (HMD) users. HMDs allow people to enjoy an immersive Virtual Reality (VR) experience. A virtual avatar can be a representative of the user in the virtual environment. However, the synchronization of the virtual avatar's expressions with those of the HMD user is limited. The major problem of wearing an HMD is that a large portion of the user's face is occluded, making facial recognition difficult in an HMD-based virtual environment. To overcome this problem, we propose a facial expression mapping technology using photo-reflective sensors. The sensors attached inside the HMD measure the reflection intensity between the sensors and the user's face. The intensity values of five basic facial expressions (Neutral, Happy, Angry, Surprised, and Sad) are used for training a classifier to estimate the facial expression of a user. In Siggraph 2019, the user can enjoy two application, the facial expression synchronization with the avatar, and simple manipulation experience for a virtual environment by facial expressions.	https://dl.acm.org/authorize?N688355	Masaaki Murakami, Kosuke Kikui, Katsuhiro Suzuki, Fumihiko Nakamura, Masaaki Fukuoka, Katsutoshi Masai, Yuta Sugiura, Maki Sugimoto
Affine interpolation in a lie group framework	Affine transformations are of vital importance in many tasks pertaining to motion design and animation. Interpolation of affine transformations is non-trivial. Typically, the given affine transformation is decomposed into simpler components which are easier to interpolate. This may lead to unintuitive results, while in some cases, such solutions may not work. In this work, we propose an interpolation framework which is based on a Lie group representation of the affine transformation. The Lie group representation decomposes the given transformation into simpler and meaningful components, on which computational tools like the exponential and logarithm maps are available in closed form. Interpolation exists for all affine transformations while preserving a few characteristics of the original transformation. A detailed analysis and several experiments of the proposed framework are included.	https://dl.acm.org/authorize?N688026	Sumukh Bansal, Aditya Tatu
Age of Sail	"Set on the open ocean in 1900, ""Age of Sail"" is the story of William Avery (voiced by Ian McShane), an old sailor adrift and alone in the North Atlantic. When Avery reluctantly rescues Lara, who has mysteriously fallen overboard, he finds redemption and hope in his darkest hours."	https://dl.acm.org/authorize?N689368	John Kahrs
Air: augmented intersection of realities	An interactive presentation of AR research on the augmentation of the interfaces between computing systems, leveraging physical space as the medium to interact, transfer, and actuate digital content.	https://dl.acm.org/authorize?N689149	Hisham Bedri, Christian Vazquez, Valentin Heun, Anna Fusté, Benjamin Reynolds
Alita: Battle Angel	"""Alita: Battle Angel"" follows the young cyborg Alita as she unearths her extraordinary past. Her palpable humanness was key to the film's success and her character represents a new standard for humanoid CG characters in a lead role."	https://dl.acm.org/authorize?N689240	Eric Saindon
Alita: Battle Angel: the art of being human	Alita: Battle Angel follows the young cyborg Alita as she unearths her extraordinary past. Her palpable humanness was key to the film's success and her character represents a new standard of photoreal digital doubles and humanoid CG character realization. Visual Effects Supervisor Nick Epstein will discuss how Weta Digital's advances in performance capture, CG biology, and facial animation brought her story to life.	https://dl.acm.org/authorize?N689209	Nick Epstein
An adaptive variational finite difference framework for efficient symmetric octree viscosity	While pressure forces are often the bottleneck in (near-)inviscid fluid simulations, viscosity can impose orders of magnitude greater computational costs at lower Reynolds numbers. We propose an implicit octree finite difference discretization that significantly accelerates the solution of the free surface viscosity equations using adaptive staggered grids, while supporting viscous buckling and rotation effects, variable viscosity, and interaction with scripted moving solids. In experimental comparisons against regular grids, our method reduced the number of active velocity degrees of freedom by as much as a factor of 7.7 and reduced linear system solution times by factors between 3.8 and 9.4. We achieve this by developing a novel adaptive variational finite difference methodology for octrees and applying it to the optimization form of the viscosity problem. This yields a linear system that is symmetric positive definite by construction, unlike naive finite difference/volume methods, and much sparser than a hypothetical finite element alternative. Grid refinement studies show spatial convergence at first order in and second order in , while the significantly smaller size of the octree linear systems allows for the solution of viscous forces at higher effective resolutions than with regular grids. We demonstrate the practical benefits of our adaptive scheme by replacing the regular grid viscosity step of a commercial liquid simulator (Houdini) to yield large speed-ups, and by incorporating it into an existing inviscid octree simulator to add support for viscous flows. Animations of viscous liquids pouring, bending, stirring, buckling, and melting illustrate that our octree method offers significant computational gains and excellent visual consistency with its regular grid counterpart.	https://dl.acm.org/authorize?N688049	Ryan Goldade, Yipeng Wang, Mridul Aanjaneya, Christopher Batty
An introduction to physics-based animation	"Real-time graphics has come a long way since the ""brute-force approach"" of rasterization had been classified ""ridiculously expensive"" in 1974. Henceforth the promise ""Ray tracing is the future and ever will be"" drove the development of ray tracing algorithms and hardware, and resulted in a major revolution of image synthesis. This course will take a look at how far out the future is, review the state of the art, and identify the current challenges for research. Not surprisingly, it looks like we are not done with ray tracing, yet."	https://dl.acm.org/authorize?N689260	Adam W. Bargteil, Tamar Shinar
An intuitive and educational programming tool with tangible blocks and AR	In recent years, there has been a trend in teaching programming in elementary schools around the world. When teaching programming to lower grader students, robots and puzzles are often used to learn programming for easy understanding. However, those tools have limitations in execution results. Higher graders often use visual programming as a learning material. However, there is a problem that visual programming requires one computer for each individual student, and many students have to learn how to use a computer first. Therefore we propose a programming tool using tangible blocks and AR. This makes it possible to learn programming intuitively with fewer restrictions. Our tool is operated using a smartphone and tangible blocks without using a computer. By using AR, it is possible to create an intuitive programming that can interact with reality. We asked teachers who have experience teaching programming to children to assess the usefulness of our tool within programming education in school. As a result, there was an opinion that it might be suitable for multi-person programming.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338570	Keisuke Hattori, Tatsunori Hirai
Anisotropic elasticity for inversion-safety and element rehabilitation	We present an analysis of anisotropic hyperelasticity, specifically transverse isotropy, that obtains closed-form expressions for the eigendecompositions of many common energies. We then use these to build fast and concise Newton implementations. We leverage our analysis in two separate applications. First, we show that existing anisotropic energies are not inversion-safe, and contain spurious stable states under large deformation. We then propose a new anisotropic strain invariant that enables the formulation of a novel, robust, and inversion-safe energy. The new energy fits completely within our analysis, so closed-form expressions are obtained for its eigensystem as well. Secondly, we use our analysis to badly-conditioned finite elements. Using this method, we can robustly simulate large deformations even when a mesh contains degenerate, zero-volume elements. We accomplish this by swapping the badly-behaved isotropic direction with a well-behaved anisotropic term. We validate our approach on a variety of examples.	https://dl.acm.org/authorize?N688014	Theodore Kim, Fernando De Goes, Hayley Iben
Approaches for immersive media curriculum implementation	The rapid expansion of availability, affordability and implementation of immersive media technologies in the market place has spawned an increasing interest in integration of design and production methodologies into higher education curriculum, while simultaneously posing a host of challenges to educators when with regards to resources, pedagogy and identifying industry needs. This panel brings together the directors of programs from Texas A&M University, The Ringling College of Art + Design, and Drexel University who have integrated immersive media into existing programs in visualization, as well as launched entire degree programs focused on the teaching design and production methodologies, to share and discuss their experiences with fellow	https://dl.acm.org/authorize?N688254	Nick Jushchyshyn, Timothy McLaughlin, Morgan Woolverton
Architecture challenges in the Android 3D graphics stack	The increasing traction of high-fidelity games on mobile devices is highlighting the challenges game developers have to face in order to optimize their content within the Android ecosystem. In this talk, we'll explain our understanding of these challenges through the lens of how Android's graphics stack works today. If you've ever wondered: • Are Android graphics drivers as buggy as I've heard? • Why is there so much difference from device to device? • Why aren't there great profilers like on console? • Why can't I just measure draw call timings like on desktop? • Why aren't graphics drivers updatable? ... then this talk is for you! We'll cover the way the hardware ecosystem for Android works, including the quirks of SOC vs. OEM vs. IP makers and how that translates to unique challenges. Then we will cover how software flows through this ecosystem and out through carriers, and the challenges that brings. We will talk about how the unique architecture features on mobile translate to new types of tooling challenges. Finally, we will talk about parallels between these combined challenges and other more traditional driver models from Windows or Mac, and discuss some of the implications thereof.	https://dl.acm.org/authorize?N689383	Pau Baiget
Are we done with ray tracing?	In computer graphics, many traditional problems are now better handled by deep-learning based data-driven methods. In applications that operate on regular 2D domains, like image processing and computational photography, deep networks are state-of-the-art, often beating dedicated hand-crafted methods by significant margins. More recently, other domains such as geometry processing, animation, video processing, and physical simulations have benefited from deep learning methods as well, often requiring application-specific learning architectures. The massive volume of research that has emerged in just a few years is often difficult to grasp for researchers new to this area. This course gives an organized overview of core theory, practice, and graphics-related applications of deep learning.	https://dl.acm.org/citation.cfm?id=3329896	Alexander Keller, Timo Viitanen, Colin Barré-Brisebois, Christoph Schied, Morgan McGuire
Arque: artificial biomimicry-inspired tail for extending innate body functions	For most mammals and vertebrate animals, tail plays an important role for their body providing variant functions to expand their mobility, or as a limb that allows manipulation and gripping. In this work, Arque, we propose an artificial biomimicry-inspired anthropomorphic tail to allow us alter our body momentum for assistive, and haptic feedback applications. The proposed tail consists of adjacent joints with a spring-based structure to handle shearing and tangential forces, and allow managing the length and weight of the target tail. The internal structure of the tail is driven by four pneumatic artificial muscles providing the actuation mechanism for the tail tip. Here we highlight potential applications for using such prosthetic tail as an extension of human body to provide active momentum alteration in balancing situations, or as a device to alter body momentum for full-body haptic feedback scenarios.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338573	Junichi Nabeshima, MHD Yamen Saraiji, Kouta Minamizawa
Arque: artificial biomimicry-lnspired tail for extending innate body functions	For most mammals and vertebrate animals, tail plays an important role for their body providing variant functions to expand their mobility, or as a limb that allows manipulation and gripping. In this work, Arque, we propose an artificial biomimicry-inspired anthropomorphic tail to allow us alter our body momentum for assistive, and haptic feedback applications. The proposed tail consists of adjacent joints with a spring-based structure to handle shearing and tangential forces, and allow managing the length and weight of the target tail. The internal structure of the tail is driven by four pneumatic artificial muscles providing the actuation mechanism for the tail tip. Here we highlight potential applications for using such prosthetic tail as an extension of human body to provide active momentum alteration in balancing situations, or as a device to alter body momentum for full-body haptic feedback scenarios.	https://dl.acm.org/authorize?N688356	Junichi Nabeshima, MHD Yamen Saraiji, Kouta Minamizawa
Atlas refinement with bounded packing efficiency	We present a novel algorithm to refine an input atlas with bounded packing efficiency. Central to this method is the use of the axis-aligned structure that converts the general polygon packing problem to a rectangle packing problem, which is easier to achieve high packing efficiency. Given a parameterized mesh with no flipped triangles, we propose a new angle-driven deformation strategy to transform it into a set of axis-aligned charts, which can be decomposed into rectangles by the motorcycle graph algorithm. Since motorcycle graphs are not unique, we select the one balancing the trade-off between the packing efficiency and chart boundary length, while maintaining bounded packing efficiency. The axis-aligned chart often contains greater distortion than the input, so we try to reduce the distortion while bounding the packing efficiency and retaining bijection. We demonstrate the efficacy of our method on a data set containing over five thousand complex models. For all models, our method is able to produce packed atlases with bounded packing efficiency; for example, when the packing efficiency bound is set to 80%, we elongate the boundary length by an average of 78.7% and increase the distortion by an average of 0.0533%. Compared to state-of-the-art methods, our method is much faster and achieves greater packing efficiency.	https://dl.acm.org/authorize?N688088	Hao-Yu Liu, Xiao-Ming Fu, Chunyang Ye, Shuangming Chai, Ligang Liu
Autism XR	Students with ASD often struggle with social communication and behaviors. Autism XR provides a safe web-based augmented reality (WebXR) environment that allows the user to privately practice their social skills and behaviors on a mobile device.	https://dl.acm.org/authorize?N689140	Laura Robinson, Keith Takens, Mary Musto, Marc Petz
Autofocals: evaluating gaze-contingent eyeglasses for presbyopes	Current-generation virtual reality (VR) displays aim to generate perceptually realistic user experiences by accurately rendering many perceptually important effects including perspective, disparity, motion parallax, and other depth cues. We introduce ocular parallax rendering, a technology that renders small amounts of gaze-contingent parallax capable of further increasing perceptual realism in VR. Ocular parallax, small depth-dependent image shifts on the retina created as the eye rotates, occurs because the centers of rotation and projection of the eye are not the same. We study the perceptual implications of ocular parallax rendering by designing and conducting a series of user experiments. We estimate perceptual detection and discrimination thresholds for this effect and demonstrate that it is clearly visible in most VR applications. However, our studies also indicate that ocular parallax rendering does not significantly improve depth perception in VR.	https://dl.acm.org/authorize?N689329	Robert Konrad, Anastasios Angelopoulos, Gordon Wetzstein
Automatic layout generation for graphical design magazines	We propose a system that automatically generates layouts for magazines that require graphical design. In this system, when images or texts are input as the content to be placed in layouts, an appropriate layout is automatically generated in consideration of content and design. The layout generation process is performed by randomized processing in accordance with a rule set of minimum conditions that must be satisfied for layouts (minimum condition rule set), where a large number of candidates are generated. The appearance, style, design, and composition of the work are evaluated by a learning-to-rank estimator, top scores are returned to the user. Users can greatly improve the efficiency of layout creation/editing by selecting from among automatically generated candidate layouts.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338574	Sou Tabata, Hiroki Yoshihara, Haruka Maeda, Kei Yokoyama
Avengers: Endgame, a new approach for combustion simulations	"Fire, from small-scale candle flames to enormous explosions, remains an area of special interest in visual effects. Even compared to regular fluid simulation, the wide range of chemical reactions with corresponding generated motion and illumination makes for highly complex visual phenomena, difficult for an artist to recreate directly. The goal of our software is to provide attractive physical and chemical simulation workflows, which enable the artist to automaticall achieve ""physically plausible"" results by default. Ideally, these results should come close to matching real-world footage if the real-world parameters are known (such as what fuel is being burned). To support this, we aim to provide a user interface for artistic direction where the controls map intuitively to changes in the visual result. More user-demanding proceduralism will only occasionally be required for final artistic tweaks or hero shots."	https://dl.acm.org/authorize?N689303	Michael B. Nielsen, Konstantinos Stamatelos, Morten Bojsen-Hansen, Robert Bridson
Beach body bros	While showcasing Oculus Medium as a production-ready sculpting tool, Beach Body Bros takes you on a ridiculous musical journey through a world of pure muscle, intense strength, and stupid greatness.	https://dl.acm.org/authorize?N689136	Tyler Hurd
Being henry	After experiencing a stroke that left him paralyzed, Henry Evans uses cutting-edge technology to keep him connected to the things he loves. In this first-person interactive documentary, see the world through Henry's eyes --- and the eyes of his robots.	https://dl.acm.org/authorize?N689141	Sarah Waintraub
Best Friend	In a near future, a lonely man is addicted to a product called «Best Friend» which offers him perfect virtual friends.	https://dl.acm.org/authorize?N689249	Nicholas Olivieri
Beyond trilinear interpolation: higher quality for free	In volume-rendering applications, it is a de facto standard to reconstruct the underlying continuous function by using trilinear interpolation, and to estimate the gradients for the shading computations by calculating central differences on the fly. In a GPU implementation, this requires seven trilinear texture samples: one for the function reconstruction, and six for the gradient estimation. In this paper, for the first time, we show that the six additional samples can be used not just for gradient estimation, but for significantly improving the quality of the function reconstruction as well. As the additional arithmetic operations can be performed in the shadow of the texture fetches, we can achieve this quality improvement for free without reducing the rendering performance at all. Therefore, our method can completely replace the standard trilinear interpolation in the practice of GPU-accelerated volume rendering.	https://dl.acm.org/authorize?N688001	Balázs Csébfalvi
Biodigital: transform data to experience, beyond data visualization	"Biodigital"" is a Sci-Fi interactive story set in the year 2117 that combines VR film, immersive 3D environments, and VR data visualization. It turns data into a cinematic experience where a user is enmeshed as a character in the story. This VR storytelling tells the tale of humanity a hundred years from now. It also encourages us to think ""How should we live in the future?"	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338542	Takahito Ito, Cesar A. Hidalgo
Bone mother: the challenges of making an indie 3D printed film	After almost 5 years with a small crew, Bone Mother is the first National Film Board of Canada film to explore 3d printing. The filmmakers used 3d printing to achieve a wide range of expressions and dialogue, but also to help build the puppets, sets and rigs. While developing new production pipelines that incorporated this technology into stop-motion, the team found that as one solution was solved, a new challenge surfaced.	https://dl.acm.org/authorize?N689388	Dale Hayward
Bonfire	You've just crashed your spaceship on a strange planet. Your only tools for survival are a fading bonfire, a surly robot, and a limited supply of nourishment cylinders. And, what's that sneaking around out there in the dark alien jungle?	https://dl.acm.org/authorize?N689354	Larry Cutler, Wei Wang
Boosting VFX production with deep learning	Machine learning techniques are not often associated with artistic work such as visual effects production. Nevertheless, these techniques can save a lot of time for artists when used in the right context. In recent years, deep learning techniques have become a widely used tool with powerful frameworks that can be employed in a production environment. We present two deep learning solutions that were integrated into our production pipeline and used in current productions. One method generates high quality images from a compressed video file that contains various compression artifacts. The other quickly locates slates and color charts used for grading in a large set of images. We discuss these particular solutions in the context of previous work, as well as the challenges of integrating a deep learning solution within a VFX production pipeline, from concept to implementation.	https://dl.acm.org/authorize?N689398	Yanir Kleiman, Simon Pabst, Patrick Nagle
Branding & marketing: strategies for global talent acquisition in today's digital media productions market	Whether you're an individual or a company, getting noticed in today's highly competitive digital media production marketplace presents numerous challenges. There is an increased reliance on effective branding strategies and marketing techniques to cut through the clutter. The panel discussion will touch on multiple issues surrounding branding and marketing for individuals and companies, highlighting unique and shared challenges.	https://dl.acm.org/authorize?N688128	Stan Szymanski
Building modern VFX infrastructure	Pixar Universal Scene Description technology is well known in the VFX world. With its promising goal to unify and ease the interchanges of data between Digital Content Creation tools (DCC), it has convinced the industry since its initial open source release in 2016. The question is no more but	https://dl.acm.org/authorize?N689313	Lucille Caillaud, Robin De Lillo, Guillaume Laforge, Jean-Christophe Morin
CD-MPM: continuum damage material point methods for dynamic fracture animation	We propose a robust method for untangling an arbitrary number of cloth layers, possibly exhibiting deep interpenetrations, to a collision-free state, ready for animation. Our method relies on an intermediate, implicit representation to solve the problem: the user selects a few garments stored in a library together with their implicit approximations, and places them over a mannequin while specifying the desired order between layers. The intersecting implicit surfaces are then combined using a new family of N-ary composition operators, specially designed for untangling layers. Garment meshes are finally projected to the deformed implicit surfaces in linear time, while best preserving triangles and avoiding loss of details. Each of the untangling operators computes the target surface for a given garment in a single step, while accounting for the order between cloth layers and their individual thicknesses. As a group, they guarantee an intersection-free output configuration. Moreover, a weight can be associated with each layer to tune their relative influence during untangling, such as leather being less deformed than cloth. Results for each layer then reflect the combined effect of the other layers, enabling us to output a plausible configuration in contact regions. As our results show, our method can be used to generate plausible, new static shapes of garments when underwear has been added, as well as collision-free configurations enabling a user to safely launch animations of arbitrarily complex layered clothing.	https://dl.acm.org/authorize?N688175	Thomas Buffet, Damien Rohmer, Loïc Barthe, Laurence Boissieux, Marie-Paule Cani
Capture4VR: from VR photography to VR video	Concepts, terminology, structures, no math, no code. Free open-source libraries do the hard work. My background: consultant, writer, director, etc.	https://dl.acm.org/authorize?N689261	Christian Richardt, Peter Hedman, Ryan S. Overbeck, Brian Cabral, Robert Konrad, Steve Sullivan
Causing chaos: physics and destruction in Unreal Engine	New high performance physics and destruction system running in Epic Game's Unreal Engine. Features include new fracture and cutting tools in the engine, dynamic strain evaluation, interactive caching, fields and Niagara support.	https://dl.acm.org/authorize?N689126	Michael Lentine, Jim Van Allen, Matthias Worch
ChicMR: immersive mixed reality system using video-see-thru HMD and 3D LiDAR scanner	In this paper, we propose a mixed-reality system that provides an immersive interaction space for both real and virtual objects. Using a custom-made video-see-thru (VST) HMD, the system streams high-resolution stereo images of what the user is currently looking at. Also, the proposed system includes a rotating 2D LiDAR that configures a geometry of the user's surrounding space. Based on the triangle mesh approach, the proposed system creates an interaction space from the point cloud data. With the constructed interaction space, the user experiences various interactions using both real and virtual objects in the mixed-reality environment. We demonstrate the feasibility and effectiveness of our system with a real-time application.	https://dl.acm.org/authorize?N688357	Jin Ha Hwang, Hyukmin Kwon, Younguk Kim, Yong-Ho Lee, Jinbaek Kim, JungKyu Kim, Bum-Jae You
Children do not play war	"""Ghost Fleet VR"" immerses viewers in the story of slavery in the Thai fishing industry. Step onboard and into the story of Tun Lin, who was sold into slavery while looking for work in Thailand."	https://dl.acm.org/authorize?N689360	Fabiano Mixo
Cinematic scientific visualization: the art of communicating science	This course is a group endeavor by Sebastian Koeh, Teseo Sehneider, Francis Williams, and Daniele Panozzo. Please contact us if you have questions or comments. For troubleshooting, please post an issue on github. We are grateful to the authors of all open souree C++ libraries we are using. In particular, libigl, tetwild, polyfem, pybind11, and Jupyter. The course will mainly use • igl (Section 2) • polyfem (Section 3) • ABC Dataset CAD Processing (Section 4) • TetWild • 3D Viewer We provide documentation for the first 3 libraries in these course notes and we refer to https://geometryprocessing.github.io/geometric-computing-python/ for a complete and live version.	https://dl.acm.org/authorize?N689262	Kalina Borkiewicz, AJ Christensen, Helen-Nicole Kostis, Greg Shirah, Ryan Wyatt
City of sparkles	Through using authentic Twitter messages, City of Sparkles portrays storytelling through a virtual reality experience in New York City and San Francisco. The audiences can feel different emotions through exploring through the world of tweets.	https://dl.acm.org/authorize?N689134	Botao Hu, Yang Liu, Ran Duan
Color fundamentals for digital content creation, visualization & exploration	What is the best representation for doing euclidean geometry on computers? This question is a fundamental one for practitioners of computer graphics, as well as those working in computer vision, 3D games, virtual reality, robotics, CAD, animation, geometric processing, discrete geometry, and related fields. While available programming languages change and develop with reassuring regularity, the underlying geometric representations tend to be based on vector and linear algebra and analytic geometry (VLAAG for short), a framework that has remained virtually unchanged for 100 years. These notes introduce (PGA) as a modern alternative for doing euclidean geometry and shows how it compares to VLAAG, both conceptually and practically. In the next section we develop a basis for this comparison by drafting a wishlist for doing euclidean geometry.	https://dl.acm.org/authorize?N689263	Theresa-Marie Rhyne
Compact snapshot hyperspectral imaging with diffracted rotation	Simulating viscoelastic polymers and polymeric fluids requires a robust and accurate capture of elasticity and viscosity. The computation is known to become very challenging under large deformations and high viscosity. Drawing inspirations from return mapping based elastoplasticity treatment for granular materials, we present a finite strain integration scheme for general viscoelastic solids under arbitrarily large deformation and non-equilibrated flow. Our scheme is based on a predictor-corrector exponential mapping scheme on the principal strains from the deformation gradient, which closely resembles the conventional treatment for elastoplasticity and allows straightforward implementation into any existing constitutive models. We develop a new Material Point Method that is fully implicit on both elasticity and inelasticity using augmented Lagrangian optimization with various preconditioning strategies for highly efficient time integration. Our method not only handles viscoelasticity but also supports existing elastoplastic models including Drucker-Prager and von-Mises in a unified manner. We demonstrate the efficacy of our framework on various examples showing intricate and characteristic inelastic dynamics with competitive performance.	https://dl.acm.org/authorize?N688163	Yu Fang, Minchen Li, Ming Gao, Chenfanfu Jiang
Compositing light field video using multiplane images	We present a variety of new compositing techniques using Multi-plane Images (MPI's) [Zhou et al. 2018] derived from footage shot with an inexpensive and portable light field video camera array. The effects include camera stabilization, foreground object removal, synthetic depth of field, and deep compositing. Traditional compositing is based around layering RGBA images to visually integrate elements into the same scene, and often requires manual 2D and/or 3D artist intervention to achieve realism in the presence of volumetric effects such as smoke or splashing water. We leverage the newly introduced DeepView solver [Flynn et al. 2019] and a light field camera array to generate MPIs stored in the DeepEXR format for compositing with realistic spatial integration and a simple workflow which offers new creative capabilities. We demonstrate using this technique by combining footage that would otherwise be very challenging and time intensive to achieve when using traditional techniques, with minimal artist intervention.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338614	Matthew DuVall, John Flynn, Michael Broxton, Paul Debevec
Computational design of fabric formwork	We present an inverse design tool for fabric formwork - a process where flat panels are sewn together to form a fabric container for casting a plaster sculpture. Compared to 3D printing techniques, the benefit of fabric formwork is its properties of low-cost and easy transport. The process of fabric formwork is akin to molding and casting but having a soft boundary. Deformation of the fabric container is governed by force equilibrium between the pressure forces from liquid fill and tension in the stretched fabric. The final result of fabrication depends on the shapes of the flat panels, the fabrication orientation and the placement of external supports. Our computational framework generates optimized flat panels and fabrication orientation with reference to a target shape, and determines effective locations for external supports. We demonstrate the function of this design tool on a variety of models with different shapes and topology. Physical fabrication is also demonstrated to validate our approach.	https://dl.acm.org/authorize?N688154	Xiaoting Zhang, Guoxin Fang, Melina Skouras, Gwenda Gieseler, Charlie C. L. Wang, Emily Whiting
Computational fabrication	Whenever you start a renderer, you need a way to see an image. The most straightforward way is to write it to a file. The catch is, there are so many formats and many of those are complex. I always start with a plain text ppm file. Here's a nice description from Wikipedia:	https://dl.acm.org/authorize?N689264	Wojciech Matusik, Adriana Schulz
Computational peeling art design	Some artists peel citrus fruits into a variety of elegant 2D shapes, depicting animals, plants, and cartoons. It is a creative art form, called This art form follows the conservation principle, i.e., each shape must be created using one entire peel. Central to this art is finding optimal cut lines so that the citruses can be cut and unfolded into the desired shapes. However, it is extremely difficult for users to imagine and generate cuts for their desired shapes. To this end, we present a computational method for citrus peeling art designs. Our key insight is that instead of solving the difficult cut generation problem, we map a designed input shape onto a citrus in an attempt to cover the entire citrus and use the mapped boundary to generate the cut paths. Sometimes, a mapped shape is unable to completely cover a citrus. Consequently, we have developed five customized ways of interaction that are used to rectify the input shape so that it is suitable for citrus peeling art. The mapping process and user interactions are iteratively conducted to satisfy a user's design intentions. A large number of experiments, including a formative user study, demonstrate the capability and practicability of our method for peeling art design and construction.	https://dl.acm.org/authorize?N688019	Hao Liu, Xiao-Teng Zhang, Xiao-Ming Fu, Zhi-Chao Dong, Ligang Liu
Conduit: a modern pipeline for the open source world	We present Portal, a modern web interface for creating and managing media and review sessions at Blue Sky Studios. Utilizing a horizontally scalable stack, Portal allows artists and production management to quickly search for and play back media, capture notes with draw-overs, and manage review sessions all from within a web browser. To help ensure success, our media tools team conducted rigorous user story mapping sessions with key management staff across the studio. As a result, Portal has become an integral part of the dailies workflow at Blue Sky.	https://dl.acm.org/authorize?N689311	Danny Rerucha, Tommy Zhu, Andy Schott, Marley Gilb, Tracy Priest, Jennifer Brola, Blessan Abraham, Mark McGuire
Content-aware generative modeling of graphic design layouts	In this paper we present a unified deep inverse rendering framework for estimating the spatially-varying appearance properties of a planar exemplar from an arbitrary number of input photographs, ranging from just a single photograph to many photographs. The precision of the estimated appearance scales from plausible when the input photographs fails to capture all the reflectance information, to accurate for large input sets. A key distinguishing feature of our framework is that it directly optimizes for the appearance parameters in a latent embedded space of spatially-varying appearance, such that no handcrafted heuristics are needed to regularize the optimization. This latent embedding is learned through a fully convolutional auto-encoder that has been designed to regularize the optimization. Our framework not only supports an arbitrary number of input photographs, but also at high resolution. We demonstrate and evaluate our deep inverse rendering solution on a wide variety of publicly available datasets.	https://dl.acm.org/authorize?N688189	DUAN GAO, Xiao Li, Yue Dong, Pieter Peers, Kun Xu, Xin Tong
Convergent turbulence refinement toward irrotational vortex	We proposed a detail refinement method to enhance the visual effect of turbulence in irrotational vortex. We restore the missing angular velocity from the particles and convert them into linear velocity to recover turbulent detail due to numerical disspation.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338605	Xiaokun Wang, Sinuo Liu, Xiaojuan Ban, Yanrui Xu, Jing Zhou, Cong Wang
Creating a modern classic visual effect	While much of the traditional terminology of visual effects survives, the role of computers has transformed the way in which filmmakers are able to realize their vision. To support these filmmakers, effects teams must understand both how visual effects have been created in the past and how to develop software to deliver them in the modern context. The assignment we present here is designed to be part of a computing programme that provides an opportunity for students to develop such software in a near realistic setting. We do this by asking the students to work in small groups to replicate a classic visual effect that was originally created in camera.	https://dl.acm.org/authorize?N688265	Ken Cameron
Creating a robust online pipeline	This talk will cover the efforts that went into creating a fully remote pipeline to produce the award-winning CG short film, La Noria and the spinoff of the pipeline into its publicly available platform; Artella. The talk will cover the creation of the main online pipeline tools which allowed the La Noria production to happen all the way through discussing the different technical and creative challenges that came along with producing a film entirely with remote artists. Using behind the scenes examples, artwork, renders, and tests from each of the departments of the film attendees of this talk will be walked through this incredibly innovative production that will set the stage for the way we view making movies and how geography won't be a barrier in the future. Additionally, Carlos Baena will bring up the different technical, creative and industry challenges as well as stories for how this film was completed and how the pipeline evolved over the course of the production into what is not available via the Artella virtual production platform.	https://dl.acm.org/authorize?N689384	Carlos Baena
Creating impactful characters: correcting human impact accelerations using high rate IMUs in dynamic activities	Human motion capture using video-based or sensor-based methods gives animators the capability to directly translate complex human motions to create lifelike character animations. Advances in motion capture algorithms have improved their accuracy for estimating human generalized motion coordinates (joint angles and body positions). However, the traditional motion capture pipeline is not well suited to measure short duration, high acceleration impacts, such as running and jumping footstrikes. While high acceleration impacts have minimal influence on generalized coordinates, they play a big role in exciting soft tissue dynamics. Here we present a method for correcting motion capture trajectories using a sparse set of inertial measurement units (IMUs) collecting at high sampling rates to produce more accurate impact accelerations without sacrificing accuracy of the generalized coordinates representing gross motions. We demonstrate the efficacy of our method by correcting human motion captured experimentally using commercial motion capture systems with high rate IMUs sampling at 400Hz during basketball jump shots and running. With our method, we automatically corrected 185 jumping impacts and 1266 running impacts from 5 subjects. Post correction, we found an average increase of 84.6% and 91.1% in pelvis vertical acceleration and ankle dorsiflexion velocity respectively for basketball jump shots, and an average increase of 110% and 237% in pelvis vertical acceleration and ankle plantarflexion velocity respectively for running. In both activities, pelvis vertical position and ankle angle had small corrections on average below 2.0cm and 0.20rad respectively. Finally, when driving a human rig with soft tissue dynamics using corrected motions, we found a 143.4% and 11.2% increase in soft tissue oscillation amplitudes in basketball jump shots and running respectively. Our methodology can be generalized to correct impact accelerations for other body segments, and provide new tools to create realistic soft tissue animations during dynamic activities for more lifelike characters and better motion reconstruction for biomechanical analyses.	https://dl.acm.org/authorize?N688092	Calvin Kuo, Ziheng Liang, Ye Fan, Jean-Sébastien Blouin, Dinesh K. Pai
Creating photoreal creatures that audiences can connect with	This Production Talk will explore the culture shift in advertising, particularly how consumer expectations for quality content has never been higher than it is today. It will explore the use of photoreal characters and creatures to allow brands to break through the noise and connect with audiences. Michael Gregory (Creative Director) and Dan Seddon (VFX supervisor) will take audiences though the key steps that need to be taken to achieve a photoreal creature, as well as considerations that need to be made when finalizing the look of the creature. Examples of Moving Picture Company's (MPC) creature and character work will be used, as well as an exploration into the proprietary tools developed and used by MPC in their creative process.	https://dl.acm.org/authorize?N689377	Michael Gregory, Dan Seddon
Creating ralphzilla: moshpit, skeleton library and automation framework	Recently the use of vehicles has increased in importance for many games. This is not only for open-world games, where the use of vehicles are a crucial element of world traversal, but also for scenario based games where the use of vehicles adds a more varied game-play experience. In many of these games, however, the characters inside the vehicles lack the animations to connect their motion to that of the vehicle. The use of a few poses or small number of animations makes in-vehicle characters too rigid and is particularly noticeable in open vehicles or those with excessive motion such as tractors, speedboats or motorbikes. This can break the connection the player has to the vehicle experience. To solve this problem, several games have used a method to control a ragdoll with physical parameters to follow the input poses [Fuller and Nilsson 2010] [Mach 2017]. However, this solution has several complications regarding controllability and stability when simulating a ragdoll and a vehicle at the same time. I would like to introduce a new approach using particle-based dynamics rather than using a ragdoll. We present two methods: a particle-based approach to physical movement (see Figure 1) and modifying goal positions to generate plausible target poses (see Figure 3).	https://dl.acm.org/authorize?N689330	Hyojong Shin, Mark Leadbeater, Ben Merrick
Creating the immersive world of BioWare's Anthem	The savage world of Anthem is volatile, lush, expansive, and full of unexpected characters. Bringing these aspects to life in a real-time (30fps) interactive environment presented a wealth of challenging problems for BioWare's technical artists and rendering engineers. These developers work with content creators to bridge art and technology through creative problem solving in areas such as performance/runtime, shaders, and artist tools. This retrospective panel will highlight some of the team's work, alongside reflections on innovation, distributed collaboration/coordination, and the successes and challenges of creating a new IP for the world to enjoy.	https://dl.acm.org/authorize?N689294	Gracie Arenas Strittmatter, Jeff Vanelle, Ben Cloward, Eve Colvin
CreativeAI: deep learning for graphics	The purpose of this course is to study how to construct a consistent lighting style for animation, as well as, the techniques used to achieve that style. The franchise and specifically is used as a case study. None of the design principles presented here are specific to , they are basic design principles found in many films, both live action and animated. It is the choice of which design principles to use and how we combine and prioritize them that creates a unique style. The goal is to show that our choices are not random, but carefully constructed to serve a story and the vision of how it needs to be told. Using a single film, which is part of a larger franchise for this study provides two advantages. First, the key to consistent style is making consistent choices. By using a single film we are able to dive deeper into the choices and their motivations and also build a stronger case for demonstrating consistency. Second, having access to before and after examples provides a much clearer visual example of the principles being employed, why they are chosen and then finishing with the mechanics of how they are actually achieved.	https://dl.acm.org/authorize?N689275	Niloy J. Mitra, Iasonas Kokkinos, Paul Guerrero, Nils Thuerey, Vladimir Kim, Leonidas Guibas
CubeHarmonic: a new musical instrument based on Rubik's cube with embedded motion sensor	A contemporary challenge involves scientific education and the connection between new technologies and the heritage of the past. CubeHarmonic (CH) joins novelty and tradition, creativity and education, science and art. It takes shape as a novel musical instrument where magnetic 3D motion tracking technology meets musical performance and composition. CH is a Rubik's cube with a note on each facet, and a chord or chord sequence on each face. The position of each facet is detected through magnetic 3D motion tracking. While scrambling the cube, the performer gets new chords and new chord sequences. CH can be used to compose, improvise, and teach music and mathematics (group theory, permutations) with colors and physical manipulation supporting abstract thinking. Furthermore, CH allows visually impaired people to enjoy Rubik's cube manipulation by using sounds instead of colors.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338572	Maria Mannone, Eri Kitamura, Jiawei Huang, Ryo Sugawara, Pascal Chiu, Yoshifumi Kitamura
CurviSlicer: slightly curved slicing for 3-axis printers	Most additive manufacturing processes fabricate objects by stacking planar layers of solidified material. As a result, produced parts exhibit a so-called staircase effect, which results from sampling slanted surfaces with parallel planes. Using thinner slices reduces this effect, but it always remains visible where layers align with the input surfaces. In this research we exploit the ability of some additive manufacturing processes to deposit material slightly out of plane to dramatically reduce these artifacts. We focus in particular on the widespread Fused Filament Fabrication (FFF) technology, since most printers in this category can deposit along slightly curved paths, under deposition slope and thickness constraints. Our algorithm curves the layers, making them either follow the natural slope of the input surface or on the contrary, make them intersect the surfaces at a steeper angle thereby improving the sampling quality. Rather than directly computing curved layers, our algorithm optimizes for a deformation of the model which is then sliced with a standard planar approach. We demonstrate that this approach enables us to encode all fabrication constraints, including the guarantee of generating collision-free toolpaths, in a convex optimization that can be solved using a QP solver. We produce a variety of models and compare print quality between curved deposition and planar slicing.	https://dl.acm.org/authorize?N688036	Jimmy Etienne, Nicolas Ray, Daniele Panozzo, Samuel Hornus, Charlie C. L. Wang, Jonàs Martínez, Sara McMains, Marc Alexa, Brian Wyvill, Sylvain Lefebvre
DMP without DMP, full-CG environments for The Lion King	presented the unique challenge of creating a full CG feature film that could cross the border of photo-realism and be perceived as live action by the audience. The director and the production design team strongly pushed for a naturalistic look, heavily influenced by the imagery of African landscapes made popular by documentaries. In order to create a world that could be fully explored by a wide variety of virtual lenses, including dramatic wides and tight telephotos, the MPC Environments Team had to abandon some traditional Digital Matte Painting techniques (DMP), and focus on delivering full CG Environments to a scale and scope they never handled before.	https://dl.acm.org/authorize?N689390	Luca Bonatti, Marco Rolandi, Julien Bolbach, Kai Wolter
Dadum: experiencing memories of my father	Dadum is a wearable AR experience that utilizes spatial tracking of the Hololensl to produce a unique AR mask-based social artwork. The artwork is a collection of masks, 3D recreations of the artist's deceased father, where users wear these AR masks as their avatar in a mixed reality multi-user space.	https://dl.acm.org/authorize?N689137	Bynhan Pham, Danilo Gasques Rodrigues, Jon Paden
Decomposed optimization time integrator for large-step elastodynamics	Simulation methods are rapidly advancing the accuracy, consistency and controllability of elastodynamic modeling and animation. Critical to these advances, we require efficient time step solvers that reliably solve all implicit time integration problems for elastica. While available time step solvers succeed admirably in some regimes, they become impractically slow, inaccurate, unstable, or even divergent in others --- as we show here. Towards addressing these needs we present the Decomposed Optimization Time Integrator (DOT), a new domain-decomposed optimization method for solving the per time step, nonlinear problems of implicit numerical time integration. DOT is especially suitable for large time step simulations of deformable bodies with nonlinear materials and high-speed dynamics. It is efficient, automated, and robust at large, fixed-size time steps, thus ensuring stable, continued progress of high-quality simulation output. Across a broad range of extreme and mild deformation dynamics, using frame-rate size time steps with widely varying object shapes and mesh resolutions, we show that DOT always converges to user-set tolerances, generally well-exceeding and always close to the best wall-clock times across all previous nonlinear time step solvers, irrespective of the deformation applied.	https://dl.acm.org/authorize?N688025	Minchen Li, Ming Gao, Timothy Langlois, Chenfanfu Jiang, Danny M. Kaufman
Deep convolutional reconstruction for gradient-domain rendering	Subsurface scattering, in which light refracts into a translucent material to interact with its interior, is the dominant mode of light transport in many types of organic materials. Accounting for this phenomenon is thus crucial for visual realism, but explicit simulation of the complex internal scattering process is often too costly. BSSRDF models based on analytic transport solutions are significantly more efficient but impose severe assumptions that are almost always violated, e.g. planar geometry, isotropy, low absorption, and spatio-directional separability. The resulting discrepancies between model and usage lead to objectionable errors in renderings, particularly near geometric features that violate planarity. This article introduces a new shape-adaptive BSSRDF model that retains the efficiency of prior analytic methods while greatly improving overall accuracy. Our approach is based on a conditional variational autoencoder, which learns to sample from a reference distribution produced by a brute-force volumetric path tracer. In contrast to the path tracer, our autoencoder directly samples outgoing locations on the object surface, bypassing a potentially lengthy internal scattering process. The distribution is conditional on both material properties and a set of features characterizing geometric variation in a neighborhood of the incident location. We use a low-order polynomial to model the local geometry as an implicitly defined surface, capturing curvature, thickness, corners, as well as cylindrical and toroidal regions. We present several examples of objects with challenging medium parameters and complex geometry and compare to ground truth simulations and prior work.	https://dl.acm.org/authorize?N688172	Delio Vicini, Vladlen Koltun, Wenzel Jakob
Deep inverse rendering for high-resolution SVBRDF estimation from an arbitrary number of images	We present a model to measure the similarity in appearance between different materials, which correlates with human similarity judgments. We first create a database of 9,000 rendered images depicting objects with varying materials, shape and illumination. We then gather data on perceived similarity from crowdsourced experiments; our analysis of over 114,840 answers suggests that indeed a shared perception of appearance similarity exists. We feed this data to a deep learning architecture with a novel loss function, which learns a feature space for materials that correlates with such perceived appearance similarity. Our evaluation shows that our model outperforms existing metrics. Last, we demonstrate several applications enabled by our metric, including appearance-based search for material suggestions, database visualization, clustering and summarization, and gamut mapping.	https://dl.acm.org/authorize?N688180	Manuel Lagunas, Sandra Malpica, Ana Serrano, Elena Garces, Diego Gutierrez, Belen Masia
Deep learning: a crash course	Light transport simulation is ruled by the radiance equation, which is an integral equation. Photorealistic image synthesis consists of computing functionals of the solution of the integral equation, which involves integration, too. However, in meaningful settings, none of the integrals can be computed analytically and, in fact, all these integrals need to be approximated using Monte Carlo and quasi-Monte Carlo methods. Generating uniformly distributed points in the unit-hypercube is at the core of all of these methods. The course teaches the algorithms behind and elaborates on the characteristics of different classes of uniformly distributed points to help selecting the points most efficient for a task.	https://dl.acm.org/authorize?N689276	Andrew Glassner
Deep reality: an underwater VR experience to promote relaxation by unconscious HR, EDA, and brain activity biofeedback	DeepReality uses biometric information for reflection and relaxation. We monitor realtime brain activity, heart rate, and electrodermal activity to decrease heart rate and respiration by subtle, almost imperceptible light flickering, sound pulsations, and slow movements of underwater 3D creatures that reflect the internal state of the viewer.	https://dl.acm.org/authorize?N689142	Judith Amores Fernandez, Pattie Maes, Anna Fusté Robert Richer
Deep reflectance fields: high-quality facial reflectance field inference from color gradient illumination	We present a novel technique to relight images of human faces by learning a model of facial reflectance from a database of 4D reflectance field data of several subjects in a variety of expressions and viewpoints. Using our learned model, a face can be relit in arbitrary illumination environments using only two original images recorded under spherical color gradient illumination. The output of our deep network indicates that the color gradient images contain the information needed to estimate the full 4D reflectance field, including specular reflections and high frequency details. While capturing spherical color gradient illumination still requires a special lighting setup, reduction to just two illumination conditions allows the technique to be applied to dynamic facial performance capture. We show side-by-side comparisons which demonstrate that the proposed system outperforms the state-of-the-art techniques in both realism and speed.	https://dl.acm.org/authorize?N688022	Abhimitra Meka, Christian Häne, Rohit Pandey, Michael Zollhöfer, Sean Fanello, Graham Fyffe, Adarsh Kowdle, Xueming Yu, Jay Busch, Jason Dourgarian, Peter Denny, Sofien Bouaziz, Peter Lincoln, Matt Whalen, Geoff Harvey, Jonathan Taylor, Shahram Izadi, Andrea Tagliasacchi, Paul Debevec, Christian Theobalt, Julien Valentin, Christoph Rhemann
Deep view synthesis from sparse photometric images	"The goal of light transport acquisition is to take images from a sparse set of lighting and viewing directions, and combine them to enable arbitrary relighting with changing view. While relighting from sparse images has received significant attention, there has been relatively less progress on view synthesis from a sparse set of ""photometric"" images---images captured under controlled conditions, lit by a single directional source; we use a spherical gantry to position the camera on a sphere surrounding the object. In this paper, we synthesize novel viewpoints across a wide range of viewing directions (covering a 60° cone) from a sparse set of just six viewing directions. While our approach relates to previous view synthesis and image-based rendering techniques, those methods are usually restricted to much smaller baselines, and are captured under environment illumination. At our baselines, input images have few correspondences and large occlusions; however we benefit from structured photometric images. Our method is based on a deep convolutional network trained to directly synthesize new views from the six input views. This network combines 3D convolutions on a plane sweep volume with a novel per-view per-depth plane attention map prediction network to effectively aggregate multi-view appearance. We train our network with a large-scale synthetic dataset of 1000 scenes with complex geometry and material properties. In practice, it is able to synthesize novel viewpoints for captured real data and reproduces complex appearance effects like occlusions, view-dependent specularities and hard shadows. Moreover, the method can also be combined with previous relighting techniques to enable changing both lighting and view, and applied to computer vision problems like multiview stereo from sparse image sets."	https://dl.acm.org/authorize?N688021	Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, Ravi Ramamoorthi
Deep-ChildAR bot: educational activities and safety care augmented reality system with deep-learning for preschool	We propose a projection-based augmented reality (AR) robot system that provides pervasive support for the education and safety of preschoolers via a deep learning framework. This system can utilize real-world objects as metaphors for educational tools by performing object detection based on deep learning in real-time, and it can help recognize the dangers of real-world objects that may pose risks to children. We designed the system in a simple and intuitive way to provide user-friendly interfaces and interactions for children. Children's experiences through the proposed system can improve their physical, cognitive, emotional, and thinking abilities.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338589	Yoon Jung Park, Hyocheol Ro, Tack-Don Han
DeepLight: learning illumination for unconstrained mobile mixed reality	We present our modern pipeline, Conduit, developed for Blue Sky's upcoming feature film, Nimona. Conduit refers to a set of tools and web services that allow artists to find, track, version and quality control their work. In addition to describing the system and implementation, we will discuss the challenges and opportunities of developing and deploying a pipeline with the intention of open sourcing the resulting toolset. We found that communicating concepts and progress updates both internally and externally throughout the development process ultimately resulted in a more robust solution.	https://dl.acm.org/authorize?N689310	Oliver Staeubli, Tim Hoff, Ryan Bland, Rebecca Hallac, Josh Smeltzer, Chris Rydalch, Karyn Buczek Monschein, Mark McGuire
Deepfovea: neural reconstruction for foveated rendering and video compression using learned natural video statistics	Hair simulation models are based on physics, but require additional controls to achieve certain looks or art directions. A common simulation control is to use hard or soft constraints on the kinematic points provided by the articulation of the scalp or explicit rigging of the hair [Kaur et al. 2018; Soares et al. 2012]. While following the rigged points adds explicit control during shot work, we want to author information during the setup phase to better follow the groomed shape automatically during simulation (Figure 1). We have found that there is no single approach that satisfies every artistic requirement, and have instead developed several practical force-and constraint-based techniques over the course of the making of We have also discovered that kinematic constraints can sometimes be adversely affected by mesh deformation and discuss how to mitigate this effect for both articulated and simulated hair.	https://dl.acm.org/authorize?N689322	Hayley Iben, Jacob Brooks, Christopher Bolwyn
Deferred neural rendering: image synthesis using neural textures	The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce , a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose , which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.	https://dl.acm.org/authorize?N688011	Justus Thies, Michael Zollhöfer, Matthias Nießner
Delaunay lofts: a new class of space-filling shapes	We have developed an approach to construct and design a new class of space-filling shapes, which we call Our approach is based on interpolation of a stack of planar tiles whose dual tilings are Delaunay diagrams. We construct control curves that interpolate Delaunay vertices. Voronoi decomposition of the volume using these control curves as Voronoi sites gives us lofted interpolation of original polygons in planar tiles. This, combined with the use of wallpaper symmetries allows for the design of space-filling shapes in 3-space. In the poster exhibition, we will also demonstrate 3D printed examples of the new class of shapes (See Figures 1 and 3).	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338576	Sai Ganesh Subramanian, Mathew Eng, Vinayak Krishnamurthy, Ergun Akleman
Demonstrating preemptive reaction: accelerating human reaction using electrical muscle stimulation without compromising agency	In this demonstration we enable preemptive force-feedback systems to speed up human reaction time without fully compromising the user's sense of agency. Typically these haptic systems speed up human reaction time by means of electrical muscle stimulation (EMS) or mechanical actuation (exoskeletons), which unfortunately, completely remove the users sense of agency. We address this by actuating the user's body, using EMS, within a particular time window (160 ms after visual stimulus), which we found to speed up reaction time by 80 ms, while retaining a sense of agency. Here, we demonstrate this at the example of two applications: (1) taking a picture of a high-speed moving object in mid-flight, or (2) hit baseball with a toy gun.	https://dl.acm.org/authorize?N688358	Jun Nishida, Shunichi Kasahara, Pedro Lopes
Depth boost: extended depth reconstruction capability on volumetric display	In this talk, we show how acoustic metamaterials can be used to build the acoustic equivalent of optical devices. We demonstrate two key devices: (1) an acoustic prism, used to send the different notes in a melody towards different directions, and (2) an auto-zoom lens, used to send sound to a moving target. We conclude, discussing potential applications and limitations.	https://dl.acm.org/authorize?N689309	Gianluca Memoli, Thomas J. Graham, Joshua T. Kybett, Arash Pouryazdan
Designing a full-body customizable haptic interface using two-dimensional signal transmission	"The concept of spatial computing has a high potential to augment our lives. We can imagine the expansion of the concept of spatial computing in the future as the expanded visual space will blend into our daily lives. In such a future, how do we feel and interact with another reality that overlaps with real space? We have researched this question from an approach using a haptic interface. We previously developed the Synesthesia Suit[Konishi et al. 2016]. This is a full body haptic suit that extends the VR experience of the game ""Rez Infinite"". It delivers the visual and musical experience of ""Rez"" to the whole body, and provides haptic feedback. However, the Synesthesia Suit is difficult to walk around in a physical space, due to the thick cables required for its operation. Basically it is especially suited for VR gaming and it was difficult to apply to spatial computing experience."	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338569	Taichi Furukawa, Nobuhisa Hanamitsu, Yoichi Kamiyama, Hideaki Nii, Charalampos Krekoukiotis, Kouta Minamizawa, Akihito Noda, Junko Yamada, Keiichi Kitamura, Daisuke Niwa, Yoshiaki Hirano
Designing chain reaction contraptions from causal graphs	Chain reaction contraptions, commonly referred to as Rube Goldberg machines, achieve simple tasks in an intentionally complex fashion via a cascading sequence of events. They are fun, engaging and satisfying to watch. Physically realizing them, however, involves hours or even days of manual trial-and-error effort. The main difficulties lie in predicting failure factors over long chains of events and robustly enforcing an expected causality between parallel chains, especially under perturbations of the layout. We present a computational framework to help design the layout of such contraptions by optimizing their robustness to possible assembly errors. Inspired by the active learning paradigm in machine learning, we propose a generic sampling-based method to progressively approximate the of a given scenario over the design space of possible scene layouts. The success or failure of any given simulation is determined from a user-specified causal graph enforcing a time ordering between expected events. Our method scales to complex causal graphs and high dimensional design spaces by dividing the graph and scene into simpler sub-scenarios. The aggregated success probability distribution is subsequently used to optimize the entire layout. We demonstrate the use of our framework through a range of real world examples of increasing complexity, and report significant improvements over alternative approaches. Code and fabrication diagrams are available on the project page.	https://dl.acm.org/authorize?N688098	Robin Roussel, Marie-Paule Cani, Jean-Claude Léon, Niloy J. Mitra
DiCE: dichoptic contrast enhancement for binocular displays	In stereoscopic displays, such as those used in VR/AR headsets, our two eyes are presented with different views. The disparity between the views is typically used to convey depth cues, but it could be used for other purposes. We devise a novel technique that takes advantage of binocular fusion to boost perceived local contrast and visual quality of images. Since the technique is based on fixed tone-curves, it has negligible computational cost and it is well suited for real-time applications, such as VR rendering. To control the trade-off between the level of enhancement and binocular rivalry, we conduct a series of experiments that lead to a new finding, explaining the factors that dominate the rivalry perception in a dichoptic presentation where two images of different contrasts are displayed. With this new inding, we demonstrate that the enhancement can be quantitatively measured and binocular rivalry is well controlled.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338578	Fangeheng Zhong, George Alex Koulieris, George Drettakis, Martin S. Banks, Mathieu Chambe, Fredo Durand, Rafal Mantiuk
Differentiable graphics with TensorFlow 2.0	This course will cover the compact volume data structure and various tools available in the open source library OpenVDB. Since its release in 2012 it has set an industry standard and has been used for visual effects in over 150 feature movies. Last year this open source project was the first to be adopted by the new Academy Software Foundation (ASWF) and the Linux Foundation. This means that the project is now changing governance and more importantly has open up for numerous external contributions. We will describe this new governance, specifically what it means for would-be contributors, and of course what has already been added to the project since last time this course was offered at SIGGRAPH in 2017.	https://dl.acm.org/authorize?N689277	Paige Bailey, Sofien Bouaziz, Shan Carter, Josh Gordon, Christian Häne, Alexander Mordvintsev, Julien Valentin, Martin Wicke
Direct delta mush skinning and variants	"A significant fraction of the world's population have experienced virtual characters through games and movies, and the possibility of online VR social experiences may greatly extend this audience. At present, the skin deformation for interactive and real-time characters is typically computed using geometric skinning methods. These methods are efficient and simple to implement, but obtaining quality results requires considerable manual ""rigging"" effort involving trial-and-error weight painting, the addition of virtual helper bones, etc. The recently introduced Delta Mush algorithm largely solves this rig authoring problem, but its iterative computational approach has prevented direct adoption in real-time engines. This paper introduces Direct Delta Mush, a new algorithm that simultaneously improves on the efficiency and control of Delta Mush while generalizing previous algorithms. Specifically, we derive a direct rather than iterative algorithm that has the same ballpark computational form as some previous geometric weight blending algorithms. Straightforward variants of the algorithm are then proposed to further optimize computational and storage cost with insignificant quality losses. These variants are equivalent to special cases of several previous skinning algorithms. Our algorithm simultaneously satisfies the goals of reasonable efficiency, quality, and ease of authoring. Further, its explicit decomposition of rotational and translational effects allows independent control over bending versus twisting deformation, as well as a skin sliding effect."	https://dl.acm.org/authorize?N688168	Binh Huy Le, J P Lewis
"Directable stadium crowds from image based modelling for ""Bohemian Rhapsody"""	In Walt Disney Animation Studio's 57th animated feature , the vastness of the internet is imagined as a bustling city where websites are buildings, characters represent algorithms, and travel from site to site. The enormous scope of bringing the world of the internet to life required the Crowds department to rethink how we go about populating our scenes. We extended the Zootopia Crowd Pipeline [El-Ali et al. 2016] to support pose reuse based on level-of-detail, and developed a procedural workflow to populate the world with millions of agents and efficiently render only those visible to camera.	https://dl.acm.org/authorize?N689338	Josh Richards, Le Joyce Tong, Moe El-Ali, Tuan Nguyen
Disney presents: the making of The Lion King	In this production session, VFX Supervisors Robert Legato, ASC and Director of Photography Caleb Deschanel ASC will reveal how they worked with Director Jon Favreau and the VFX and Animation team at MPC Film (Moving Picture Company) to develop a new approach to filmmaking, harnessing the latest filmmaking technologies and creative talent to bring The Lion King to a new generation. As the sole provider of visual effects and animation, the MPC team was led by: MPC VFX supervisors Adam Valdez and Elliot Newman, whom worked closely with Legato, Deschanel and Animation Supervisor Andy Jones, crafting every photo-real frame of the movie.	https://dl.acm.org/authorize?N689291	Rob Legato, Caleb Deschanel
Display methods of projection augmented reality based on deep learning pose estimation	In this paper, we propose three display methods for projection-based augmented reality. In spatial augmented reality (SAR), determining where information, objects, or contents are to be displayed is a difficult and important issue. We use deep learning models to estimate user pose and suggest ways to solve the issue based on this data. Finally, each method can be appropriately applied according to various the applications and scenarios.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338608	Hyocheol Ro, Yoon Jung Park, Jung-Hyun Byun, Tack-Don Han
Distortion-free wide-angle portraits on camera phones	Photographers take wide-angle shots to enjoy expanding views, group portraits that never miss anyone, or composite subjects with spectacular scenery background. In spite of the rapid proliferation of wide-angle cameras on mobile phones, a wider field-of-view (FOV) introduces a stronger perspective distortion. Most notably, faces are stretched, squished, and skewed, to look vastly different from real-life. Correcting such distortions requires professional editing skills, as trivial manipulations can introduce other kinds of distortions. This paper introduces a new algorithm to undistort faces without affecting other parts of the photo. Given a portrait as an input, we formulate an optimization problem to create a content-aware warping mesh which adapts to the stereographic projection on facial regions, and seamlessly evolves to the perspective projection over the background. Our new energy function performs effectively and reliably for a large group of subjects in the photo. The proposed algorithm is fully automatic and operates at an interactive rate on the mobile platform. We demonstrate promising results on a wide range of FOVs from 70° to 120°.	https://dl.acm.org/authorize?N688016	YiChang Shih, Wei-Sheng Lai, Chia-Kai Liang
Doctor Who: The Runaway	"Step inside the TARDIS with the Doctor in this beautiful, animated, interactive story from the ""Doctor Who"" team. You've been in a collision. You wake inside the TARDIS. The Doctor introduces you to the person, or thing, you collided with. He's a strange and magnificent ball of living energy called Volta. Part surly teenager, part bomb, Volta is very unstable. In fact, he's primed to explode. Big time. Unless he can be returned to his home planet, sharpish. The problem is, a squad of galactic busybodies has other plans for Volta. Bad ones. Drawn into a frantic chase, you become the Doctor's unlikely assistant as she races against time to get Volta home to his parents. Armed with a sonic screwdriver, it is down to you to help the Doctor as she faces the forces of evil, and teenage angst, in this animated, 13-minute VR adventure from the team behind ""Doctor Who"" Series 11."	https://dl.acm.org/authorize?N689352	Mathias Chelebourg
Drawing sound in MR space	What if we can draw sound in mid-air? We propose a multi-participant, interactive, audiovisual art experience using mixed reality (MR). In the MR space, participants can draw virtual lines in mid-air to create audiovisual effects.	https://dl.acm.org/authorize?N689143	Taku Ota, Ryo Komatsubara, Katsutoshi Hata, Takahisa Mitsumori, Hidefumi Ohmura, Ken Sonobe, Ryu Nakagawa
Dust and cobwebs for Toy Story 4	The Toy Story universe makes its home at small scales, with the camera sometimes just a few centimeters from surfaces where typical shader approaches are unable to provide the desired level of detail. For environments like the Second Chance Antiques Store for Toy Story 4, the Set Extensions team developed systems to generate dimensional, granular elements such as dust, small debris, and cobwebs to enhance storytelling and ambiance. In addition to improving realism, these elements help indicate how hidden or exposed an area is from human observation and elevated the sense of drama and history.	https://dl.acm.org/authorize?N689391	Hosuk Chang, David Luoh
Eclipse	Eclipse is a glimpse into the future of entertainment. A hyperreality experience, Eclipse provides a complete free roaming experience on a limited footprint with unique features, such as full body awareness and haptic floors. A collaborative sci-fi experience for two to four players.	https://dl.acm.org/authorize?N689144	Aymeric Favre, Flavien Galliot, Frederic Cussey, Frederic Plantard, Frederic Lecompte, Jonathan Astruc, Darrin Taylor, Paul Etienne Duclos
Effectiveness of facial animated avatar and voice transformer in elearning programming course	The advancement in technology brought about the introduction of eLearning to educational institutes. By supplementing traditional courses with eLearning materials, instructors are able to introduce new learning methods without completely deviating from standard education programs [Basogain et al. 2017]. Some of the most popular forms of E-Learning include online courses [Aparicio and Bacao 2013], [Goyal 2012], video clips of lectures, and gamification of courses and materials [Plessis 2017]. This paper introduces and evaluates the performance of eLearning videos featuring anime styled avatars (a.k.a VTuber) speaking in vocoder transformed audios and how they compare with the traditional lecturer videos.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338540	Rex Hsieh, Akihiko Shirai, Hisashi Sato
Efficient and conservative fluids using bidirectional mapping	"We introduce variable thickness, viscous vortex filaments. These can model such varied phenomena as underwater bubble rings or the intricate ""chandeliers"" formed by ink dropping into fluid. Treating the evolution of such filaments as an instance of Newtonian dynamics on a Riemannian configuration manifold we are able to extend classical work in the dynamics of vortex filaments through inclusion of viscous drag forces. The latter must be accounted for in low Reynolds number flows where they lead to significant variations in filament thickness and form an essential part of the observed dynamics. We develop and document both the underlying theory and associated practical numerical algorithms."	https://dl.acm.org/authorize?N688174	Marcel Padilla, Albert Chern, Felix Knöppel, Ulrich Pinkall, Peter Schröder
Efficient mask expansion for green-screen keying using color distributions	Masks are heavily used in image and video processing, particularly in the context of green screen keying. Designing good masks is a difficult task that involves painting over small details of images. Usually, only a rough mask is created. We propose an algorithm to expand such a mask, using color similarity. Our approach is fast, even on 4K images, and compares favorably with standard tools used in keying.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338595	Alexandre Derouet-Jourdan, Marc Salvati, Xiaoxiong Xing, Takuro Nishikawa
Eigen zoetrope	Zoetrope is an animation display that produces the illusion of motion via a displayed sequence of pictures; this device was invented in 1833. The traditional Zoetrope consists of a rotational disk and a strobe light. The disk has some plates that are pasted together showing sequential pictures and is rotated rapidly. The strobe light is emitted with synchronization to the rotation angle To display the animation. The Zoetrope has been improved and there are many kinds of the zoetropes in SIGGRAPH community: interactive zoetrope [Smoot et al. 2010], ZoeMatrope [Miyashita et al. 2016], and Magic Zoetrope [Yokota and Hashida 2018].	https://dl.acm.org/authorize?N688359	Gou Koutaki
Ellipsoidal path connections for time-gated rendering	During the last decade, we have been witnessing the continued development of new time-of-flight imaging devices, and their increased use in numerous and varied applications. However, physics-based rendering techniques that can accurately simulate these devices are still lacking: while existing algorithms are adequate for certain tasks, such as simulating transient cameras, they are very inefficient for simulating time-gated cameras because of the large number of wasted path samples. We take steps towards addressing these deficiencies, by introducing a procedure for efficiently sampling paths with a predetermined length, and incorporating it within rendering frameworks tailored towards simulating time-gated imaging. We use our open-source implementation of the above to empirically demonstrate improved rendering performance in a variety of applications, including simulating proximity sensors, imaging through occlusions, depth-selective cameras, transient imaging in dynamic scenes, and non-line-of-sight imaging.	https://dl.acm.org/authorize?N688083	Adithya Pediredla, Ashok Veeraraghavan, Ioannis Gkioulekas
Enhancement of CT images for visualization	Modern medical science strongly depends on imaging technologies for accurate diagnose and treatment planning. Raw medical images generally require post-processing - like edge and contrast enhancement, and noise removal - for visualization. In this paper, a clustering-based contrast enhancement technique is presented for computed tomography (CT) images.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338602	Anam Mehmood, Ishtiaq Rasool Khan, Hassan Dawood, Hussain Dawood
Enhancing emotional intelligence in project management: strategies for better outcomes and community with limited financial overhead	"During the SIGGRAPH 2018 BOF ""Emphasizing Empathy in Pipeline Project Management,"" group consensus stated that highly effective project management can only be achieved when emphasis is placed on demonstrated empathy for any and all project contributors at the project management level and when challenges are framed as opportunities to enhance both the team and the project manager's own emotional intelligence. The reality faced in the industry, however, can present unique challenges, specifically relating to toxic cultural folkways, lack of leadership support, and lack of designated monetary resources. Based on subsequent discussions borne from the initial presentation with industry professionals and team leaders, it seems imperative to address not only the theories of Emotional Intelligence in greater depth, but also to acknowledge the potential obstacles in applying this basic theory in the real world. This talk aims to illuminate opportunities for individual production professionals to both challenge their own perceptions of the industry culture and make effective changes pertaining to their management and communication styles to affect positive change in their work environment, increase employee morale, and build community, barring financial allotment, to the overall benefit of their team members and their project health."	https://dl.acm.org/authorize?N689399	Claudia E. Davis
Expedition Reef for Educators	"An educational resource for K-12 teachers that leverage assets from the planetarium show ""Expedition Reef."""	https://dl.acm.org/authorize?N689242	Ryan Wyatt
Experiences of treating phantom limb pain using immersive virtual reality	Phantom limb pain (PLP) is a phenomenon that affects millions of amputees worldwide. Its causes are poorly understood, and traditional forms of pain relief are largely ineffective. For over a decade virtual reality (VR) has shown tantalising possibilities of treating or managing this debilitating condition. Until recently however, the cost, complexity and fragility of VR hardware made exploring this unorthodox approach at any meaningful scale challenging; patients have had to travel to the location of specialist equipment to participate in studies, and missed appointments, dropouts or broken hardware have hampered data-gathering. Improvements in 'consumer grade' VR headsets now makes larger trials of this visual approach to pain management viable. We describe a trial of a VR system for PLP reduction utilising lightweight, standalone and low-cost VR hardware suitable for independent home use.	https://dl.acm.org/authorize?N689381	James Marsh, Stephen Pettifer, Cliff Richardson, Jai Kulkarni
Exploration of using face tracking to reduce GPU rendering on current and future auto-stereoscopic displays	Future auto-stereoscopic displays offer us an amazing possibility of virtual reality without the need for head mounted displays. Since fundamentally though we only need to generate viewpoints for known observers, the classical approach to render all views at once is wasteful in terms of GPU resources and limits the scale of an auto-stereoscopic display. We present a technique that reduces GPU consumption when using an auto-stereoscopic displays by giving the display a context awareness of its observers. The technique was first applied to the Looking Glass device on the Unity3D platform. Rather than rendering 45 different views at the same time, for each observer, the framework only requires six views that are visible to both eyes based on the tracked eye positions. Given the current specifications of this device, the framework helps save 73% GPU consumption for Looking Glass if it was to render a 8 X 8 resolution scene, and the saved GPU consumption increases as the resolution increases. This technique can be applied to reduce future GPU requirements for auto-stereoscopic displays in the future.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338577	Xingyu Pan, Mengya Zheng, Abraham Campbell
Exploring color variations for vector graphics	We propose a novel and intuitive method for exploring recoloring variations of vector graphics. Compared with existing methods, ours is specifically tailored for vector graphics, where color distributions are sparser and are explicitly stored using constructs like solid colors or gradients, independent from other semantical and spatial relationships. Our method tries to infer some of them before formulating color transfer as a transport problem between the weighted color distributions of the reference and the target vector graphics. We enable creative exploration by providing fine-grain control over the resulting transfer, allowing users to modify relative color distributions in real-time.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338552	Sayan Ghosh, Jose Echevarria, Vineet Batra, Ankit Phogat
EyeHacker: gaze-based automatic reality manipulation	In this study, we introduce EyeHacker, which is an immersive virtual reality (VR) system that spatiotemporally mixes the live and recorded/edited scenes based on the measurement of the users' gaze. This system updates the transition risk in real time by utilizing the gaze information of the users (i.e., the locus of attention) and the optical flow of scenes. Scene transitions are allowed when the risk is less than the threshold, which is modulated by the head movement data of the users (i.e., the faster their head movement, the higher will be the threshold). Using this algorithm and experience scenario prepared in advance, visual reality can be manipulated without being noticed by users (i.e., eye hacking). For example, consider a situation in which the objects around the users perpetually disappear and appear. The users would often have a strange feeling that something was wrong and, sometimes, would even find what happened but only later; they cannot visually perceive the changes in real time. Further, with the other variant of risk algorithms, the system can implement a variety of experience scenarios, resulting in reality confusion.	https://dl.acm.org/authorize?N688350	Daichi Ito, Sohei Wakisaka, Atsushi Izumihara, Tomoya Yamaguchi, Atsushi Hiyama, Masahiko Inami
FACS at 40: facial action coding system panel	Creating emotionally impactful characters has been one of the biggest challenges in computer graphics. Key to this has been understanding how faces express emotion. In 1978 research started in coding human expressions. The Facial Action Coding System (FACS) would become one of the cornerstones of facial animation of digital characters. Today FACS is recognized as the gold standard in scientific facial research and animation. Forty years on, we discuss the strengths and limitations of FACS, its relevance in the age of Machine Learning and what people are doing to improve upon FACS in animation. The panel features leading experts: J.P. Lewis, Erika Rosenberg, Vladimir Mastilovic, Mark Sagar and hosted by Mike Seymour.	https://dl.acm.org/authorize?N688129	Mike Seymour
Facial pipeline in playmobil: the movie	In this paper, we present the technical pipeline that has been deployed at ON Animation studios to manage the specificity of facial animation on the Playmobil movie. According to the artistic requirement of this production, we developed a complete texture-free solution that gives the artists the ability to animate 2D facial features on a three-dimensional face while having a realtime raytraced feedback in the viewport. This approach provides full control over the shapes and since the final result is computed at render time, the visual style can be controlled until the very end of the workflow.	https://dl.acm.org/authorize?N689372	Jeremy Ringard, Claude Levastre
"Fast, interpolationless character animation through ""ephemeral"" rigging"	"Presbyopia, the loss of accommodation due to the stiffening of the crystalline lens in the eye, affects nearly 20% of the population worldwide. Traditional forms of presbyopia correction use fixed focal elements that inherently trade off field of view or stereo vision for a greater range of distances at which the wearer can see clearly. However, none of these offer the same natural refocusing enjoyed in youth. In this work, we built a new type of presbyopia correction, dubbed ""autofocal,"" which externally mimics the natural accommodation response of the eye by combining data from eye trackers and a depth sensor, and then automatically drives focus-tunable lenses. We evaluated autofocals against progressives and mono-vision in a user study; compared to these traditional corrections, autofocals maintain better visual acuity at all tested distances, allow for faster and more accurate visual task performance, and are easier to refocus with for a majority of wearers."	https://dl.acm.org/authorize?N689328	Nitish Padmanaban, Robert K. Konrad, Gordon Wetzstein
Finding hexahedrizations for small quadrangulations of the sphere	This paper tackles the challenging problem of constrained hexahedral meshing. An algorithm is introduced to build combinatorial hexahedral meshes whose boundary facets exactly match a given quadrangulation of the topological sphere. This algorithm is the first practical solution to the problem. It is able to compute small hexahedral meshes of quadrangulations for which the previously known best solutions could only be built by hand or contained thousands of hexahedra. These challenging quadrangulations include the boundaries of transition templates that are critical for the success of general hexahedral meshing algorithms. The algorithm proposed in this paper is dedicated to building combinatorial hexahedral meshes of small quadrangulations and ignores the geometrical problem. The key idea of the method is to exploit the equivalence between quad flips in the boundary and the insertion of hexahedra glued to this boundary. The tree of all sequences of flipping operations is explored, searching for a path that transforms the input quadrangulation into a new quadrangulation for which a hexahedral mesh is known. When a small hexahedral mesh exists, a sequence transforming into the boundary of a cube is found; otherwise, a set of pre-computed hexahedral meshes is used. A novel approach to deal with the large number of problem symmetries is proposed. Combined with an efficient backtracking search, it allows small shellable hexahedral meshes to be found for all even quadrangulations with up to 20 quadrangles. All 54, 943 such quadrangulations were meshed using no more than 72 hexahedra. This algorithm is also used to find a construction to fill arbitrary domains, thereby proving that any ball-shaped domain bounded by quadrangles can be meshed with no more than 78 hexahedra. This very significantly lowers the previous upper bound of 5396	https://dl.acm.org/authorize?N688008	Kilian Verhetsel, Jeanne Pellerin, Jean-François Remacle
First Man: Redefining in-camera FX	As the lead VFX house DNEG's work on First Man was centered around trying to achieve some of the most realistic and immersive in-camera VFX shots ever seen. The team used a blend of cutting-edge in-camera VFX techniques, special effects, scale models, and never-before-seen footage from NASA's archive to tell the story of Neil Armstrong's journey to the moon and back. Join Michelle Eisenreich, DNEG VFX Producer, (and DNEG's Academy award-winning DFX Supervisor Tristan Myles - note: depending on Tristan's availability TBC) as she/they share(s) insights on how the First Man VFX crew were able to realise this by using one of the biggest LED screens ever built on a movie set.	https://dl.acm.org/authorize?N689208	Peter Farkas, Paul Lambert
Flap flap away: animation cycle multiplexing	The birds of required several technological advancements and techniques to achieve the simple graphic style of the film. One technology was a re-designed wing rig with unique mechanics that allowed for clean lines and graphic shapes rather than our previous anatomical based wing rig. The production style also required extreme posing involving sliding limbs, large open mouth ranges and jiggly eyes. These requirements were achieved with a combination of new workflow techniques, updates to the pipeline and the creation and updating of proprietary deformers.	https://dl.acm.org/authorize?N689326	James Gu, Ozgur Aydogdu, Steven Song
Flood action VR: a virtual reality framework for disaster awareness and emergency response training	Natural disasters constitute unexpected and severe threats with devastating effects on communities worldwide. Recent studies emphasize the importance of public awareness and training of first responders in disaster preparedness and response activities. This paper presents a virtual reality framework that creates a realistic 3D gaming environment with real-time and historical weather and disaster conditions. Main goals of the project are to increase public awareness about disasters by using gamification techniques, and train and evaluate emergency responders by simulating real-life scenarios. The system is supported by voice recognition to interact with the virtual world, and analyze user's actions and voice to detect emotional and psychological state.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338550	Yusuf Sermet, Ibrahim Demir
Fluid-measurement technology using flow birefringence of nanocellulose	We propose a potential fluid-measurement technology aimed at supporting biomechanics research of water sports using fluid simulation and motion analysis. Cellulose nanofibers introduced into the water as tracer particles to visualize the movement of water. An optical property of nanofibers, called flow birefringence, makes water flows brighter than their surroundings when placed between right and left circularly polarized plates. We tested the capability of the technology in a water tank and succeeded in using an existing particle-tracking method-particle image velocimetry (PIV)-to measure the flows from a pump in the tank.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338565	Shogo Yamashita, Takaaki Kasuga, Shunichi Suwa, Takashi Miyaki, Masaya Nogi, Jun Rekimoto
For the throne: the making of 'Game of Thrones': Season 8	"The eighth and final season of ""Game of Thrones"" was the most anticipated show in recent television history. The cinematic saga spanned six VFX-heavy episodes, from the spectacular Battle of Winterfell to the destruction of Red Keep, and its aftermath. In this talk, Weta Digital, Image Engine, Pixomondo, and Scanline team up to reveal the VFX they created for the climactic conclusion to ""Game of Thrones,"" and how their collective work on the long-running series created a new standard of cinematic television that has forever changed the landscape of TV viewing. The visual effects supervisor from each facility will delve in to the creation over 3000 VFX shots across every episode, featuring dragons, large-scale environments, and epic battles. They will describe how they used cutting-edge film techniques to produce some of the most complex VFX ever seen on TV, and, in doing so, rewrote the rules of what can be accomplished on the small screen."	https://dl.acm.org/authorize?N689207	Martin Hill, Thomas Schelesny, Mohsen Mousavi, Sven Martin
Foundational principles & technologies for the metaverse	In Avengers: Endgame we wanted to improve the physical accuracy of our simulations and level of artistic control for the look of our combustion simulations, i.e. explosions, grenades and fire. Our in-house Simulation RnD department developed an improved combustion solver, internally referred as Combustion 2. This paper will focus on some of the technical aspects of the fluid solver, however the presentation will pivot more towards the production work related to its adoption on Avengers: Endgame.	https://dl.acm.org/authorize?N689302	Gerardo Aguilera, John Johansson
Foveated AR: dynamically-foveated augmented reality display	"We present a near-eye augmented reality display with resolution and focal depth dynamically driven by gaze tracking. The display combines a traveling microdisplay relayed off a concave half-mirror magnifier for the high-resolution foveal region, with a wide field-of-view peripheral display using a projector-based Maxwellian-view display whose nodal point is translated to follow the viewer's pupil during eye movements using a traveling holographic optical element. The same optics relay an image of the eye to an infrared camera used for gaze tracking, which in turn drives the foveal display location and peripheral nodal point. Our display supports accommodation cues by varying the focal depth of the microdisplay in the foveal region, and by rendering simulated defocus on the ""always in focus"" scanning laser projector used for peripheral display. The resulting family of displays significantly improves on the field-of-view, resolution, and form-factor tradeoff present in previous augmented reality designs. We show prototypes supporting 30, 40 and 60 cpd foveal resolution at a net 85° × 78° field of view per eye."	https://dl.acm.org/authorize?N688044	Jonghyun Kim, Youngmo Jeong, Michael Stengel, Kaan Akşit, Rachel Albert, Ben Boudaoud, Trey Greer, Joohwan Kim, Ward Lopes, Zander Majercik, Peter Shirley, Josef Spjut, Morgan McGuire, David Luebke
Foveated displays: toward classification of the emerging field	Recent advances in head-mounted displays (HMDs) provide new levels of immersion by delivering imagery straight to human eyes. The high spatial and temporal resolution requirements of these displays pose a tremendous challenge for real-time rendering and video compression. Since the eyes rapidly decrease in spatial acuity with increasing eccentricity, providing high resolution to peripheral vision is unnecessary. Upcoming VR displays provide real-time estimation of gaze, enabling gaze-contingent rendering and compression methods that take advantage of this acuity falloff. In this setting, special care must be given to avoid visible artifacts such as a loss of contrast or addition of flicker.	https://dl.acm.org/authorize?N689321	Anton Kaplanyan, Anton Sochenov, Thomas Leimkühler, Mikhail Okunev, Todd Goodall, Gizem Rufo
Fractional gaussian fields for modeling and rendering of spatially-correlated media	Transmission of radiation through spatially-correlated media has demonstrated deviations from the classical exponential law of the corresponding uncorrelated media. In this paper, we propose a general, physically-based method for modeling such correlated media with non-exponential decay of transmittance. We describe spatial correlations by introducing the Fractional Gaussian Field (FGF), a powerful mathematical tool that has proven useful in many areas but remains under-explored in graphics. With the FGF, we study the effects of correlations in a unified manner, by modeling both high-frequency, noise-like fluctuations and -th order fractional Brownian motion (fBm) with a stochastic continuity property. As a result, we are able to reproduce a wide variety of appearances stemming from different types of spatial correlations. Compared to previous work, our method is the first that addresses both short-range and long-range correlations using physically-based fluctuation models. We show that our method can simulate different extents of randomness in spatially-correlated media, resulting in a smooth transition in a range of appearances from exponential falloff to complete transparency. We further demonstrate how our method can be integrated into an energy-conserving RTE framework with a well-designed importance sampling scheme and validate its ability compared to the classical transport theory and previous work.	https://dl.acm.org/authorize?N688090	Jie Guo, Yanjun Chen, Bingyang Hu, Ling-Qi Yan, Yanwen Guo, Yuntao Liu
From comic book to movie screen: achieving symbiosis between rigging and creature effects for Venom	The primary challenge at the heart of Visual Effects lies in the ability to translate the director's creative brief into compelling visuals using a combination of art and technology. In the case of a key requirement was to keep the character design as faithful to the comic books as possible. This talk describes the challenges tackled by the Rigging, Effects, and R&D departments at DNEG in order to bring this classic antihero to life in a photorealistic manner.	https://dl.acm.org/authorize?N689378	Charlie Banks, William Gabriele, Marco Dambros, Erica Vigilante, Jesus R Nieto, Martin Pražák, Sylvain Brugnot
From light to sound: prisms and auto-zoom lenses	We developed a system for visualizing golf putting trajectories that can be used in live broadcasting. The trajectory computer graphics (CGs) in a golf putting scene are useful for visualizing the results of past plays and the shape of the green. In addition, displaying past trajectories that were shot near the position of the next player helps TV viewers predict the ball movement of the next play. Visualizing the putting trajectories in this way offers TV viewers a new style of watching live golf tournaments and helps make the programs more understandable and exiting.	https://dl.acm.org/authorize?N689300	Masaki Takahashi, Takahito Ito, Hidehiko Okubo, Hideki Mitsumine
Fully automatic colorization for anime character considering accurate eye colors	In this paper, we propose a method to colorize line drawings of anime characters' faces with colors from a reference image. Previous studies using reference images often fail to realize fully-automatic colorization, especially for small areas, e.g., eye colors in the resulting image may differ from the reference image. The proposed method accurately colorizes eyes in the input line drawing using automatically computed hints. The hints are round patches used to specify the positions and corresponding colors extracted from the eye areas of a reference image.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338585	Kenta Akita, Yuki Morimoto, Reiji Tsuruno
Fundamental solutions for water wave animation	Due to the enormous amount of detail and the interplay of various biological phenomena, modeling realistic ecosystems of trees and other plants is a challenging and open problem. Previous research on modeling plant ecologies has focused on representations to handle this complexity, mostly through geometric simplifications, such as points or billboards. In this paper we describe a multi-scale method to design large-scale ecosystems with individual plants that are realistically modeled and faithfully capture biological features, such as growth, plant interactions, different types of tropism, and the competition for resources. Our approach is based on leveraging inter- and intra-plant self-similarities for efficiently modeling plant geometry. We focus on the interactive design of plant ecosystems of up to 500K plants, while adhering to biological priors known in forestry and botany research. The introduced parameter space supports modeling properties of nine distinct plant ecologies while each plant is represented as a 3D surface mesh. The capabilities of our framework are illustrated through numerous models of forests, individual plants, and validations.	https://dl.acm.org/authorize?N688186	Miłosz Makowski, Torsten Hädrich, Jan Scheffczyk, Dominik L. Michels, Sören Pirk, Wojtek Pałubicki
GPGPU acceleration of environmental and movement datasets	Due to the increased availability and accuracy of GPS sensors, the field of movement ecology has been able to benefit from larger datasets of movement data. As miniaturisation and the efficiency of electronic components have improved, additional sensors have been coupled with GPS tracking to enable features related to the animal's state at a given position to be recorded. This capability is especially relevant to understand how environmental conditions may affect movement.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338584	Daniel Bird, Stephen Laycock
Game Changer	A macho toy arcade prize is quick to judge a young girl who wants to win him, and goes on a life-changing journey in an attempt to stop her from winning enough tickets to take him home.	https://dl.acm.org/authorize?N689239	Aviv Mano
GauGAN: semantic image synthesis with spatially adaptive normalization	"We propose GauGAN, a GAN-based image synthesis model that can generate photo-realistic images given an input semantic layout. It is built on spatially-adaptive normalization, a simple but effective normalization layer. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and non-linearity layers. We show that this is sub-optimal as the normalization layers tend to ""wash away"" semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Our proposed method outperforms the previous methods by a large margin. Furthermore, the new method enables natural extension to control the style of the synthesized images. Given a style guide image, our style encoder network captures the style into a latent code, which our image generator network combines with the semantic layout via spatially-adaptive normalization to generate a photo-realistic image that respects both the style of the guide image and content of the semantic layout. Our method will enable people without drawing skills to effectively express their imagination. GauGAN in the inference time is a simple convolutional neural network. It runs real-time on most modern GPU cards. GauGAN is one of the recent research efforts in advancing GANs for real-time image rendering. We believe this is of interest to the SIGGRAPH and real-time communities."	https://dl.acm.org/authorize?N689127	Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu
Gaussian-product subdivision surfaces	Probabilistic distribution models like Gaussian mixtures have shown great potential for improving both the quality and speed of several geometric operators. This is largely due to their ability to model large fuzzy data using only a reduced set of atomic distributions, allowing for large compression rates at minimal information loss. We introduce a new surface model that utilizes these qualities of Gaussian mixtures for the definition and control of a parametric smooth surface. Our approach is based on an enriched mesh data structure, which describes the probability distribution of spatial surface locations around each vertex via a Gaussian covariance matrix. By incorporating this additional covariance information, we show how to define a smooth surface via a nonlinear probabilistic subdivision operator based on products of Gaussians, which is able to capture rich details at fixed control mesh resolution. This entails new applications in surface reconstruction, modeling, and geometric compression.	https://dl.acm.org/authorize?N688080	Reinhold Preiner, Tamy Boubekeur, Michael Wimmer
Gaze-contingent ocular parallax rendering for virtual reality	"There is not yet consensus in the field on what constitutes a ""foveated display"". We propose a compromise between the perspectives of rendering, imaging, physiology and vision science that defines a foveated display as a display designed to function in the context of user gaze. This definition enables us to describe 2 axes of foveation, gaze interaction and resolution distribution, which we then subdivide to provide useful categories for classification. We view this proposal as the start of a discussion among the community rather than a final taxonomy."	https://dl.acm.org/authorize?N689320	Josef Spjut, Ben Boudaoud
Geometric algebra and computer graphics	In the past few years the movie industry has switched over from stochastic rasterisation approaches to using physically based light transport simulation: path tracing in production has become ubiquitous across studios. The new approach came with undisputed advantages such as consistent lighting, progressive previews, and fresh code bases. But also abandoning 30 years of experience meant some hard cuts affecting all stages such as lighting, look development, geometric modelling, scene description formats, the way we schedule for multi-threading, just to name a few. This means there is a rich set of people involved and as an expert in either one of these aspects it is easy to lose track of the big picture. This is part I of a full-day course, and it focuses on the necessary background knowledge. In this part, we would like to provide context for everybody interested in understanding the challenges behind writing renderers intended for movie production work. In particular we will give an insight into movie production requirements for new students and academic researchers. On the other side we will lay a solid mathematical foundation to develop new ideas to solve problems in this context. To further illustrate, part II of the course will focus on materials (acquisition and production requirements) and showcase practical efforts by prominent professionals in the field, pointing out unexpected challenges encountered in new shows and unsolved problems as well as room for future work wherever appropriate.	https://dl.acm.org/authorize?N689279	Charles G. Gunn, Steven De Keninck
Geometric computing with python	Path guiding is a family of adaptive variance reduction techniques in physically-based rendering, which includes methods for sampling both direct and indirect illumination, surfaces and volumes but also for sampling optimal path lengths and making splitting decisions. Since adoption of path tracing as a de facto standard in the VFX industry several years ago, there has been an increased interest in producing high-quality images with low amount of Monte Carlo samples per pixel. Path guiding, which has received attention in the research community in the past few years, has proven to be useful for this task and therefore has been adopted by Weta Digital. Recently, it has also been implemented in the Walt Disney Animation Studios' Hyperion and Pixar's Renderman. The goal of this course is to share our practical experience with path guiding in production and to provide self-contained overview of recently published techniques and to discuss their pros and cons. We also take audience through theoretical background of various path guiding methods which are mostly based on machine learning - used to adapt sampling distributons based on observed samples - and zero-variance random walk theory - used as a framework for combining different sampling decisions in an optimal way. At the end of our course we discuss open problems and invite researchers to further develop path guiding in their future work.	https://dl.acm.org/authorize?N689278	Sebastian Koch, Teseo Schneider, Francis Williams, Daniele Panozzo
Geometry-aware scattering compensation for 3D printing	Commercially available full-color 3D printing allows for detailed control of material deposition in a volume, but an exact reproduction of a target surface appearance is hampered by the strong subsurface scattering that causes nontrivial volumetric cross-talk at the print surface. Previous work showed how an iterative optimization scheme based on accumulating absorptive materials at the surface can be used to find a volumetric distribution of print materials that closely approximates a given target appearance. In this work, we first revisit the assumption that pushing the absorptive materials to the surface results in minimal volumetric cross-talk. We design a full-fledged optimization on a small domain for this task and confirm this previously reported heuristic. Then, we extend the above approach that is critically limited to color reproduction on planar surfaces, to arbitrary 3D shapes. Our method enables high-fidelity color texture reproduction on 3D prints by effectively compensating for internal light scattering within arbitrarily shaped objects. In addition, we propose a content-aware gamut mapping that significantly improves color reproduction for the pathological case of thin geometric features. Using a wide range of sample objects with complex textures and geometries, we demonstrate color reproduction whose fidelity is superior to state-of-the-art drivers for color 3D printers.	https://dl.acm.org/authorize?N688166	Denis Sumin, Tobias Rittig, Vahid Babaei, Thomas Nindel, Alexander Wilkie, Piotr Didyk, Bernd Bickel, Jaroslav Křivánek, Karol Myszkowski, Tim Weyrich
Ghost Fleet VR	"Based on the sci-fi short story by Philip K. Dick, ""The Great C"" is a cinematic narrative featuring a thrilling storyline, stunning environments, and a powerful soundtrack. ""The Great C"" was developed from the ground up to push the boundaries of storytelling in VR."	https://dl.acm.org/authorize?N689361	Lucas Gath
GlideReality: a highly immersive VR System augmented by a novel multi-modal and multi-contact cutaneous wearable display	The user's palm plays a relevant role in the detection and manipulation of objects. The GlideReality system was designed to increase the immersive Virtual Reality (VR) experience and make it more interactive, providing multi-contact and multi-modal haptic stimuli on the user's palm using a novel wearable haptic display LinkGlide. The proposed device, which consists of three five-bar inverted linkages generatingthree contact points,is used toprovide tactile stimuli to the users in the VR environment. The force sensors, located at each of the contact points, provide feedback to the control system to generate the required stimuli with a specific force. In addition, an impedance control was developed to generate the sensation of objects stiffness. The system consists of an HTC Vive Pro Headset for room scale VR, the trackers for the hand position, and the game engine Unity 3D for the integration of the system and the application development. The GlideReality system provides highly immersive interaction with virtual objects in the different applications: BallFeel, PressFeel, and AnimalFeel. With this system, we can potentially achieve a highly immersive VR experience and make it more interactive.	https://dl.acm.org/authorize?N688351	Miguel Altamirano Cabrera, Dzmitry Tsetserukou
Global adaptive sampling hierarchies in production ray tracing	Microfacet-based reflection models are widely used in visual effects applications ranging from computer games to animation and feature film rendering. However, the standard microfacet BRDF supports single scattering only. Light that is scattered more than once is not accounted for, which can lead to significant energy loss. As physically based rendering becoming more prevalent in production, the lack of energy preservation has become problematic. This has lead to several recent works on multiple scattering. Heitz et al. [2016] presented a volumetric approach to model multiple scattering accurately but its stochastic evaluation increases variance. Xie and Hanrahan [2018] presented an analytical multiple scattering model that is efficient but has a singularity in the direction of mirror reflection.	https://dl.acm.org/authorize?N689333	Feng Xie, Anton Kaplanyan, Warren Hunt, Pat Hanrahan
Gloomy eyes	When the sun got tired of humans, it decided to hide and never rise again. The darkness awoke the dead from their graves. A zombie kid, Gloomy, and a mortal girl, Nena, fall in love and immerse in a deep connection that not even the most powerful man can destroy.	https://dl.acm.org/authorize?N689138	Antoine Cayrol, German Heller
Glove puppetry cloud theater through a virtual reality network	Chinese glove puppetry is a traditional art with a long history and widely spread in the folk ever. This paper presents a collaborative work in multi-user virtual reality system for puppetry's opera. According to our developed system, each user has a unique perspective on any shared virtual world and interaction through our virtual reality network. This system achieves human-computer interaction and realizes the interaction between people. In addition, it brings an entertaining experience to users and easy to operate for all ages. In order to cultural preservation, we can record a grandmaster's puppet show performance in our system. Our research not only delivers a balance of art and technology in culture creativity, but also preserve folk art.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338564	Der-Lor Way, Weng-Kei Lau, Tzu Ying Huang
Golf training system using sonification and virtual shadow	This paper proposed a golf training system using real-time audio-visual feedback. The system captures user's motion with an optical motion tracking system and projects his/her posture as a virtual shadow on the ground. Unlike other golf training systems, our system enables the user to keep his/her gaze on the ball. The model swing motion of the expert golfer is also overlaid so that the user can understand the difference between his/her motion and expert's. Moreover, the system provides audio feedback using sound image localization. It enables the user to understand the position and orientation of the club face, which is often out of sight of the user during his/her swing motion.	https://dl.acm.org/authorize?N688352	Atsuki Ikeda, Yuka Tanaka, Dong-Hyun Hwang, Homare Kon, Hideki Koike
Graph matching based anime colorization with multiple references	We propose a graph matching-based anime-colorization method from line drawings using multiple reference images. A graph structure of each frame in an input line drawing sequence helps to find correspondences of regions to be colorized between frames. However, it is difficult to find precise correspondences of whole frames only from a single reference image, because the graph structure tends to change drastically during the sequence. Therefore, our method first finds an optimal image from multiple reference images according to a cost function that represents shape similarity between nodes and compatibility of node pairs. While it is necessary to prepare several manually colored reference images, our method is still effective in reducing the effort required for colorization in anime production. We demonstrate the effectiveness of our method using actual images from our production.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338560	Akinobu Maejima, Hiroyuki Kubo, Takuya Funatomi, Tatsuo Yotsukura, Satoshi Nakamura, Yasuhiro Mukaigawa
Grasshopper: dreamworks environmental motion system	Rig speed plays a critical role in animation pipelines. Real-time performance provides instant feedback to artists, thereby allowing quick iterations and ultimately leading to better quality animation. A complete approach to real-time performance requires both playback and manipulation at interactive speeds. A pose-based caching system (PBCS) addresses the former, but the manipulation of complex rigs remains slow. This paper speeds up rig manipulation by taking advantage of modern multi-core architectures and the GPU, and by constructing rigs that evaluate efficiently on parallel processing hardware. This complete approach, including tool updates and rig optimizations, was used successfully to significantly improve interactive rig manipulation performance on	https://dl.acm.org/authorize?N689336	John Kahwaty, Walter Yoder, Andy Lin, Gene S. Lee, David Suroviec
Groovy assignment: the VR ride: a cross disciplinary assignment in computer graphics and interactivity	"In this Groovy Assignment submission, we present a VR Ride assignment that challenges students to create a fully interactive VR computer graphics experience integrated with a themed ride. For this assignment, a ride is an apparatus that fully supports the users weight, utilizes the user's body motions as a primary input for computer interactivity, and provides haptic feedback relevant to the VR experience. The assignment is designed to inspire and motivate creative thinking and cross disciplinary collaboration with faculty and students from outside the scope of those traditionally involved in programs focused on computer graphics and interaction. The assignment can be easily scaled, by utilizing wholly existing, found or purchased platforms (exercise equipment such as a rowing machine or stationary bike) for use with small groups of students if desired. In the example presented, students from Electrical Engineering, Mechanical Engineering, Industrial Design, Game Design, VR & Immersive Media, Animation & VFX and Game Design programs collaborated using primarily found or recycled components to build a bespoke, human powered ""VR Cycle"" ride, integrated with original VR experiences developed for the ride."	https://dl.acm.org/authorize?N688266	Nick Jushchyshyn, Robert Lloyd, Erik Sundquist
HAPTIC PLASTeR: soft, thin, light and flexible haptic display using DEA composed of slide-ring material for daily life	"Recently, many wearable haptic displays have been widely explored aiming the enriched user experience through various application such as the virtual reality (VR) and Telexistence. Many such proposed wearable haptic displays so far are composed of rigid materials such as motors, voice coil actuators and speakers [Minamizawa et al. 2007]. Therefore, in recent years, haptic displays composed of soft materials such as dielectric elastomer actuators (DEAs) have been proposed [Koo et al. 2008; Park et al. 2015]. However, the polymers used in such DEAs have hysteresis-loss property as a main physical limitation, which results in different output displacement property during the actuation cycles. In addition, these DEAs consist of a property that requires ""pre-stretching"", i.e., a strong force is required at the time of initial actuation. As such, these properties requires specialized actuation mechanisms for DEAs to be widely used as haptic displays."	https://dl.acm.org/authorize?N688353	Tadatoshi Kurogi, Yuji Yonehara, Roshan Lalintha Peiris, Takeshi Fujiwara, Kouta Minamizawa
Hand modeling and simulation using stabilized magnetic resonance imaging	"We demonstrate how to acquire complete human hand bone anatomy (meshes) in multiple poses using magnetic resonance imaging (MRI). Such acquisition was previously difficult because MRI scans must be long for high-precision results (over 10 minutes) and because humans cannot hold the hand perfectly still in non-trivial and badly supported poses. We invent a manufacturing process whereby we use lifecasting materials commonly employed in film special effects industry to generate hand molds, personalized to the subject, and to each pose. These molds are both ergonomic and encasing, and they stabilize the hand during scanning. We also demonstrate how to efficiently segment the MRI scans into individual bone meshes in all poses, and how to correspond each bone's mesh to same mesh connectivity across all poses. Next, we interpolate and extrapolate the MRI-acquired bone meshes to the entire range of motion of the hand, producing an accurate data-driven animation-ready rig for bone meshes. We also demonstrate how to acquire not just bone geometry (using MRI) in each pose, but also a matching highly accurate surface geometry (using optical scanners) in each pose, modeling skin pores and wrinkles. We also give a soft tissue Finite Element Method simulation ""rig"", consisting of novel tet meshing for stability at the joints, spatially varying geometric and material detail, and quality constraints to the acquired skeleton kinematic rig. Given an animation sequence of hand joint angles, our FEM soft tissue rig produces quality hand surface shapes in arbitrary poses in the hand range of motion. Our results qualitatively reproduce important features seen in the photographs of the subject's hand, such as similar overall organic shape and fold formation."	https://dl.acm.org/authorize?N688160	Bohan Wang, George Matcuk, Jernej Barbič
Handheld multi-frame super-resolution	Compared to DSLR cameras, smartphone cameras have smaller sensors, which limits their spatial resolution; smaller apertures, which limits their light gathering ability; and smaller pixels, which reduces their signal-to-noise ratio. The use of color filter arrays (CFAs) requires demosaicing, which further degrades resolution. In this paper, we supplant the use of traditional demosaicing in single-frame and burst photography pipelines with a multiframe super-resolution algorithm that creates a complete RGB image directly from a burst of CFA raw images. We harness natural hand tremor, typical in handheld photography, to acquire a burst of raw frames with small offsets. These frames are then aligned and merged to form a single image with red, green, and blue values at every pixel site. This approach, which includes no explicit demosaicing step, serves to both increase image resolution and boost signal to noise ratio. Our algorithm is robust to challenging scene conditions: local motion, occlusion, or scene changes. It runs at 100 milliseconds per 12-megapixel RAW input burst frame on mass-produced mobile phones. Specifically, the algorithm is the basis of the feature, as well as the default merge method in mode (whether zooming or not) on Google's flagship phone.	https://dl.acm.org/authorize?N688073	Bartlomiej Wronski, Ignacio Garcia-Dorado, Manfred Ernst, Damien Kelly, Michael Krainin, Chia-Kai Liang, Marc Levoy, Peyman Milanfar
Harmonic triangulations	We introduce the notion of harmonic triangulations: a harmonic triangulation simultaneously minimizes the Dirichlet energy of all piecewise linear functions. By a famous result of Rippa, Delaunay triangulations are the harmonic triangulations of planar point sets. We prove by explicit counterexample that in 3D a harmonic triangulation does not exist in general. However, we show that bistellar flips are harmonic: if they decrease Dirichlet energy for one set of function values, they do so for all. This observation gives rise to the notion of locally harmonic triangulations. We demonstrate that locally harmonic triangulations can be efficiently computed, and efficiently reduce sliver tetrahedra. The notion of harmonic triangulation also gives rise to a scalar measure of the quality of a triangulation, which can be used to prioritize flips and optimize the position of vertices. Tetrahedral meshes generated by optimizing this function generally show better quality than Delaunay-based optimization techniques.	https://dl.acm.org/authorize?N688009	Marc Alexa
Hedgehog	A little boy speaks about hedgehogs all the time to everybody.	https://dl.acm.org/authorize?N689232	Vaibhav Keswani
HeroMirror interactive: a gesture controlled augmented reality gaming experience	Appropriately chosen user interfaces are essential parts of immersive augmented reality experiences. Regular user interfaces cannot be efficiently used for interactive, real-time augmented reality applications. In this study, a gesture controlled educational gaming experience is described where gesture recognition relies on deep learning methods. Our implementation is able to replace a depth-camera based gesture recognition system using conventional camera while ensuring the same level of recognition accuracy.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338554	Tamás Matuszka, Ferenc Czuczor, Zoltán Sóstai
Heterotopias	Heterotopias is an interactive VR experience based on Michel Foucault's lecture, Des espaces autres. It leverages eye-tracking technology, transforming blinks into cinematic cuts. Suspended in a hanging chair, the user progresses through a series of virtual spaces resembling an abbreviated life cycle - a well, a garden, and a mausoleum.	https://dl.acm.org/authorize?N689145	Noa Kaplan, Szilvia Ruszev
Hierarchical russian roulette for vertex connections	While bidirectional path tracing is a well-established light transport algorithm, many samples are required to obtain high-quality results for specular-diffuse-glossy or glossy-diffuse-glossy reflections especially when they are highly glossy. To improve the efficiency for such light path configurations, we propose a technique for vertex connections. Our technique accelerates a huge number of Russian roulette operations according to an approximate scattering lobe at an eye-subpath vertex for many cached light-subpath vertices. Our method dramatically reduces the number of random number generations needed for Russian roulette by introducing a hierarchical rejection algorithm which assigns random numbers in a top-down fashion. To efficiently reject light vertices in each hierarchy, we also introduce an efficient approximation of anisotropic scattering lobes used for the probability of Russian roulette. Our technique is easy to integrate into some existing bidirectional path tracing-based algorithms which cache light-subpath vertices (e.g., probabilistic connections, and vertex connection and merging). In addition, unlike existing many-light methods, our method does not restrict multiple importance sampling strategies thanks to the simplicity of Russian roulette. Although the proposed technique does not support perfectly specular surfaces, it significantly improves the efficiency for caustics reflected on extremely glossy surfaces in an unbiased fashion.	https://dl.acm.org/authorize?N688081	Yusuke Tokuyoshi, Takahiro Harada
Hierarchy models: building blocks for procedural rigging	The use of animation cycle multiplexing technique was first deployed on at Dreamworks to accomplish the ambitious task of animating many winged characters with limited resources. It has since been adopted through software platform changes, working with award winning software Premo, to its current form in It would not have been possible to accomplish the film with the level of visual complexity due to production budget and time constraints.	https://dl.acm.org/authorize?N689325	Sandy Kao, Simon Otto
Holding the shape in hair simulation	This talk presents DreamWorks' Feather System , which is used for grooming body feathers and modeling scales interactively in real time. It is also used for feather motion such as secondary motion or wind, for feather special effects such as ruffling or puffing up, and for feather finaling. The system has been used in several shows at DreamWorks including	https://dl.acm.org/authorize?N689323	Nicholas Augello, David Tonnesen, Arunachalam Somasundaram
How to Train Your Dragon: the hidden what?	"How to Train Your Dragon: The Hidden World represents the final installment of the Dragon trilogy whose releases spanned 9 years during which technology changed dramatically. The filmmakers will discuss how the third film evolved to embrace new technology, particularly Physically Based Rendering and improved simulation capabilities, while remaining true to the design principles established in the first movie. It was critical to the storytelling that the Hidden World, home of the dragons, needed to be expansive and endless despite being underground. It needed to be rooted in the naturalistic style of the franchise, yet feel alien to our human world. We will discuss the creative design challenges and technical hurdles faced in bringing this world to life. Due to the introduction of new software and pipeline, built on USD, as well as schedule challenges, the production process resembled less of our traditional linear pipeline typical of feature animation production with much more back and forth between departments working at the same time. The panelists will explore how this ""controlled chaos"" that characterized production of The Hidden World impacted the final film."	https://dl.acm.org/authorize?N689293	Dave Walvoord, Lawrence Lee, Munira Tayabji, Paolo deGuzman, Pablo Valle, Chris De St. Jeor
Hummingbird: dreamworks feather system	MPC's proprietary grooming software - - has been used to create all hair, fur and feathers on our characters with great success for over a decade. However, the creation of feather grooms has always been a time consuming and technically challenging task for our Groom department, often keeping a senior artist occupied for months. Due to narrower deadlines and a constant push for higher quality, we recently extended our feather tool set, which has allowed our artists to significantly streamline their feather workflow. After adopting our new geometry-based feather system in production, our Groom artists have been able to reduce the time frame for finalizing a hero character from months to weeks.	https://dl.acm.org/authorize?N689324	Rasmus Haapaoja, Christoph Genzwürker
Hyperparameter optimization in black-box image processing using differentiable proxies	"Nearly every commodity imaging system we directly interact with, or indirectly rely on, leverages power efficient, application-adjustable black-box hardware image signal processing (ISPs) units, running either in dedicated hardware blocks, or as proprietary software modules on programmable hardware. The configuration parameters of these black-box ISPs often have complex interactions with the output image, and must be adjusted prior to deployment according to application-specific quality and performance metrics. Today, this search is commonly performed by ""golden eye"" experts or algorithm developers leveraging domain expertise. We present a system to optimize the parameters of black-box hardware and software image processing pipelines according to any arbitrary (i.e., application-specific) metric. We leverage a mapping between the configuration space and evaluation metrics, parameterized by a convolutional neural network that we train in an end-to-end fashion with imaging hardware in-the-loop. Unlike prior art, our allow for high-dimension parameter search with stochastic first-order optimizers, without explicitly modeling any lower-level image processing transformations. As such, we can efficiently optimize black-box image processing pipelines for a variety of imaging applications, reducing application-specific configuration times from months to hours. Our optimization method is fully automatic, even with black-box hardware in the loop. We validate our method on experimental data for real-time display applications, object detection, and extreme low-light imaging. The proposed approach outperforms manual search qualitatively and quantitatively for all domain-specific applications tested. When applied to traditional denoisers, we demonstrate that---just by changing hyperparameters---traditional algorithms can outperform recent deep learning methods by a substantial margin on recent benchmarks."	https://dl.acm.org/authorize?N688072	Ethan Tseng, Felix Yu, Yuting Yang, Fahim Mannan, Karl ST. Arnaud, Derek Nowrouzezahrai, Jean-François Lalonde, Felix Heide
ILM 2019 - Behind the Magic	Industrial Light & Magic (ILM) highlights its best feature film visual effects and animation work from the past year.	https://dl.acm.org/authorize?N689357	Brent Segura-Bowers
IR surface reflectance estimation and material type recognition using two-stream net and kinect camera	Recently, material type recognition using color or light field camera has been studied. However, visual pattern based approaches for material type recognition without direct acquisition of surface reflectance show limited performance. In this work, we propose IR surface reflectance estimation using ToF (Time-of-Flight) active sensor such as Kinect and perform surface material type recognition based on both color and reflectance clues. Two stream deep neural network consists of convolutional neural network encoding visual clue and recurrent neural network encoding reflectance characteristic is proposed for material classification. Estimated IR surface reflectance and material type recognition evaluation on our Color-IR Material Data set show promising performance compared to prior approaches.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338557	SeokYeong Lee, HwaSup Lim, SangChul Ahn, SeungKyu Lee
ISSv2 and OpenISS distributed system for real-time interaction for performing arts	v2 is a configurable toolbox which provides multimodal interaction and serves as a platform for artists to enhance their performance through the use of depth and colour data from a 3D capture device. Its newest iteration was presented as part of ChineseCHI in 2018. This latest iteration of ISSv2 is powered by an open source core named OpenISS. The core allows the ISSv2 platform to be run as a distributed system. Video and depth capture are done from a computer acting as a server with a client component for displaying the applied effects and video from a web browser. This has the added benefit of allowing the artist to broadcast their performance live and opens the way for audience interaction. There are two primary motivations behind creating an open source core for the ISS: first, open source tech allows more people to participate in the development process as well as understand how the technology works while spreading maintenance responsibilities to their respective parties. Secondly, having a core allows parts of they system to be switched out at will without having to modify it all at once, this is particularly relevant with respect to capture devices.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338539	Serguei A. Mokhov, Deschanel Li, Haotao Lai, Jashanjot Singh, Yiran Shen, Jonathan Llewellyn, Miao Song, Sudhir P. Mudur
Il divino: Michelangelo's Sistine ceiling in VR	Experience Michelangelo's Sistine ceiling in virtual reality. Move around the Chapel or take a guided tour and use a virtual scaffold to see the work up close as few have seen it before.	https://dl.acm.org/authorize?N689146	Christopher Evans
Image ditching: manipulating images with audio effects	Image glitching and data-bending are used to introduce image formats, data manipulation, and data visualization to beginning CS students and non-major students taking computing courses with no coding required.	https://dl.acm.org/authorize?N688267	Erik Brunvand
Immersive game for dental anesthesia training with haptic feedback	Training anesthetic application is a challenge for teaching dentistry, given the complexity of the procedure and the risks involved. Through an immersive virtual reality environment, the system presented here offers a playful way of learning, through game elements. This serious game allows the user to practice anesthesia technique with or without aids (tactile or visual) and to receive scores at different levels of difficulty. An important differential is the possibility of the syringe being driven automatically (tactile aid) in order to reproduce trajectories inserted by experienced instructors. Thus, the student can feel as if the instructor was conducting the learner's hand while guiding her or him.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338592	Matheus A. O. Ribeiro, Allan A. Tori, Romero Tori, Fátima L. S. Nunes
Immersivemote: combining foveated AI and streaming for immersive remote operations	Immersivemote is a novel technology combining our former foveated streaming solution with our novel foveated AI concept. While we have previously shown that foveated streaming can achieve 90% bandwidth savings, as compared to existing streaming solutions, foveated AI is designed to enable real-time video augmentations that are controlled through eye-gaze. The combined solution is therefore capable of effectively interfacing remote operators with mission critical information obtained, in real time, from task-aware machine understanding of the scene and IoT data.	https://dl.acm.org/authorize?N689382	Pietro Lungaro, Konrad Tollmar
Implicit untangling: a robust solution for modeling layered clothing	We present a new algorithm to automatically schedule Halide programs for high-performance image processing and deep learning. We significantly improve upon the performance of previous methods, which considered a limited subset of schedules. We define a parameterization of possible schedules much larger than prior methods and use a variant of beam search to search over it. The search optimizes runtime predicted by a cost model based on a combination of new derived features and machine learning. We train the cost model by generating and featurizing hundreds of thousands of random programs and schedules. We show that this approach operates effectively with or without autotuning. It produces schedules which are on average almost twice as fast as the existing Halide autoscheduler without autotuning, or more than twice as fast with, and is the first automatic scheduling algorithm to significantly outperform human experts on average.	https://dl.acm.org/authorize?N688176	Andrew Adams, Karima Ma, Luke Anderson, Riyadh Baghdadi, Tzu-Mao Li, Michaël Gharbi, Benoit Steiner, Steven Johnson, Kayvon Fatahalian, Frédo Durand, Jonathan Ragan-Kelley
InNervate immersion: case study of dynamic simulations in AR/VR environments for learning muscular innervation	We present a collaborative immersive technology effort, InNervate AR and InNervate VR. These applications meet the need to expand on existing anatomy education platforms by implementing a more dynamic and interactive user interface. This user interface allows for exploration of the complex relationship between motor nerve deficits and their effects upon the canine anatomy's ability to produce movement. Preliminary AR user studies provided us with positive feedback in the quality of learning. The studies show that the dynamic touch interactions in AR definitely benefit students' critical reasoning and spatial visualization in learning motor nerve and muscle relationships. However, users seek a more immersive VR-based learning environment, without the distractions that an AR experience may offer. Based on this feedback, a VR version of this learning experience was created. Preliminary responses show that users are satisfied with this VR environment which allows them to manipulate and control the anatomical content with full-body interactions.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338580	Margaret Cook, Amber Ackley, Karla Chang Gonzalez, Austin Payne, Jinsil Hwaryoung Seo, Caleb Kicklighter, Michelle Pine, Timothy McLaughlin
Inside Hurricane Maria in 360 Degrees	Follow Blue, a highly intelligent Velociraptor, on her quest for survival, as she scours for food and water, searches for signs of life, and fights against some of the island's most threatening predators and the foreseeable destruction of Isla Nublar.	https://dl.acm.org/authorize?N689363	Gregory Shirah
Instafalls: how to train your waterfalls	This article describes the workflow for delivering real-time FX elements to animation in an omni-directional way and how they get processed for further stages, as it was used on 'Avengers: Endgame' at Weta Digital. We will discuss our procedural approach to solving some of the challenges around what we call 'AnimFx', focusing on production requirements and our ensuing technological solution.	https://dl.acm.org/authorize?N689347	Tobias Mack, Ashraf Ghoniem, Ruben Mayor, Gerardo Aguilera
Integrate USD the nodal way, a visual VFX pipeline	Hierarchy Models provide an encapsulation mechanism for joint hierarchies that yield an essential building block for procedural rigging. With Hierarchy Models, joints travel through dependency graphs (DGs) as an atomic entity. Operation nodes in the DG can modify all aspects of input hierarchy and even perform topological modifications like adding or removing joints. The Hierarchy Model reduces complexity in character rigs, improves separation between data and behavior, provides a clean interface, and simplifies understanding and debugging rigs. It offers geometric evaluation optimizations, and promotes parallelism in the DG structure.	https://dl.acm.org/authorize?N689314	Michael Hutchinson, Sandy Kao, Kevin Ochs, Gilbert Davoud, Alex Powell
InteractionFusion: real-time reconstruction of hand poses and deformable objects in hand-object interactions	Hand-object interaction is challenging to reconstruct but important for many applications like HCI, robotics and so on. Previous works focus on either the hand or the object while we jointly track the hand poses, fuse the 3D object model and reconstruct its rigid and nonrigid motions, and perform all these tasks in real time. To achieve this, we first use a DNN to segment the hand and object in the two input depth streams and predict the current hand pose based on the previous poses by a pre-trained LSTM network. With this information, a unified optimization framework is proposed to jointly track the hand poses and object motions. The optimization integrates the segmented depth maps, the predicted motion, a spatial-temporal varying rigidity regularizer and a real-time contact constraint. A nonrigid fusion technique is further involved to reconstruct the object model. Experiments demonstrate that our method can solve the ambiguity caused by heavy occlusions between hand and object, and generate accurate results for various objects and interacting motions.	https://dl.acm.org/authorize?N688093	Hao Zhang, Zi-Hao Bo, Jun-Hai Yong, Feng Xu
Interactive and automatic navigation for 360° video playback	A common way to view a 360° video on a 2D display is to crop and render a part of the video as a normal field-of-view (NFoV) video. While users can enjoy natural-looking NFoV videos using this approach, they need to constantly make manual adjustment of the viewing direction not to miss interesting events in the video. In this paper, we propose an interactive and automatic navigation system for comfortable 360° video playback. Our system finds a virtual camera path that shows the most salient areas through the video, generates a NFoV video based on the path, and plays it in an online manner. A user can interactively change the viewing direction while watching a video, and the system instantly updates the path reflecting the intention of the user. To enable online processing, we design our system consisting of an offline pre-processing step, and an online 360° video navigation step. The pre-processing step computes optical flow and saliency scores for an input video. Based on these, the online video navigation step computes an optimal camera path reflecting user interaction, and plays a NFoV video in an online manner. For improved user experience, we also introduce optical flow-based camera path planning, saliency-aware path update, and adaptive control of the temporal window size. Our experimental results including user studies show that our system provides more pleasant experience of watching 360° videos than existing approaches.	https://dl.acm.org/authorize?N688153	Kyoungkook Kang, Sunghyun Cho
Interactive cinematic scientific visualization in unity	Cinematic scientific visualizations turn complex scientific phenomena and concepts into stunning graphics and make them easier for the general public to comprehend. Adding interactivity to cinematic scientific visualizations is highly beneficial especially for educational purposes, as it keeps the viewers engaged and promotes active learning [Cano et al. 2017]. Although there are existing software tools such as VisIt that are capable of handling large data sets and allow for interactive exploration, they are usually designed for scientists and not meant for producing cinematic visualizations for the general public. Creating aesthetically pleasing visualizations of scientific data helps to better communicate the scientific concepts, increase impact, and reach a broader audience [Borkiewicz et al. 2018]. As existing examples of visualizations that are both interactive and cinematic have mainly been produced with custom software, there is a lack of easily accessible tools for developing this type of scientific visualization.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338588	Jasmine Y. Shih, Kalina Borkiewicz, AJ Christensen, Donna Cox
Interactive hand pose estimation using a stretch-sensing soft glove	We propose a stretch-sensing soft glove to interactively capture hand poses with high accuracy and without requiring an external optical setup. We demonstrate how our device can be fabricated and calibrated at low cost, using simple tools available in most fabrication labs. To reconstruct the pose from the capacitive sensors embedded in the glove, we propose a deep network architecture that exploits the spatial layout of the sensor itself. The network is trained only once, using an inexpensive off-the-shelf hand pose reconstruction system to gather the training data. The per-user calibration is then performed on-the-fly using only the glove. The glove's capabilities are demonstrated in a series of ablative experiments, exploring different models and calibration methods. Comparing against commercial data gloves, we achieve a 35% improvement in reconstruction accuracy.	https://dl.acm.org/authorize?N688096	Oliver Glauser, Shihao Wu, Daniele Panozzo, Otmar Hilliges, Olga Sorkine-Hornung
Interactive spatial augmented reality system for Chinese opera	In this research, the authors designed an interactive spatial augmented reality system for stage performance based on the technologies of UWB positioning and Bluetooth triggering. The position of the actor is obtained through the antenna tag carried by the actor and the signal base station placed on the stage. Special effects can be triggered through the Bluetooth module according to the actor. The system has a higher degree of freedom in practical applications, which can present an interactive spatial augmented reality effect, and therefore provide new possibilities for the application of spatial augmented reality in the stage performance. The system could bring better immersive experience to the audiences, and it also brings new possibilities for the aesthetic creation of opera.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338566	YanXiang Zhang, YiRun Shen, WeiWei Zhang, ZiQiang Zhu, PengFei Ma
Interactive virtual reality orchestral music	The authors developed a VR orchestral application for interactive music experience, allowing virtual musical instruments in an orchestral piece to be repositioned spatially, dynamically and interactively in VR space. This can be done for changing environments where 3D audio technology is used to restructure traditional orchestral pieces into a new music art form. User experience surveys were undertaken on two kinds of users, with the results showing that the VR orchestral system developed in this paper could bring some special advantages in the musical experience.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338547	YanXiang Zhang, Li Tao, YiRun Shen, Elieisar Clayton, Fangbemi Abassin
Introduction to real-time ray tracing	The last few years have seen a decisive move of the movie making industry towards rendering using physically based methods, mostly implemented in terms of path tracing. While path tracing reached most VFX houses and animation studios at a time when a physically based approach to rendering and especially material modelling was already firmly established, the new tools brought with them a whole new balance, and many new workflows have evolved to find a new equilibrium. Letting go of instincts based on hard-learned lessons from a previous time has been challenging for some, and many different takes on a practical deployment of the new technologies have emerged. While the language and toolkit available to the technical directors keep closing the gap between lighting in the real world and the light transport simulations ran in software, an understanding of the limitations of the simulation models and a good intuition of the trade-offs and approximations at play are of fundamental importance to make efficient use of the available resources. In this course, the novel workflows emerged during the transitions at a number of large facilities are presented to a wide audience including technical directors, artists, and researchers. This is the second part of a two part course. While the first part focuses on background and implementation, the second one focuses on material acquisition and modeling, GPU rendering, and pipeline evolution.	https://dl.acm.org/authorize?N689270	Morgan McGuire, Peter Shirley, Chris Wyman
Jurassic world: blue	In a last desperate attempt to communicate, a man decides to lock his boyfriend inside his mind. A raging stream of consciousness unfolds, shedding light on a flawed relationship.	https://dl.acm.org/authorize?N689364	Felix Lajeunesse, Paul Raphael
Kaiju Confidential	A comedy about big creatures and small slights. Grigon (Blake Anderson) may not be the toughest beast on the block, but he's certainly the most neurotic. When he discovers the legendary Mega-Hydra (Paul F. Tompkins) rampaging on his turf, it becomes a stand-off of passive-aggressive proportions.	https://dl.acm.org/authorize?N689353	Ethan Shaftel
Kinky Kitchen	Chosen kitchen utensils and groceries are leading a life of their own. In an intimate moment filled with lust, they interact in an untypical way. The viewer experiences the pleasure of their sensuality through sounds that at first set them on the wrong track.	https://dl.acm.org/authorize?N689243	Bea Hoeller
KleinPAT: optimal mode conflation for time-domain precomputation of acoustic transfer	A typical rainfall scenario contains tens of thousands of dynamic sound sources. A characteristic of the large-scale scene is the strong randomness in raindrop distribution, which makes it notoriously expensive to synthesize such sounds with purely physical methods. Moreover, the raindrops hitting different surfaces (liquid or various solids) can emit distinct sounds, for which prior methods with unified impact sound models are ill-suited. In this paper, we present a physically-based statistical simulation method to synthesize realistic rain sound, which respects surface materials. We first model the raindrop sound with two mechanisms, namely the initial impact and the subsequent pulsation of entrained bubbles. Then we generate material sound textures (MSTs) based on a specially designed signal decomposition and reconstruction model. This allows us to distinguish liquid surface with bubble sound and different solid surfaces with MSTs. Furthermore, we build a basic rain sound (BR-sound) bank with the proposed raindrop sound clustering method based on a statistical model, and design a sound source activator for simulating spatial propagation in an efficient manner. This novel method drastically decreases the computational cost while producing convincing sound results. Various experiments demonstrate the effectiveness of our sound simulation model.	https://dl.acm.org/authorize?N688178	Shiguang Liu, Haonan Cheng, Yiying Tong
LayerCode: optical barcodes for 3D printed shapes	With the advance of personal and customized fabrication techniques, the capability to embed information in physical objects becomes evermore crucial. We present , a tagging scheme that embeds a carefully designed barcode pattern in 3D printed objects as a deliberate byproduct of the 3D printing process. The LayerCode concept is inspired by the structural resemblance between the parallel black and white bars of the standard barcode and the universal layer-by-layer approach of 3D printing. We introduce an encoding algorithm that enables the 3D printing layers to carry information without altering the object geometry. We also introduce a decoding algorithm that reads the LayerCode tag of a physical object by just taking a photo. The physical deployment of LayerCode tags is realized on various types of 3D printers, including Fused Deposition Modeling printers as well as Stereolithography based printers. Each offers its own advantages and tradeoffs. We show that LayerCode tags can work on complex, nontrivial shapes, on which all previous tagging mechanisms may fail. To evaluate LayerCode thoroughly, we further stress test it with a large dataset of complex shapes using virtual rendering. Among 4,835 tested shapes, we successfully encode and decode on more than 99% of the shapes.	https://dl.acm.org/authorize?N688167	Henrique Teles Maia, Dingzeyu Li, Yuan Yang, Changxi Zheng
Layered reconstruction of stippling art	Given a point set ⊆ , reconstruction refers to the process of identifying a vector shape that best approximates the input. Although this field was pioneered since 1983 by Edelsbrunner [Edelsbrunner et al. 2006] and has been heavily studied since then, the general problem still remains open, ill-posed and challenging. Solving it is essential for a wide range of applications, from image processing, pattern recognition and sketching to wireless networks.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338598	Amal Dev Parakkat, Pooran Memari, Marie-Paule Cani
Learning character-agnostic motion for motion retargeting in 2D	Analyzing human motion is a challenging task with a wide variety of applications in computer vision and in graphics. One such application, of particular importance in computer animation, is the retargeting of motion from one performer to another. While humans move in three dimensions, the vast majority of human motions are captured using video, requiring 2D-to-3D pose and camera recovery, before existing retargeting approaches may be applied. In this paper, we present a new method for retargeting video-captured motion between different human performers, without the need to explicitly reconstruct 3D poses and/or camera parameters. In order to achieve our goal, we learn to extract, directly from a video, a high-level latent motion representation, which is invariant to the skeleton geometry and the camera view. Our key idea is to train a deep neural network to decompose temporal sequences of 2D poses into three components: motion, skeleton, and camera view-angle. Having extracted such a representation, we are able to re-combine motion with novel skeletons and camera views, and decode a retargeted temporal sequence, which we compare to a ground truth from a synthetic dataset. We demonstrate that our framework can be used to robustly extract human motion from videos, bypassing 3D reconstruction, and outperforming existing retargeting methods, when applied to videos in-the-wild. It also enables additional applications, such as performance cloning, video-driven cartoons, and motion retrieval.	https://dl.acm.org/authorize?N688020	Kfir Aberman, Rundi Wu, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or
Learning from human-robot interactions in modeled scenes	There is increasing interest in using robots in simulation to understand and improve human-robot interaction (HRI). At the same time, the use of simulated settings to gather training data promises to help address a major data bottleneck in allowing robots to take advantage of powerful machine learning approaches. In this paper, we describe a prototype system that combines the robot operating system (ROS), the simulator Gazebo, and the Unity game engine to create human-robot interaction scenarios. A person can engage with the scenario using a monitor wall, allowing simultaneous collection of realistic sensor data and traces of human actions.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338546	Mark Murnane, Max Breitmeyer, Francis Ferraro, Cynthia Matuszek, Don Engel
Learning to fly: computational controller design for hybrid UAVs with reinforcement learning	Hybrid unmanned aerial vehicles (UAV) combine advantages of multicopters and fixed-wing planes: vertical take-off, landing, and low energy use. However, hybrid UAVs are rarely used because controller design is challenging due to its complex, mixed dynamics. In this paper, we propose a method to automate this design process by training a mode-free, model-agnostic neural network controller for hybrid UAVs. We present a neural network controller design with a novel error convolution input trained by reinforcement learning. Our controller exhibits two key features: First, it does not distinguish among flying modes, and the same controller structure can be used for copters with various dynamics. Second, our controller works for real models without any additional parameter tuning process, closing the gap between virtual simulation and real fabrication. We demonstrate the efficacy of the proposed controller both in simulation and in our custom-built hybrid UAVs (Figure 1, 8). The experiments show that the controller is robust to exploit the complex dynamics when both rotors and wings are active in flight tests.	https://dl.acm.org/authorize?N688097	Jie Xu, Tao Du, Michael Foshey, Beichen Li, Bo Zhu, Adriana Schulz, Wojciech Matusik
Learning to optimize halide with tree search and random programs	We propose a new modal sound synthesis method that rapidly estimates all acoustic transfer fields of a linear modal vibration model, and greatly reduces preprocessing costs. Instead of performing a separate frequency-domain Helmholtz radiation analysis for each mode, our method partitions vibration modes into using optimal , then performs a single time-domain wave simulation for each chord. We then perform on each chord's time-domain radiation field using a specialized QR solver, and thereby extract the frequency-domain transfer functions of each mode. The precomputed transfer functions are represented for fast far-field evaluation, e.g., using multipole expansions. In this paper, we propose to use a single scalar-valued Far-field Acoustic Transfer (FFAT) cube map. We describe a GPU-accelerated vector wavesolver that achieves high-throughput acoustic transfer computation at accuracy sufficient for sound synthesis. Our implementation, KleinPAT, can achieve hundred- to thousand-fold speedups compared to existing Helmholtz-based transfer solvers, thereby enabling large-scale generation of modal sound models for audio-visual applications.	https://dl.acm.org/authorize?N688177	Jui-Hsien Wang, Doug L. James
Level ex: tracing all kinds of rays... on mobile	Presented are new methods to raytrace and simulate raytracing of different types of rays through a range of different participating media in real-time, including: *SDF ray-tracing to recreated refractive physics-based viscous fluids that interface with a surrounding soft-body environment and other physics objects in the scene. SDFs are blended seamlessly with each other and environment objects. Special considerations are made to maximize cache and memory-coherence on tile-based mobile GPU architectures. *X-Rays - Which follow logarithmic attenuation functions and fresnel-like behaviors as they are absorbed and scattered through different materials on their way from the emitter to the detector plate. X-ray tracing behaves more like a transparent shadowing technique than anything else. * Ultrasonic Sound waves - Used in real-time ultrasound imaging, these rays break all the rules - you can't even rely on their propagation speed to stay constant. Dozens of different artifact types (shadows, ringing, etc.) must be simulated through the tracing behavior. For example - highly reflective objects outside of the ultrasound beam 'slice' may bounce back into the frame, creating the appearance of 'ghost' objects that aren't actually there. Various mobile-friendly thread bundling approaches are taken to cast and bounce rays in the scene. Worth noting The SDF ray-marching technique in Pulm Ex was shown last year but has been drastically improved with refractions, performance optimizations, and new SDF shapes, blends, and techniques.	https://dl.acm.org/authorize?N689128	Sam Glassenberg, Matthew Yaeger
Light pruning on Toy Story 4	DNEG is constantly improving its global tools and processes to make them more efficient and artist-friendly, while leveraging state-of-the-art technologies and trends in the industry. The special requirements for Sony Pictures' movie were the perfect opportunity for us to improve our Image Based Lighting (IBL) workflows. In this paper we present the a semi-automated system to allow fast and artist-friendly extraction of numerous lights from HDR images (HDRIs), using computer vision. The system uses , a USD-based implementation of light descriptions, to allow cross-DCC usage and pipeline integration.	https://dl.acm.org/authorize?N689318	Stefano Cieri, Alexander Schwank
Lighting design for stylized animation	This course will introduce students, researchers and digital artists to the recent results in perceptual research on virtual characters. It covers both how technical and artistic aspects that constitute the appearance of a virtual character influence human perception. We will report results of studies that addressed the influence of low-level cues like facial proportions, shading or level of detail and higher-level cues such as behavior or artistic stylization. We will place emphasis on aspects that are encountered during character development, animation, and achieving consistency between the visuals and storytelling. The insights that we present in this course will serve as an additional toolset to anticipate the effect of certain design decisions and to create more convincing characters, especially in the case where budgets or time are limited.	https://dl.acm.org/authorize?N689271	Dave Walvoord, DreamWorks Animation
Liquid printed pneumatics	Liquid Printed Pneumatics is a project developed by the MIT, Self-Assembly Lab and Swiss designer Christophe Guberan that focuses on the 3D printing of pneumatically activated objects. Rapid Liquid Printing (RLP), a new additive manufacturing process developed at the lab, is used to create shape changing devices and objects.	https://dl.acm.org/authorize?N688354	Bjorn Sparrman, Schendy Kernizan, Jared Laucks, Skylar Tibbits, Christophe Guberan
LiquidMask: utilizing liquid-based haptic for multiple tactile sensation in immersive virtual reality	We present LiquidMask, a liquid-based haptic device which can simultaneously produce thermal changes and vibration responses by filling liquid into the mask. To demonstrate our device, we also design a diving game to show the utility of LiquidMask.	https://dl.acm.org/authorize?N689255	Yi-Ya Liao, Ya-Fang Hong, Ping-Hsuan Han, Ju-Chun Ko
Local light field fusion: practical view synthesis with prescriptive sampling guidelines	We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000X fewer views. We demonstrate our approach's practicality with an augmented reality smart-phone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.	https://dl.acm.org/authorize?N688074	Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar
Luminance-contrast-aware foveated rendering	Current rendering techniques struggle to fulfill quality and power efficiency requirements imposed by new display devices such as virtual reality headsets. A promising solution to overcome these problems is foveated rendering, which exploits gaze information to reduce rendering quality for the peripheral vision where the requirements of the human visual system are significantly lower. Most of the current solutions model the sensitivity as a function of eccentricity, neglecting the fact that it also is strongly influenced by the displayed content. In this work, we propose a new luminance-contrast-aware foveated rendering technique which demonstrates that the computational savings of foveated rendering can be significantly improved if local luminance contrast of the image is analyzed. To this end, we first study the resolution requirements at different eccentricities as a function of luminance patterns. We later use this information to derive a low-cost predictor of the foveated rendering parameters. Its main feature is the ability to predict the parameters using only a low-resolution version of the current frame, even though the prediction holds for high-resolution rendering. This property is essential for the estimation of required quality before the full-resolution image is rendered. We demonstrate that our predictor can efficiently drive the foveated rendering technique and analyze its benefits in a series of user experiments.	https://dl.acm.org/authorize?N688043	Okan Tarhan Tursun, Elena Arabadzhiyska-Koleva, Marek Wernikowski, Radosław Mantiuk, Hans-Peter Seidel, Karol Myszkowski, Piotr Didyk
Machine-learning denoising in feature film production	"We present our experience deploying and using machine learning denoising of Monte Carlo renders in the production of animated feature films such as Pixar's , Disney Animation's and Industrial Light & Magic's visual effects work on photo-realistic films such as We show what it took to move from an R&D implementation of ""Denoising with Kernel Prediction and Asymmetric Loss Functions"" [Vogels et al. 2018] to a practical tool in a production pipeline."	https://dl.acm.org/authorize?N689395	Henrik Dahlberg, David Adler, Jeremy Newlin
MagicPAPER: tabletop interactive projection device based on tangible interaction	This study proposes a tabletop projection device that can be implemented by combining physical objects with interactive projections. Users can interact on kraft papers using daily tools, such as marker pens, toothbrushes, colored blocks, and square wooden blocks. The input of the proposed device is a multifunction sensor, and the output is a tabletop projector. Using MagicPAPER, four types of interactions are implemented, namely drawing, gesture recognition, brushing, and building blocks. The abstract and poster discuss the design motivations and system descriptions of MagicPAPER.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338575	Qin Wu, Jiayuan Wang, Sirui Wang, Tong Su, Chenmei Yu
MagniFinger: fingertip-mounted microscope for augmenting human perception	We propose , a fingertip-worn microscopy device that augments the limited abilities of human visual and tactile sensory systems in micrometer-scale environments. makes use of the finger's dexterous motor skills to achieve precise and intuitive control while allowing the user to observe the desired position simply by placing a fingertip. To implement the fingertip-sized device and its tactile display, we have built a system comprising a ball lens, an image sensor, and a thin piezoelectric actuator. Vibration-based tactile feedback is displayed based on the luminance of a magnified image, providing the user with the feeling of touching the magnified world.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338563	Noriyasu Obushi, Sohei Wakisaka, Shunichi Kasahara, Atsushi Hiyama, Masahiko Inami
MagniFinger: magnified perception by a fingertip probe microscope	We propose , a fingertip-worn microscopy device that augments the limited abilities of human visual and tactile sensory systems in micrometer-scale environments. makes use of the finger's dexterous motor skills to achieve precise and intuitive control while allowing the user to observe the desired position simply by placing a fingertip. To implement the fingertip-sized device and its tactile display, we have built a system comprising a ball lens, an image sensor, and a thin piezoelectric actuator. Vibration-based tactile feedback is displayed based on the luminance of a magnified image, providing the user with the feeling of touching the magnified world.	https://dl.acm.org/authorize?N688365	Noriyasu Obushi, Sohei Wakisaka, Shunichi Kasahara, Atsushi Hiyama, Masahiko Inami
Marooned	"""Marooned"" tells the story of a cantankerous and selfish robot named C-0R13, stranded on an abandoned lunar outpost, who longs to return to Earth. With a partially built ship and his last power source, the determined robot will stop at nothing to achieve his goal."	https://dl.acm.org/authorize?N689350	Andrew Erekson
Mary and the monster: chapter one	"Mary and the Monster: Chapter One is a shared AR experience that brings to life ""Frankenstein"" through the eyes of a young Mary Shelley. Using Parallux technology and the Magic Leap One, audiences are immersed in the author's world as she conjures the most famous gothic tale of all time."	https://dl.acm.org/authorize?N689256	Kris Layng, Ken Perlin, Sebastian Herscher, Gabe Zetter
Massively parallel layout generation in real time	Conceiving an artwork requires designers to create assets and organize (or layout) them in a harmonious, self-orating story. While creativity is fundamental to both aspects, the latter can be bolstered with automated techniques. We present a first true SIMD formulation for the layout generation and leverage CUDA-enabled GPU to scan through millions of possible permutations and rank them on aesthetic appeal using weighted parameters such as symmetry, alignment, density, size balance, etc. The entire process happens in real-time using a GPU-accelerated implementation of replica exchange Monte Carlo Markov Chain method. The exploration of design space is rapidly narrowed by performing distant jumps from poorly ranked layouts, and fine tuning the highly ranked ones. Several iterations are carried out until desired rank or system convergence is achieved. In contrast to existing approaches, our technique generates aesthetically better layouts and runs more than two orders of magnitude faster.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338596	Vineet Batra, Ankit Phogat, Tarun Beri
Matching prescription & visual acuity: towards AR for humans	An increasingly important part of usuable near-eye displays to allow use by users who use vision correction such as that provided by glasses and contact lenses. Recent research indicates that over 20% of world population is myopic, and this percentage is increasing [Holden et al. 2016]. Commercial prototypes have offered an additional prescription lens pair or a glasses-compatible design, but both of these approaches increase the size and weight of the device. Ideally, a user's prescription should be considered from the optical design stage for the smallest form factor.	https://dl.acm.org/authorize?N688366	Jonghyun Kim, Michael Stengel, Jui-Yi Wu, Ben Boudaoud, Josef Spjut, Kaan Akşit, Rachel Albert, Trey Greer, Youngmo Jeong, Ward Lopes, Zander Majercik, Peter Shirley, Morgan McGuire, David Luebke
Mayday - Final Chapter	We use pop-up book to present a person's life. Along with the increase of age, different choice leads us to the success, lost, fulfilled, or regret. Therefore, when the loved one leaves, the life from that day is called the rest of life.	https://dl.acm.org/authorize?N689241	Muh Chen
Meet in rain: a serious game to help the better appreciation of Chinese poems	is a serious game on Chinese poetry. While invoking various events, players must complete given tasks, which help them better appreciate the poems, by exploring imaginary sceneries that depict Chinese poems. Its visual design also mimics Chinese paintings in the era when the poems were created. As only a few serious games exist for Chinese poetry and they mostly focus on knowledge acquisition, our work provides a rare design exemplar of a serious game that is designed with the intention to foster aesthetic appreciation for a cultural subject.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338587	Ye-Ning Jiang, Hiroki Nishino
Melody slot machine	"We developed an interactive music system called the ""Melody Slot Machine, "" which provides an experience of manipulating a music performance. The melodies used in the system are divided into multiple segments, and each segment has multiple variations of melodies. By turning the dials manually, users can switch the variations of melodies freely. When you pull the slot lever, the melody of all segments rotates, and melody segments are randomly selected. Since the performer displayed in a hologram moves in accordance with the selected variation of melody, users can enjoy the feeling of manipulating the performance."	https://dl.acm.org/authorize?N688367	Masatoshi Hamanaka, Takayuki Nakatsuka, Shigeo Morishima
Mesh wrap based on affine-invariant coordinates	We present a new technique to transfer the mesh connectivity between 3D models of different shapes. In contrast to prior work, our method is designed to wrap meshes under large, locally non-rigid deformations, which are commonly found in feature animations. To achieve this goal, we enrich the traditional iterative closest point scheme with mesh coordinates that parametrize the edge spans of a desired tessellation invariant to locally affine transformations. As a result, we produce surfaces that wrap a target geometry accurately, while resembling the patch layout of the source mesh. Our implementation also offers an interactive workflow to assist the authoring of curve correspondences. We employed this tool to wrap 600 humanoid assets to a reference mesh connectivity, spanning characters modeled over the last 15 years at Pixar.	https://dl.acm.org/authorize?N689370	Fernando de Goes, Alonso Martinez
Mesh-driven generation and animation of groomed feathers	This talk presents DreamWorks' , an environmental motion system for creating believable animation for grass, plants, and other vegetation. The system was used extensively on the complex and expansive sets in and is currently being used on more productions at DreamWorks.	https://dl.acm.org/authorize?N689335	Chris De St. Jeor, Chris Michael, Kurt Phillips, Arunachalam Somasundaram
MeshCNN: a network with an edge	Polygonal meshes provide an efficient representation for 3D shapes. They explicitly captureboth shape surface and topology, and leverage non-uniformity to represent large flat regions as well as sharp, intricate features. This non-uniformity and irregularity, however, inhibits mesh analysis efforts using neural networks that combine convolution and pooling operations. In this paper, we utilize the unique properties of the mesh for a direct analysis of 3D shapes using , a convolutional neural network designed specifically for triangular meshes. Analogous to classic CNNs, MeshCNN combines specialized convolution and pooling layers that operate on the mesh edges, by leveraging their intrinsic geodesic connections. Convolutions are applied on edges and the four edges of their incident triangles, and pooling is applied via an edge collapse operation that retains surface topology, thereby, generating new mesh connectivity for the subsequent convolutions. MeshCNN learns which edges to collapse, thus forming a task-driven process where the network exposes and expands the important features while discarding the redundant ones. We demonstrate the effectiveness of MeshCNN on various learning tasks applied to 3D meshes.	https://dl.acm.org/authorize?N688045	Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, Daniel Cohen-Or
Meu	Meu allows users to connect to their inner emotional states and communicate them to others by providing customizable avatars along with unique algorithms that connect your movements to reactive music, visual effects, and haptics vibrations. Your body language and dances create a Meu that your friends can then experience.	https://dl.acm.org/authorize?N689147	Sarah Hashkes
Mica	In this experience you meet Mica, the human center of spatial computing and AI. Together, you develop an emotional connection and create artwork. Inspired by collaborative art practices, this experience establishes you and Mica as a unified force, working together to find meaning and create beauty.	https://dl.acm.org/authorize?N689257	John Monos
Mica: a photoreal character for spatial computing	Mica is an autonomous, photoreal human character that runs on the Magic Leap One spatial computing platform. The past ten years have seen tremendous improvements in virtual character technology on screen, in film and games. Spatial computing hardware such as Magic Leap One allows us to take the next step: interacting with virtual characters off the screen. We believe that virtual characters who are aware of and can interact directly with users in real-world environments will provide a uniquely compelling experience.	https://dl.acm.org/authorize?N689385	James Bancroft, Nafees Bin Zafar, Sean Comer, Takashi Kuribayashi, Jonathan Litt, Thomas Miller
MindPalace	"""Traveling While Black"" is a cinematic VR experience that immerses the viewer in the long history of restriction of movement for black Americans, and the creation of safe spaces in our communities."	https://dl.acm.org/authorize?N689375	Carl Krause, Dominik Stockhausen
Mixing sauces: a viscosity blending model for shear thinning fluids	The materials around us usually exist as of constituents, each constituent with possibly a different property. How can we describe the material property of such a mixture is the core question of this paper. We propose a nonlinear blending model that can capture intriguing flowing behaviors that can differ from that of the individual constituents (Fig. 1). We used a laboratory device, , to measure the flowing properties of various fluid-like foods, and found that an has nice agreements with the measured data even for the mixtures of these foods. We then constructed a blending model such that it qualitatively agrees with the measurements and is closed in the parameter space of the elastic Herschel-Bulkley model. We provide validations through comparisons between the measured and estimated properties using our model, and comparisons between simulated examples and captured footages. We show the utility of our model for producing interesting behaviors of various mixtures.	https://dl.acm.org/authorize?N688040	Kentaro Nagasawa, Takayuki Suzuki, Ryohei Seto, Masato Okada, Yonghao Yue
Mortal Kombat 11: high fidelity cached simulations in real-time	Mortal Kombat 11 introduces an Alembic-based asset pipeline that enables artists to leverage new workflows previously unattainable in real-time games. Blood and gore are the cornerstone of the franchise, and the art direction for our Fatalities and Fatal Blows focuses on close-up, high-fidelity, slow-motion shots showing extreme amounts of blood, which traditional sprite particles would struggle to achieve.	https://dl.acm.org/authorize?N689394	Jason Nadro, Matt Battaglia, Aren Voorhees
Multi-resolution approach to computing locally injective maps on meshes	Computing injective mappings with low distortions on meshes is an important problem for its wide range of practical applications in computer graphics, geometric modeling and physical simulations. Such tasks as surface parametrization or shape deformation are often reduced to minimizing non-convex and non-linear geometric energies defined over triangulated domains. These energies are commonly expressed in a finite element manner as a weighted sum of distortion densities over simplixes :[MATHS HERE]where (2) enforces to preserve orientation of each simplex, and ( ) is a linear system of the given positional constraints. The orientation constraints are particularly important in parametrization problems, since they avoid undesirable foldover artifacts in the texture, while positional constraints are widely used in shape deformation applications, such as point-to-point deformations, deformations with fixed anchors, and more.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338591	Alexander Naitsat, Yehoshua Y. Zeevi
Multi-robot collaborative dense scene reconstruction	We present an autonomous scanning approach which allows multiple robots to perform collaborative scanning for dense 3D reconstruction of unknown indoor scenes. Our method plans scanning paths for several robots, allowing them to efficiently coordinate with each other such that the collective scanning coverage and reconstruction quality is maximized while the overall scanning effort is minimized. To this end, we define the problem as a dynamic task assignment and introduce a novel formulation based on Optimal Mass Transport (OMT). Given the currently scanned scene, a set of task views are extracted to cover scene regions which are either unknown or uncertain. These task views are assigned to the robots based on the OMT optimization. We then compute for each robot a smooth path over its assigned tasks by solving an approximate traveling salesman problem. In order to showcase our algorithm, we implement a multi-robot auto-scanning system. Since our method is computationally efficient, we can easily run it in real time on commodity hardware, and combine it with online RGB-D reconstruction approaches. In our results, we show several real-world examples of large indoor environments; in addition, we build a benchmark with a series of carefully designed metrics for quantitatively evaluating multi-robot autoscanning. Overall, we are able to demonstrate high-quality scanning results with respect to reconstruction quality and scanning efficiency, which significantly outperforms existing multi-robot exploration systems.	https://dl.acm.org/authorize?N688039	Siyan Dong, Kai Xu, Qiang Zhou, Andrea Tagliasacchi, Shiqing Xin, Matthias Nießner, Baoquan Chen
Multi-task audio-driven facial animation	We propose an effective method to solve multiple characters audio-driven facial animation (ADFA) problem in an end-to-end fashion via deep neural network. In this paper each character's ADFA considered as a single task, and our goal is to solve ADFA problem in multi-task setting. To this end, we present MulTaNet for multi-task audio-driven facial animation (MTADFA), which learns a cross-task unified feature mapping from audio-to-vertex that capture shared information across multiple related tasks, while try to find within-task prediction network encoding character-dependent topological information. Extensive experiments indicate that MulTaNet generates more natural-looking and stable facial animation, meanwhile shows better generalization capacity to unseen languages compare to previous approaches.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338541	Youngsoo Kim, Shounan An, Youngbak Jo, Seungje Park, Shindong Kang, Insoo Oh, Duke Donghyun Kim
Multi-view facial capture using binary spherical gradient illumination	High resolution facial capture has received significant attention in computer graphics due to its application in the creation of photorealistic digital humans for various applications ranging from film and VFX to games and VR. Here, the state of the art method for high quality acquisition of facial geometry and reflectance employs polarized spherical gradient illumination [Ghosh et al. 2011; Ma et al. 2007]. The technique has had a significant impact in facial capture for film VFX, recently receiving a Technical Achievement award from the Academy of Motion Picture Arts and Sciences [Aca 2019]. However, the method imposes a few constraints due to the employment of polarized illumination, and requires the camera viewpoints to be located close to the equator of the LED sphere for appropriate diffuse-specular separation for multiview capture [Ghosh et al. 2011]. The employment of polarization for reflectance separation also reduces the amount of light available for exposures and requires double the number of photographs (in cross and parallel polarization states), increasing the capture time and the number of photographs required for each face scan.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338611	Alexander Lattas, Mingqian Wang, Stefanos Zafeiriou, Abhijeet Ghosh
Multi-view relighting using a geometry-aware network	We propose the first learning-based algorithm that can relight images in a plausible and controllable manner given multiple views of an outdoor scene. In particular, we introduce a neural network that utilizes multiple geometry cues (normal maps, specular direction, etc.) and source and target shadow masks computed from a noisy obtained by multi-view stereo. Our model is a three-stage pipeline: two subnetworks refine the source and target shadow masks, and a third performs the final relighting. Furthermore, we introduce a novel representation for the shadow masks, which we call They reproject the colors from all views into the shadowed pixels and enable our network to cope with inacuraccies in the proxy and the non-locality of the shadow casting interactions. Acquiring large-scale multi-view relighting datasets for real scenes is challenging, so we train our network on photorealistic synthetic data. At train time, we also compute a noisy stereo-based geometric proxy, this time from the synthetic renderings. This allows us to bridge the gap between the real and synthetic domains. Our model generalizes well to real scenes. It can alter the illumination of drone footage, image-based renderings, textured mesh reconstructions, and even internet photo collections.	https://dl.acm.org/authorize?N688023	Julien Philip, Michaël Gharbi, Tinghui Zhou, Alexei A. Efros, George Drettakis
Multiple scattering using machine learning	A longstanding problem with the use of shading normals is the discontinuity introduced into the cosine falloff where part of the hemisphere around the shading normal falls below the geometric surface. Our solution is to add a geometrically derived shadowing function that adds minimal additional shadowing while falling smoothly to zero at the terminator. Our shadowing function is simple, robust, efficient and production proven.	https://dl.acm.org/authorize?N689334	Matt Jen-Yuan Chiang, Yining Karl Li, Brent Burley
Muscle-based facial retargeting with anatomical constraints	We present a physically based facial retargeting algorithm that is suitable for use in high-end production. Given an actor's facial performance, we first run a targeted muscle simulation on the actor in order to determine the actor blendshape muscles that best match the performance. The deformation of the actor blendshape muscles are then transferred onto the creature to obtain the corresponding creature blendshape muscles. Finally, the resulting creature blendshape muscles are used to drive a creature muscle simulation which yields the retargeted performance. In order to ensure the anatomically correct placement of the muscles, cranium, and jaw, we introduce novel anatomically motivated constraints during the transfer process. Compared to previous approaches, these constraints not only improve the expressiveness of the retargeted creature performance to the actor performance but also eliminate spurious visual artifacts.	https://dl.acm.org/authorize?N689371	Matthew Cong, Ronald Fedkiw
My favorite samples	Derivatives occur frequently in computer graphics and arise in many different contexts. Gradients and often Hessians of objective functions are required for efficient optimization. Gradients of potential energy are used to compute forces. Constitutive models are frequently formulated from an energy density, which must be differentiated to compute stress. Hessians of potential energy or energy density are needed for implicit integration. As the methods used in computer graphics become more accurate and sophisticated, the complexity of the functions that must be differentiated also increases. The purpose of this course is to show that it is practical to compute derivatives even for functions that may seem impossibly complex. This course provides practical strategies and techniques for planning, computing, testing, debugging, and optimizing routines for computing first and second derivatives of real-world routines. This course will also introduce and explore auto differentiation, which encompasses a variety of techniques for obtaining derivatives automatically. The goal of this course is not to introduce the concept of derivatives, how to use them, or even how to calculate them per se. This is not intended to be a calculus course; we will assume that our audience is familiar with multivariable calculus. Instead, the emphasis is on implementing derivatives of complicated computational procedures in computer programs and actually getting them to work.	https://dl.acm.org/citation.cfm?id=3329901	Alexander Keller, Iliyan Georgiev, Abdalla Ahmed, Per Christensen, Matt Pharr
NASA surveys hurricane damage to puerto rico's forests	Hurricane Maria transformed the lush rainforests of Puerto Rico, leaving lots of openings in the forest canopy. NASA scientists studied the island's forests before and after the storm using a portable lidar instrument onboard a small aircraft to create 3D views of the forest.	https://dl.acm.org/authorize?N689247	Alex Kekesi
Navigating intrinsic triangulations	We present a data structure that makes it easy to run a large class of algorithms from computational geometry and scientific computing on extremely poor-quality surface meshes. Rather than changing the geometry, as in traditional remeshing, we consider which connect vertices by straight paths along the exact geometry of the input mesh. Our key insight is that such a triangulation can be encoded implicitly by storing the direction and distance to neighboring vertices. The resulting then allows geometric and topological queries to be made on-demand by tracing paths across the surface. Existing algorithms can be easily translated into the intrinsic setting, since this data structure supports the same basic operations as an ordinary triangle mesh (vertex insertions, edge splits, ). The output of intrinsic algorithms can then be stored on an ordinary mesh for subsequent use; unlike previous data structures, we use a constant amount of memory and do not need to explicitly construct an unless it is specifically requested. Working in the intrinsic setting incurs little computational overhead, yet we can run algorithms on extremely degenerate inputs, including all manifold meshes from the data set. To evaluate our data structure we implement several fundamental geometric algorithms including intrinsic versions of Delaunay refinement and optimal Delaunay triangulation, approximation of Steiner trees, adaptive mesh refinement for PDEs, and computation of Poisson equations, geodesic distance, and flip-free tangent vector fields.	https://dl.acm.org/authorize?N688000	Nicholas Sharp, Yousuf Soliman, Keenan Crane
Neck strap haptics: an algorithm for non-visible VR information using haptic perception on the neck	In this poster, we propose a new haptic rendering algorithm that dynamically modulates wave parameters to convey distance, direction, and object type by utilizing neck perception and the Hapbeat-Duo, a haptic device composed of two actuators linked by a neck strap. This method is useful for various VR use cases because it provides feedback without disturbing users' movement. In our experiment, we presented haptic feedback of sine waves which were dynamically modulated according to direction and distance between a player and a target. These waves were presented to both sides of the users' necks independently. As a result, players could reach invisible targets and immediately know they had reached the targets. The proposed algorithm allows the neck to become as important a receptive part of body as eyes, ears, and hands.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338562	Yusuke Yamazaki, Shoichi Hasegawa, Hironori Mitake, Akihiko Shirai
Neural AR: immersive augmented reality with real-time neural style transfer	In this project, we propose a method of rendering on a neural network base that enables high-immersion AR and implement that method in demonstrating an AR game that runs interactively in realtime.	https://dl.acm.org/authorize?N689258	Daiki Taniguchi
Neural pixel error detection	Current video quality control entails a manual review of every frame for every video for pixel errors. A pixel error is a single or small group of anomalous pixels displaying incorrect colors, arising from multiple sources in the video production pipeline. The detection process is difficult, time consuming, and rife with human error. In this work, we present a novel approach for automated pixel error detection, applying simple machine learning techniques to great effect. We use an autoencoder architecture followed by statistical post-processing to catch all tested live action pixel anomalies while keeping the false positive rate to a minimum. We discuss previous dead pixel detection methods in image processing, and compare to other machine learning approaches.	https://dl.acm.org/authorize?N689397	Erika Varis Doggett, Anna M. C. Wolak, P. Daphne Tsatsoulis, Nicholas McCarthy
Neural volumes: learning dynamic renderable volumes from images	Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.	https://dl.acm.org/authorize?N688010	Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh
NeuroSkinning: automatic skin binding for production characters with deep graph networks	We present a deep-learning-based method to automatically compute skin weights for skeleton-based deformation of production characters. Given a character mesh and its associated skeleton hierarchy in rest pose, our method constructs a graph for the mesh, each node of which encodes the mesh-skeleton attributes of a vertex. An end-to-end deep graph convolution network is then introduced to learn the mesh-skeleton binding patterns from a set of character models with skin weights painted by artists. The network can be used to predict the skin weight map for a new character model, which describes how the skeleton hierarchy influences the mesh vertices during deformation. Our method is designed to work for non-manifold meshes with multiple disjoint or intersected components, which are common in game production and require complex skeleton hierarchies for animation control. We tested our method on the datasets of two commercial games. Experiments show that the predicted skin weight maps can be readily applied to characters in the production pipeline to generate high-quality deformations.	https://dl.acm.org/authorize?N688169	Lijuan Liu, Youyi Zheng, Di Tang, Yi Yuan, Changjie Fan, Kun Zhou
Nitro	Nitro is a mixed reality game for the Magic Leap that lets users drive a small remote control car around a virtual obstacle course using motion controls and hand gestures. They can steer by line of sight or by using a screen attached to their controller showing the car's perspective.	https://dl.acm.org/authorize?N689259	J. J. Castillo, Greg J Tamargo, Marc Huet
Noise reduction with image inpainting: an application in clinical data diagnosis	For cytology, pathology or histology image analysis, whether performed by computer-aided algorithms or human experts, a general issue is to exclude the disturbance caused by noisy objects, especially when appeared with high similarities in shape, color and texture with target cell or tissues. In this paper, we introduce a novel model to reduce such type of noisy objects with large quantity and distribution in the microscope images based on deep learning and hand-craft features. The model experimentally reduces the false positives without effect on objects of interest for cancer detection. Moreover, it also provides much distinct images for human experts for the final diagnosis.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338593	Jing Ke, Junwei Deng, Yizhou Lu
Nuclear dissent	"Interactive documentary ""Nuclear Dissent"" is a cautionary tale about nuclear destruction, told through the lens of some of the world's bravest activists and experts. From the weapon tests to the protest, the fallout and the propaganda, the documentary explores nuclear impacts and inspires everyday people to take action."	https://dl.acm.org/authorize?N689139	Grace Hwang, Heather Phenix, Pablo Vio
OVS+Tumor: a tool for enhanced lung tumor annotation in VR for machine learning training and analysis	OVS+Tumor creates a seamless VR environment designed for intuitive interaction aiding in the complex task of parsing through 3D CT-scans and annotating candidate tumors. Through interactive subsetting and on-the-fly iso-cloud generation, a wider range of users beyond just domain experts (radiologists/surgeons) can generate a viable machine-learning training dataset.	https://dl.acm.org/authorize?N689251	Daniel Crichton, Heather Kincaid, Ashish Mahabal, Sudhir Srivastava, Santiago Lombeyda, George Djorgovski, Christos Patriotis
OceanGAN: a deep learning alternative to physics-based ocean rendering	Physics-based models for ocean dynamics and optical raytracing are used extensively for rendering maritime scenes in computer graphics [Darles et al. 2011]. Raytracing models can provide high-fidelity representations of an ocean image with full control of the underlying environmental conditions, sensor specifications, and viewing geometry. However, the computational expense of rendering ocean scenes can be high. This work demonstrates an alternative approach to ocean raytracing via machine learning, specifically Generative Adversarial Networks (GANs) [Goodfellow et al. 2014]. In this paper, we demonstrate that a GAN trained on several thousand small scenes produced by a raytracing model can be used to generate megapixel scenes roughly an order of magnitude faster with a consistent wave spectrum and minimal processing artifacts.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338559	Christopher Ratto, Mimi Szeto, David Slocum, Kevin Del Bene
Old Soldier	War has a way of wearing down the most weathered soldiers. For legendary Horde warrior Varok Saurfang, this one could very well be his last.	https://dl.acm.org/authorize?N689231	Marc Messenger
Ollie	Ollie is a VR animation tool designed for beginners in 3D animation. Ollie introduces key concepts in animation (timing, easing, squash and stretch, and more) to new animators in a highly visual way, making the animation process feel playful, approachable, and encouraging.	https://dl.acm.org/authorize?N689250	Zachary Flores, Annie Oh, Sagar Ramesh, David Deedwania, Carson Hall, Drew Okenfuss
On bubble rings and ink chandeliers	This paper investigates the use of fundamental solutions for animating detailed linear water surface waves. We first propose an analytical solution for efficiently animating circular ripples in closed form. We then show how to adapt the method of fundamental solutions (MFS) to create ambient waves interacting with complex obstacles. Subsequently, we present a novel wavelet-based discretization which outperforms the state of the art MFS approach for simulating time-varying water surface waves with moving obstacles. Our results feature high-resolution spatial details, interactions with complex boundaries, and large open ocean domains. Our method compares favorably with previous work as well as known analytical solutions. We also present comparisons between our method and real world examples.	https://dl.acm.org/authorize?N688185	Camille Schreck, Christian Hafner, Chris Wojtan
On the accurate large-scale simulation of ferrofluids	We present an approach to the accurate and efficient large-scale simulation of the complex dynamics of ferrofluids based on physical principles. Ferrofluids are liquids containing magnetic particles that react to an external magnetic field without solidifying. In this contribution, we employ smooth magnets to simulate ferrofluids in contrast to previous methods based on the finite element method or point magnets. We solve the magnetization using the analytical solution of the smooth magnets' field, and derive the bounded magnetic force formulas addressing particle penetration. We integrate the magnetic field and force evaluations into the fast multipole method allowing for efficient large-scale simulations of ferrofluids. The presented simulations are well reproducible since our approach can be easily incorporated into a framework implementing a Fast Multipole Method and a Smoothed Particle Hydrodynamics fluid solver with surface tension. We provide a detailed analysis of our approach and validate our results against real wet lab experiments. This work can potentially open the door for a deeper understanding of ferrofluids and for the identification of new areas of applications of these materials.	https://dl.acm.org/authorize?N688048	Libo Huang, Torsten Hädrich, Dominik L. Michels
Optimal multiple importance sampling	Multiple Importance Sampling (MIS) is a key technique for achieving robustness of Monte Carlo estimators in computer graphics and other fields. We derive optimal weighting functions for MIS that provably minimize the variance of an MIS estimator, given a set of sampling techniques. We show that the resulting variance reduction over the balance heuristic can be higher than predicted by the variance bounds derived by Veach and Guibas, who assumed only non-negative weights in their proof. We theoretically analyze the variance of the optimal MIS weights and show the relation to the variance of the balance heuristic. Furthermore, we establish a connection between the new weighting functions and control variates as previously applied to mixture sampling. We apply the new optimal weights to integration problems in light transport and show that they allow for new design considerations when choosing the appropriate sampling techniques for a given integration problem.	https://dl.acm.org/authorize?N688082	Ivo Kondapaneni, Petr Vevoda, Pascal Grittmann, Tomáš Skřivan, Philipp Slusallek, Jaroslav Křivánek
Optimal transport-based polar interpolation of directional fields	We propose an algorithm that interpolates between vector and frame fields on triangulated surfaces, designed to complement field design methods in geometry processing and simulation. Our algorithm is based on a construction, leveraging a conservation law from the Hopf-Poincaré theorem to match singular points using ideas from optimal transport; the remaining detail of the field is interpolated using straightforward machinery. Our model is designed with topology in mind, sliding singular points along the surface rather than having them appear and disappear, and it caters to all surface topologies, including boundary and generator loops.	https://dl.acm.org/authorize?N688033	Justin Solomon, Amir Vaxman
Optimizing large scale crowds in ralph breaks the internet	Composed of over 550,000 crowd agents, Ralphzilla of , one of the largest movie monsters ever created and presented a huge technical and artistic challenge. We introduce a new crowd solver, Moshpit, which performs high resolution inter-body collision among crowd agents. We will also explain how Moshpit was incorporated into Disney's Skeleton Library (SL) and proprietary pipeline automation framework.	https://dl.acm.org/authorize?N689339	Dong Joo Byun, Alberto Luceño Ros, Alexander Moaveni, Marc Bryant, Joyce Le Tong, Moe El-Ali
Optimizing rig manipulation with GPU and parallel evaluation	"To deliver photoreal, dynamic and directable rock concert audiences for ""Bohemian Rhapsody"" to a demanding client brief, lead VFX vendor DNEG developed a novel crowd simulation solution based on multi-view video capture and image based modeling. Over 350 choreographed performances by individual crowd extras, totalling more than 70 hours of footage, was acquired on set using a video camera array. A system was developed to convert the video data to lightweight 3D sprites that could be quickly laid out, synchronised, edited and rendered on a large scale. Efficient artist workflow tools and scalable video processing technology was developed so that crew with little previous experience in crowd simulation could fill a virtual Wembley Stadium with a dancing, cheering crowd responding to Freddie Mercury's electrifying performance."	https://dl.acm.org/authorize?N689337	Ted Waine, May Leung, Paul Norris
Parametrization quantization with free boundaries for trimmed quad meshing	The generation of quad meshes based on surface parametrization techniques has proven to be a versatile approach. These techniques quantize an initial seamless parametrization so as to obtain an integer grid map implying a pure quad mesh. State-of-the-art methods following this approach have to assume that the surface to be meshed either has no boundary, or has a boundary which the resulting mesh is supposed to be aligned to. In a variety of applications this is not desirable and non-boundary-aligned meshes or grid-parametrizations are preferred. We thus present a technique to robustly generate integer grid maps which are either boundary-aligned, non-boundary-aligned, or partially boundary-aligned, just as required by different applications. We thereby generalize previous work to this broader setting. This enables the reliable generation of trimmed quad meshes with partial elements along the boundary, preferable in various scenarios, from tiled texturing over design and modeling to fabrication and architecture, due to fewer constraints and hence higher overall mesh quality and other benefits in terms of aesthetics and flexibility.	https://dl.acm.org/authorize?N688006	Max Lyon, Marcel Campen, David Bommes, Leif Kobbelt
Partial zoom on small display for people suffering from presbyopia	When presbyopic people use digital devices, they often zoom in the display, because it is not in focus when they move it close to face. We have proposed the automatic display zoom system for presbyopic people [Fang and Funahashi 2018]. However, some of the information on the small display has gone out of it after zooming-in (Fig. 2(a) to (b)). It is necessary to scroll it frequently, and a bother. On the other hand, a conventional partial zoom means like a magnifying glass is also usually provided (Fig. 2(a) to (c)). The part around a zoomed area is cut off, and it is necessary to move the glass frequently too. People sometimes want to skim through sentences and understand an overview. By the way, although it is difficult to read blurry words (Fig. 3(a)), you can guess and read a sentence includes the blurry words when some other words are clear (Fig. 3(b)). Therefore, we reconsider the zoom-in method for presbyopic people. For example, the area paid attention is zoomed in to read clearly, and the magnification rate of the area around it is gradually reduced to zoom-out rate so that all information is displayed in the small display even though some words are zoomed out. It is expected that you can guess and read also the unzoomed-in words like blurred words around the clear zoomed-in words. We propose a suitable partial zoom-in function that allows you to skim a document.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338581	Huiyi Fang, Kenji Funahashi, Shinji Mizuno, Yuji Iwahori
Passage	A little boy stands up to an enemy with a totem he built out of his tribe's remains.	https://dl.acm.org/authorize?N689245	Igor Coric
Perceptual rasterization for head-mounted display image synthesis	"We suggest a rasterization pipeline tailored towards the needs of HMDs, where latency and field-of-view requirements pose new challenges beyond those of traditional desktop displays. Instead of image warping for low latency, or using multiple passes for foveation, we show how both can be produced directly in a single perceptual rasterization pass. We do this with per-fragment ray-casting. This is enabled by derivations of tight space-time-fovea pixel bounds, introducing just enough flexibility for the requisite geometric tests, but retaining most of the simplicity and efficiency of the traditional rasterizaton pipeline. To produce foveated images, we rasterize to an image with spatially varying pixel density. To compensate for latency, we extend the image formation model to directly produce ""rolling"" images where the time at each pixel depends on its display location. Our approach overcomes limitations of warping with respect to disocclusions, object motion and view-dependent shading, as well as geometric aliasing artifacts in other foveated rendering techniques. A set of perceptual user studies demonstrates the efficacy of our approach."	https://dl.acm.org/authorize?N688042	Sebastian Friston, Tobias Ritschel, Anthony Steed
Photon surfaces for robust, unbiased volumetric density estimation	"We generalize photon planes to : a new family of unbiased volumetric density estimators which we combine using multiple importance sampling. To derive our new estimators, we start with the extended path integral which duplicates the vertex at the end of the camera and photon subpaths and couples them using a blurring kernel. To make our formulation unbiased, however, we use a delta kernel to couple these two end points. Unfortunately, sampling the resulting singular integral using Monte Carlo is impossible since the probability of generating a contributing light path by independently sampling the two subpaths is zero. Our key insight is that we can eliminate the delta kernel and make Monte Carlo estimation practical by integrating any three dimensions analytically, and integrating only the remaining dimensions using Monte Carlo. We demonstrate the practicality of this approach by instantiating a collection of estimators which analytically integrate the distance along the camera ray and two arbitrary sampling dimensions along the photon subpath (e.g., distance, direction, surface area). This generalizes photon planes to curved ""photon surfaces"", including new ""photon cone"", ""photon cylinder"", ""photon sphere"", and multiple new ""photon plane"" estimators. These estimators allow us to handle light paths not supported by photon planes, including single scattering, and surface-to-media transport. More importantly, since our estimators have complementary strengths due to analytically integrating different dimensions of the path integral, we can combine them using multiple importance sampling. This combination mitigates singularities present in individual estimators, substantially reducing variance while remaining fully unbiased. We demonstrate our improved estimators on a number of scenes containing homogeneous media with highly anisotropic phase functions, accelerating both multiple scattering and single scattering compared to prior techniques."	https://dl.acm.org/authorize?N688091	Xi Deng, Shaojie Jiao, Benedikt Bitterli, Wojciech Jarosz
Photon: a modular, research-oriented rendering system	To develop a graphics project with ease and confidence, the reliability and extensibility of the underlying framework are essential. While there are existing options, e.g., pbrt-v3 [Pharr et al. 2016] and Mitsuba [Jakob 2010], they either focus on education or not being updated for a long time. We would like to present an alternative solution named	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338586	Tzu-Chieh Chang, Ming Ouhyoung
Physically-based statistical simulation of rain sound	We propose a new method for reconstructing an implicit surface from an un-oriented point set. While existing methods often involve non-trivial heuristics and require additional constraints, such as normals or labelled points, we introduce a direct definition of the function from the points as the solution to a constrained quadratic optimization problem. The definition has a number of appealing features: it uses a single parameter (parameter-free for exact interpolation), applies to any dimensions, commutes with similarity transformations, and can be easily implemented without discretizing the space. More importantly, the use of a global smoothness energy allows our definition to be much more resilient to sampling imperfections than existing methods, making it particularly suited for sparse and non-uniform inputs.	https://dl.acm.org/authorize?N688179	Zhiyang Huang, Nathan Carr, Tao Ju
Physics-based combustion simulation in bifrost	We present a novel approach to retiming of fluid simulations, which is a common yet challenging practice in visual effects productions. Unlike traditional techniques that are limited to dense simulations and only account for bulk motion by the fluid velocities, our approach also works on sparse simulations and attempts to account for two (vs one) of the fundamental processes governing fluid dynamics, namely the effect of hyperbolic advection and parabolic diffusion, be it physical or numerical. This allows for smoother transitions between the existing and newly generated simulation volumes, thereby preserving the overall look of a retimed fluid animation, while significantly outperforming both forward simulations, which tend to change the look, and guided inverse simulations, which are known to be computationally expensive.	https://dl.acm.org/authorize?N689304	Ken Museth
Physics-based full-body soccer motion control for dribbling and shooting	Playing with a soccer ball is not easy even for a real human because of dynamic foot contacts with the moving ball while chasing and controlling it. The problem of online full-body soccer motion synthesis is challenging and has not been fully solved yet. In this paper, we present a novel motion control system that produces physically-correct full-body soccer motions: dribbling forward, dribbling to the side, and shooting, in response to an online user motion prescription specified by a motion type, a running speed, and a turning angle. This system performs two tightly-coupled tasks: data-driven motion prediction and physics-based motion synthesis. Given example motion data, the former synthesizes a reference motion in accordance with an online user input and further refines the motion to make the character kick the ball at a right time and place. Provided with the reference motion, the latter then adopts a Model Predictive Control (MPC) framework to generate a physically-correct soccer motion, by solving an optimal control problem that is formulated based on dynamics for a full-body character and the moving ball together with their interactions. Our demonstration shows the effectiveness of the proposed system that synthesizes convincing full-body soccer motions in various scenarios such as adjusting the desired running speed of the character, changing the velocity and the mass of the ball, and maintaining balance against external forces.	https://dl.acm.org/authorize?N688029	Seokpyo Hong, Daseong Han, Kyungmin Cho, Joseph S. Shin, Junyong Noh
PickHits: hitting experience generation with throwing motion via a handheld mechanical device	Experiences of hitting targets cause a great feeling. We propose a system for generating this experience computationally. This system consists of external tracking cameras and a handheld device for holding and releasing a thrown object. As a proof-of-concept system, we developed the system based on two key elements: low-latency release device and constant model-based prediction. During the user's throwing motion, the ballistic trajectory of the thrown object is predicted in real time, and when the trajectory coincides with the desired one, the object is released. We found that we can generate a computational hitting experience within a limited range space.	https://dl.acm.org/authorize?N688368	Azumi Maekawa, Seito Matsubara, Atsushi Hiyama, Masahiko Inami
Pieces of the past, maya treasure hunt: a virtual reality game experience	We present an educational virtual reality (VR) puzzle game set in an archaeological context. We digitally documented the site architecture and a selection of excavated artefact using structure from motion (SfM) mapping, reconstructed the site during Classic Period (AD 250-900) based on the current state and archaeological findings, and created the natural environment using procedural modeling. With this collection of resources, we created a holistic landscape of the Mayan site of Cahal Pech. The player can link the Mayan ruin between its current state and the past through collecting artefact and evidence, and discover the architectural beauty and historical richness of this site.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338604	Jiawei Huang
PinocchioVR	In the Pinocchio fairy tale, the nose of a boy extends when he lies. Inspired by this tale, we created a Pinocchio VR system that presents the feeling of the nose extending through body ownership illusion. This illusion is created by pulling the nose while presenting the visuals of the nose growing in the head mounted display. Our research that explored different combinations of haptics and visuals indicated that the the minimum requirement for this body ownership illusion is the visuals and the 'nose-pulling' haptic sensation. Thus, the PinocchioVR system consists of a head mounted display and an integrated haptic nose-pulling mechanism. Furthermore, in this demonstration, we explore olfactory and vibrotactile sensations as multimodal effects to present new experiences such as interacting with far away objects with your nose, extending the nose to smell foods at a distance, and even, experience what it would feel like to hang clothes on your elongated nose.	https://dl.acm.org/authorize?N688369	Makoto Uju, Kenichiro Shirota, Roshan Peiris, Kouta Minamizawa
Pixel ripped 1989	A game within a game, original, fun, and packed full of surprises that will keep you guessing along the way. Pixel Ripped truly is a nostalgic, mad trip down memory lane. Search for retro-Easter eggs as you fight the mundane distractions of life and get down to what's most important - gaming!	https://dl.acm.org/authorize?N689130	Ana Ribeiro, Fernanda Martins
PlanIT: planning and instantiating indoor scenes with relation graph and spatial prior networks	Layout is fundamental to graphic designs. For visual attractiveness and efficient communication of messages and ideas, graphic design layouts often have great variation, driven by the contents to be presented. In this paper, we study the problem of content-aware graphic design layout generation. We propose a deep generative model for graphic design layouts that is able to synthesize layout designs based on the visual and textual semantics of user inputs. Unlike previous approaches that are oblivious to the input contents and rely on heuristic criteria, our model captures the effect of visual and textual contents on layouts, and implicitly learns complex layout structure variations from data without the use of any heuristic rules. To train our model, we build a large-scale magazine layout dataset with fine-grained layout annotations and keyword labeling. Experimental results show that our model can synthesize high-quality layouts based on the visual semantics of input images and keyword-based summary of input text. We also demonstrate that our model internally learns powerful features that capture the subtle interaction between contents and layouts, which are useful for layout-aware design retrieval.	https://dl.acm.org/authorize?N688188	Xinru Zheng, Xiaotian Qiao, Ying Cao, Rynson W. H. Lau
Porting your VR title to oculus quest	Survios, a virtual reality (VR) game developer dedicated to building active, immersive experiences that push the limits of VR innovation, ported their VR boxing title, Creed: Rise to Glory, to the newly anticipated Oculus Quest. Porting to this new mobile VR platform is complex and demands extra creativity from developers when compared to porting to the previous generation of consoles. In this paper, Eugene Elkin, Senior Software Engineer at Survios, will share insights and learnings from the process, covering target hardware and its capabilities, rendering techniques, game optimization, and more.	https://dl.acm.org/authorize?N689386	Eugene Elkin
Practical dynamic lighting for large-scale game environments	Dynamic lighting techniques are vital in giving artists rapid feedback and reducing iteration time. In this technical postmortem, we will talk about dynamic lighting techniques for large-scale game environments that reflect changes in the time of day and include many dynamic lights. Moreover, a unified atmospheric scattering technique with clouds will also be discussed.	https://dl.acm.org/authorize?N689306	Kyungjoon Cho, Kwanghyeon Go, Daeil Kim
Practical lighting on 	Pixar films have recently seen drastically rising light counts via procedural generation, resulting in longer render times and slower interactive workflows. Here we present a fully automated, scalable, and error-free light pruning pipeline deployed on Toy Story 4 that reduces final render times by 15--50% in challenging cases, accelerates interactive lighting, and automates a previously manual and error-prone task.	https://dl.acm.org/authorize?N689317	Vaibhav Vavilala
Practical measurement and modeling of spectral skin reflectance	Accurate modeling and rendering of human skin appearance has been a long standing goal in computer graphics. Of particular importance has been the realistic modeling and rendering of layered subsurface scattering in skin for which various bio-physical models have been proposed based on the spectral distribution of chromophores in the epidermal and dermal layers of skin [Donner and Jensen 2006; Donner et al. 2008; Jimenez et al. 2010]. However, measurement of the spectral parameters of absorption and scattering of light for such bio-phyisical models has been a challenge in computer graphics. Previous works have either borrowed parameters for skin-type from tissue-optics literature [Donner and Jensen 2006], or employed extensive multispectral imaging for inverse rendering detailed spatially varying parameters for a patch of skin [Donner et al. 2008]. Closest to our approach, Jimenez et al. [2010] employed observations under uniform broadband illumination to estimate two dominant parameters (melanin and hemoglobin concentrations) for driving a qualitative appearance model for facial animation.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338607	Yuliya Gitlina, Daljit Singh Dhillon, Giuseppe Claudio Guarnera, Abhijeet Ghosh
Predictive and proactive pipelines: approaches to monitoring and optimizing CG film production	The spatial computing affordances of virtual and augmented reality introduce new ethical and privacy dilemmas. This panel will explore the many implications of biometric data (eye tracking, facial tracking, gait detection, emotional sentiment analysis, galvanic skin response, EEG, EMG, and ECG) to contextually-aware computing that can scan and identify your immediate surroundings. There are many unknown ethical thresholds with immersive computing, and this panel will discuss our own moral intuitions on the topic while inviting the audience to share their own questions and insights for how to navigate this landscape.	https://dl.acm.org/authorize?N688120	Sean Palmer, Kaori Ogino, Aidan Sarsfield, Munira Tayabji, Mark Hills, Renee Tam, Pavani Rao Boddapati, Claudia Chung Sanii
Preparing students to take the next step: school to work transition (demo reels and beyond)	Panelists from various industry sub-segments involved with computer graphics and interactive techniques discuss preparation students must have to get initial access and employment in industry. Individual representatives talk both generally, and specifically (as examples) about their own companies. What entry-level applicants should have (and not have) on resumes, portfolios, and demo reels will be discussed and shown. Industry segments represented include game development/design, animation, special/visual effects, and production for motion pictures. Discussion includes preparation, training, and attributes students need to enter the workforce. Examining both short- and long-term needs, the panelists take a step back and look at trends and changes that have taken place and may take place. Questions considered include how schools can help students transition to industry, and what students can do on their own to be pro-active in obtaining requisite credentials. Getting noticed is a subject for discussion as well as what students should do/include and not do/include in demo reels and resumes as a way to help educators help students.	https://dl.acm.org/authorize?N688268	Glenn Goldman, Jimmy Ockey, Sylvie Tehbelian, Elizabeth Zavitsanos, Francisco Javier Romero Rodriguez
Preserving virtual reality artworks: a museum perspective	As artists increasingly engage with virtual reality (VR) technologies, the artworks they produce are beginning to enter the collections of cultural heritage institutions. Museums, libraries and archives are therefore assessing how these complex works might be brought into collections and how they might be stabilised to ensure they can be exhibited in the long-term. Reporting on ongoing research at Tate in London, in this talk we will introduce our perspective as conservators of time-based media (broadly understood as art with a technological component that unfolds over time) on the challenges we face in preserving virtual reality artworks. We expect this to be of interest to SIGGRAPH attendees who are considering the legacy of their creations and the ways in which virtual reality artworks (and related technologies) might be stabilised in order to secure their future.	https://dl.acm.org/authorize?N689380	Jack McConchie, Tom Ensom
Procedural approach to animation driven effects for Avengers: Endgame	LAIKA the animation studio is known for stop-motion and a unified fusion of art and stunning visual effects technology. This presentation will cover the techniques used to generate water effects and a collapsing bridge of ice for LAIKA's Missing Link. Building on techniques for incorporating stylized water effects with stop motion animation developed for LAIKA's Kubo and the Two Strings [Montgomery 2016], the FX team used SideFX Houdini's [SideFX 2019b] guided ocean and narrow band FLIP tools to guide the action and manage the larger scale and more numerous shots required for Missing Link. The collapsing ice bridge sequence presented an unusual challenge integrating stop motion animated characters with an animated CG set piece. To accomplish this difficult task, the normal course of production was reversed, and the CG animation was used to guide the character animation.	https://dl.acm.org/authorize?N689348	David Horsley, Peter Stuart
Procedural organic modeling	David Bachman is a professor of Mathematics at Pitzer College in Claremont, CA, where also teaches computer science, and has co-taught classes on design with faculty in the art department. He received a PhD in 1999 from the University of Texas at Austin, and has since published over 20 research articles, three books, and received two grants from the National Science Foundation. For the last nine years David has been combining techniques from mathematics, biology, and computer science to produce 3D printed artwork, which has been shown in galleries across the country.	https://dl.acm.org/authorize?N688269	David Bachman
Procedural phasor noise	Procedural pattern synthesis is a fundamental tool of Computer Graphics, ubiquitous in games and special effects. By calling a single procedure in every pixel - or voxel - large quantities of details are generated at low cost, enhancing textures, producing complex structures within and along surfaces. Such procedures are typically implemented as pixel shaders. We propose a novel procedural pattern synthesis technique that exhibits desirable properties for modeling highly contrasted patterns, that are especially well suited to produce surface and microstructure details. In particular, our synthesizer affords for a precise control over the profile, orientation and distribution of the produced stochastic patterns, while allowing to grade all these parameters spatially. Our technique defines a stochastic smooth phase field - - that is then fed into a periodic function (e.g. a sine wave), producing an oscillating field with prescribed main frequencies and preserved contrast oscillations. In addition, the profile of each oscillation is directly controllable (e.g. sine wave, sawtooth, rectangular or any 1D profile). Our technique builds upon a reformulation of Gabor noise in terms of a that affords for a clear separation between local intensity and phase. Applications range from texturing to modeling surface displacements, as well as multi-material microstructures in the context of additive manufacturing.	https://dl.acm.org/authorize?N688002	Thibault Tricard, Semyon Efremov, Cédric Zanni, Fabrice Neyret, Jonàs Martínez, Sylvain Lefebvre
Procedural system assisted authoring of open-world content for Marvel's Spider-Man	"Crimes and vignettes are placed throughout the game city space using procedural systems to find appropriate locations. Editing of roads or buildings necessitates re-authoring the placement of dynamic encounters in that area of the environment. In many cases the dynamic encounters may have already undergone ""final"" manual adjustment to the space. To accommodate iterations on city layout, we add three main improvements to our procedural systems: preserve hand-authored work if it continues to meet specifications; place encounter components with higher fidelity; and provide the artists and designers guidance for crimes and vignettes needing more attention."	https://dl.acm.org/authorize?N689392	David Santiago
Progressive embedding	Tutte embedding is one of the most common building blocks in geometry processing algorithms due to its simplicity and provable guarantees. Although provably correct in infinite precision arithmetic, it fails in challenging cases when implemented using floating point arithmetic, largely due to the induced exponential area changes. We propose , with similar theoretical guarantees to Tutte embedding, but more resilient to the rounding error of floating point arithmetic. Inspired by progressive meshes, we collapse edges on an invalid embedding to a valid, simplified mesh, then insert points back while maintaining validity. We demonstrate the robustness of our method by computing embeddings for a large collection of disk topology meshes. By combining our robust embedding with a variant of the matchmaker algorithm, we propose a general algorithm for the problem of mapping multiply connected domains with arbitrary hard constraints to the plane, with applications in texture mapping and remeshing.	https://dl.acm.org/authorize?N688087	Hanxiao Shen, Zhongshi Jiang, Denis Zorin, Daniele Panozzo
Project jua	Project Jua is an interactive VR experience that raises awareness of the impact of not having easy access to electricity, taking the player to a rural farm in Kenya to experience firsthand the challenges of living off the grid to discover an African solar power solution that is improving lives.	https://dl.acm.org/authorize?N689131	Ioulia Isserlis, Brian First, Max Sacker, Ayuba Audu, Raymond Ononiwu, Dunni Abiodun
Project nira: instant interactive real-time access to multi-gigabyte sized 3D assets on any device	Here is some text in the style of last year's RTL lineup descriptions: We demonstrate a collaboration platform for viewing massive 3D assets in real-time using any web browser on any device. Technologies such as high framerate server-side rendering, low-latency video encoding, and artist friendly markup tools are combined to accelerate production pipelines. Additional text below: Nira is a powerful new collaborative design and art review platform for 3D production pipelines. Nira removes the burden of having to send proprietary files back and forth for reviews by providing a unified platform for viewing, reviewing, tracking, and version comparing of all assets. Upon upload of a production asset, the Nira asset ingestion pipeline converts the asset to a format optimized for very fast loading and display performance within its custom renderer. When the asset is subsequently viewed, Nira renders and then encodes the frame buffer to an efficient and high quality video stream, which is then decoded within a web browser on the viewer's device at 60 fps in real-time. The client device need only be capable of decoding a video stream, so this allows large production assets to be interactively viewed on lower powered mobile devices like never before. In addition to viewing capabilities, Nira also includes a host of collaborative drawing and markup tools allowing for effective visual communication between team members and stakeholders.	https://dl.acm.org/authorize?N689129	Arash Keissami, Andrew Johnson, Dario Manesku
PuppetMaster: robotic animation of marionettes	We present a computational framework for robotic animation of real-world string puppets. Also known as marionettes, these articulated figures are typically brought to life by human puppeteers. The puppeteer manipulates rigid handles that are attached to the puppet from above via strings. The motions of the marionette are therefore governed largely by gravity, the pull forces exerted by the strings, and the internal forces arising from mechanical articulation constraints. This seemingly simple setup conceals a very challenging and nuanced control problem, as marionettes are, in fact, complex coupled pendulum systems. Despite this, in the hands of a master puppeteer, marionette animation can be nothing short of mesmerizing. Our goal is to enable autonomous robots to animate marionettes with a level of skill that approaches that of human puppeteers. To this end, we devise a predictive control model that accounts for the dynamics of the marionette and kinematics of the robot puppeteer. The input to our system consists of a string puppet design and a target motion, and our trajectory planning algorithm computes robot control actions that lead to the marionette moving as desired. We validate our methodology through a series of experiments conducted on an array of marionette designs and target motions. These experiments are performed both in simulation and using a physical robot, the human-sized, dual arm ABB YuMi IRB 14000.	https://dl.acm.org/authorize?N688158	Simon Zimmermann, Roi Poranne, James M. Bern, Stelian Coros
Puppeteered rain: interactive illusion of levitating water drops by position-dependent strobe projection	Light projection onto falling water produces distinct and impressive experience which is suitable for entertainment and advertising installations in public spaces [Barnum et al. 2010; Eitoku et al. 2006]. One of popular and classical techniques used in illuminating water for such purposes is strobe lighting, which presents optical illusion of levitating --- or slowly falling or rising --- water drops depending on the relation between water dropping and strobe lighting frequencies (e.g. [Pevnick 1981; Rosenthal 1984]).	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338603	Shingo Kagami, Kotone Higuchi, Koichi Hashimoto
Purl	"""Purl,"" directed by Kristen Lester and produced by Gillian Libbert-Duncan, features an earnest ball of yarn named Purl who gets a job in a fast-paced, high energy, bro-tastic startup. Yarn hijinks ensue as she tries to fit in, but how far is she willing to go to get the acceptance she yearns for, and in the end, is it worth it?"	https://dl.acm.org/authorize?N689248	Kristen Lester
Quixel's rebirth: megascans environment breakdown	One of the aims of the project was to prove that the kinds of results we achieved with Rebirth are accessible, not only for studios with huge budgets and large teams but also for freelancers and independents. If an individual wanted to download and use the assets that we did in Rebirth it would cost the equivalent of around $75. While Rebirth may look complex, in reality, we used a small set of assets throughout the entire cinematic. As each Megascans asset is quite visually complex, this allowed us to repurpose the same assets across many different shots whilst creating a varied look.	https://dl.acm.org/authorize?N689120	Galen Davis
Rapid 3D building modeling by sketching	This paper proposes an intuitive tool for users to create 3D architectural models through 2D sketch input. A user only needs to draw the outline of a frontal or oblique view of a building. Our system recognizes the parts drawn in a sketch and estimates their types. The estimated information is then used to compose the corresponding 3D model. Besides, our system provides additional assistant tools for rapid editing. The modeling process can be iterative and incremental. To accomplish a building complex, a user can gradually create their models from one view to another. Our experiment shows that the proposed interface with sketch analysis tools eases the process of 3D building modeling.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338545	Chia-Yu Chen, I-Chen Lin
Real-time pose and shape reconstruction of two interacting hands with a single depth camera	We present a novel method for real-time pose and shape reconstruction of two strongly interacting hands. Our approach is the first two-hand tracking solution that combines an extensive list of favorable properties, namely it is marker-less, uses a single consumer-level depth camera, runs in real time, handles inter- and intra-hand collisions, and automatically adjusts to the user's hand shape. In order to achieve this, we embed a recent parametric hand pose and shape model and a dense correspondence predictor based on a deep neural network into a suitable energy minimization framework. For training the correspondence prediction network, we synthesize a two-hand dataset based on physical simulations that includes both hand pose and shape annotations while at the same time avoiding inter-hand penetrations. To achieve real-time rates, we phrase the model fitting in terms of a nonlinear least-squares problem so that the energy can be optimized based on a highly efficient GPU-based Gauss-Newton optimizer. We show state-of-the-art results in scenes that exceed the complexity level demonstrated by previous work, including tight two-hand grasps, significant inter-hand occlusions, and gesture interaction.	https://dl.acm.org/authorize?N688094	Franziska Mueller, Micah Davis, Florian Bernard, Oleksandr Sotnychenko, Mickeal Verschoor, Miguel A. Otaduy, Dan Casas, Christian Theobalt
"Real-time procedural VFX characters in unity's real-time short film ""The Heretic"""	"""The Heretic"", the latest real-time short film by Unity's Demo Team features two entirely vfx-based characters, Boston and Morgan. VFX-based characters are notoriously hard to conceptualize, especially in motion. Building and using real-time tools allowed us to explore the space of possibilities and quickly arrive at designs we were satisfied with. The character of Boston is made up of steel wires navigating the environment, conforming to the shape of a bird-like creature. Animating thousands of wires in a traditional way was unthinkable. The challenge then lied in building tools allowing to express the intention of the artists, allow for emergent behavior, but also make it directable at the same time. Boston is implemented as a set of tools, scripts, and shaders within the Unity engine. The character Morgan doesn't have a clearly- defined physical manifestation, morphs between its female and male forms, and constantly varies its size. Morgan is implemented using the Unity Visual Effect Graph, extended with additional tools and features. The presentation will be of value to all creative communities that base their process on real-time technology - both within game development and real-time filmmaking. From a film production perspective, such characters are typically entirely within the domain of expertise of post-production / VFX studios. With the introduction of realtime-based VFX characters, these characters can be included much earlier in the production process and are more open for experimentation and better connected to the rest of the production elements. From a game development perspective, real- time VFX-based characters allow for bolder and more unconventional creative ideas to be conceived and executed and thereby contribute to richer and more elaborate virtual worlds. They allow the creatives to express more complex, abstract or surreal ideas and develop interesting and original aesthetics."	https://dl.acm.org/authorize?N689121	Veselin Efremov, Adrian Lazar
Real-time structure aware color stippling	In computer graphics, stippling is a widely used non-photorealistic rendering technique. As the art of representing images with dots, one of the key problems is the placement of dots. In general, they should be distributed evenly, and with some randomness at the same time. Blue noise methods provide these characteristics and are used by state-of-the-art gray-scale algorithms to distribute dots. Color stippling, however, is more challenging as each channel should have even distribution at the same time. Existing approaches cast color stippling as a multi-class blue noise sampling problem and provide high quality results at the cost of a very long processing time. In this paper, we propose a real-time structure aware method for color stippling, based on samples generated from an incremental voronoi set. Our method can handle an arbitrary input color vector for stippling and produce significantly better results than previous methods, at real-time frame rate. We evaluate the perceptual quality of our stippling with a user study and its numerical performance by measuring the MSE between the reconstructed image from the stippling and the input image. As a result, the real time performance of our method makes interactive stippling editing possible, providing the artist with an effective tool to explore quickly a wide space of color image stippling.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338606	Lei Ma, Hong Deng, Beibei Wang, Yanyun Chen, Tamy Boubekeur
Real-time, single camera, digital human development	We have built a real-time (60 fps) photo-realistic facial motion capture system which uses a single camera, proprietary deep learning software, and Unreal Engine 4 to create photo-real digital humans and creatures. Our system uses thousands of frames of realistic captured 3D facial performance of an actor (generated from automated offline systems) instead of a traditional FACS-based facial rig to produce an accurate model of how an actor's face moves. This 3D data is used to create a real-time machine learning model which uses a single image to accurately describe the exact facial pose in under 17 milliseconds. The motion of the face is highly realistic and includes region based blood flow, wrinkle activation, and pore structure changes, driven by geometry deformations in real-time. The facial performance of the actor can be transferred to a character with extremely high fidelity, and switching the machine learning models is instantaneous. We consider this a significant advancement over other real-time avatar projects in development. Building on top of our real-time facial animation technology, we seek to make interaction with our avatars more immersive and emotive. We built an AR system for the actor who is driving the human / character to see and interact with people in VR or others viewing in AR. With this technique, the character you are interacting with in VR can make correct eye contact, walk around you, and interact as if you were together all while still achieving the highest quality capture. This process allows for a much more tangible VR / AR experience than any other system. Another goal of ours is to achieve photo-real avatar telepresence with minimal latency. We have been able to successfully live-drive our digital humans from our office in Los Angeles to our office in Vancouver.	https://dl.acm.org/authorize?N689122	Doug Roble, Darren Hendler, Jeremy Buttell, Melissa Cell, Jason Briggs, Chad Reddick, Lonnie Iannazzo, Deer Li, Mark Williams, Lucio Moser, Cydney Wong, Dimitry Kachkovski, Jason Huang, Kai Zhang, David McLean, Rickey Cloudsdale, Dan Milling, Ron Miller, JT Lawrence, Chinyu Chien
Reconsideration of ouija board motion in terms of haptic illusions (IV): effect of haptic cue and another player	The Ouija board game is associated with a type of involuntary motion known as an ideomotor action. We sought to clarify the conditions under which this motion occurs by evaluating the effect that visual and haptic movement cues have on its occurrence. Using our lateral skin deformation device, we found that the simultaneous presentation of visual and tactile illusory motion and force produced larger ideomotor actions than when either modality presented alone, an effect that was further potentiated by the presence of another player (an avatar).	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338567	Takahiro Shitara, Vibol Yem, Hiroyuki Kajimoto
Recreating BoPeep for Toy Story 4	In , audiences rediscover BoPeep, who returns after nearly 20 years away from the big screen. In adapting her design, we considered not only the cultural context of reviving one of our industry's first female characters, informing our story and design, but also the technology now available, which drove explorations in shading and simulation, among other areas. Our talk describes BoPeep's journey through production: from initial research into decades-old reference and visualization, to the modern results and strides we've taken across both creative and technical specialties.	https://dl.acm.org/authorize?N689379	Mariana Galindo, Carrie Hobson, Radford Hurn, Patty Kihm, Tanja Krampfert, Mara MacMahon, George Nguyen, Becki Tower
RedMax: efficient & flexible approach for articulated dynamics	It is well known that the dynamics of articulated rigid bodies can be solved in time using a recursive method, where is the number of joints. However, when elasticity is added between the bodies , damped springs), with linearly implicit integration, the stiffness matrix in the equations of motion breaks the tree topology of the system, making the recursive method inapplicable. In such cases, the only alternative has been to form and solve the system matrix, which takes ( ) time. We propose a new approach that is capable of solving the linearly implicit equations of motion in near linear time. Our method, which we call RedMax, is built using a combined reduced/maximal coordinate formulation. This hybrid model enables direct flexibility to apply arbitrary combinations of constraints and contact modeling in both reduced and maximal coordinates, as well as mixtures of implicit and explicit forces in either coordinate representation. We highlight RedMax's flexibility with seamless integration of deformable objects with two-way coupling, at a standard additional cost. We further highlight its flexibility by constructing an efficient internal (joint) and external (environment) frictional contact solver that can leverage bilateral joint constraints for rapid evaluation of frictional articulated dynamics.	https://dl.acm.org/authorize?N688159	Ying Wang, Nicholas J. Weidner, Margaret A. Baxter, Yura Hwang, Danny M. Kaufman, Shinjiro Sueda
Reincarnation: virtual reality recreation of Yves Tanguy's world	Reincarnation is a virtual reality art experience, built upon a series of French surrealist painter Yves Tanguy's paintings in combination with my creation of pseudo-natural beings. It seeks and argues the animism in various matters and challenges the anthropocentric worldview in an artificial intelligence era.	https://dl.acm.org/authorize?N689148	Mengyu Chen, Jing Yan
Remote control experiment with displaybowl and 360-degree video	DisplayBowl is a bowl-shaped hemispherical display for showing omnidirectional images with direction data. It provides users with a novel way of observing 360-degree video streams, which improves the awareness of the surroundings when operating a remote-controlled vehicle compared to conventional flat displays and HMDs. In this paper, we present a user study, in which we asked participants to control a remote drone using an omnidirectional video streaming, to compare the uniqueness and advantages of three displays: a flat panel display, a head-mounted display and DisplayBowl.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338568	Shio Miyafuji, Soichiro Toyohara, Toshiki Sato, Hideki Koike
Remote spatial programming and collaboration using a real-time volumetric capture space	Collaboratively program a machine through a spatial drag-and-drop interface while being captured real-time in volumetric-video. We explore the future of industrial programming by using AR to build a drag-and-drop spatial interface. One person programs a machine using their phone while another can collaborate remotely through a volumetric-video	https://dl.acm.org/authorize?N689252	Hisham Bedri, Christian Vazquez, Valentin Heun, Anna Fusté, Benjamin Reynolds
Retiming of fluid simulations for VFX: distributed non-linear fluid retiming by sparse bi-directional advection-diffusion	We present our FX retiming workflow developed on Blue Sky Studios' latest feature, Spies in Disguise. Retiming refers to the slowing down and speeding up of FX assets in a shot. These include point particles, rigid bodies, volumetric elements like smoke and fire, and fluids. Our solution is shown to be robust and efficient, even for the challenging cases of retiming heavily dynamic events, such as fire and explosions.	https://dl.acm.org/authorize?N689315	Ravindra Dwivedi, Jon Barry, Sean C McDuffee
Rodent: generating renderers without writing a generator	Monte-Carlo Renderers must generate many color samples to produce a noise-free image, and for each of those, they must evaluate complex mathematical models representing the appearance of the objects in the scene. These models are usually in the form of shaders: Small programs that are executed during rendering in order to compute a value for the current sample. Renderers often compile and optimize shaders just before rendering, taking advantage of the knowledge of the scene. In principle, the entire renderer could benefit from a-priori code generation. For instance, scheduling can take advantage of the knowledge of the scene in order to maximize hardware usage. However, writing such a configurable renderer eventually means writing a compiler that translates a scene description into machine code. In this paper, we present a framework that allows generating entire renderers for CPUs and GPUs without having to write a dedicated compiler: First, we provide a rendering library in a functional/imperative language that elegantly abstracts the individual rendering concepts using higher-order functions. Second, we use to combine and specialize the individual components of a renderer according to a particular scene. Our results show that the renderers we generate outperform equivalent high-performance implementations written with state-of-the-art ray tracing libraries on the CPU and GPU.	https://dl.acm.org/authorize?N688095	Arsène Pérard-Gayot, Richard Membarth, Roland Leißa, Sebastian Hack, Philipp Slusallek
SAGNet: structure-aware generative network for 3D-shape modeling	We present SAGNet, a structure-aware generative model for 3D shapes. Given a set of segmented objects of a certain class, the geometry of their parts and the pairwise relationships between them (the structure) are jointly learned and embedded in a latent space by an autoencoder. The encoder intertwines the geometry and structure features into a single latent code, while the decoder disentangles the features and reconstructs the geometry and structure of the 3D model. Our autoencoder consists of two branches, one for the structure and one for the geometry. The key idea is that during the analysis, the two branches exchange information between them, thereby learning the dependencies between structure and geometry and encoding two augmented features, which are then fused into a single latent code. This explicit intertwining of information enables separately controlling the geometry and the structure of the generated models. We evaluate the performance of our method and conduct an ablation study. We explicitly show that encoding of shapes accounts for both similarities in structure and geometry. A variety of quality results generated by SAGNet are presented.	https://dl.acm.org/authorize?N688046	Zhijie Wu, Xiang Wang, Di Lin, Dani Lischinski, Daniel Cohen-Or, Hui Huang
SPOT: sliced partial optimal transport	"Optimal transport research has surged in the last decade with wide applications in computer graphics. In most cases, however, it has focused on the special case of the so-called ""balanced"" optimal transport problem, that is, the problem of optimally matching positive measures of equal total mass. While this approach is suitable for handling probability distributions as their total mass is always equal to one, it precludes other applications manipulating disparate measures. Our paper proposes a fast approach to the optimal transport of constant distributions supported on point sets of different cardinality via one-dimensional slices. This leads to one-dimensional partial assignment problems akin to alignment problems encountered in genomics or text comparison. Contrary to one-dimensional balanced optimal transport that leads to a trivial linear-time algorithm, such partial optimal transport, even in 1-d, has not seen any closed-form solution nor very efficient algorithms to date. We provide the first efficient 1-d partial optimal transport solver. Along with a quasilinear time problem decomposition algorithm, it solves 1-d assignment problems consisting of up to millions of Dirac distributions within fractions of a second in parallel. We handle higher dimensional problems via a slicing approach, and further extend the popular iterative closest point algorithm using optimal transport - an algorithm we call We illustrate our method on computer graphics applications such a color transfer and point cloud registration."	https://dl.acm.org/authorize?N688034	Nicolas Bonneel, David Coeurjolly
Sample-based Monte Carlo denoising using a kernel-splatting network	It has been shown that rendering in the gradient domain, i.e., estimating finite difference gradients of image intensity using correlated samples, and combining them with direct estimates of pixel intensities by solving a screened Poisson problem, often offers fundamental benefits over merely sampling pixel intensities. The reasons can be traced to the frequency content of the light transport integrand and its interplay with the gradient operator. However, while they often yield state of the art performance among algorithms that are based on Monte Carlo sampling alone, gradient-domain rendering algorithms have, until now, not generally been competitive with techniques that combine Monte Carlo sampling with post-hoc noise removal using sophisticated non-linear filtering. Drawing on the power of modern convolutional neural networks, we propose a novel reconstruction method for gradient-domain rendering. Our technique replaces the screened Poisson solver of previous gradient-domain techniques with a novel dense variant of the U-Net autoencoder, additionally taking auxiliary feature buffers as inputs. We optimize our network to minimize a perceptual image distance metric calibrated to the human visual system. Our results significantly improve the quality obtained from gradient-domain path tracing, allowing it to overtake state-of-the-art comparison techniques that denoise traditional Monte Carlo samplings. In particular, we observe that the correlated gradient samples --- that offer information about the smoothness of the integrand unavailable in standard Monte Carlo sampling --- notably improve image quality compared to an equally powerful neural model that does not make use of gradient samples.	https://dl.acm.org/authorize?N688171	Markus Kettunen, Erik Härkönen, Jaakko Lehtinen
Scalable muscle-actuated human simulation and control	Many anatomical factors, such as bone geometry and muscle condition, interact to affect human movements. This work aims to build a comprehensive musculoskeletal model and its control system that reproduces realistic human movements driven by muscle contraction dynamics. The variations in the anatomic model generate a spectrum of human movements ranging from typical to highly stylistic movements. To do so, we discuss scalable and reliable simulation of anatomical features, robust control of under-actuated dynamical systems based on deep reinforcement learning, and modeling of pose-dependent joint limits. The key technical contribution is a scalable, two-level imitation learning algorithm that can deal with a comprehensive full-body musculoskeletal model with 346 muscles. We demonstrate the predictive simulation of dynamic motor skills under anatomical conditions including bone deformity, muscle weakness, contracture, and the use of a prosthesis. We also simulate various pathological gaits and predictively visualize how orthopedic surgeries improve post-operative gaits.	https://dl.acm.org/authorize?N688028	Seunghwan Lee, Moonseok Park, Kyoungmin Lee, Jehee Lee
Scented graphics: exploration in inkjet scented-printing	"This paper describes ""Scented Graphics,"" our artistic experiments with a scented-printing technique that utilizes an inkjet printer. By mixing water-soluble aromatic oils with inks, an inkjet printer can be utilized to control scent mixing with high precision at almost no extra cost. Such features are hardly provided by the existing scented-printing services at the same level of flexibility and cost-effectiveness. We mimicked the Swiss graphic design in our experiment as the style to facilitate the preliminary investigation of this technique. As scent mixing is a fundamental technique in olfactory art, our experiments can be beneficial as design exemplars as a preliminary exploration of the technique."	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338610	Yi-Ching Kang, Hiroki Nishino
Sculpting color spaces	Color correction with a long chain of keyers and math operators, even in the hands of experienced artists, often induces artifacts such as muddy colors or malformed edges. Inspired by tools which display a 3D color histogram [COL 2007], and Flame's Color Wrapper [WRA 2019], we embarked on building a user-friendly 3D color space sculpting toolset which allows a user to make complex and elegant color alterations quickly, intuitively and effectively. In this paper, we will show how smooth transitions can be achieved by our tool through a combination of soft-selection, LUT auto-filling, and tetrahedral interpolation. Multiple approaches for interactive selection and highlighting were introduced to overcome the inaccurate and esoteric manipulation when applying similar 3D based tools to point clouds of colors. Our tool has been robustly integrated with the color correction pipeline through the ability to import and export industry standard 3D LUT files. We have found success for our tool in a number of color-manipulation-related tasks, such as: denoising, despilling, and standard grading.	https://dl.acm.org/authorize?N689396	Yanli Zhao, Darryl Gouder, Rob Pieké
Semantic photo manipulation with a generative image prior	Despite the recent success of GANs in synthesizing images conditioned on inputs such as a user sketch, text, or semantic labels, manipulating the high-level attributes of an existing natural photograph with GANs is challenging for two reasons. First, it is hard for GANs to precisely reproduce an input image. Second, after manipulation, the newly synthesized pixels often do not fit the original image. In this paper, we address these issues by adapting the image prior learned by GANs to image statistics of an individual image. Our method can accurately reconstruct the input image and synthesize new content, consistent with the appearance of the input image. We demonstrate our interactive system on several semantic image editing tasks, including synthesizing new objects consistent with background, removing unwanted objects, and changing the appearance of an object. Quantitative and qualitative comparisons against several existing methods demonstrate the effectiveness of our method.	https://dl.acm.org/authorize?N688004	David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba
Shading atlas streaming demonstration	Streaming high quality rendering for virtual reality applications requires minimizing perceived latency. Shading Atlas Streaming (SAS) [Mueller et al. 2018] is a novel object-space rendering framework suitable for streaming virtual reality content. SAS decouples server-side shading from client-side rendering, allowing the client to perform framerate upsampling and latency compensation autonomously for short periods of time. The shading information created by the server in object space is temporally coherent and can be efficiently compressed using standard MPEG encoding. SAS compares favorably to previous methods for remote image-based rendering in terms of image quality and network bandwidth efficiency. SAS allows highly efficient parallel allocation in a virtualized-texture-like memory hierarchy, solving a common efficiency problem of object-space shading. With SAS, untethered virtual reality headsets can benefit from high quality rendering without paying in increased latency. Visitors will be able to try SAS by roaming the exhibit area wearing a Snapdragon 845 based headset connected via consumer WiFi.	https://dl.acm.org/authorize?N688360	Joerg H. Mueller, Thomas Neff, Philip Voglreiter, Mina Makar, Markus Steinberger, Dieter Schmalstieg
ShapeSense: a 2D shape rendering VR device with moving surfaces that controls mass properties and air resistance	"We introduce ""ShapeSense,"" a VR device with moving surfaces for rendering various shape perceptions with a single device. Shape-Sense can simultaneously control the mass properties and air resistance in order to represent the target shape seen in VR. The results of user studies showed that our proposed device can reproduce various shape perceptions and is superior to the conventional device, which only considers mass properties for shape rendering."	https://dl.acm.org/authorize?N688361	Yuhu Liu, Takeru Hashimoto, Shigeo Yoshida, Takuji Narumi, Tomohiro Tanikawa, Michitaka Hirose
Share Your Gifts	With researchers finding that 7 out of 10 people aren't acting on their creative potential, one thing is clear: Creativity is being threatened. Apple believes that creativity moves the world forward, which is why this holiday season the brand created an inspiring message aimed at creators everywhere.	https://dl.acm.org/authorize?N689246	Buck
ShareHaptics: a modular haptic feedback system using shape memory alloy for mixed reality shared space applications	We present ShareHaptics, a novel modular system to provide tactile and pressure feedback in mixed reality applications using a novel actuator: shape memory alloy (SMA). We apply it to fingers, wrist and foot ankle. Although it can be used for haptic feedback in a diverse set of use cases, we specifically focus on collaborative applications: ShareHaptics allows to haptically jack-in to a remote environment via a custom glove and ankle braces. We demonstrate a wide range of applications: watching sports, gaming, and collaborative discussions and skill transfer.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338597	Takuro Nakao, Stevanus Kevin Santana, Megumi Isogai, Shinya Shimizu, Hideaki Kimata, Kai Kunze, Yun Suen Pai
Silly rubber: an implicit material point method for simulating non-equilibrated viscoelastic and elastoplastic solids	We present two new approaches for animating dynamic fracture involving large elastoplastic deformation. In contrast to traditional mesh-based techniques, where sharp discontinuity is introduced to split the continuum at crack surfaces, our methods are based on Continuum Damage Mechanics (CDM) with a variational energy-based formulation for crack evolution. Our first approach formulates the resulting dynamic material damage evolution with a Ginzburg-Landau type phase-field equation and discretizes it with the Material Point Method (MPM), resulting in a coupled momentum/damage solver rooted in phase field fracture: PFF-MPM. Although our PFF-MPM approach achieves convincing fracture with or without plasticity, we also introduce a return mapping algorithm that can be analytically solved for a wide range of general non-associated plasticity models, achieving more than two times speedup over traditional iterative approaches. To demonstrate the efficacy of the algorithm, we also develop a Non-Associated Cam-Clay (NACC) plasticity model with a novel fracture-friendly hardening scheme. Our NACC plasticity paired with traditional MPM composes a second approach to dynamic fracture, as it produces a breadth of organic, brittle material fracture effects on its own. Though NACC and PFF can be combined, we focus on exploring their material effects separately. Both methods can be easily integrated into any existing MPM solver, enabling the simulation of various fracturing materials with extremely high visual fidelity while requiring little additional computational overhead.	https://dl.acm.org/authorize?N688164	Joshuah Wolper, Yu Fang, Minchen Li, Jiecong Lu, Ming Gao, Chenfanfu Jiang
Single image portrait relighting	Lighting plays a central role in conveying the essence and depth of the subject in a portrait photograph. Professional photographers will carefully control the lighting in their studio to manipulate the appearance of their subject, while consumer photographers are usually constrained to the illumination of their environment. Though prior works have explored techniques for relighting an image, their utility is usually limited due to requirements of specialized hardware, multiple images of the subject under controlled or known illuminations, or accurate models of geometry and reflectance. To this end, we present a system for : a neural network that takes as input a RGB image of a portrait taken with a standard cellphone camera in an unconstrained environment, and from that image produces a relit image of that subject as though it were illuminated according to any provided environment map. Our method is trained on a small database of 18 individuals captured under different directional light sources in a controlled light stage setup consisting of a densely sampled sphere of lights. Our proposed technique produces quantitatively superior results on our dataset's validation set compared to prior works, and produces convincing qualitative relighting results on a dataset of hundreds of real-world cellphone portraits. Because our technique can produce a 640 × 640 image in only 160 milliseconds, it may enable interactive user-facing photographic applications in the future.	https://dl.acm.org/authorize?N688024	Tiancheng Sun, Jonathan T. Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, Ravi Ramamoorthi
SkateVR	An installation for those who are ready to jump on a skateboard and collect the maximum points in a virtual world, even if they know to control a real skate or not.	https://dl.acm.org/authorize?N689253	Dima Gangaliuc, Tony Tampei
Skinning vector graphics with GANs	We propose a novel method for editing vector graphics which enables users to intuitively modify complex Bézier geometry. Our method uses a Generative Adversarial Network (GAN) to automatically predict salient points for any arbitrary geometry defined by cubic Bézier curves, which are used as locations for a Linear Blend Skinning transformation. Further, we bind input geometry to a triangle mesh, to decouple the complexity of input geometry from mesh topology. Finally, to reconstruct Bézier curves from the transformed mesh, we formulate a linear optimization problem and solve it in performant manner to ensure real time feedback, without increasing the number of Bézier segments.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338544	Ankit Phogat, Matthew Fisher, Danny M. Kaufman, Vineet Batra
Sliding the pieces into place: rigging the pigeons of spies in disguise	"I present an alternative CG character animation methodology that eschews both keyframes and conventional hierarchical rigging. Primary rig controls have no hierarchy or built-in behavior-instead the animator calls for ""ephemeral"" rig behavior as needed. The system also facilitates ""interpolationless"" animation by removing keyframes as we know them, replacing them with discrete poses and inbetweening tools."	https://dl.acm.org/authorize?N689327	Raf Anzovin
Space walk: a combination of subtle redirected walking techniques integrated with gameplay and narration	Redirected walking (RDW) denotes a collection of techniques for immersive virtual environments (IVEs), in which users are unknowingly guided on paths in the real world that vary from the paths they perceive in the IVE. For this Emerging Technologies exhibit we present a playful virtual reality (VR) experience that introduces a combination of those RDW techniques such as bending gains, rotation gains, and impossible spaces, which are all subtly integrated with the gameplay and narration to perfectly fit the given environment. Those perceptual tricks allow users to explore a virtual space station of 45 m in a room-scale setup by natural walking only.	https://dl.acm.org/authorize?N688362	Eike Langbehn, Frank Steinicke
Spectral coarsening of geometric operators	We introduce a novel approach to measure the behavior of a geometric operator before and after coarsening. By comparing eigenvectors of the input operator and its coarsened counterpart, we can quantitatively and visually analyze how well the spectral properties of the operator are maintained. Using this measure, we show that standard mesh simplification and algebraic coarsening techniques fail to maintain spectral properties. In response, we introduce a novel approach for We show that it is possible to significantly reduce the sampling density of an operator derived from a 3D shape without affecting the low-frequency eigenvectors. By marrying techniques developed within the algebraic multigrid and the functional maps literatures, we successfully coarsen a variety of isotropic and anisotropic operators while maintaining sparsity and positive semi-definiteness. We demonstrate the utility of this approach for applications including operatorsensitive sampling, shape matching, and graph pooling for convolutional neural networks.	https://dl.acm.org/authorize?N688150	Hsueh-Ti Derek Liu, Alec Jacobson, Maks Ovsjanikov
Spheres	Space isn't silent. In fact, it is full of sound. For thousands of years, we've looked to the stars to find our place in the Universe, but for the first time we listen to its music. SPHERES is a three-chapter journey to uncover the hidden songs of the cosmos.	https://dl.acm.org/authorize?N689132	Dylan Golden, Nick Boyer, Eliza McNitt, Arnaud Colinart, Jess Engel
Spider-Man: Into the Spider-Verse	"Phil Lord and Christopher Miller, the creative minds behind ""The Lego Movie"" and ""21 Jump Street,"" bring their unique talents to a fresh vision of a different Spider-Man universe, with a groundbreaking visual style that's the first of its kind. ""Spider-Man™: Into the Spider-Verse"" introduces Brooklyn teen Miles Morales, and the limitless possibilities of the Spider-Verse, where more than one can wear the mask."	https://dl.acm.org/authorize?N689351	Danny Dimian
Spooky action at a distance: real-time VR interaction for non real-time remote robotics	We control robots through a simulated environment in game engine using VR and interact with it intuitively. A major breakthrough of this system is that, even if real-time robot control is not possible, the user can interact with the environment in real-time to complete tasks. Our system consists of a robot, vision sensor (RGB-D camera), game engine, and VR headset with controllers. The robot-side visual is provided as a scanned 3D geometry snapshot. We leverage point cloud as a visualization. Given the information to the user, two steps are required to control the robot. First, object annotation is needed. Given virtual 3d objects, the user is asked to place them roughly where they are in VR, therefore making the process intuitive. Next, computer vision based optimization refines the position to an accuracy level required for robot grasping. Optimization runs using non-blocking threads to maintain real-time experience. Second, the user needs to interact with objects. A robot simulation and UI will assist the process. A virtual robot gripper will provide a stable grasp estimation when it is brought close to a target. Once the object is picked up, placing it is also assisted. As in our example with block construction, each block's alignment with other blocks is assisted using its geometric characteristics, facilitating accurate placement. During the process, robot actions are simulated then visualized. The simulation and assistance is processed in real-time. Once interaction is given, simulated actions are sent and executed. Interaction and annotation processes can be queued without waiting for a robot to complete each step. Additionally, the user can easily abort planned actions then redo them. Our system demonstrates how powerful it is to combine game engine technologies, VR, and robots with computer vision/graphics algorithms to achieve semantic control over time and space.	https://dl.acm.org/authorize?N689124	Pavel Savkin, Nathan Quinn, Lochlainn Wilson
Star-shaped metrics for mechanical metamaterial design	We present a method for designing mechanical metamaterials based on the novel concept of Voronoi diagrams induced by star-shaped metrics. As one of its central advantages, our approach supports interpolation between arbitrary metrics. This capability opens up a rich space of structures with interesting aesthetics and a wide range of mechanical properties, including isotropic, tetragonal, orthotropic, as well as smoothly graded materials. We evaluate our method by creating large sets of example structures, provided as accompanying material. We validate the mechanical properties predicted by simulation through tensile tests on a set of physical prototypes.	https://dl.acm.org/authorize?N688037	Jonàs Martínez, Mélina Skouras, Christian Schumacher, Samuel Hornus, Sylvain Lefebvre, Bernhard Thomaszewski
Step right up, everyone's a winner: the making of Toy Story 4	Toy Story 4 continues the rich history of the previous three films, while also expanding the world and exposing the characters to new and exciting challenges. For Woody, a journey far beyond the kids' rooms and neighborhoods he's always known introduces him to new faces that test his assumptions of what it means to be there for a child. And of course, he'll need the help of some old friends to navigate the adventures he encounters. In this session, the crew will discuss how the sets, characters, cinematography and even pipeline were all developed to celebrate this next chapter. Please keep your hands inside the ride at all times.	https://dl.acm.org/authorize?N689292	Bob Moyer, Bill Reeves, Derek Reeves, Thomas Jordan, Steve Karski, Sajan Skaria, Amy Jones, Ariela Federov
Streamlining IBL workflows with computer vision and USD	We present a learning-based method to infer plausible high dynamic range (HDR), omnidirectional illumination given an unconstrained, low dynamic range (LDR) image from a mobile phone camera with a limited field of view (FOV). For training data, we collect videos of various reflective spheres placed within the camera's FOV, leaving most of the background unoccluded, leveraging that materials with diverse reflectance functions reveal different lighting cues in a single exposure. We train a deep neural network to regress from the LDR background image to HDR lighting by matching the LDR ground truth sphere images to those rendered with the predicted illumination using image-based relighting, which is differentiable. Our inference runs at interactive frame rates on a mobile device, enabling realistic rendering of virtual objects into real scenes for mobile mixed reality. Training on auto-exposed and white-balanced videos, we improve the realism of rendered objects compared to the state-of-the art methods for both indoor and outdoor scenes.	https://dl.acm.org/authorize?N689319	Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, Paul Debevec
Stuffed	An emotionally unstable cat makes a strange encounter that helps him to grow up and learn to overcome his anger.	https://dl.acm.org/authorize?N689233	Élise Simoulin
Stylizing video by example	We introduce a new example-based approach to video stylization, with a focus on preserving the visual quality of the style, user controllability and applicability to arbitrary video. Our method gets as input one or more keyframes that the artist chooses to stylize with standard painting tools. It then automatically propagates the stylization to the rest of the sequence. To facilitate this while preserving visual quality, we developed a new type of guidance for state-of-art patch-based synthesis, that can be applied to any type of video content and does not require any additional information besides the video itself and a user-specified mask of the region to be stylized. We further show a temporal blending approach for interpolating style between keyframes that preserves texture coherence, contrast and high frequency details. We evaluate our method on various scenes from real production setting and provide a thorough comparison with prior art.	https://dl.acm.org/authorize?N688152	Ondřej Jamriška, Šárka Sochorová, Ondřej Texler, Michal Lukáč, Jakub Fišer, Jingwan Lu, Eli Shechtman, Daniel Sýkora
Surface2Volume: surface segmentation conforming assemblable volumetric partition	Users frequently seek to fabricate objects whose outer surfaces consist of regions with different surface attributes, such as color or material. Manufacturing such objects in a single piece is often challenging or even impossible. The alternative is to partition them into single-attribute volumetric parts that can be fabricated separately and then assembled to form the target object. Facilitating this approach requires partitioning the input model into parts that to the surface segmentation and that can be moved apart with no collisions. We propose , a partition algorithm capable of producing such parts, each of which is affiliated with a single attribute, the outer surface of whose assembly conforms to the input surface geometry and segmentation. In computing the partition we strictly enforce conformity with surface segmentation and assemblability, and optimize for ease of fabrication by minimizing part count, promoting part simplicity, and simplifying assembly sequencing. We note that computing the desired partition requires solving for three types of variables: per-part assembly trajectories, partition topology, i.e. the connectivity of the interface surfaces separating the different parts, and the geometry, or location, of these interfaces. We efficiently produce the desired partitions by addressing one type of variables at a time: first computing the assembly trajectories, then determining interface topology, and finally computing interface locations that allow parts assemblability. We algorithmically identify inputs that necessitate sequential assembly, and partition these inputs gradually by computing and disassembling a subset of assemblable parts at a time. We demonstrate our method's robustness and versatility by employing it to partition a range of models with complex surface segmentations into assemblable parts. We further validate our framework via output fabrication and comparisons to alternative partition techniques.	https://dl.acm.org/authorize?N688035	Chrystiano Araújo, Daniela Cabiddu, Marco Attene, Marco Livesu, Nicholas Vining, Alla Sheffer
SurfaceBrush: from virtual reality drawings to manifold surfaces	Popular Virtual Reality (VR) tools allow users to draw varying-width, ribbonlike 3D brush strokes by moving a hand-held controller in 3D space. Artists frequently use dense collections of such strokes to draw virtual 3D shapes. We propose , a surfacing method that converts such VR drawings into user-intended manifold free-form 3D surfaces, providing a novel approach for modeling 3D shapes. The inputs to our method consist of dense collections of artist-drawn stroke ribbons described by the positions and normals of their central polylines, and ribbon widths. These inputs are highly distinct from those handled by existing surfacing frameworks and exhibit different sparsity and error patterns, necessitating a novel surfacing approach. We surface the input stroke drawings by identifying and leveraging local coherence between nearby artist strokes. In particular, we observe that strokes intended to be adjacent on the artist imagined surface often have similar tangent directions along their respective polylines. We leverage this local stroke direction consistency by casting the computation of the user-intended manifold surface as a constrained matching problem on stroke polyline vertices and edges. We first detect and smoothly connect adjacent similarly-directed sequences of stroke edges producing one or more manifold partial surfaces. We then complete the surfacing process by identifying and connecting adjacent similarly directed edges along the borders of these partial surfaces. We confirm the usability of the SurfaceBrush interface and the validity of our drawing analysis via an observational study. We validate our stroke surfacing algorithm by demonstrating an array of manifold surfaces computed by our framework starting from a range of inputs of varying complexity, and by comparing our outputs to reconstructions computed using alternative means.	https://dl.acm.org/authorize?N688041	Enrique Rosales, Jafet Rodriguez, ALLA SHEFFER
"Swing into another dimension: the making of ""Spider-Man: Into the Spider-Verse"""	"This production session will explore the art and innovation behind the creation of the Academy Award®-winning ""Spider-Man: Into the Spider-Verse."" The filmmaking team behind the first-ever animated Spider-Man feature film took significant risks to develop an all-new visual style inspired by the graphic look of comic books. The hand of the artist is visible in every frame, including misalignments and bleeding colors, imperfections rarely seen in CG animation. The entire look of the film was driven by artists' intentions, in which design and style was more important than accuracy or realism. The presentation will delve into the new technology developed and the changes to both the pipeline and workflow required to accommodate working in this new visual style. Every department at Sony Pictures Imageworks was asked to reconsider what it means to make an animated feature in the spirit of this revolutionary comic book style and to bring something new to the look of the film. Various new techniques were developed including the rigging and animating of facial line work, 2D hand-drawn effects and stylized rendering."	https://dl.acm.org/authorize?N689200	Danny Dimian, Joshua Beveridge, Bret St. Clair, Geeta Basantani
Symmetric moving frames	A basic challenge in field-guided hexahedral meshing is to find a spatially-varying frame that is adapted to the domain geometry and is continuous up to symmetries of the cube. We introduce a fundamentally new representation of such based on Cartan's method of moving frames. Our key observation is that cross fields and ordinary frame fields are locally characterized by identical conditions on their Hence, by using derivatives as the principal representation (and only later recovering the field itself), one avoids the need to explicitly account for symmetry during optimization. At the discrete level, derivatives are encoded by skew-symmetric matrices associated with the edges of a tetrahedral mesh; these matrices encode arbitrarily large rotations along each edge, and can robustly capture singular behavior even on coarse meshes. We apply this representation to compute 3D cross fields that are as smooth as possible everywhere but on a prescribed network of singular curves---since these fields are adapted to curve tangents, they can be directly used as input for field-guided mesh generation algorithms. Optimization amounts to an easy nonlinear least squares problem that behaves like a convex program in the sense that it always appears to produce the same result, independent of initialization. We study the numerical behavior of this procedure, and perform some preliminary experiments with mesh generation.	https://dl.acm.org/authorize?N688032	Etienne Corman, Keenan Crane
Synthesis of biologically realistic human motion using joint torque actuation	Using joint actuators to drive the skeletal movements is a common practice in character animation, but the resultant torque patterns are often unnatural or infeasible for real humans to achieve. On the other hand, physiologically-based models explicitly simulate muscles and tendons and thus produce more human-like movements and torque patterns. This paper introduces a technique to transform an optimal control problem formulated in the muscle-actuation space to an equivalent problem in the joint-actuation space, such that the solutions to both problems have the same optimal value. By solving the equivalent problem in the joint-actuation space, we can generate human-like motions comparable to those generated by musculotendon models, while retaining the benefit of simple modeling and fast computation offered by joint-actuation models. Our method transforms constant bounds on muscle activations to nonlinear, state-dependent torque limits in the joint-actuation space. In addition, the metabolic energy function on muscle activations is transformed to a nonlinear function of joint torques, joint configuration and joint velocity. Our technique can also benefit policy optimization using deep reinforcement learning approach, by providing a more anatomically realistic action space for the agent to explore during the learning process. We take the advantage of the physiologically-based simulator, OpenSim, to provide training data for learning the torque limits and the metabolic energy function. Once trained, the same torque limits and the energy function can be applied to drastically different motor tasks formulated as either trajectory optimization or policy learning.	https://dl.acm.org/authorize?N688027	Yifeng Jiang, Tom Van Wouwe, Friedl De Groote, C. Karen Liu
Synthetic defocus and look-ahead autofocus for casual videography	In cinema, large camera lenses create beautiful shallow depth of field (DOF), but make focusing difficult and expensive. Accurate cinema focus usually relies on a script and a person to control focus in realtime. Casual videographers often crave cinematic focus, but fail to achieve it. We either sacrifice shallow DOF, as in smartphone videos; or we struggle to deliver accurate focus, as in videos from larger cameras. This paper is about a new approach in the pursuit of cinematic focus for casual videography. We present a system that synthetically renders refocusable video from a deep DOF video shot with a smartphone, and analyzes video frames to deliver context-aware autofocus for the current frame. To create refocusable video, we extend recent machine learning methods designed for still photography, contributing a new dataset for machine training, a rendering model better suited to cinema focus, and a filtering solution for temporal coherence. To choose focus accurately for each frame, we demonstrate autofocus that looks at upcoming video frames and applies AI-assist modules such as motion, face, audio and saliency detection. We also show that autofocus benefits from machine learning and a large-scale video dataset with focus annotation, where we use our RVR-LAAF GUI to create this sizable dataset efficiently. We deliver, for example, a shallow DOF video where the autofocus transitions onto each person she begins to speak. This is impossible for conventional camera autofocus because it would require seeing into the future.	https://dl.acm.org/authorize?N688085	Xuaner Zhang, Kevin Matzen, Vivien Nguyen, Dillon Yao, You Zhang, Ren Ng
Synthetic silviculture: multi-scale modeling of plant ecosystems	"We present a new framework for interior scene synthesis that combines a high-level relation graph representation with spatial prior neural networks. We observe that prior work on scene synthesis is divided into two camps: object-oriented approaches (which reason about the set of objects in a scene and their configurations) and space-oriented approaches (which reason about what objects occupy what regions of space). Our insight is that the object-oriented paradigm excels at high-level of how a room should be laid out, while the space-oriented paradigm performs well at a layout by placing objects in precise spatial configurations. With this in mind, we present PlanIT, a layout-generation framework that divides the problem into two distinct and phases. PlanIT represents the ""plan"" for a scene via a relation graph, encoding objects as nodes and spatial/semantic relationships between objects as edges. In the planning phase, it uses a deep graph convolutional generative model to synthesize relation graphs. In the instantiation phase, it uses image-based convolutional network modules to guide a search procedure that places objects into the scene in a manner consistent with the graph. By decomposing the problem in this way, PlanIT generates scenes of comparable quality to those generated by prior approaches (as judged by both people and learned classifiers), while also providing the modeling flexibility of the intermediate relationship graph representation. These graphs allow the system to support applications such as scene synthesis from a partial graph provided by a user."	https://dl.acm.org/authorize?N688187	Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X. Chang, Daniel Ritchie
T. rex: skeleton crew	"""T. rex: Skeleton Crew"" is a multiplayer, wireless VR experience designed for AMNH's exhibition ""T. rex: The Ultimate Predator."" Together, three players assemble a T. rex skeleton and then witness it come to life as the environment transforms from a museum gallery to the Cretaceous world 66 million years ago."	https://dl.acm.org/authorize?N689133	Vivian Trakinski, Nicholas Bartzokas, Laura Moustakerski, Eozin Che, Lauren Kushner, Shay Krasinski, Fredric Kessler, Jason Morfoot, Mark Norell, Kent Stevens, Gregory Erickson, Zhao Chuang, Chris Chin, Jad Boniface, Dario Laverde, Piotr Baczynski, Bartosz Roslonski, Tomasz Figas, Robert Zjawinski, Mateusz Kozowski, Marcin Szadkowski, Kamil Sroka, Tomasz Zelazko, Eugenia Goncharova, Matt McCorkle
Taming the shadow Terminator	We present a practical approach to model close-up water interaction with characters. We specifically focus on high-fidelity surface tension and adhesion effects as water sheds off skin. We show that an existing particle-in-cell (FLIP/APIC) solver can be adapted to capture small-scale water-solid interaction dynamics and discuss the role and implementation details of the relevant key components: treatment of surface tension and viscosity, enforcement of contact angle, and maintenance of contact with fast-moving collision geometry. The method allows for resolution of effects on a scale of a fraction of a millimeter and is performant enough to be able to cover a whole human body with a layer of water. We demonstrate successful use of the approach in a shot from	https://dl.acm.org/authorize?N689345	Alexey Stomakhin, Andrew Moffat, Gary Boyle
Tangent-space optimization for interactive animation control	Character animation tools are based on a keyframing metaphor where artists pose characters at selected keyframes and the software automatically interpolates the frames inbetween. Although the quality of the interpolation is critical for achieving a fluid and engaging animation, the tools available to adjust the result of the automatic inbetweening are rudimentary and typically require manual editing of spline parameters. As a result, artists spend a tremendous amount of time posing and setting more keyframes. In this pose-centric workflow, animators use combinations of forward and inverse kinematics. While forward kinematics leads to intuitive interpolations, it does not naturally support positional constraints such as fixed contact points. Inverse kinematics can be used to fix certain points in space at keyframes, but can lead to inferior interpolations, is slow to compute, and does not allow for positional contraints at non-keyframe frames. In this paper, we address these problems by formulating the control of interpolations with positional constraints over time as a space-time optimization problem in the tangent space of the animation curves driving the controls. Our method has the key properties that it (1) allows the manipulation of positions and orientations over time, extending inverse kinematics, (2) does not add new keyframes that might conflict with an artist's preferred keyframe style, and (3) works in the space of artist editable animation curves and hence integrates seamlessly with current pipelines. We demonstrate the utility of the technique in practice via various examples and use cases.	https://dl.acm.org/authorize?N688156	Loïc Ciccone, Cengiz Öztireli, Robert W. Sumner
TeeVR: spatial template-based acquisition, modeling, and rendering of large-scale indoor spaces	Conventional image-based rendering has limited applicability for large-scale spaces. In this study, we demonstrate an efficient alternative to conventional image-based rendering. Our key approach is based on a spatial template (ST), which solely includes architectural geometric primitives. The predictability of ST improves the efficiency of acquisition, storage, and rendering. Thereby, our system can be applied to the modeling and rendering of larger indoor spaces.	https://dl.acm.org/authorize?N688363	Nathan Doh, Hyunga Choi, Bumchul Jang, Sangmin Ahn, Hyojin Jung, Sungkil Lee
TeleSight: enabling asymmetric collaboration in VR between HMD user and Non-HMD users	"In this paper, we propose ""TeleSight"", proof-of-concept prototype enabling asymmetric collaboration in VR between HMD user and Non-HMD users. TeleSight provides two interaction layers for designing asymmetric interaction, a physical layer for direct tangible interaction with players in VR space and visual layer that visually expresses VR space. Each layer is constructed by an avatar robot that imitates the motion of the HMD user, and a projection system. Non-HMD users can understand what happens in the virtual world by each interaction layers. Also, Non-HMD users can interact tangibly with the players in VR space throughout avatar robot. TeleSight provides that cooperative co-located gameplay between HMD user and Non-HMD users through experience scenarios using two layers."	https://dl.acm.org/authorize?N688364	Taichi Furukawa, Daisuke Yamamoto, Moe Sugawa, Roshan Peiris, Kouta Minamizawa
Temporal and spatial anti-aliasing for rendering reflection on a water surface	The ocean surface is highly dynamic. It moves rapidly and thus its shading changes rapidly as well. Usually, this doesn't pose any problems if the shading is smooth. However, for a surface that has a strong highlight or bright reflection moving rapidly, it causes an inaccurate and unnatural flickering. In the traditional rendering algorithms, each frame is rendered independently at a discrete time, resulting in serious temporal aliasing artifacts. Particularly, for a wavy water surface, reflection vectors may not hit the light source even though they actually hit for part of the frame time. Removing such aliasing in real-time is an active research area and many methods have been proposed [Jimenez et al. 2011]. They can improve the fidelity and efficiency of the rendering method. However, their focus is on spatial anti-aliasing and most of them do not address the temporal aliasing problem, particularly the one observed in rendering a reflected image of a light source on the water surface.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338599	Namo Podee, Nelson Max, Kei Iwasaki, Yoshinori Dobashi
Tensor maps for synchronizing heterogeneous shape collections	Establishing high-quality correspondence maps between geometric shapes has been shown to be the fundamental problem in managing geometric shape collections. Prior work has focused on computing efficient maps between pairs of shapes, and has shown a quantifiable benefit of joint map synchronization, where a collection of shapes are used to improve (denoise) the pairwise maps for consistency and correctness. However, these existing map synchronization techniques place very strong assumptions on the input shapes collection such as all the input shapes fall into the same category and/or the majority of the input pairwise maps are correct. In this paper, we present a multiple map synchronization approach that takes a heterogeneous shape collection as input and simultaneously outputs consistent dense pairwise shape maps. We achieve our goal by using a novel tensor-based representation for map synchronization, which is efficient and robust than all prior matrix-based representations. We demonstrate the usefulness of this approach across a wide range of geometric shape datasets and the applications in shape clustering and shape co-segmentation.	https://dl.acm.org/authorize?N688151	Qixing Huang, Zhenxiao Liang, Haoyun Wang, Simiao Zuo, Chandrajit Bajaj
Text-based editing of talking-head video	Editing talking-head video to change the speech content or to remove filler words is challenging. We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame. To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. We demonstrate a large variety of edits, such as the addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis.	https://dl.acm.org/authorize?N688013	Ohad Fried, Ayush Tewari, Michael Zollhöfer, Adam Finkelstein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, Maneesh Agrawala
The Bolt Connection	After a heist that goes wrong, a frail robot who was the driver of a robot mafia ends up in possession of loot he shouldn't have: a human heart. Tempted by the opportunity to be like his bosses, he grafts the heart to himself and discovers the feeling of being alive. But, those brief instants of life have a price.	https://dl.acm.org/authorize?N689230	Nicolas Lebas
The Great C	Journey inside Hurricane Maria two days before it devastated Puerto Rico. This visualization is based on volumetric precipitation data from the NASA-Japan Global Precipitation Measurement Core Observatory satellite. The 3D tour reveals structures inside the hurricane that fueled the storm's intensification to a Category 5 within 24 hours.	https://dl.acm.org/authorize?N689362	Steve Miller
The Heretic (part 1)	In a future when humans and technology are intertwined, we have yet to discover the origins of our fairy tales.	https://dl.acm.org/authorize?N689244	Veselin Efremov
The Ostrich Politic	Ostriches carry on their daily activities burying their heads, believing it's an instinctive behavior. However, one day, research by phylogeneticist Dr. Kays proves otherwise.	https://dl.acm.org/authorize?N689358	Mohammad Houhou
The Stained Club	Finn has stains on his skin. One day, he meets a group of cool kids with different stains on their bodies. One day, he understands that these stains aren't just pretty.	https://dl.acm.org/authorize?N689359	Mélanie Lopez
The Tree	In a world of drought, an old man spends his days collecting drops of water to quench the thirst of a dead tree.	https://dl.acm.org/authorize?N689355	Basil Malek-Abuhamdan
The VFX of Netflix series	From the tragic tales of orphans to a joint force of super siblings to sinister forces threatening 1908s Indiana, the VFX teams on Netflix series have delivered some of the year's most astounding visuals. We've queued three of them up for bingeing en masse. Join creatives behind A SERIES OF UNFORTUNATE EVENTS, THE UMBRELLA ACADEMY and STRANGER THINGS as they present some of the work, techniques, and passion that brought these worlds and characters into being.	https://dl.acm.org/authorize?N689206	Sean Santiago, Chris White, Aladino Debert, Yvon Jardel, Paul Graff, Sue Rowe, Everett Burrell
The beauty of breaking rhythms: affective robot motion design using Jo-Ha-Ky&umacr; of bunraku puppet	"One of the UNESCO intangible cultural heritages Bunraku puppets can play one of the most beautiful puppet motions in the world. The sophisticated motions of the Bunraku puppet can express emotions interactively with a fixed facial expression overcoming the so-called ""Uncanny Valley"", simultaneously. In the present paper, we study the Bunraku motions using the famous concept so-called ""Jo(Introduction)-Ha(Breaking)-Kyū(Rapid)"". These emotional motions are synchronized with the Jo-ha-kyū. As a result, an android robot can express different affective motion synchronized to the different type of emotional chant or narration with a story that we call Jōruri."	https://dl.acm.org/authorize?N689373	Yang Chen, Ran Dong, Dongsheng Cai, Shinobu Nakagawa, Tomonari Higaki, Nobuyoshi Asai
The ethical and privacy implications of mixed reality	XR has never been more immersive, entertaining and dynamic than it is now. So why is the virtual world still such a lonely place? Shared virtual experiences are slowing becoming a reality, with the potential to transform the way XR is used in education, entertainment, telepresence and the enterprise. But will technology and content finally coevolve to support realistic, realtime, multi-person interactions? This panel explores the expanding dimensions of shared experiences in XR, offering views on emerging trends, applications and breakthrough technologies.	https://dl.acm.org/authorize?N688121	Kent Bye, Diane Hosfelt, Sam Chase, Matt Miesnieks, Taylor Beck
The face of art: landmark detection and geometric style in portraits	"Facial Landmark detection in natural images is a very active research domain. Impressive progress has been made in recent years, with the rise of neural-network based methods and large-scale datasets. However, it is still a challenging and largely unexplored problem in the artistic portraits domain. Compared to natural face images, artistic portraits are much more diverse. They contain a much wider style variation in both geometry and texture and are more complex to analyze. Moreover, datasets that are necessary to train neural networks are unavailable. We propose a method for artistic augmentation of natural face images that enables training deep neural networks for landmark detection in artistic portraits. We utilize conventional facial landmarks datasets, and transform their content from natural images into ""artistic face"" images. In addition, we use a feature-based landmark correction step, to reduce the dependency between the different facial features, which is necessary due to position and shape variations of facial landmarks in artworks. To evaluate our landmark detection framework, we created an ""Artistic-Faces"" dataset, containing 160 artworks of various art genres, artists and styles, with a large variation in both geometry and texture. Using our method, we can detect facial features in artistic portraits and analyze their geometric style. This allows the definition of signatures for artistic styles of artworks and artists, that encode both the geometry and the texture style. It also allows us to present a geometric-aware style transfer method for portraits."	https://dl.acm.org/authorize?N688015	Jordan Yaniv, Yael Newman, Ariel Shamir
The future of shared experiences: XR is a lonely world	This Special Session lightning round features 12 dynamic trailblazers from SIGGRAPH history who are continually empowered by innovation into present day explorations. These women interweave stories across time that transformed digital media and shaped SIGGRAPH; while at the same time these women personally benefited from the interdisciplinary graphics community.	https://dl.acm.org/authorize?N688122	Aaron Sisto, Victor Luo, Anand Agarawala, Jameson Detweiler, Varun Mani, Mark Mine, Ben Grossman
The last oasis	A man-made disaster has devastated much of the earth's ecosystem. Ogini Orog, a brilliant scientist, has built himself a safe bunker using his wits to survive the hostile environment and is looking for artifacts in order to extract and save the last remains or nature. Explore each room to uncover clues.	https://dl.acm.org/authorize?N689254	Goro Fujita
"The making of ""Age of Sail"""	"""Age of Sail"" tells the story of William Avery, an old sailor adrift and alone in the North Atlantic. When Avery rescues Lara, who has mysteriously fallen overboard, he finds redemption and hope in his darkest hours. In this production talk we'll go behind the scenes in the making of this multi-platform immersive animated short. Some of the unique challenges we'll discuss: bringing a 2D illustrated style to life in the medium of 6DoF VR with new non-photorealistic rendering techniques; immersing the viewer in a storm-tossed ocean without making them seasick; adapting a single story to multiple mediums (desktop/mobile VR, 360° video, and 2D film); creating a better sounding and more responsive sound mix with multiple surround formats and a new spatialization model; and optimizing it all to run at 60fps on a mobile phone."	https://dl.acm.org/authorize?N689387	John Kahrs, Theresa Latzko, Cassidy Curtis, Scot Stafford
"The making of ""How to Train Your Dragon: The Hidden World"""	"A breakdown of the techniques and approaches taken by the filmmakers in the production of ""How to Train Your Dragon: The Hidden World."""	https://dl.acm.org/authorize?N689234	Dave Walvoord
"The making of Marvel Studios' ""Avengers: Endgame"""	The fourth installment in the Avengers saga is the culmination of 22 interconnected films and has drawn audiences to witness the turning point of this epic journey. Our beloved heroes now truly understand how fragile our world is, and the sacrifices that must be made to protect it. It is a story of friendship, teamwork, and setting aside our differences to overcome an immense obstacle. Join Marvel Studios, Digital Domain, ILM, and Weta Digital as they discuss how the most diverse collection of heroes, environments, and visual effects were assembled into this ultimate climactic final chapter.	https://dl.acm.org/authorize?N689290	Victoria Alonso, Dan DeLeeuw, Jen Underdahl, Kelly Port, Russell Earl, Matt Aitken, Gerardo Ramirez
TileGAN: synthesis of large-scale non-homogeneous textures	We tackle the problem of texture synthesis in the setting where many input images are given and a large-scale output is required. We build on recent generative adversarial networks and propose two extensions in this paper. First, we propose an algorithm to combine outputs of GANs trained on a smaller resolution to produce a large-scale plausible texture map with virtually no boundary artifacts. Second, we propose a user interface to enable artistic control. Our quantitative and qualitative results showcase the generation of synthesized high-resolution maps consisting of up to hundreds of megapixels as a case in point.	https://dl.acm.org/authorize?N688003	Anna Frühstück, Ibraheem Alhashim, Peter Wonka
Transfantome: transformation into bodies of various scale and structure in multiple spaces	"Transfantome is a novel robot interaction in which the user seamlessly changes or simultaneously uses different ""Bodies"" of different sizes, structures, and positions. By combining various multiple bodies, we expand our range of power, dexterity, and the place we act, and it brings us unexplored experience or highly efficient work. We map a virtual ""Proxy Body"" to the target physical ""bodies"" like miniature robots or huge construction machines and control them. By moving and scaling the Proxy Body in the virtual environment that reproduces the target bodies and their environment, we can smoothly change bodies which have separated view, size, and structure. As a result, we can handle various bodies together intuitively. For example, by using a powerful giant robot and a dextrous small one, it is possible to carry out the work like rescuing injured person while removing heavy debris efficiently and carefully."	https://dl.acm.org/authorize?N688375	Atsushi Izumihara, Tomoya Sasaki, Masahiro Ogino, Reona Takamura, Masahiko Inami
TriWild: robust triangulation with curve constraints	We propose a robust 2D meshing algorithm, , to generate curved triangles reproducing smooth feature curves, leading to coarse meshes designed to match the simulation requirements necessary by applications and avoiding the geometrical errors introduced by linear meshes. The robustness and effectiveness of our technique are demonstrated by batch processing an SVG collection of 20k images, and by comparing our results against state of the art linear and curvilinear meshing algorithms. We demonstrate for our algorithm the practical utility of computing diffusion curves, fluid simulations, elastic deformations, and shape inflation on complex 2D geometries.	https://dl.acm.org/authorize?N688007	Yixin Hu, Teseo Schneider, Xifeng Gao, Qingnan Zhou, Alec Jacobson, Denis Zorin, Daniele Panozzo
Undersea	Undersea is a room-scale, spatial computing experience for the Magic Leap One headset. Relax and observe underwater life in a dynamically generated, coral reef biome. Distinct vistas and creatures, presented in a photo-real art style, provide an opportunity to feel a sense of presence and connection between the creatures, the environment, and the user.	https://dl.acm.org/authorize?N689265	Magic Leap Team
Unsupervised incremental learning for hand shape and pose estimation	We present an unsupervised incremental learning method for refining hand shape and pose estimation. We propose a refiner network ( ) that can augment a state-of-the-art hand tracking system ( ) by refining its estimations on unlabeled data. At each input depth frame, the estimations from the are iteratively refined by using a model-fitting strategy. During this process, the adapts to the input data characteristics by incremental learning. We show that our method provides more accurate hand shape and pose estimates on both a standard dataset and real data.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338553	Pratik Kalshetti, Parag Chaudhuri
Untold HERstories: an homage to SIGGRAPH	The attendees at the SIGGRAPH Conference run the gambit from indie contractors to multi-million-dollar studios. After attending the conference for the past 7 years, I kept hearing the same thing from many small studios and/or individuals. They feel a lot of the emerging technology that is premiered at SIGGRAPH seems out of reach or impractical for them to utilize. Our panel works to challenge this assumption. By bringing in industry professionals from various disciplines and levels, we can show how anyone from an independent contractor to a large studio can implement new tools and technologies. Specifically, our talk will focus on encounters with AR/VR/MR and how we worked it into our pipeline. Whether someone is out there making the next big movie or trying to pitch to a client in a conference room setting, they will find attending this panel useful. We will talk about the practical applications of AR/VR/MR and how we have explored these technologies over the years.	https://dl.acm.org/authorize?N688123	Donna J. Cox, Ellen Sandor, Janine Fron
Using moments to represent bounded signals for spectral rendering	Prefiltering the reflectance of a displacement-mapped surface while preserving its overall appearance is challenging, as smoothing a displacement map causes complex changes of illumination effects such as shadowing-masking and interreflection. In this paper, we introduce a new method that prefilters displacement maps and BRDFs jointly and constructs SVBRDFs at reduced resolutions. These SVBRDFs preserve the appearance of the input models by capturing both shadowing-masking and interreflection effects. To express our appearance-preserving SVBRDFs efficiently, we leverage a new representation that involves spatially varying NDFs and a novel scaling function that accurately captures micro-scale changes of shadowing, masking, and interreflection effects. Further, we show that the 6D scaling function can be factorized into a 2D function of surface location and a 4D function of direction. By exploiting the smoothness of these functions, we develop a simple and efficient factorization method that does not require computing the full scaling function. The resulting functions can be represented at low resolutions (e.g., 4 for the spatial function and 15 for the angular function), leading to minimal additional storage. Our method generalizes well to different types of geometries beyond Gaussian surfaces. Models prefiltered using our approach at different scales can be combined to form mipmaps, allowing accurate and anti-aliased level-of-detail (LoD) rendering.	https://dl.acm.org/authorize?N688182	Lifan Wu, Shuang Zhao, Ling-Qi Yan, Ravi Ramamoorthi
VFX fractal toolkit: integrating fractals into VFX pipeline	This paper proposes an innovative industry practice regarding fractal geometry generating and rendering processes. VFX Fractal Toolkit (VFT) aims to provide powerful, yet intuitive and artist-friendly workflows for exploration and generating of vast amounts of fractals. VFT allows for node-based description of fractals implemented in VFT is built specifically for Visual Effects (VFX) pipelines and employs standard practices. It aims to provide artists with a toolset which would help them explore fractal forms of generative art directly in VFX applications.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338543	Juraj Tomori
VR facial animation via multiview image translation	A key promise of Virtual Reality (VR) is the possibility of remote social interaction that is more immersive than any prior telecommunication media. However, existing social VR experiences are mediated by inauthentic digital representations of the user (i.e., stylized avatars). These stylized representations have limited the adoption of social VR applications in precisely those cases where immersion is most necessary (e.g., professional interactions and intimate conversations). In this work, we present a bidirectional system that can animate avatar heads of both users' full likeness using consumer-friendly headset mounted cameras (HMC). There are two main challenges in doing this: unaccommodating camera views and the image-to-avatar domain gap. We address both challenges by leveraging constraints imposed by multiview geometry to establish precise image-to-avatar correspondence, which are then used to learn an end-to-end model for real-time tracking. We present designs for a HMC, aimed at data-collection and model building, and a tracking HMC for use during interactions in VR. Correspondence between the avatar and the HMC-acquired images are automatically found through self-supervised multiview image translation, which does not require manual annotation or one-to-one correspondence between domains. We evaluate the system on a variety of users and demonstrate significant improvements over prior work.	https://dl.acm.org/authorize?N688012	Shih-En Wei, Jason Saragih, Tomas Simon, Adam W. Harley, Stephen Lombardi, Michal Perdoch, Alexander Hypes, Dawei Wang, Hernan Badino, Yaser Sheikh
VR hair salon for avatars	While hair is an essential component of virtual humans, it is also one of the most challenging and time-consuming digital assets to create. Existing automatic techniques lack the generality and flexibility for users to create the exact intended hairstyles. Meanwhile, manual authoring interfaces often require considerable skills and experiences from character modelers, and are difficult to navigate for intricate 3D hair structures. We propose an interactive hair modeling system that can help create complex hairstyles that would otherwise take weeks or months with existing tools. Modelers, including novice users, can focus on the overall intended hairstyles and local hair deformations, as our system intelligently suggests the desired hair parts. Our method combines the flexibility of manual authoring and the convenience of data-driven automation. Since hair contains intricate 3D structures such as buns, knots, and strands, they are inherently challenging to create from scratch using traditional 2D interfaces. Our system provides a new 3D hair authoring interface for immersive interaction in virtual reality (VR). We use a strip-based representation, which is commonly adopted in real-time games due to rendering efficiency and modeling flexibility. The output strips can be converted to other hair formats such as strands. Users can draw high-level guide strips, from which our system predicts the most plausible hairstyles in the dataset via a trained deep neural network. Each hairstyle in our dataset is composed of multiple variations, serving as blendshapes to fit the user drawings via global blending and local deformation. The fitted hair models are visualized as interactive suggestions, that the user can select, modify, or ignore. We conducted a user study to confirm that our system can significantly reduce manual labor while improve the output quality for modeling a variety of hairstyles that are challenging to create using existing techniques.	https://dl.acm.org/authorize?N689135	Hao Li, Jun Xing, Koki Nagano, Liwen Hu, Li-Yi Wei
VR minecraft for art	We have created a VR art museum within Minecraft and added features to make it more interactive and engaging. A scavenger hunt was added, and a player can build a sculpture after completing the scavenger hunt. The user is then provided with different colored blocks to create a painting of their own.	https://dl.acm.org/authorize?N689266	Kyungjin Yoo, Ryan Havel, Nikhil Patel
VRProp-net: real-time interaction with virtual props	Virtual and Augmented Reality (VR and AR) are two fast growing mediums, not only in the entertainment industry but also in health, education and engineering. A good VR or AR application seamlessly merges the real and virtual world, making the user feels fully immersed. Traditionally, a computer-generated object can be interacted with using controllers or hand gestures [HTC 2019; Microsoft 2019; Oculus 2019]. However, these motions can feel unnatural and do not accurately represent the motion of interacting with a real object. On the other hand, a physical object can be used to control the motion of a virtual object. At present, this can be done by tracking purely rigid motion using an external sensor [HTC 2019]. Alternatively, a sparse number of markers can be tracked, for example using a motion capture system, and the positions of these used to drive the motion of an underlying non-rigid model. However, this approach is sensitive to changes in marker position and occlusions and often involves costly non-standard hardware [Vicon 2019]. In addition, these approaches often require a virtual model to be manually sculpted and rigged which can be a time consuming process. Neural networks have been shown to be successful tools in computer vision, with several key methods using networks for tracking rigid and non-rigid motion in RGB images [Andrychowicz et al. 2018; Kanazawa et al. 2018; Pumarola et al. 2018]. While these methods show potential, they are limited to using multiple RGB cameras or large, costly amounts of labelled training data.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338548	Catherine Taylor, Robin McNicholas, Darren Cosker
Variational implicit point set surfaces	Denoising has proven to be useful to efficiently generate high-quality Monte Carlo renderings. Traditional pixel-based denoisers exploit summary statistics of a pixel's sample distributions, which discards much of the samples' information and limits their denoising power. On the other hand, sample-based techniques tend to be slow and have difficulties handling general transport scenarios. We present the first convolutional network that can learn to denoise Monte Carlo renderings directly from the samples. Learning the mapping between samples and images creates new challenges for the network architecture design: the order of the samples is arbitrary, and they should be treated in a permutation invariant manner. To address these challenges, we develop a novel kernel-predicting architecture that individual samples onto nearby pixels. Splatting is a natural solution to situations such as motion blur, depth-of-field and many light transport paths, where it is easier to predict which pixels a sample contributes to, rather than a approach that needs to figure out, for each pixel, which samples (or nearby pixels) are relevant. Compared to previous state-of-the-art methods, ours is robust to the severe noise of low-sample count images (e.g. 8 samples per pixel) and yields higher-quality results both visually and numerically. Our approach retains the generality and efficiency of pixel-space methods while enjoying the expressiveness and accuracy of the more complex sample-based approaches.	https://dl.acm.org/authorize?N688170	Michaël Gharbi, Tzu-Mao Li, Miika Aittala, Jaakko Lehtinen, Frédo Durand
Vector based glyph style transfer	In this work, we solve the problem of real-time transfer of geometric style from a single glyph to the entire glyph set of a vector font. In our solution, a single glyph is defined as one or more closed Bézier paths which is further broken down in primitives to define a set of segments. The modification to these segments is percolated to the entire glyph set by comparing the set of segments across glyphs using techniques like the order and direction of segments and the spatial placement of segments. Once the target segments in other glyphs is identified the transformation from style glyph is applied to the target glyph. Furthermore, we establish user-controlled policies for percolation of style like mapping line segment modification to curve segments. This extension to the algorithm enables the user to create multiple variations of a glyph.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338600	Praveen Kumar Dhanuka, Nirmal Kumawat, Nipun Jindal
Vibration-minimizing motion retargeting for robotic characters	Creating animations for robotic characters is very challenging due to the constraints imposed by their physical nature. In particular, the combination of fast motions and unavoidable structural deformations leads to mechanical oscillations that negatively affect their performances. Our goal is to automatically transfer motions created using traditional animation software to robotic characters while avoiding such artifacts. To this end, we develop an optimization-based, dynamics-aware motion retargeting system that adjusts an input motion such that visually salient low-frequency, large amplitude vibrations are suppressed. The technical core of our animation system consists of a differentiable dynamics simulator that provides constraint-based two-way coupling between rigid and flexible components. We demonstrate the efficacy of our method through experiments performed on a total of five robotic characters including a child-sized animatronic figure that features highly dynamic drumming and boxing motions.	https://dl.acm.org/authorize?N688157	Shayan Hoshyari, Hongyi Xu, Espen Knoop, Stelian Coros, Moritz Bächer
Vidgets: modular mechanical widgets for mobile devices	We present , a family of mechanical widgets, specifically push buttons and rotary knobs that augment mobile devices with tangible user interfaces. When these widgets are attached to a mobile device and a user interacts with them, the widgets' nonlinear mechanical response shifts the device slightly and quickly, and this subtle motion can be detected by the accelerometer commonly equipped on mobile devices. We propose a physics-based model to understand the nonlinear mechanical response of widgets. This understanding enables us to design tactile force profiles of these widgets so that the resulting accelerometer signals become easy to recognize. We then develop a lightweight signal processing algorithm that analyzes the accelerometer signals and recognizes how the user interacts with the widgets in real time. Vidgets widgets are low-cost, compact, reconfigurable, and power efficient. They can form a diverse set of physical interfaces that enrich users' interactions with mobile devices in various practical scenarios. We demonstrate their use in three applications: photo capture with single-handed zoom, control of mobile games, and making a playable mobile music instrument.	https://dl.acm.org/authorize?N688155	Chang Xiao, Karl Bayer, Changxi Zheng, Shree K. Nayar
Virtual reality mirror therapy rehabilitation for post-stroke patients	Strokes have a wide range of occurrence among people of all races and genders. The consequences of a stroke often include significant muscle weakness on one side of the body that must be physically exercised to attempt to restore its previous strength and mobility. In traditional mirror therapy, the partially disabled hand or leg is hidden by a mirror. The patient sees a reflection of the healthy side of their body where the disabled limb should be, in order to stimulate brain to operate the partially disabled hand/leg in the proper way. The patient will interact with mirror using unaffected hand and observe its reflection.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338601	Levan Sulimanov, Marc Olano
Visual knitting machine programming	Industrial knitting machines are commonly used to manufacture complicated shapes from yarns; however, designing patterns for these machines requires extensive training. We present the first general visual programming interface for creating 3D objects with complex surface finishes on industrial knitting machines. At the core of our interface is a new, augmented, version of the stitch mesh data structure. The augmented stitch mesh stores low-level knitting operations per-face and encodes the dependencies between faces using directed edge labels. Our system can generate knittable augmented stitch meshes from 3D models, allows users to edit these meshes in a way that preserves their knittability, and can schedule the execution order and location of each face for production on a knitting machine. Our system is general, in that its knittability-preserving editing operations are sufficient to transform between any two machine-knittable stitch patterns with the same orientation on the same surface. We demonstrate the power and flexibility of our pipeline by using it to create and knit objects featuring a wide range of patterns and textures, including intarsia and Fair Isle colorwork; knit and purl textures; cable patterns; and laces.	https://dl.acm.org/authorize?N688018	Vidya Narayanan, Kui Wu, Cem Yuksel, James McCann
Visual simulation of ice and frost with sketch input	"Visual effects (called ""VFX"" in this poster) have expanded the range of expression and been necessary to enhance the mood for the story in films and videos. A variety of software tools which support producing VFX has been developed."	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338571	Mikiko Amano, Takayuki Itoh
Visual smoothness of polyhedral surfaces	Representing smooth geometric shapes by polyhedral meshes can be quite difficult in situations where the variation of edges and face normals is prominently visible. Especially problematic are saddle-shaped areas of the mesh, where typical vertices with six incident edges are ill suited to emulate the more symmetric smooth situation. The importance of a faithful discrete representation is apparent for certain special applications like freeform architecture, but is also relevant for simulation and geometric computing. In this paper we discuss what exactly is meant by a good representation of saddle points, and how this requirement is stronger than a good approximation of a surface plus its normals. We characterize good saddles in terms of the normal pyramid in a vertex. We show several ways to design meshes whose normals enjoy small variation (implying good saddle points). For this purpose we define a discrete energy of polyhedral surfaces, which is related to a certain total absolute curvature of smooth surfaces. We discuss the minimizers of both functionals and in particular show that the discrete energy is minimal not for triangle meshes, but for principal quad meshes. We demonstrate our procedures for optimization and interactive design by means of meshes intended for architectural design.	https://dl.acm.org/authorize?N688086	Davide Pellis, Martin Kilian, Felix Dellinger, Johannes Wallner, Helmut Pottmann
Visualization of putting trajectories in live golf broadcasting	"Science-fiction notions of ""the metaverse"" are slowly becoming a reality as products such as Fortnite, Minecraft, and Roblox bring immersive social experiences to hundreds of millions of people and blur the boundaries between games and social networks. This talk will explore the foundational technologies, economies, and freedoms required to build this future medium as a force for good."	https://dl.acm.org/authorize?N689301	Tim Sweeney
Visualization of ultra-thin semi-transparent metallic films by wave simulations and ray-tracing rendering	Plasmonic color generation describes structural color arising from resonant interaction between visible light and metallic nanostructures, causing selective frequencies of light to be scattered and/or absorbed [Kristensen et al. 2017; Sun and Xia 2003]. The perceived color from such metallic nanostructures is highly dependent on viewing angle and the color appearance can change with color of the viewing background. Plasmonic color generation is a rapidly emerging research area with potential advantages over conventional pigment printing technology including higher printing resolution and robustness, greater compatibility for integration and functionalization, and reduced resource requirements [Mudachathi and Tanaka 2017; Zhu et al. 2017]. Structural color from plasmonic nanostructures has already been used to improve security measures in currency notes and credit cards [Lee et al. 2018].	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338594	Wei Sen Loi, Kenneth J. Chau
Visualizing expert motion for guidance in a VR ski simulator	While humans are quite good at copying motions from others, it is difficult to do so in a dynamic sport such as skiing. Hence, we propose a virtual reality ski training system, which visualizes prerecorded expert motion in different ways and enables users to learn by copying. The system is based on a commercial indoor ski simulator, a VR headset, and two VR trackers to capture the ski's motion. Users can control their skis on the virtual ski slope and improve their skills by following a digital avatar of the expert skier replayed in front of them. We investigate 3 types of visualizations for training: Graphs to visualize the angle of feet compared to the expert, periodic copies of the expert's pose to show the spatial and temporal motion of the key movements, and a more minimal ribbon-trace of the leading skier to point out the optimized trajectory.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338561	Takayuki Nozawa, Erwin Wu, Florian Perteneder, Hideki Koike
Volume-aware design of composite molds	We propose a novel technique for the automatic design of molds to cast highly complex shapes. The technique generates composite, two-piece molds. Each mold piece is made up of a hard plastic shell and a flexible silicone part. Thanks to the thin, soft, and smartly shaped silicone part, which is kept in place by a hard plastic shell, we can cast objects of unprecedented complexity. An innovative algorithm based on a volumetric analysis defines the layout of the internal cuts in the silicone mold part. Our approach can robustly handle thin protruding features and intertwined topologies that have caused previous methods to fail. We compare our results with state of the art techniques, and we demonstrate the casting of shapes with extremely complex geometry.	https://dl.acm.org/authorize?N688165	Thomas Alderighi, Luigi Malomo, Daniela Giorgi, Bernd Bickel, Paolo Cignoni, Nico Pietroni
Vox-cells: voxel-based visualization of volume data for enhanced understanding and exploration in virtual reality (VR)	We present Vox-Cells, a volume visualization framework designed for real-time volume investigation and exploration. We seek to treat data as a first-class citizen with a 1:1 relationship between the data and its corresponding representation. CPU-GPU transfer is minimized, and novel approaches to volume construction and lighting are explored in order to maximize performance for deployment on consumer grade Virtual Reality (VR) Head-mounted displays (HMD).	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338551	Rowan T. Hughes, Campbell Strong, John McGhee
Voxel printing using procedural art-directable technologies	A procedural art-directable workflow is developed for voxel 3D printing using existing digital effects technologies. Customised for the Stratasys J750's unique materials, the system produces large-scale prosthetic eyes as a case study for film and display work.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338555	Tor Robinson, William Furneaux
Wallpaper pattern alignment along garment seams	Despite recent developments towards on-demand, individualized garment design and fabrication, the majority of processes in the fashion industry are still inefficient and heavily dependent on manual work. A significant amount of recent research in this area has been focused on supporting designers to digitally create sewing patterns and shapes, but there is little work on textured fabrics. Aligning textile patterns like stripes or plaid along garment seams requires an experienced tailor and is thus reserved only for expensive, high-end garments. We present an interactive algorithm for automatically aligning repetitive textile patterns along seams for a given garment, allowing a user to make design choices at each step of our pipeline. Our approach is based on the 17 wallpaper groups and the symmetries they exhibit. We exploit these symmetries to optimize the alignment of the sewing pattern with the textured fabric for each of its pieces, determining where to cut the fabric. We optionally alter the sewing pattern slightly for a perfect fit along seams, without visibly changing the 3D shape of the garment. The pieces can then be cut automatically by a CNC or laser cutter. Our approach fits within the pipeline of digital garment design, eliminating the difficult, manual step of aligning and cutting the garment pieces by hand.	https://dl.acm.org/authorize?N688017	Katja Wolff, Olga Sorkine-Hornung
Warp-and-project tomography for rapidly deforming objects	Computed tomography has emerged as the method of choice for scanning complex shapes as well as interior structures of stationary objects. Recent progress has also allowed the use of CT for analyzing deforming objects and dynamic phenomena, although the deformations have been constrained to be either slow or periodic motions. In this work we improve the tomographic reconstruction of time-varying geometries undergoing faster, non-periodic deformations. Our method uses a warp-and-project approach that allows us to introduce an essentially continuous time axis where consistency of the reconstructed shape with the projection images is enforced for the specific time and deformation state at which the image was captured. The method uses an efficient, time-adaptive solver that yields both the moving geometry as well as the deformation field. We validate our method with extensive experiments using both synthetic and real data from a range of different application scenarios.	https://dl.acm.org/authorize?N688031	Guangming Zang, Ramzi Idoughi, Ran Tao, Gilles Lubineau, Peter Wonka, Wolfgang Heidrich
Wave-based non-line-of-sight imaging using fast 	Imaging objects outside a camera's direct line of sight has important applications in robotic vision, remote sensing, and many other domains. Time-of-flight-based non-line-of-sight (NLOS) imaging systems have recently demonstrated impressive results, but several challenges remain. Image formation and inversion models have been slow or limited by the types of hidden surfaces that can be imaged. Moreover, non-planar sampling surfaces and non-confocal scanning methods have not been supported by efficient NLOS algorithms. With this work, we introduce a wave-based image formation model for the problem of NLOS imaging. Inspired by inverse methods used in seismology, we adapt a frequency-domain method, migration, for solving the inverse NLOS problem. Unlike existing NLOS algorithms, migration is both fast and memory efficient, it is robust to specular and other complex reflectance properties, and we show how it can be used with non-confocally scanned measurements as well as for non-planar sampling surfaces. migration is more robust to measurement noise than alternative methods, generally produces better quality reconstructions, and is easy to implement. We experimentally validate our algorithms with a new NLOS imaging system that records room-sized scenes outdoors under indirect sunlight, and scans persons wearing retroreflective clothing at interactive rates.	https://dl.acm.org/authorize?N688161	David B. Lindell, Gordon Wetzstein, Matthew O'Toole
Wearable soft pneumatic ring with multi-mode controlling for rich haptic effects	Common mechanical actuators for haptic feedback are generally dedicated to creating single kind of feedback, e.g., vibrotactile only, the pressure only, or shear force only, [Choi and Kuchenbecker 2013; Girard et al. 2016; Pacchierotti et al. 2017]. This is against the fact that highly realistic fully immersive VR/AR sometimes requires rather complete and rich multi-mode haptic feedback. For instance, when rubbing your finger on a wooden desk, the fingertip simultaneously senses both the high-frequency vibration due to the roughness of the surface texture and the quasi-static pressure due to pushing force, and your brain combines them to feel it as a wooden desk. The lack of any of the involved physical signal may seriously deteriorate the realism. This may be one of the reasons why current haptic interface technology for VR/AR environments is not at the same level as visual interfaces.	https://dl.acm.org/doi/abs/abs/10.1145/3306214.3338613	Aishwari Talhan, Hwangil Kim, Seokhee Jeon
Weaving geodesic foliations	We study discrete geodesic foliations of surfaces---foliations whose leaves are all approximately geodesic curves---and develop several new variational algorithms for computing such foliations. Our key insight is a relaxation of vector field integrability in the discrete setting, which allows us to optimize for curl-free unit vector fields that remain well-defined near singularities and robustly recover a scalar function whose gradient is well aligned to these fields. We then connect the physics governing surfaces woven out of thin ribbons to the geometry of geodesic foliations, and present a design and fabrication pipeline for approximating surfaces of arbitrary geometry and topology by triaxially-woven structures, where the ribbon layout is determined by a geodesic foliation on a sixfold branched cover of the input surface. We validate the effectiveness of our pipeline on a variety of simulated and fabricated woven designs, including an example for readers to try at home.	https://dl.acm.org/authorize?N688089	Josh Vekhter, Jiacheng Zhuo, Luisa F Gil Fandino, Qixing Huang, Etienne Vouga
What time is it?: efficient and robust FX retiming workflow for spies in disguise	"Since the adoption of Global Illumination on and Renderman RIS on , Pixar has pushed closer and closer to photorealism with physically-based lighting and shading. With each show, Lighting artists have needed to create, organize, and creatively balance more and more light sources of increasing complexity. Because Pixar has also traditionally worked with a ""fixed"" camera exposure, the light colors and intensities chosen by the artist would also drive the overall brightness of the final image. , which takes place in both an antiques mall and a traveling carnival, was a challenge to this workflow. The large variety of light sources in the antiques mall would be difficult to group into uniform categories; complex light animation on carnival rides required light intensities driven by upstream assets; and seeing the same lights under day and night illumination meant that production-wide choices for light intensities could not be made under a single, fixed exposure. We needed assets that behaved like in the real world and when placed together in a scene. We developed a method on for tagging modeled assets with physical light properties that would automatically be converted into functioning light sources in a shot by a script called Bakelite. This pipeline gave the Lighting department more time for creative iteration with minimal setup, allowed pre-lighting visualization of shots by upstream departments, and ensured a final image that was both rich in surface detail and also accurate in HDR."	https://dl.acm.org/authorize?N689316	Yaa-Lirng Tu, Tim Babb, Hosuk Chang, Bill Reeves
Why you should(n't) build your own game engine	Developing a modern game engine from the ground up has become an increasingly rare opportunity, and with good reason. It is a costly commitment and coupled with the existing technologies readily available at reasonable pricing models, it is a hard sell for any startup to take on such a burden. This paper focuses on a few key issues when developing such a technology base to serve as both a guide and a warning. Rather than discussing the implementation details and features of the engine, the paper will delve into the importance of efficient workflows; the challenges of outsourcing, and finally the lessons learned from building the technology and a game that runs on it.	https://dl.acm.org/authorize?N689305	Andrés Rivela
Wild Love	While on a romantic getaway, Alan and Beverly cause a fatal accident. This crime won't remain unpunished...	https://dl.acm.org/authorize?N689356	Maryka Laudet
Wolves in the walls: chapter 2, it's all over	Not everything is as it seems when 8-year-old Lucy's imagination proves to be a reality. Based on the work by renowned author Neil Gaiman, Wolves in the Walls draws you into a storybook where only you can help Lucy discover what's truly hiding inside the walls of her house.	https://dl.acm.org/authorize?N689267	Peter Billington
Worldspace painting data visualization	We invented AR interaction for paintings where they have been historically static, taking greater advantage of AR's interactive nature to providing a compact and stable mobile experience that is both engaging and minimally intrusive to the art gallery experience. We provide context-specific information on parts of each painting.	https://dl.acm.org/authorize?N689268	Kyungjin Yoo, Dean Foster
X-Shells: a new class of deployable beam structures	We present , a new class of deployable structures formed by an ensemble of elastically deforming beams coupled through rotational joints. An X-shell can be assembled conveniently in a flat configuration from standard elastic beam elements and then deployed through force actuation into the desired 3D target state. During deployment, the coupling imposed by the joints will force the beams to twist and buckle out of plane to maintain a state of static equilibrium. This complex interaction of discrete joints and continuously deforming beams allows interesting 3D forms to emerge. Simulating X-shells is challenging, however, due to unstable equilibria at the onset of beam buckling. We propose an optimization-based simulation framework building on a discrete rod model that robustly handles such difficult scenarios by analyzing and appropriately modifying the elastic energy Hessian. This real-time simulation method forms the basis of a computational design tool for X-shells that enables interactive design space exploration by varying and optimizing design parameters to achieve a specific design intent. We jointly optimize the assembly state and the deployed configuration to ensure the geometric and structural integrity of the deployable X-shell. Once a design is finalized, we also optimize for a sparse distribution of actuation forces to efficiently deploy it from its flat assembly state to its 3D target state. We demonstrate the effectiveness of our design approach with a number of design studies that highlight the richness of the X-shell design space, enabling new forms not possible with existing approaches. We validate our computational model with several physical prototypes that show excellent agreement with the optimized digital models.	https://dl.acm.org/authorize?N688038	J. Panetta, M. Konaković-Luković, F. Isvoranu, E. Bouleau, M. Pauly
iMapper: interaction-guided scene mapping from monocular videos	Next generation smart and augmented reality systems demand a computational understanding of monocular footage that captures humans in physical spaces to reveal plausible object arrangements and human-object interactions. Despite recent advances, both in scene layout and human motion analysis, the above setting remains challenging to analyze due to regular occlusions that occur between objects and human motions. We observe that the between object arrangements and human actions is often strongly correlated, and hence can be used to help recover from these occlusions. We present iMapper, a data-driven method to identify such human-object interactions and utilize them to infer layouts of occluded objects. Starting from a monocular video with detected 2D human joint positions that are potentially noisy and occluded, we first introduce the notion of as space-time snapshots where informative human-object interactions happen. Then, we propose a global optimization to retrieve and fit interactions from a database to the detected salient interactions in order to best explain the input video. We extensively evaluate the approach, both quantitatively against manually annotated ground truth and through a user study, and demonstrate that iMapper produces plausible scene layouts for scenes with medium to heavy occlusion. Code and data are available on the project page.	https://dl.acm.org/authorize?N688047	Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin Yumer, Niloy J. Mitra
the bOnd	Far away in an alien forest, an ancient healer and her faithful beast seek a sacred glade. Here, she connects with the web of life --- and sees the world through others' eyes. But, there's an imbalance --- a dark presence. As the forest's guardian, she must confront it.	https://dl.acm.org/authorize?N689369	Olly Reid
