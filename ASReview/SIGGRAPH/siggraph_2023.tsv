title	abstract	url	authors
"""F O R M S"" - Creating new visual perceptions of dance movement through machine learning"	"""FORMS"" is a new digital art concept that combines the fields of dance movement and machine learning techniques, specifically human pose detection, to create a real-time and interactive visual experience. This project aims to explore the relationship between dance and visual art by creating a framework that generates abstract and literal visual models from the dancers' movements. The main objective of this project is to enhance the perception of dance movement by providing a new layer of visual composition. The proposed framework provides different visual forms based on human pose detection, creating a novel and real-time visual expression of the dance movement. The human pose detection model used in this project is based on state-of-the-art deep learning techniques, which analyze the positions and movements of different parts of the human body in real-time. This model allows the framework to capture movements of the dancers and translate them into unique visual forms. The case study showcases the potential of ""FORMS"" by demonstrating how professional young dancers can use the framework to enrich their performance and create new visual perceptions of dance movement. This study contributes to the cultivation of body awareness, understanding of the dance movement and overall enrichment of the art experience. The use of machine learning techniques showcases the potential of technology to enhance and expand the boundaries of artistic expression. The ""FORMS"" project is a novel and interdisciplinary approach that bridges the fields of art and technology, providing a new way to experience and perceive the dance movement."	https://dl.acm.org/doi/abs/10.1145/3588028.3603673	Maria Rita Nogueira, João Braz Simões, Paulo Menezes
3D Character Motion Authoring From Temporally-Sparse Photos	This paper presents a neural network-based learning approach that enables seamless generation of 3D human motion in-between photos to accelerate the process of 3D character motion authoring. This new approach allows users to freely edit (replace, insert, or delete) input photos and specify the transition length to generate a kinematically coherent sequence of 3D human poses and shapes in-between the given photos. We demonstrate through qualitative and subjective evaluations that our approach is capable of generating high-fidelity, natural 3D pose and shape transitions.	https://dl.acm.org/doi/abs/10.1145/3588028.3603656	Jen-Chun Lin, Wen-Li Wei
3D Gaussian Splatting for Real-Time Radiance Field Rendering	Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.	https://dl.acm.org/doi/abs/10.1145/3592433	Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, George Drettakis
3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models	We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation, and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation. Code: https://1zb.github.io/3DShape2VecSet/.	https://dl.acm.org/doi/abs/10.1145/3592442	Biao Zhang, Jiapeng Tang, Matthias Nießner, Peter Wonka
50 Years of Changes–How to Brace Yourself!: SIGGRAPH 2023 Retrospective Panel	Having lived through the 50 years of changes, the panelists attempt to put them in calibrated perspective, not merely for the sake of fond reminiscence–and fun!–but as a guide to those who face the looming future changes.	https://dl.acm.org/doi/abs/10.1145/3587422.3597996	James F. Blinn, James D. Foley, Donald P. Greenberg, Alvy Ray Smith, Andries van Dam, John Turner Whitted
A Calling. From the Desert. To the Sea	Fearing that her younger sister will be wed off Just like she was, Ahlam lures Yasmin to run away with her by telling a story of a mythical sea that lies beyond the desolate desert and its mountains. Yasmin's childish curiosity overturns her father's apprehension and his warnings of the monster that resides beyond the borders of their house. In-search of this mystical sea, the two sisters journey away far from their home, predetermined fate and secluded life with their father. Along the way, young Yasmin is forced to face her inner demons and further her understanding of the world she lives in.	https://dl.acm.org/doi/abs/10.1145/3577024.3588640	Murad Abu Eisheh, Mario Bertsch
A Contact Proxy Splitting Method for Lagrangian Solid-Fluid Coupling	We present a robust and efficient method for simulating Lagrangian solid-fluid coupling based on a new operator splitting strategy. We use variational formulations to approximate fluid properties and solid-fluid interactions, and introduce a unified two-way coupling formulation for SPH fluids and FEM solids using interior point barrier-based frictional contact. We split the resulting optimization problem into a fluid phase and a solid-coupling phase using a novel time-splitting approach with augmented , and propose efficient custom linear solvers. Our technique accounts for fluids interaction with nonlinear hyperelastic objects of different geometries and codimensions, while maintaining an algorithmically guaranteed non-penetrating criterion. Comprehensive benchmarks and experiments demonstrate the efficacy of our method.	https://dl.acm.org/doi/abs/10.1145/3592115	Tianyi Xie, Minchen Li, Yin Yang, Chenfanfu Jiang
A Demonstration of Morphing Identity: Exploring Self-Other Identity Continuum through Interpersonal Facial Morphing	We explored continuous changes in self-other identity by designing an interpersonal facial morphing experience where the facial images of two users are blended and then swapped over time. To explore this with diverse social relationships, we conducted qualitative and quantitative investigations through public exhibitions. We found that there is a window of self-identification as well as a variety of interpersonal experiences in the facial morphing process. From these insights, we synthesized a Self-Other Continuum represented by a sense of agency and facial identity. This continuum has implications in terms of the social and subjective aspects of interpersonal communication, which enables further scenario design and could complement findings from research on interactive devices for remote communication.	https://dl.acm.org/doi/abs/10.1145/3588037.3595394	Kye Shimizu, Santa Naruse, Jun Nishida, Shunichi Kasahara
A Full-Wave Reference Simulator for Computing Surface Reflectance	Computing light reflection from rough surfaces is an important topic in computer graphics. Reflection models developed based on geometric optics fail to capture wave effects such as diffraction and interference, while existing models based on physical optics approximations give erroneous predictions under many circumstances (e.g. when multiple scattering from the surface cannot be ignored). We present a scalable 3D full-wave simulator for computing reference solutions to surface scattering problems, which can be used to evaluate and guide the development of approximate models for rendering. We investigate the range of validity for some existing wave optics based reflection models; our results confirm these models for low-roughness surfaces but also show that prior rendering methods do not accurately predict the scattering behavior of some types of surfaces. Our simulator is based on the boundary element method (BEM) and accelerated using the adaptive integral method (AIM), and is implemented to execute on modern GPUs. We demonstrate the simulator on domains up to 60 × 60 × 10 wavelengths, involving surface samples with significant height variations. Furthermore, we propose a new system for efficiently computing BRDF values for large numbers of incident and outgoing directions at once, by combining small simulations to characterize larger areas. Our simulator will be released as an open-source toolkit for computing surface scattering.	https://dl.acm.org/doi/abs/10.1145/3592414	Yunchen Yu, Mengqi Xia, Bruce Walter, Eric Michielssen, Steve Marschner
A Gentle Introduction to ReSTIR Path Reuse in Real-Time	"In recent years, reservoir-based spatiotemporal importance resampling (ReSTIR) algorithms appeared out of nowhere to take parts of the realtime rendering community by storm, with sample reuse speeding direct lighting from millions of dynamic lights [1], diffuse multi-bounce lighting [2], participating media [3], and even complex global illumination paths [4]. Highly optimized variants (e.g. [5]) can give 100x efficiency improvement over traditional ray- and path-tracing methods; this is key to achieve 30 or 60 Hz framerates. In production engines, tracing even one ray or path per pixel may only be feasible on the highest-end systems, so maximizing image quality per sample is vital. ReSTIR builds on the math in Talbot et al.'s [6] resampled importance sampling (RIS), which previously was not widely used or taught, leaving many practitioners missing key intuitions and theoretical grounding. A firm grounding is vital, as seemingly obvious ""optimizations"" arising during ReSTIR engine integration can silently introduce conditional probabilities and dependencies that, left ignored, add uncontrollable bias to the results. In this course, we plan to: 1. Provide concrete motivation and intuition for why ReSTIR works, where it applies, what assumptions it makes, and the limitations of today's theory and implementations; 2. Gently develop the theory, targeting attendees with basic Monte Carlo sampling experience but without prior knowledge of resampling algorithms (e.g., Talbot et al. [6]); 3. Give explicit algorithmic samples and pseudocode, pointing out easily-encountered pitfalls when implementing ReSTIR; 4. Discuss actual game integrations, highlighting the gotchas, challenges, and corner cases we encountered along the way, and highlighting ReSTIR's practical benefits."	https://dl.acm.org/doi/abs/10.1145/3587423.3595511	Chris Wyman, Markus Kettunen, Daqi Lin, Benedikt Bitterli, Cem Yuksel, Wojciech Jarosz, Pawel Kozlowski
A Practical Walk-on-Boundary Method for Boundary Value Problems	We introduce the (WoB) method for solving boundary value problems to computer graphics. WoB is a grid-free Monte Carlo solver for certain classes of second order partial differential equations. A similar Monte Carlo solver, the walk-on-spheres (WoS) method, has been recently popularized in computer graphics due to its advantages over traditional spatial discretization-based alternatives. We show that WoB's intrinsic properties yield further advantages beyond those of WoS. Unlike WoS, WoB naturally supports various boundary conditions (Dirichlet, Neumann, Robin, and mixed) for both interior and exterior domains. WoB builds upon boundary integral formulations, and it is mathematically more similar to light transport simulation in rendering than the random walk formulation of WoS. This similarity between WoB and rendering allows us to implement WoB on top of Monte Carlo ray tracing, and to incorporate advanced rendering techniques (e.g., bidirectional estimators with multiple importance sampling, the virtual point lights method, and Markov chain Monte Carlo) into WoB. WoB does not suffer from the intrinsic bias of WoS near the boundary and can estimate solutions precisely on the boundary. Our numerical results highlight the advantages of WoB over WoS as an attractive alternative to solve boundary value problems based on Monte Carlo.	https://dl.acm.org/doi/abs/10.1145/3592109	Ryusuke Sugimoto, Terry Chen, Yiti Jiang, Christopher Batty, Toshiya Hachisuka
A Practical Wave Optics Reflection Model for Hair and Fur	Traditional fiber scattering models, based on ray optics, are missing some important visual aspects of fiber appearance. Previous work [Xia et al. 2020] on wave scattering from ideal extrusions demonstrated that diffraction produces strong forward scattering and colorful effects that are missing from ray-based models. However, that work was unable to include some important surface characteristics such as surface roughness and tilted cuticle scales, which are known to be important for fiber appearance. In this work, we take an important step to study wave effects from rough fibers with arbitrary 3D microgeometry. While the full-wave simulation of realistic 3D fibers remains intractable, we developed a 3D wave optics simulator based on a physical optics approximation, using a GPU-based hierarchical algorithm to greatly accelerate the calculation. It simulates surface reflection and diffractive scattering, which are present in all fibers and typically dominate for darkly pigmented fibers. The simulation provides a detailed picture of first order scattering, but it is not practical to use for production rendering as this would require tabulation per fiber geometry. To practically handle geometry variations in the scene, we propose a model based on wavelet noise, capturing the important statistical features in the simulation results that are relevant for rendering. Both our simulation and practical model show similar granular patterns to those observed in optical measurement. Our compact noise model can be easily combined with existing scattering models to render hair and fur of various colors, introducing visually important colorful glints that were missing from all previous models.	https://dl.acm.org/doi/abs/10.1145/3592446	Mengqi Xia, Bruce Walter, Christophe Hery, Olivier Maury, Eric Michielssen, Steve Marschner
A Proposal of Acquiring and Analyzing Method for Distributed Litter on the Street using Smartphone Users as Passive Mobility Sensors	With increased environmental protection activities, smartphone-enabled cleaning activities to deter street littering are gaining attention. We propose a method to analyze litter-on-road images captured by a smartphone camera mounted on a bicycle for users who do not require conscious care (Fig. 1). First, the user mounts the smartphone on a bicycle and starts the developed application, which creates a still image by capturing videos. The still images were then categorized using machine learning, and the type of trash was annotated in the images. Finally, to predict the distribution of trash, the probability of its influence on the environment, such as convenience stores and bars, was calculated using the machine learning model. This paper discusses our developed system's efficacy for acquiring and analyzing methods on the road. As a fast effort, we verify the accuracy of tagging PET bottles, cans, food trays, and masks using a learning model generated by Detectron2.	https://dl.acm.org/doi/abs/10.1145/3588028.3603684	Hikaru Hagura, Ryuta Yamaguchi, Tomoki Yoshihisa, Shinji Shimojo, Yukiko Kawai
A Software-Agnostic Small Studio and Education Pipeline	Building a production pipeline from scratch that supports iteration and creative control, for a university or small studio, can be difficult to achieve with smaller budgets and major time constraints. We developed a USD pipeline that allows our production to work seamlessly between multiple production software packages, playing to the strengths of each. With these tools, we were able to build our entire animation pipeline in just months and adapt it on the fly to the needs of our production. This pipeline also allowed artists to choose different software based on the strength of each package, all while referencing the same asset and layout files.	https://dl.acm.org/doi/abs/10.1145/3587424.3595578	Zach Wood, Anna Hales, Seth Holladay, Craig Van Dyke, Matt Minson
A Temporal Coherent Topology Optimization Approach for Assembly Planning of Bespoke Frame Structures	We present a computational framework for planning the assembly sequence of bespoke frame structures. Frame structures are one of the most commonly used structural systems in modern architecture, providing resistance to gravitational and external loads. Building frame structures requires traversing through several partially built states. If the assembly sequence is planned poorly, these partial assemblies can exhibit substantial deformation due to self-weight, slowing down or jeopardizing the assembly process. Finding a good assembly sequence that minimizes intermediate deformations is an interesting yet challenging combinatorial problem that is usually solved by heuristic search algorithms. In this paper, we propose a new optimization-based approach that models sequence planning using a series of topology optimization problems. Our key insight is that enforcing temporal coherent constraints in the topology optimization can lead to sub-structures with small deformations while staying consistent with each other to form an assembly sequence. We benchmark our algorithm on a large data set and show improvements in both performance and computational time over greedy search algorithms. In addition, we demonstrate that our algorithm can be extended to handle assembly with static or dynamic supports. We further validate our approach by generating a series of results in multiple scales, including a real-world prototype with a mixed reality assistant using our computed sequence and a simulated example demonstrating a multi-robot assembly application.	https://dl.acm.org/doi/abs/10.1145/3592102	Ziqi Wang, Florian Kennel-Maushart, Yijiang Huang, Bernhard Thomaszewski, Stelian Coros
AI-Mediated 3D Video Conferencing	We present an AI-mediated 3D video conferencing system that can reconstruct and autostereoscopically display a life-sized talking head using consumer-grade compute resources and minimal capture equipment. Our 3D capture uses a novel 3D lifting method that encodes a given 2D input into an efficient triplanar neural representation of the user, which can be rendered from novel viewpoints in real-time. Our AI-based techniques drastically reduce the cost for 3D capture, while providing a high-fidelity 3D representation on the receiver's end at the cost of traditional 2D video streaming. Additional advantages of our AI-based approach include the ability to accommodate both photorealistic and stylized avatars, and the ability to enable mutual eye contact in multi-directional video conferencing. We demonstrate our system using a tracked stereo display for a personal viewing experience as well as a lightfield display for a room-scale multi-viewer experience.	https://dl.acm.org/doi/abs/10.1145/3588037.3595385	Michael Stengel, Koki Nagano, Chao Liu, Matthew Chan, Alex Trevithick, Shalini De Mello, Jonghyun Kim, David Luebke
Action-Origami Inspired Haptic Devices for Virtual Reality	Origami offers an innovative way to implement haptic interaction with minimum actuation, particularly in immersive encountered-type haptics and robotics. This paper presents two novel action-origami-inspired haptic devices for Virtual Reality (VR). The Zipper Flower Tube is a rigid-foldable origami structure that can provide different stiffness sensations to simulate the elastic response of a material. The Shiftly is a shape-shifting haptic display that employs origami to enable a real-time experience of different shapes and edges of virtual objects or the softness of materials. The modular approach of our action origami haptic devices provides a high-fidelity, energy-efficient and low-cost solution for interacting with virtual materials and objects in VR.	https://dl.acm.org/doi/abs/10.1145/3588037.3595393	Khrystyna Vasylevska, Tobias Batik, Hugo Brument, Kiumars Sharifmoghaddam, Georg Nawratil, Emanuel Vonach, Soroosh Mortezapoor, Hannes Kaufmann
Actualities: Seamless Live Performance with the Physical and Virtual Audiences in Multiverse	During the pandemic, the popularity of VR concerts increased, providing the audience with a novel experience of watching performances where performers and online attendees typically exist in different spaces and times. However, as the pandemic recovered gradually, people returned to live performances physically due to the diminished sense of co-presence experienced in VR concerts. We present Actualities, a seamless live performance consisting of onsite and online form with a multiverse interaction to create an engaging experience for all participants. By utilizing various sensors, we capture signals from the performance venue and digitalize all onsite elements into virtual scenes. The visual content is projected onto screens for the onsite audience, while being simultaneously broadcast via live-streaming to the online audience.	https://dl.acm.org/doi/abs/10.1145/3588027.3595594	Ke-Fan Lin, Yu-Chih Chou, Yu-Hsiang Weng, Yvone Tsai Chen, Zin-Yin Lim, Chi-Po Lin, Ping-Hsuan Han, Tse-Yu Pan
Airbnb Aircover	"Airbnb's new ""Aircover"" protection program has your backside (and money) covered during your booking experience and stay."	https://dl.acm.org/doi/abs/10.1145/3577024.3589022	Justin Cone
Alice in Gravityland: Augmenting Gravity Experiences with Around-the-Head Vibrotactile Feedback and Illusory Tactile Motion	Alice in Gravityland is a VR adventure exploring three gravity experiences with novel, around-the-head vibrotactile feedback using illusory tactile motion. Players are able to 1) change the direction of gravity, 2) navigate through zero gravity, and 3) defy gravity as they walk on walls. The haptic feedback helps improve players' sense of directionality to improve immersion when experiencing gravity events. Inspired by Lewis Carroll's Alice's Adventures in Wonderland (1865), the game invites players to alter gravity to solve puzzles and experience gravity in a unique way through this multi-sensory VR adventure.	https://dl.acm.org/doi/abs/10.1145/3588027.3595591	Luca E. Taglialatela, Chiao-Ju Chang, Shu-Wen Chen, Yi-Han Chang, Chang-Min Chen, Zih-Huei Yang, Tsung-Min Lin, Shih-Yu Ma, Neng-Hao Yu, Mike Y. Chen
An Extensible, Data-Oriented Architecture for High-Performance, Many-World Simulation	Training AI agents to perform complex tasks in simulated worlds requires millions to billions of steps of experience. To achieve high performance, today's fastest simulators for training AI agents adopt the idea of batch simulation: using a single simulation engine to simultaneously step many environments in parallel. We introduce a framework for productively authoring novel training environments (including custom logic for environment generation, environment time stepping, and generating agent observations and rewards) that execute as high-performance, GPU-accelerated batched simulators. Our key observation is that the entity-component-system (ECS) design pattern, popular for expressing CPU-side game logic today, is also well-suited for providing the structure needed for high-performance batched simulators. We contribute the first fully-GPU accelerated ECS implementation that natively supports batch environment simulation. We demonstrate how ECS abstractions impose structure on a training environment's logic and state that allows the system to efficiently manage state, amortize work, and identify GPU-friendly coherent parallel computations within and across different environments. We implement several learning environments in this framework, and demonstrate GPU speedups of two to three orders of magnitude over open source CPU baselines and 5-33× over strong baselines running on a 32-thread CPU. An implementation of the OpenAI hide and seek 3D environment written in our framework, which performs rigid body physics and ray tracing in each simulator step, achieves over 1.9 million environment steps per second on a single GPU.	https://dl.acm.org/doi/abs/10.1145/3592427	Brennan Shacklett, Luc Guy Rosenzweig, Zhiqiang Xie, Bidipta Sarkar, Andrew Szot, Erik Wijmans, Vladlen Koltun, Dhruv Batra, Kayvon Fatahalian
An Interactive Showcase of RCSketch: Sketch, Build, and Control Your Dream Vehicles	We present RCSketch, the award-winning interactive system that lets anyone sketch their dream vehicles in 3D, build moving structures of those vehicles, and control them from multiple viewpoints. Visitors to this interactive showcase are able to use our system and design vehicles of their own and perform a wide variety of realistic movements across the vast digital landscape onboard their vehicles.	https://dl.acm.org/doi/abs/10.1145/3588037.3595398	Hanbit Kim, Jaeho Sung, Joon Hyub Lee, Seok-Hyung Bae
An Introduction to Quantum Computing	Quantum computing is a radically new approach to creating algorithms and programs. By exploiting the unusual behavior of quantum objects, this new technology invites us to re-imagine the computer graphics methods we know and love in revolutionary new ways. This course presents a math-free introduction to quantum computing.	https://dl.acm.org/doi/abs/10.1145/3587423.3595538	Andrew Glassner
An investigation of changes in taste perception by varying polygon resolution of foods in virtual environments	In recent years, metaverse has received considerable attention. We believe that as this technology develops, humanity can dine in a virtual space while maintaining a sense of immersion. Therefore, we investigated whether the taste of food is influenced by the number of polygons of CG models using VR/AR technology. We created CG models and overlaid the image onto the actual food via HMD. Then the subjects consumed the food which CG image overlaid and answered a questionnaire. Results showed that the higher the number of polygons, the less hardness was perceived and the toon-like model was more likely to affect the taste.	https://dl.acm.org/doi/abs/10.1145/3588028.3603689	Taiyo Taguchi, Yurie Watanabe, Tomokazu Ishikawa
Anatomically Detailed Simulation of Human Torso	"Many existing digital human models approximate the human skeletal system using rigid bodies connected by rotational joints. While the simplification is considered acceptable for legs and arms, it significantly lacks fidelity to model rich torso movements in common activities such as dancing, Yoga, and various sports. Research from biomechanics provides more detailed modeling for parts of the torso, but their models often operate in isolation and are not fast and robust enough to support computationally heavy applications and large-scale data generation for full-body digital humans. This paper proposes a new torso model that aims to achieve high fidelity both in perception and in functionality, while being computationally feasible for simulation and optimal control tasks. We build a detailed human torso model consisting of various anatomical components, including facets, ligaments, and intervertebral discs, by coupling efficient finite-element and rigid-body simulations. Given an existing motion capture sequence without dense markers placed on the torso, our new model is able to recover the underlying torso bone movements. Our method is remarkably robust that it can be used to automatically ""retrofit"" the entire Mixamo motion database of highly diverse human motions without user intervention. We also show that our model is computationally efficient for solving trajectory optimization of highly dynamic full-body movements, without relying any reference motion. Physiological validity of the model is validated against established literature."	https://dl.acm.org/doi/abs/10.1145/3592425	Seunghwan Lee, Yifeng Jiang, C. Karen Liu
ArrangementNet: Learning Scene Arrangements for Vectorized Indoor Scene Modeling	We present a novel vectorized indoor modeling approach that converts point clouds into building information models (BIM) with concise and semantically segmented polygonal meshes. Existing methods detect planar shapes and connect them to complete the scene. Some focus on floor plan reconstruction as a simplified problem to better analyze connectivity between planes of floors and walls. However, the connectivity analysis is still challenging and ill-posed with incomplete point clouds as input. We propose ArrangementNet to estimate scene arrangements from an incomplete point cloud, which we can easily convert into a BIM model. ArrangementNet is a novel graph neural network that consumes noisy over-partitioned initial arrangements extracted through non-learning techniques and outputs high-quality scene arrangement. The core of ArrangementNet is an extended graph convolution that leverages co-linear and co-face relationships in the arrangement and improves the quality of prediction in complex scenes. We apply ArrangementNet to improve floor plan and ceiling arrangements and enrich them with semantic objects as scene arrangements for scene generation. Our approach faithfully models challenging scenes obtained from laser scans or multiview stereo and shows significant improvement in BIM model reconstruction compared to the state-of-the-art. Our code is available at https://github.com/zssjh/ArrangementNet.	https://dl.acm.org/doi/abs/10.1145/3592122	Jingwei Huang, Shanshan Zhang, Bo Duan, Yanfeng Zhang, Xiaoyang Guo, Mingwei Sun, Li Yi
Art Simulates Life: 3D Visualization Takes Pediatric Hospitalist Simulations to the Next Level	Children's National Hospital is a leading pediatric teaching hospital with medical students, residency programs, fellowships, and research initiatives. We develop virtual 3D patients to improve medical training by simulating plausible, life-threatening scenarios in infants and children for Pediatric Hospitalists. Medical simulation is widely used to enhance the readiness of medical professionals [Davila and Price 2023; Motola et al. 2013]. Children's National's Division of Pediatric Hospital Medicine's Resuscitation Simulation Team (HRST) leads training for attending Hospitalists responsible for the inpatient care of sick and injured children across our entire regional network. Our 3D patient simulations provide realistic depictions of nuanced physical findings that are essential to identify uncommon, yet life threatening medical conditions in children. 3D models applicable for pediatric simulations are underrepresented compared to those used for adult patients.	https://dl.acm.org/doi/abs/10.1145/3588028.3603662	Holly Bloom, Kevin Creamer, Lana Ismail
Asymmetrical VR for Education	In collocated VR classes, instructors need to guide their students, while also remaining aware of the physical environment in order to ensure students' safety. It is hard to do both simultaneously. We present a system that utilizes hand-held devices for non-VR instructors, enabling them to explore VR content and interact with students who are fully immersed in VR. The instructor can observe the VR environment or switch between different students' first-person views by using commonly available hand-held devices, such as smartphones and tablets. The instructor can also use hand-held devices to interact with the VR world itself. The students can see the real-time video stream of the physical environment as well as a video stream of the instructor. The system enables seamless communication and collaboration, thereby helping to create a better and richer educational experience for VR classes.	https://dl.acm.org/doi/abs/10.1145/3588027.3595600	Keru Wang, Zhu Wang, Ken Perlin
Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models	Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of , where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes ( , colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of , where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed , we guide the model to refine the cross-attention units to to all subject tokens in the text prompt and strengthen --- or --- their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts. Code is available at our project page: https://attendandexcite.github.io/Attend-and-Excite/.	https://dl.acm.org/doi/abs/10.1145/3592116	Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or
Augmented Haptic VR Experience Combining Two Weight-Shifting Versatile Controllers	We designed a VR controller to integrate experimental haptic technology into a practical controller. The device consists of two independent controllers, each with a weight-shifting module that can provide vibration, impact, and shape perception yet is sufficiently compact to be handled as a conventional commodity controller. Combining two controllers allows the device to be held differently for various applications.	https://dl.acm.org/doi/abs/10.1145/3588037.3595399	Yuhu Liu, Yuri Ishikawa, Yohei Fukuma, Yusuke Nakagawa
Avatar: The Way of Water	Thirteen years after the original film, audiences can be taken back to the world of Pandora and experience the emotional narrative of Avatar the Way of Water, depicted through the expressive nature of the Na'vi and vibrant and immersive environments.	https://dl.acm.org/doi/abs/10.1145/3577024.3588892	Eric Saindon
AvatarReX: Real-time Expressive Full-body Avatars	We present AvatarReX, a new method for learning NeRF-based full-body avatars from video data. The learnt avatar not only provides expressive control of the body, hands and the face together, but also supports real-time animation and rendering. To this end, we propose a compositional avatar representation, where the body, hands and the face are separately modeled in a way that the structural prior from parametric mesh templates is properly utilized without compromising representation flexibility. Furthermore, we disentangle the geometry and appearance for each part. With these technical designs, we propose a dedicated deferred rendering pipeline, which can be executed at a real-time framerate to synthesize high-quality free-view images. The disentanglement of geometry and appearance also allows us to design a two-pass training strategy that combines volume rendering and surface rendering for network training. In this way, patch-level supervision can be applied to force the network to learn sharp appearance details on the basis of geometry estimation. Overall, our method enables automatic construction of expressive full-body avatars with real-time rendering capability, and can generate photo-realistic images with dynamic details for novel body motions and facial expressions.	https://dl.acm.org/doi/abs/10.1145/3592101	Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, Yebin Liu
B-rep Matching for Collaborating Across CAD Systems	Large Computer-Aided Design (CAD) projects usually require collaboration across many different CAD systems as well as applications that interoperate with them for manufacturing, visualization, or simulation. A fundamental barrier to such collaborations is the ability to refer to parts of the geometry (such as a specific face) robustly under geometric and/or topological changes to the model. Persistent referencing schemes are a fundamental aspect of most CAD tools, but models that are shared across systems cannot generally make use of these internal referencing mechanisms, creating a challenge for collaboration. In this work, we address this issue by developing a novel learning-based algorithm that can automatically find correspondences between two CAD models using the standard representation used for sharing models across CAD systems: the Boundary-Representation (B-rep). Because our method works directly on B-reps it can be generalized across different CAD applications enabling collaboration.	https://dl.acm.org/doi/abs/10.1145/3592125	Benjamin Jones, James Noeckel, Milin Kodnongbua, Ilya Baran, Adriana Schulz
BStick: Hand-held Haptic Controller for Virtual Reality Applications	This study proposes Bstick-Mark2, a handheld virtual reality (VR) haptic controller that monitors and controls input from five fingers in real-time with pressure sensors and linear motors. Bstick-Mark2 is designed and fabricated to enable users to use both hands along with a head-mounted display (HMD) while freely roaming based on Bluetooth technologies. When a user holds the haptic controller with fingers, the data input from five pressure sensors is transmitted to microcontroller unit (MCU) to independently control the movements of five linear motors. The pressure and position data of linear motors are sent to a computer connected to a VR display through a Bluetooth module embedded in the controller and utilized in interaction with a virtual object and virtual hand movements using the Unity game engine. Bstick-Mark2 can withstand 22 N of force per finger to maintain the pressing force of a male's finger and is compact to enable users to easily handle using their hands. It enables to make sensations of grabbing and controlling while interacting with VR content.	https://dl.acm.org/doi/abs/10.1145/3588028.3603654	Ginam Ko, Jaeseok Yoon, Byungseok Jung, SangHun Nam
Beyond Chainmail: Computational Modeling of Discrete Interlocking Materials	We present a method for computational modeling, mechanical characterization, and macro-scale simulation of discrete interlocking materials (DIM)---3D-printed chainmail fabrics made of quasi-rigid interlocking elements. Unlike conventional elastic materials for which deformation and restoring force are directly coupled, the mechanics of DIM are governed by contacts between individual elements that give rise to anisotropic deformation constraints. To model the mechanical behavior of these materials, we propose a computational approach that builds on three key components. ( ): we explore the space of feasible deformations using native-scale simulations at the per-element level. ( ): based on this simulation data, we introduce the concept of strain-space boundaries to represent deformation limits for in- and out-of-plane deformations, and ( ): we use the strain-space boundaries to drive an efficient macro-scale simulation model based on homogenized deformation constraints. We evaluate our method on a set of representative discrete interlocking materials and validate our findings against measurements on physical prototypes.	https://dl.acm.org/doi/abs/10.1145/3592112	Pengbin Tang, Stelian Coros, Bernhard Thomaszewski
Blended Latent Diffusion	The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space and eliminating the need for resource-intensive CLIP gradient calculations at each diffusion step. We first enable LDM to perform local image edits by blending the latents at each step, similarly to Blended Diffusion. Next we propose an optimization-based solution for the inherent inability of LDM to accurately reconstruct images. Finally, we address the scenario of performing local edits using thin masks. We evaluate our method against the available baselines both qualitatively and quantitatively and demonstrate that in addition to being faster, it produces more precise results.	https://dl.acm.org/doi/abs/10.1145/3592450	Omri Avrahami, Ohad Fried, Dani Lischinski
Blender’s Simulation Nodes: A workshop on creating a melting effect with Geometry Nodes in Blender	Geometry Nodes is a procedural modeling and animation system that has been part of Blender since 2021, with Blender 2.92. It was initially focused on set dressing, procedural modeling and initial hair grooming. However in Blender 3.6 similation nodes were finally added. The class will be a hands-on show and tell of Geometry Nodes with particular emphasis on the brand new simulation systems.	https://dl.acm.org/doi/abs/10.1145/3588029.3599739	Dalai Felinto, Simon Thommes
BodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer	Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system is trained with either the Trinity speech-gesture dataset or the Talking With Hands 16.2M dataset. The results show that our system can produce more realistic, appropriate, and diverse body gestures compared to existing state-of-the-art approaches.	https://dl.acm.org/doi/abs/10.1145/3592456	Kunkun Pang, Dafei Qin, Yingruo Fan, Julian Habekost, Takaaki Shiratori, Junichi Yamagishi, Taku Komura
Boom	A couple of dumb birds tries its best to protect its eggs from a volcano eruption.	https://dl.acm.org/doi/abs/10.1145/3577024.3596488	Gabriel Augerai, Romain Augier, Laurie Pereira De Figueiredo, Charles Di Cicco, Yannick Jacquin
Boundary Value Caching for Walk on Spheres	Grid-free Monte Carlo methods such as can be used to solve elliptic partial differential equations without mesh generation or global solves. However, such methods independently estimate the solution at every point, and hence do not take advantage of the high spatial regularity of solutions to elliptic problems. We propose a fast caching strategy which first estimates solution values and derivatives at randomly sampled points along the boundary of the domain (or a local region of interest). These cached values then provide cheap, output-sensitive evaluation of the solution (or its gradient) at interior points, via a boundary integral formulation. Unlike classic boundary integral methods, our caching scheme introduces zero statistical bias and does not require a dense global solve. Moreover we can handle imperfect geometry (e.g., with self-intersections) and detailed boundary/source terms without repairing or resampling the boundary representation. Overall, our scheme is similar in spirit to methods from photorealistic rendering: it suppresses the typical salt-and-pepper noise characteristic of independent Monte Carlo estimates, while still retaining the many advantages of Monte Carlo solvers: progressive evaluation, trivial parallelization, geometric robustness, We validate our approach using test problems from visual and geometric computing.	https://dl.acm.org/doi/abs/10.1145/3592400	Bailey Miller, Rohan Sawhney, Keenan Crane, Ioannis Gkioulekas
Brain-Machine Interface for neurorehabilitation and human augmentation: Applications of BMI technology and prospects	Although there is no distinctive header, this is the abstract. This submission template allows authors to submit their papers for review to an ACM Conference or Journal without any output design specifications incorporated at this point in the process. The ACM manuscript template is a single column document that allows authors to type their content into the pre-existing set of paragraph formatting styles applied to the sample placeholder text here. Throughout the document you will find further instructions on how to format your text. If your conference's review process will be double-blind: The submitted document should not include author information and should not include acknowledgments, citations or discussion of related work that would make the authorship apparent. Submissions containing author identifying information may be subject to rejection without review. Upon acceptance, the author and affiliation information must be added to your paper.	https://dl.acm.org/doi/abs/10.1145/3588037.3605555	Junichi Ushiba, Masaaki Hayashi, Seitaro Iwama
BreakBug	An old tinkerer spends hours and hours building a machine that helps him to revive his youth, after a life time of break dancing. But the story takes a dramatic turn when he gets distracted for a brief moment, and doesn't watch his back.	https://dl.acm.org/doi/abs/10.1145/3577024.3589025	Michael Ralla
Building Maps on the Web Using RStudio, Leaflet, & Shiny	This lab will provide a hands-on introduction to visualization of spatial data using interactive maps that can be deployed as public web pages. We will use a combination of RStudio, the Shiny package, and the Leaflet open-source library to provide an introduction on how to combine data and maps to create public web pages. Attendees will gain an overview of RStudio, Leaflet, and Shiny Applications. They will learn how to install packages for leaflet and Shiny, create and customize different types of leaflet maps, including a choropleth, and develop a Shiny application deployable on the web.	https://dl.acm.org/doi/abs/10.1145/3588029.3599737	Ann McNamara
Building a Real-Time System on GPUs for Simulation and Rendering of Realistic 3D Liquid in Video Games	Modern video games employ a variety of sophisticated algorithms to produce groundbreaking 3D rendering of water, which are pushing the visual boundaries and interactive experience of rich virtual environments. However, simulation and rendering of a large number of water particles is very time consuming and it is very hard to achieve real-time frame rate (due to the huge computational cost required), e.g., those found in feature movies and offline products [Flip Fluids2022], or tools (e.g., Houdini and Blender). That is why most water visual effects in modern games are either simulated as only a 2D shallow-water in which simulation is calculated in 2D grid and projected onto the heightfield, as in [Fluid Flux 2022], or are baked at preprocessing stage which does not allow the player to dynamically interact with the liquid at runtime, as a result, lots of the fun of interactivity is lost. This course will discuss the state-of-the-art and production-proven techniques involved in building a real-time system on GPUs for simulation and rendering of realistic 3D liquid with millions of particles. It also discusses how to integrate the system into modern game engines (like UE5), with some show-cases of real applications in gaming environments.	https://dl.acm.org/doi/abs/10.1145/3587423.3595537	Baoquan Liu, Minxuan Wang, Yifan Yang, Yuewei Shao
Building a Virtual Weakly-Compressible Wind Tunnel Testing Facility	Virtual wind tunnel testing is a key ingredient in the engineering design process for the automotive and aeronautical industries as well as for urban planning: through visualization and analysis of the simulation data, it helps optimize lift and drag coefficients, increase peak speed, detect high pressure zones, and reduce wind noise at low cost prior to manufacturing. In this paper, we develop an efficient and accurate virtual wind tunnel system based on recent contributions from both computer graphics and computational fluid dynamics in high-performance kinetic solvers. Running on one or multiple GPUs, our massively-parallel lattice Boltzmann model meets industry standards for accuracy and consistency while exceeding current mainstream industrial solutions in terms of efficiency --- especially for unsteady turbulent flow simulation at very high Reynolds number (on the order of 10 ) --- due to key contributions in improved collision modeling and boundary treatment, automatic construction of multiresolution grids for complex models, as well as performance optimization. We demonstrate the efficacy and reliability of our virtual wind tunnel testing facility through comparisons of our results to multiple benchmark tests, showing an increase in both accuracy and efficiency compared to state-of-the-art industrial solutions. We also illustrate the fine turbulence structures that our system can capture, indicating the relevance of our solver for both VFX and industrial product design.	https://dl.acm.org/doi/abs/10.1145/3592394	Chaoyang Lyu, Kai Bai, Yiheng Wu, Mathieu Desbrun, Changxi Zheng, Xiaopei Liu
CT2Hair: High-Fidelity 3D Hair Modeling using Computed Tomography	We introduce CT2Hair, a fully automatic framework for creating high-fidelity 3D hair models that are suitable for use in downstream graphics applications. Our approach utilizes real-world hair wigs as input, and is able to reconstruct hair strands for a wide range of hair styles. Our method leverages computed tomography (CT) to create density volumes of the hair regions, allowing us to see through the hair unlike image-based approaches which are limited to reconstructing the visible surface. To address the noise and limited resolution of the input density volumes, we employ a coarse-to-fine approach. This process first recovers guide strands with estimated 3D orientation fields, and then populates dense strands through a novel neural interpolation of the guide strands. The generated strands are then refined to conform to the input density volumes. We demonstrate the robustness of our approach by presenting results on a wide variety of hair styles and conducting thorough evaluations on both real-world and synthetic datasets. Code and data for this paper are at github.com/facebookresearch/CT2Hair.	https://dl.acm.org/doi/abs/10.1145/3592106	Yuefan Shen, Shunsuke Saito, Ziyan Wang, Olivier Maury, Chenglei Wu, Jessica Hodgins, Youyi Zheng, Giljoo Nam
Calligraphy Experience System That Conveys the Relationship Between Kanji and Nature	To date, systems dealing with Kanji characters have been produced mainly for the purpose of learning, but it is important not only to learn the meaning and writing order of Kanji characters, but also to understand the cultural aspects of them. In regions where Kanji characters are used, various cultures, including Kanji characters, are influenced by nature, so in this study, we propose a calligraphy experience system that conveys the relationship between nature and Kanji characters. When a user looks at the writing order projected on the tabletop and writes Kanji characters created from natural shapes with a brush, the written characters change into the original natural shapes. The user's handwriting could be transformed into a natural form by using projected images.	https://dl.acm.org/doi/abs/10.1145/3588027.3595590	Kei Kobayashi, Kazuma Nagata
Champollion, the Egyptian	With this virtual reality experience, you will slip into Champollion's skin to relive a unique experience and historical milestone. Lit by torchlight, guided by Champollion's voice, the visitor-explorer enters the temple, recreated from David Roberts' watercolors. A unique experience by Agnès Molia and Gordon to explore an exceptional Egyptian site!	https://dl.acm.org/doi/abs/10.1145/3577025.3585272	Gordon, Agnès Molina
Changing the Way We Look at Cells	Conventional cell imaging kills active cultures to collect biological data. What if it didn't have to?	https://dl.acm.org/doi/abs/10.1145/3577024.3588740	Molly Huttner, Cameron Slayden
Channeling Creativity Through a Deeper Understanding of AI Image Generation	AI-generated images burst onto the scene about a year ago, with tools like Stable Diffusion, Midjourney, and DALL·E 2 all making their debut in 2022. How do these models work, and how can they be used in a production setting? In this talk, we will give an overview of how models like DALL·E 2 work and how to leverage their architectures to make them truly useful tools in the creative process. Although there are differences between each specific model architecture, the takeaways from understanding this particular stack are transferable to the others.	https://dl.acm.org/doi/abs/10.1145/3588029.3599743	Joyce Lee, Natalie Summers
Charge	In an energy-scarce dystopia, an old destitute man breaks into a battery factory but soon finds himself confronted by a deadly security droid and no way out.	https://dl.acm.org/doi/abs/10.1145/3577024.3585012	Hjalti Hjalmarsson, Francesco Siddi
ChatAvatar: Creating Hyper-realistic Physically-based 3D Facial Assets through AI-Driven Conversations	ChatAvatar revolutionizes the process of creating hyper-realistic 3D facial assets with physically-based rendering (PBR) textures through AI-driven conversations, specifically text-to-avatar interactions. By leveraging advanced diffusion technology and our comprehensive Production-Ready Facial Assets dataset, we generate CG-friendly assets that adhere to industry standards and seamlessly integrate into popular platforms like Unity, Unreal Engine, and Maya. This groundbreaking technology opens up new horizons for immersive virtual experiences, pushing the boundaries of realism and interactivity.	https://dl.acm.org/doi/abs/10.1145/3588430.3597244	Qixuan Zhang, Longwen Zhang, Lan Xu, Di Wu, Jingyi Yu
ColorfulCurves: Palette-Aware Lightness Control and Color Editing via Sparse Optimization	Color editing in images often consists of two main tasks: changing hue and saturation, and editing lightness or tone curves. State-of-the-art palette-based recoloring approaches entangle these two tasks. A user's only lightness control is changing the lightness of individual palette colors. This is inferior to state-of-the-art commercial software, where lightness editing is based on flexible tone curves that remap lightness. However, tone curves are only provided globally or per color channel (e.g., RGB). They are unrelated to the image content. Neither tone curves nor palette-based approaches support direct image-space edits---changing a pixel to a desired hue, saturation, and lightness. solves both of these problems by uniting palette-based and tone curve editing. In , users directly edit palette colors' hue and saturation, per-palette tone curves, or image pixels (hue, saturation, and lightness). solves an optimization problem in real-time to find a sparse edit that satisfies all user constraints. Our expert study found overwhelming support for over experts' preferred tools.	https://dl.acm.org/doi/abs/10.1145/3592405	Cheng-Kang Ted Chao, Jason Klein, Jianchao Tan, Jose Echevarria, Yotam Gingold
Complex Wrinkle Field Evolution	We propose a new approach for representing wrinkles, designed to capture complex and detailed wrinkle behavior on coarse triangle meshes, called Complex Wrinkle Fields. Complex Wrinkle Fields consist of an almost-everywhere-unit complex-valued phase function over the surface; a frequency one-form; and an amplitude scalar, with a soft compatibility condition coupling the frequency and phase. We develop algorithms for interpolating between two such wrinkle fields, for visualizing them as displacements of a Loop-subdivided refinement of the base mesh, and for making smooth local edits to the wrinkle amplitude, frequency, and/or orientation. These algorithms make it possible, for the first time, to create and edit animations of wrinkles on triangle meshes that are smooth in space, evolve smoothly through time, include singularities along with their complex interactions, and that represent frequencies far finer than the surface resolution.	https://dl.acm.org/doi/abs/10.1145/3592397	Zhen Chen, Danny Kaufman, Mélina Skouras, Etienne Vouga
Composite Motion Learning with Task Control	We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behaviors, we introduce a sample-efficient method for training composite control policies in an incremental manner, where we reuse a pre-trained policy as the meta policy and train a cooperative policy that adapts the meta one for new composite tasks. We show the applicability of our approach on a variety of challenging multi-objective tasks involving both composite motion imitation and multiple goal-directed control. Code is available at .	https://dl.acm.org/doi/abs/10.1145/3592447	Pei Xu, Xiumin Shang, Victor Zordan, Ioannis Karamouzas
Computational Exploration of Multistable Elastic Knots	We present an algorithmic approach to discover, study, and design multistable elastic knots. Elastic knots are physical realizations of closed curves embedded in 3-space. When endowed with the material thickness and bending resistance of a physical wire, these knots settle into equilibrium states that balance the forces induced by elastic deformation and self-contacts of the wire. In general, elastic knots can have many distinct equilibrium states, i.e. they are mechanical systems. We propose a computational pipeline that combines randomized spatial sampling and physics simulation to efficiently find stable equilibrium states of elastic knots. Leveraging results from knot theory, we run our pipeline on thousands of different topological knot types to create an extensive data set of multistable knots. By applying a series of filters to this data, we discover new transformable knots with interesting geometric and physical properties. A further analysis across knot types reveals geometric and topological patterns, yielding constructive principles that generalize beyond the currently tabulated knot types. We show how multistable elastic knots can be used to design novel deployable structures and engaging recreational puzzles. Several physical prototypes at different scales highlight these applications and validate our simulation.	https://dl.acm.org/doi/abs/10.1145/3592399	Michele Vidulis, Yingying Ren, Julian Panetta, Eitan Grinspun, Mark Pauly
Computational Long Exposure Mobile Photography	Long exposure photography produces stunning imagery, representing moving elements in a scene with motion-blur. It is generally employed in two modalities, producing either a foreground or a background blur effect. Foreground blur images are traditionally captured on a tripod-mounted camera and portray blurred moving foreground elements, such as silky water or light trails, over a perfectly sharp background landscape. Background blur images, also called panning photography, are captured while the camera is tracking a moving subject, to produce an image of a sharp subject over a background blurred by relative motion. Both techniques are notoriously challenging and require additional equipment and advanced skills. In this paper, we describe a computational burst photography system that operates in a hand-held smartphone camera app, and achieves these effects fully automatically, at the tap of the shutter button. Our approach first detects and segments the salient subject. We track the scene motion over multiple frames and align the images in order to preserve desired sharpness and to produce aesthetically pleasing motion streaks. We capture an under-exposed burst and select the subset of input frames that will produce blur trails of controlled length, regardless of scene or camera motion velocity. We predict inter-frame motion and synthesize motion-blur to fill the temporal gaps between the input frames. Finally, we composite the blurred image with the sharp regular exposure to protect the sharpness of faces or areas of the scene that are barely moving, and produce a final high resolution and high dynamic range (HDR) photograph. Our system democratizes a capability previously reserved to professionals, and makes this creative style accessible to most casual photographers.	https://dl.acm.org/doi/abs/10.1145/3592124	Eric Tabellion, Nikhil Karnad, Noa Glaser, Ben Weiss, David E. Jacobs, Yael Pritch
Contact Edit: Artist Tools for Intuitive Modeling of Hand-Object Interactions	Posing high-contact interactions is challenging and time-consuming, with hand-object interactions being especially difficult due to the large number of degrees of freedom (DOF) of the hand and the fact that humans are experts at judging hand poses. This paper addresses this challenge by elevating contact areas to first-class primitives. We provide (EAD) tools to model interactions based on contact areas, directly manipulate contact areas, and compute corresponding poses automatically. To make these operations intuitive and fast, we present a novel axis-based contact model that supports real-time approximately isometry-preserving operations on triangulated surfaces, permits movement between surfaces, and is both robust and scalable to large areas. We show that use of our contact model facilitates high quality posing even for unconstrained, high-DOF custom rigs intended for traditional keyframe-based animation pipelines. We additionally evaluate our approach with comparisons to prior art, ablation studies, user studies, qualitative assessments, and extensions to full-body interaction.	https://dl.acm.org/doi/abs/10.1145/3592117	Arjun Sriram Lakshmipathy, Nicole Feng, Yu Xi Lee, Moshe Mahler, Nancy Pollard
Content-Preserving Motion Stylization using Variational Autoencoder	This work proposes a motion style transfer network that transfers motion style between different motion categories using variational autoencoders. The proposed network effectively transfers style among various motion categories and can create stylized motion unseen in the dataset. The network contains a content-conditioned module to preserve the characteristic of the content motion, which is important for real applications. We implement the network with variational autoencoders, which enable us to control the intensity of the style and mix different styles to enrich the motion diversity.	https://dl.acm.org/doi/abs/10.1145/3588028.3603679	Chen-Chieh Liao, Jong-Hwan Kim, Hideki Koike, Dong-Hyun Hwang
Coupling Conduction, Convection and Radiative Transfer in a Single Path-Space: Application to Infrared Rendering	In the past decades, Monte Carlo methods have shown their ability to solve PDEs, independently of the dimensionality of the integration domain and for different use-cases (e.g. light transport, geometry processing, physics simulation). Specifically, the path-space formulation of transport equations is a key ingredient to define tractable and scalable solvers, and we observe nowadays a strong interest in the definition of simulation systems based on Monte Carlo algorithms. We also observe that, when simulating combined physics (e.g. thermal rendering from a heat transfer simulation), there is a lack of coupled Monte Carlo algorithms allowing to solve all the physics at once, in the same path space, rather than combining several independent MC estimators, a combination that would make the global solver critically sensitive to the complexity of each simulation space. This brings to our proposal: a coupled, single path-space, Monte Carlo algorithm for efficient multi-physics problems solving. In this work, we combine our understanding and knowledge of Physics and Computer Graphics to demonstrate how to formulate and arrange different simulation spaces into a single path space. We define a tractable formalism for coupled heat transfer simulation using Monte Carlo, and we leverage the path-space construction to interactively compute multiple simulations with different conditions in the same scene, in terms of boundary conditions and observation time. We validate our proposal in the context of infrared rendering with different thermal simulation scenarios: e.g., room temperature simulation, visualization of heat paths within materials (detection of thermal bridges), heat diffusion capacity of thermal exchanger. We expect that our theoretical framework will foster collaboration and multidisciplinary studies. The perspectives this framework opens are detailed and we suggest a research agenda towards the resolution of coupled PDEs at the interface of Physics and Computer Graphics.	https://dl.acm.org/doi/abs/10.1145/3592121	Mégane Bati, Stéphane Blanco, Christophe Coustet, Vincent Eymet, Vincent Forest, Richard Fournier, Jacques Gautrais, Nicolas Mellado, Mathias Paulin, Benjamin Piaud
Creating the Way of Water	No detail was too small to bring Avatar: The Way of Water and its 2,225 water shots to life. With an underwater world simulated by tidal changes, and water sequences enhanced by bubbles, spray, foam, and marine snow details, Pandora has never felt so real.	https://dl.acm.org/doi/abs/10.1145/3577024.3588890	Pavani Rao Boddapati
Crossed half-silvered Mirror Array: Fabrication and Evaluation of a See-Through Capable DIY Crossed Mirror Array	Crossed mirror arrays (CMAs) have recently been employed in simple retinal projection augmented reality (AR) devices owing to their wide field of view and nonfocal nature. However, they remain inadequate for AR devices for everyday use owing to the limited visibility of the physical environment. This study aims to enhance the transmittance of the CMA by fabricating it with half-silvered acrylic mirrors. Further, we evaluated the transmittance and quality of the retinal display. The proposed CMA successfully achieved sufficient retinal projection and higher see-through capability, making it more suitable for use in AR devices than conventional CMAs.	https://dl.acm.org/doi/abs/10.1145/3588028.3603644	Kensuke Katori, Kenta Yamamoto, Ippei Suzuki, Tatsuki Fushimi, Yoichi Ochiai
DJuggling: Sonification of expressive movement performance	In the real-time demo, we demonstrate how to create a musical performance using juggling movement as an instrument. We have equipped juggling balls with accelerometers, gyroscopes, and WiFi sensors. The system measures acceleration and rotation in a small HW footprint, allowing us to map various events to music. We provide hardware and software platforms to ease the creation of real-time live performances for artists and researchers alike. A movement-driven controller can be used to create music, trigger media or lights in theater performances, serve as a lighting console or VR controller, or track performance in sports or scientific experiments. We provide OSC and MIDI APIs that are widely used in before mentioned fields.	https://dl.acm.org/doi/abs/10.1145/3588430.3597246	Vojtěch Žák Leischner, Pavel Husa
DNA replication of the lagging strand	DNA replication is one of the most essential tasks of the human body. Dive in and see this intricate molecular dance unfold in vivid detail right before your eyes.	https://dl.acm.org/doi/abs/10.1145/3577024.3588981	Peter Mindek, Tobias Klein, Alfredo De Biasio
DOC: Differentiable Optimal Control for Retargeting Motions onto Legged Robots	Legged robots are designed to perform highly dynamic motions. However, it remains challenging for users to retarget expressive motions onto these complex systems. In this paper, we present a (DOC) framework that facilitates the transfer of rich motions from either animals or animations onto these robots. Interfacing with either motion capture or animation data, we formulate retargeting objectives whose parameters make them agnostic to differences in proportions and numbers of degrees of freedom between input and robot. Optimizing these parameters over the manifold spanned by optimal state and control trajectories, we minimize the retargeting error. We demonstrate the utility and efficacy of our modeling by applying DOC to a (MPC) formulation, showing retargeting results for a family of robots of varying proportions and mass distribution. With a hardware deployment, we further show that the retargeted motions are physically feasible, while MPC ensures that the robots retain their capability to react to unexpected disturbances.	https://dl.acm.org/doi/abs/10.1145/3592454	Ruben Grandia, Farbod Farshidian, Espen Knoop, Christian Schumacher, Marco Hutter, Moritz Bächer
Deep Learning for Physics Simulation	Numerical simulation of physical systems has become an increasingly important scientific tool supporting various research fields. Despite its remarkable success, simulating intricate physical systems typically requires advanced domain-specific knowledge, meticulous implementation, and enormous computational resources. With the surge of deep learning in the last decade, there has been a growing interest in the machine-learning and graphics communities to address these limitations of numerical simulation with deep learning. This course provides a gentle introduction to this topic for audiences interested in exploring this trend but with little to modest machine-learning or physics-simulation backgrounds. We begin with a brief overview of the numerical simulation framework on which we ground our discussion of deep-learning methods. Next, the course provides a possible classification of several hybrid simulation strategies based on the roles of learning and physics insights incorporated. We then review the implications of such deep-learning strategies and discuss some practical considerations in combining deep learning and physics simulation. Finally, we briefly mention several advanced machine-learning techniques for further exploration. The full course information can be found in https://people.iiis.tsinghua.edu.cn/~taodu/dl4sim/.	https://dl.acm.org/doi/abs/10.1145/3587423.3595518	Tao Du
Deformable Neural Radiance Fields for Object Motion Blur Removal	In this paper, we present a novel approach to remove object motion blur in 3D scene renderings using deformable neural radiance fields. Our technique adapts the hyperspace representation to accommodate shape changes induced by object motion blur. Experiments on Blender-generated datasets demonstrate the effectiveness of our method in producing higher-quality images with reduced object motion blur artifacts.	https://dl.acm.org/doi/abs/10.1145/3588028.3603692	Kazuhito Sato, Shugo Yamaguchi, Tsukasa Takeda, Shigeo Morishima
Demonstrating JumpMod: Haptic Backpack that Modifies Users' Perceived Jump	Vertical force-feedback is extremely rare in mainstream interactive experiences. This happens because existing haptic devices capable of sufficiently strong forces that would modify a user's jump require grounding (e.g., motion platforms or pulleys) or cumbersome actuators (e.g., large propellers attached or held by the user). To enable interactive experiences to feature jump-based haptics without sacrificing wearability, we propose JumpMod, an untethered backpack that modifies one's sense of jumping. JumpMod achieves this by moving a weight up/down along the user's back, which modifies perceived jump momentum—creating accelerated & decelerated jump sensations. Our device can render five distinct effects: jump higher, land harder/softer, pulled higher/lower, which we demonstrate at SIGGRAPH 2023 Emerging Technologies in two jump-based VR experiences.	https://dl.acm.org/doi/abs/10.1145/3588037.3595387	Romain Nith, Jacob Serfaty, Samuel Shatzkin, Alan Shen, Pedro Lopes
Dense, Interlocking-Free and Scalable Spectral Packing of Generic 3D Objects	Packing 3D objects into a known container is a very common task in many industries such as packaging, transportation, and manufacturing. This important problem is known to be NP-hard and even approximate solutions are challenging. This is due to the difficulty of handling interactions between objects with arbitrary 3D geometries and a vast combinatorial search space. Moreover, the packing must be for real-world applications. In this work, we first introduce a novel packing algorithm to search for placement locations given an object. Our method leverages a discrete voxel representation. We formulate collisions between objects as correlations of functions computed efficiently using Fast Fourier Transform (FFT). To determine the best placements, we utilize a novel cost function, which is also computed efficiently using FFT. Finally, we show how interlocking detection and correction can be addressed in the same framework resulting in interlocking-free packing. We propose a challenging benchmark with thousands of 3D objects to evaluate our algorithm. Our method demonstrates state-of-the-art performance on the benchmark when compared to existing methods in both density and speed.	https://dl.acm.org/doi/abs/10.1145/3592126	Qiaodong Cui, Victor Rong, Desai Chen, Wojciech Matusik
Deployable strip structures	We introduce the new concept of C-mesh to capture kinetic structures that can be deployed from a collapsed state. Quadrilateral C-meshes enjoy rich geometry and surprising relations with differential geometry: A structure that collapses onto a flat and straight strip corresponds to a Chebyshev net of curves on a surface of constant Gaussian curvature, while structures collapsing onto a circular strip follow surfaces which enjoy the linear-Weingarten property. Interestingly, allowing more general collapses actually leads to a smaller class of shapes. Hexagonal C-meshes have more degrees of freedom, but a local analysis suggests that there is no such direct relation to smooth surfaces. Besides theory, this paper provides tools for exploring the shape space of C-meshes and for their design. We also present an application for freeform architectural skins, namely paneling with spherical panels of constant radius, which is an important fabrication-related constraint.	https://dl.acm.org/doi/abs/10.1145/3592393	Daoming Liu, Davide Pellis, Yu-Chou Chiang, Florian Rist, Johannes Wallner, Helmut Pottmann
Dictionary Fields: Learning a Neural Basis Decomposition	We present Dictionary Fields, a novel neural representation which decomposes a signal into a product of factors, each represented by a classical or neural field representation, operating on transformed input coordinates. More specifically, we factorize a signal into a coefficient field and a basis field, and exploit periodic coordinate transformations to apply the same basis functions across multiple locations and scales. Our experiments show that Dictionary Fields lead to improvements in approximation quality, compactness, and training time when compared to previous fast reconstruction methods. Experimentally, our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when reconstructing 3D signed distance fields, and higher compactness for radiance field reconstruction tasks. Furthermore, Dictionary Fields enable generalization to unseen images/3D scenes by sharing bases across signals during training which greatly benefits use cases such as image regression from partial observations and few-shot radiance field reconstruction.	https://dl.acm.org/doi/abs/10.1145/3592135	Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, Andreas Geiger
Differentiable Stripe Patterns for Inverse Design of Structured Surfaces	Stripe patterns are ubiquitous in nature and everyday life. While the synthesis of these patterns has been thoroughly studied in the literature, their potential to control the mechanics of structured materials remains largely unexplored. In this work, we introduce Differentiable Stripe Patterns---a computational approach for automated design of physical surfaces structured with stripe-shaped bi-material distributions. Our method builds on the work by Knöppel and colleagues [2015] for generating globally-continuous and equally-spaced stripe patterns. To unlock the full potential of this design space, we propose a gradient-based optimization tool to automatically compute stripe patterns that best approximate macromechanical performance goals. Specifically, we propose a computational model that combines solid shell finite elements with XFEM for accurate and fully-differentiable modeling of elastic bi-material surfaces. To resolve non-uniqueness problems in the original method, we furthermore propose a robust formulation that yields unique and differentiable stripe patterns. We combine these components with equilibrium state derivatives into an end-to-end differentiable pipeline that enables inverse design of mechanical stripe patterns. We demonstrate our method on a diverse set of examples that illustrate the potential of stripe patterns as a design space for structured materials. Our simulation results are experimentally validated on physical prototypes.	https://dl.acm.org/doi/abs/10.1145/3592114	Juan Sebastian Montes Maestre, Yinwei Du, Ronan Hinchet, Stelian Coros, Bernhard Thomaszewski
Differential Operators on Sketches via Alpha Contours	A vector sketch is a popular and natural geometry representation depicting a 2D shape. When viewed from afar, the disconnected vector strokes of a sketch and the empty space around them visually merge into and , respectively. Positive and negative spaces are the key elements in the composition of a sketch and define what we perceive as the shape. Nevertheless, the notion of positive or negative space is mathematically ambiguous: While the strokes unambiguously indicate the interior or boundary of a 2D shape, the empty space may or may not belong to the shape's exterior. For standard discrete geometry representations, such as meshes or point clouds, some of the most robust pipelines rely on discretizations of differential operators, such as Laplace-Beltrami. Such discretizations are not available for vector sketches; defining them may enable numerous applications of classical methods on vector sketches. However, to do so, one needs to define the positive space of a vector sketch, or the Even though extracting this 2D sketch shape is mathematically ambiguous, we propose a robust algorithm, , constructing its conservative estimate: a 2D shape containing all the input strokes, which lie in its interior or on its boundary, and aligning tightly to a sketch. This allows us to define popular differential operators on vector sketches, such as Laplacian and Steklov operators. We demonstrate that our construction enables robust tools for vector sketches, such as As-Rigid-As-Possible sketch deformation and functional maps between sketches, as well as solving partial differential equations on a vector sketch.	https://dl.acm.org/doi/abs/10.1145/3592420	Mariia Myronova, William Neveu, Mikhail Bessmeltsev
Digital Dance Studio VR (DDS-VR): An innovative user-focused immersive software application for digital choreographic composition, planning, teaching, learning, and rehearsal.	The Digital Dance Studio VR (DDS-VR) is an innovative user-focused immersive software application for choreographic composition, planning, teaching, learning, and rehearsal. It offers a simple and intuitive immersive interface for creation and manipulation of choreographic sequences in virtual space, permitting exploration of spatial and temporal patterning, musical accompaniment, environment and design aesthetics, and allowing users to change the position, number, rhythm and orientation of dancers. It provides a suite of modular tools for choreographers, dancers, and anyone interested in exploring movement in a digital context – including Film and TV applications in blocking/storyboarding of fight and crowd sequences. The DDS-VR application can empower users to create, visualize, and share digital choreography outside of the physical studio, saving considerable time and money on studio and personnel hire, and bypassing the more professionalized use of real-time graphics software such as Unity and Unreal.	https://dl.acm.org/doi/abs/10.1145/3588027.3595602	Alexander Whitley, Sönke Kirchhof, Daniel Strutt
Dimix: A Cross-Dimensional Mixed Reality System Based on Latent Diffusion Model	Dimix is a system that integrates 2D images generated by Latent Diffusion Models (LDMs) [et al. 2022] with 3D and interactive Mixed Reality (MR) experiences. This system extracts regions from MR scenes based on panoramic images, adds optional mask images, and applies image-inpainting using LDMs to output an LDMs-generated image (LDMs image) that naturally extends the scene. Furthermore, by performing depth estimation on the images and reconstructing them as 3D scenes, the LDMs image can be treated in the same manner as 3D objects. This enables both basic features like occlusion and collision detection and highly interactive operations such as ink painting, achieving high immersion and realism. Additionally, the system incorporates real-time object detection to constrain the inpainting area, making the LDMs image more convincing. All processing is performed in real-time, allowing users to interact with the world in 3D without waiting for loading or any other preparation by simply uploading their preferred panorama image to the application. To our best knowledge, Dimix is the first system to seamlessly integrate LDMs images into interactive and three-dimensional MR experiences. Users can immerse themselves in an unprecedented space where the real world seamlessly blends with a world generated by a neural network, offering a glimpse into the future of MR experiences.	https://dl.acm.org/doi/abs/10.1145/3588027.3595592	Daiki Taniguchi
Down the Rabbit Hole:: Experiencing Alice in Wonderland Syndrome through Virtual Reality	Alice in Wonderland Syndrome (AIWS) is a rare perceptual disorder affecting visual processing, the perception of one's body and the experience of time. This condition can be congenital or result from various insults to the brain. There is growing interest in AIWS in providing a window into how different areas of the brain work together to construct reality. We developed a virtual reality (VR) simulation of this condition as a psychoeducational tool for students in the psychological and medical sciences and care givers to experience the different perceptual distortions common in AIWS and an opportunity to reflect on the nature of perception.	https://dl.acm.org/doi/abs/10.1145/3588028.3603678	Madeline Verheydt, Evan Staben, Kyle Carncross, Meredith Minear
DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance	Emerging Metaverse applications demand accessible, accurate and easy-to-use tools for 3D digital human creations in order to depict different cultures and societies as if in the physical world. Recent large-scale vision-language advances pave the way for novices to conveniently customize 3D content. However, the generated CG-friendly assets still cannot represent the desired facial traits for human characteristics. In this paper, we present Dream-Face, a progressive scheme to generate personalized 3D faces under text guidance. It enables layman users to naturally customize 3D facial assets that are compatible with CG pipelines, with desired shapes, textures and fine-grained animation capabilities. From a text input to describe the facial traits, we first introduce a coarse-to-fine scheme to generate the neutral facial geometry with a unified topology. We employ a selection strategy in the CLIP embedding space to generate coarse geometry, and subsequently optimize both the detailed displacements and normals using Score Distillation Sampling (SDS) from the generic Latent Diffusion Model (LDM). Then, for neutral appearance generation, we introduce a dual-path mechanism, which combines the generic LDM with a novel texture LDM to ensure both the diversity and textural specification in the UV space. We also employ a two-stage optimization to perform SDS in both the latent and image spaces to significantly provide compact priors for fine-grained synthesis. It also enables learning the mapping from the compact latent space into physically-based textures (diffuse albedo, specular intensity, normal maps, etc.). Our generated neutral assets naturally support blendshapes-based facial animations, thanks to the unified geometric topology. We further improve the animation ability with personalized deformation characteristics. To this end, we learn the universal expression prior in a latent space with neutral asset conditioning using the cross-identity hypernetwork, we subsequently train a neural facial tracker from video input space into the pre-trained expression space for personalized fine-grained animation. Extensive qualitative and quantitative experiments validate the effectiveness and generalizability of DreamFace. Notably, DreamFace can generate realistic 3D facial assets with physically-based rendering quality and rich animation ability from video footage, even for fashion icons or exotic characters in cartoons and fiction movies.	https://dl.acm.org/doi/abs/10.1145/3592094	Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, Jingyi Yu
ETER: Elastic Tessellation for Real-Time Pixel-Accurate Rendering of Large-Scale NURBS Models	We present ETER, an elastic tessellation framework for rendering large-scale NURBS models with pixel-accurate and crack-free quality at real-time frame rates. We propose a highly parallel adaptive tessellation algorithm to achieve pixel accuracy, measured by the screen space error between the exact surface and its triangulation. To resolve a bottleneck in NURBS rendering, we present a novel evaluation method based on uniform sampling grids and accelerated by GPU Tensor Cores. Compared to evaluation based on hardware tessellation, our method has achieved a significant speedup of 2.9 to 16.2 times depending on the degrees of the patches. We develop an efficient crack-filling algorithm based on conservative rasterization and visibility buffer to fill the tessellation-induced cracks while greatly reducing the jagged effect introduced by conservative rasterization. We integrate all our novel algorithms, implemented in CUDA, into a GPU NURBS rendering pipeline based on Mesh Shaders and hybrid software/hardware rasterization. Our performance data on a commodity GPU show that the rendering pipeline based on ETER is capable of rendering up to 3.7 million patches (0.25 billion tessellated triangles) in real-time (30FPS). With its advantages in performance, scalability, and visual quality in rendering large-scale NURBS models, a real-time tessellation solution based on ETER can be a powerful alternative or even a potential replacement for the existing pre-tessellation solution in CAD systems.	https://dl.acm.org/doi/abs/10.1145/3592419	Ruicheng Xiong, Yang Lu, Cong Chen, Jiaming Zhu, Yajun Zeng, Ligang Liu
Effect-based Multi-viewer Caching for Cloud-native Rendering	With cloud computing becoming ubiquitous, it appears as virtually everything can be offered However, real-time rendering in the cloud forms a notable exception, where the cloud adoption stops at running individual game instances in compute centers. In this paper, we explore whether a cloud-native rendering architecture is viable and scales to multi-client rendering scenarios. To this end, we propose world-space and on-surface caches to share rendering computations among viewers placed in the same virtual world. We discuss how caches can be utilized on an effect-basis and demonstrate that a large amount of computations can be saved as the number of viewers in a scene increases. Caches can easily be set up for various effects, including ambient occlusion, direct illumination, and diffuse global illumination. Our results underline that the image quality using cached rendering is on par with screen-space rendering and due to its simplicity and inherent coherence, cached rendering may even have advantages in single viewer setups. Analyzing the runtime and communication costs, we show that cached rendering is already viable in multi-GPU systems. Building on top of our research, cloud-native rendering may be just around the corner.	https://dl.acm.org/doi/abs/10.1145/3592431	Alexander Weinrauch, Wolfgang Tatzgern, Pascal Stadlbauer, Alexis Crickx, Jozef Hladky, Arno Coomans, Martin Winter, Joerg H. Mueller, Markus Steinberger
Efficient Embeddings in Exact Arithmetic	We provide a set of tools for generating planar embeddings of triangulated topological spheres. The algorithms make use of Schnyder labelings and realizers. A new representation of the realizer based on dual trees leads to a simple linear time algorithm mapping from weights per triangle to barycentric coordinates and, more importantly, also in the reverse direction. The algorithms can be implemented so that all coefficients involved are 1 or -1. This enables integer computation, making all computations exact. Being a Schnyder realizer, mapping from positive triangle weights guarantees that the barycentric coordinates form an embedding. The reverse direction enables an algorithm for fixing flipped triangles in planar realizations, by mapping from coordinates to weights and adjusting the weights (without forcing them to be positive). In a range of experiments, we demonstrate that all algorithms are orders of magnitude faster than existing robust approaches.	https://dl.acm.org/doi/abs/10.1145/3592445	Ugo Finnendahl, Dimitrios Bogiokas, Pablo Robles Cervantes, Marc Alexa
Efficient Rendering of Glossy Materials by Interpolating Prefiltered Environment Maps based on Primary Normals	We propose a method to improve the speed of the rendering of glossy surfaces including anisotropy reflection, using prefiltering. The key idea is to prefilter an environment light map for multiple primary normals. Furthermore, we propose an interpolation method to smoothly connect the boundaries generated by switching multiple prefiltered environment maps. As a result, we are able to render objects with glossy surface more accurately even in real-time.	https://dl.acm.org/doi/abs/10.1145/3588028.3603659	Shunya Motegi, Takuya Funatomi, Yasuhiro Mukaigawa, Aki Takayanagi, Hayato Kikuta, Hiroyuki Kubo
EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors	Human and environment sensing are two important topics in Computer Vision and Graphics. Human motion is often captured by inertial sensors, while the environment is mostly reconstructed using cameras. We integrate the two techniques together in EgoLocate, a system that simultaneously performs human motion capture (mocap), localization, and mapping in real time from sparse body-mounted sensors, including 6 inertial measurement units (IMUs) and a monocular phone camera. On one hand, inertial mocap suffers from large translation drift due to the lack of the global positioning signal. EgoLo-cate leverages image-based simultaneous localization and mapping (SLAM) techniquesto locate the human in the reconstructed scene. Onthe other hand, SLAM often fails when the visual feature is poor. EgoLocate involves inertial mocap to provide a strong prior for the camera motion. Experiments show that localization, a key challenge for both two fields, is largely improved by our technique, compared with the state of the art of the two fields. Our codes are available for research at https://xinyu-yi.github.io/EgoLocate/.	https://dl.acm.org/doi/abs/10.1145/3592099	Xinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav Golyanik, Shaohua Pan, Christian Theobalt, Feng Xu
Elementary Paper Animatronics	Paper Animatronics is a new way for elementary school kids to engage with subject matter through project-based learning. We're upgrading the classic shoe box diorama, and empowering kids to bring it to life by adding servo motors, sound and lights to create compelling characters and shows. In this workshop designed for teachers, parents and other advocates for creative education, you will get to work hands-on with our new paper animatronics kits. These make it easy to create talking characters that you can voice in real-time or use in more complex, scripted shows where things move and light up on cue using a synchronized Arduino program. Through these activities, you will see how kids learn to be creative across both technical and artistic disciplines as they explore class subject matter.	https://dl.acm.org/doi/abs/10.1145/3588029.3599744	Paul H. Dietz
Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models	Text-to-image personalization aims to teach a pre-trained diffusion model to reason about novel, user provided concepts, embedding them into new scenes guided by natural language prompts. However, current personalization approaches struggle with lengthy training times, high storage requirements or loss of identity. To overcome these limitations, we propose an encoder-based approach. Our key insight is that by on a large set of concepts from a given domain, we can improve generalization and create a model that is more amenable to quickly adding novel concepts from the same domain. Specifically, we employ two components: First, an encoder that takes as an input a single image of a target concept from a given domain, a specific face, and learns to map it into a word-embedding representing the concept. Second, a set of regularized weight-offsets for the text-to-image model that learn how to effectively injest additional concepts. Together, these components are used to guide the learning of unseen concepts, allowing us to personalize a model using only a single image and as few as 5 training steps --- accelerating personalization from dozens of minutes to , while preserving quality. Code and trained encoders will be available at our project page.	https://dl.acm.org/doi/abs/10.1145/3592133	Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or
End-to-end Procedural Material Capture with Proxy-Free Mixed-Integer Optimization	Node-graph-based procedural materials are vital to 3D content creation within the computer graphics industry. Leveraging the expressive representation of procedural materials, artists can effortlessly generate diverse appearances by altering the graph structure or node parameters. However, manually reproducing a specific appearance is a challenging task that demands extensive domain knowledge and labor. Previous research has sought to automate this process by converting artist-created material graphs into differentiable programs and optimizing node parameters against a photographed material appearance using gradient descent. These methods involve implementing differentiable filter nodes [Shi et al. 2020] and training differentiable neural proxies for generator nodes to optimize continuous and discrete node parameters [Hu et al. 2022a] jointly. Nevertheless, Neural Proxies exhibits critical limitations, such as long training times, inaccuracies, fixed resolutions, and confined parameter ranges, which hinder their scalability towards the broad spectrum of production-grade material graphs. These constraints fundamentally stem from the absence of faithful and efficient implementations of generic noise and pattern generator nodes, both differentiable and non-differentiable. Such deficiency prevents the direct optimization of continuous and discrete generator node parameters without relying on surrogate models. We present , an improved differentiable procedural material library, along with a fully-automated, end-to-end procedural material capture framework that combines gradient-based optimization and gradient-free parameter search to match existing production-grade procedural materials against user-taken flash photos. Diffmat v2 expands the range of differentiable material graph nodes in Diffmat [Shi et al. 2020] by adding generic noise/pattern generator nodes and user-customizable per-pixel filter nodes. This allows for the complete translation and optimization of procedural materials across various categories without the need for external proprietary tools or pre-cached noise patterns. Consequently, our method can capture a considerably broader array of materials, encompassing those with highly regular or stochastic geometries. We demonstrate that our end-to-end approach yields a closer match to the target than MATch [Shi et al. 2020] and Neural Proxies [Hu et al. 2022a] when starting from initially unmatched continuous and discrete parameters.	https://dl.acm.org/doi/abs/10.1145/3592132	Beichen Li, Liang Shi, Wojciech Matusik
Eventfulness for Interactive Video Alignment	Humans are remarkably sensitive to the alignment of visual events with other stimuli, which makes synchronization one of the hardest tasks in video editing. A key observation of our work is that most of the alignment we do involves salient localizable events that occur sparsely in time. By learning how to recognize these events, we can greatly reduce the space of possible synchronizations that an editor or algorithm has to consider. Furthermore, by learning descriptors of these events that capture additional properties of visible motion, we can build active tools that adapt their notion of eventfulness to a given task as they are being used. Rather than learning an automatic solution to one specific problem, our goal is to make a much broader class of interactive alignment tasks significantly easier and less time-consuming. We show that a suitable visual event descriptor can be learned entirely from stochastically-generated synthetic video. We then demonstrate the usefulness of learned and adaptive eventfulness by integrating it in novel interactive tools for applications including audio-driven time warping of video and the extraction and application of sound effects across different videos.	https://dl.acm.org/doi/abs/10.1145/3592118	Jiatian Sun, Longxiulin Deng, Triantafyllos Afouras, Andrew Owens, Abe Davis
Evolutionary Piecewise Developable Approximations	We propose a novel method to compute high-quality piecewise developable approximations for triangular meshes. Central to our approach is an evolutionary genetic algorithm for optimizing the combinatorial and discontinuous fitness function, including the approximation error, the number of patches, the patch boundary length, and the penalty for small patches and narrow regions within patches. The genetic algorithm's operations (i.e., initialization, selection, mutation, and crossover) are explicitly designed to minimize the fitness function. The main challenge is evaluating the fitness function's approximation error as it requires developable patches, which are difficult or time-consuming to obtain. Resolving the challenge is based on a critical observation: the approximation error and the mapping distortion between an input surface and its developable approximation are positively correlated empirically. To efficiently measure distortion without explicitly generating developable shapes, we creatively use conformal mapping techniques. Then, we control the mapping distortion at a relatively low level to achieve high shape similarity in the genetic algorithm. The feasibility and effectiveness of our method are demonstrated over 240 complex examples. Compared with the state-of-the-art methods, our results have much smaller approximation errors, fewer patches, shorter patch boundaries, and fewer small patches and narrow regions.	https://dl.acm.org/doi/abs/10.1145/3592140	Zheng-Yu Zhao, Mo Li, Zheng Zhang, Qing Fang, Ligang Liu, Xiao-Ming Fu
Example-Based Procedural Modeling Using Graph Grammars	We present a method for automatically generating polygonal shapes from an example using a graph grammar. Most procedural modeling techniques use grammars with manually created rules, but our method can create them automatically from an example. Our graph grammars generate graphs that are locally similar to a given example. We disassemble the input into small pieces called primitives and then reassemble the primitives into new graphs. We organize all possible locally similar graphs into a hierarchy and find matching graphs within the hierarchy. These matches are used to create a graph grammar that can construct every locally similar graph. Our method generates graphs using the grammar and then converts them into a planar graph drawing to produce the final shape.	https://dl.acm.org/doi/abs/10.1145/3592119	Paul Merrell
Example-based Motion Synthesis via Generative Motion Matching	"We present GenMM, a generative model that ""mines"" as many diverse motions as possible from a single or few example sequences. In stark contrast to existing data-driven methods, which typically require long offline training time, are prone to visual artifacts, and tend to fail on large and complex skeletons, GenMM inherits the training-free nature and the superior quality of the well-known method. GenMM can synthesize a high-quality motion within a fraction of a second, even with highly complex and large skeletal structures. At the heart of our generative framework lies the generative motion matching module, which utilizes the bidirectional visual similarity as a generative cost function to motion matching, and operates in a multi-stage framework to progressively refine a random guess using exemplar motion matches. In addition to diverse motion generation, we show the versatility of our generative framework by extending it to a number of scenarios that are not possible with motion matching alone, including motion completion, key frame-guided generation, infinite looping, and motion reassembly."	https://dl.acm.org/doi/abs/10.1145/3592395	Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen
Expansion Cones: A Progressive Volumetric Mapping Framework	Volumetric mapping is a ubiquitous and difficult problem in Geometry Processing and has been the subject of research in numerous and various directions. While several methods show encouraging results, the field still lacks a general approach with guarantees regarding map bijectivity. Through this work, we aim at opening the door to a new family of methods by providing a novel framework based on the concept of Starting from an initial map of a tetrahedral mesh whose image may contain degeneracies but no inversions, we incrementally adjust vertex images to expand degenerate elements. By restricting movement to so-called , it is done in such a way that the number of degenerate elements decreases in a strictly monotonic manner, without ever introducing any inversion. Adaptive local refinement of the mesh is performed to facilitate this process. We describe a prototype algorithm in the realm of this framework for the computation of maps from ball-topology tetrahedral meshes to convex or star-shaped domains. This algorithm is evaluated and compared to state-of-the-art methods, demonstrating its benefits in terms of bijectivity. We also discuss the associated cost in terms of sometimes significant mesh refinement to obtain the necessary degrees of freedom required for establishing a valid mapping. Our conclusions include that while this algorithm is only of limited immediate practical utility due to efficiency concerns, the general framework has the potential to inspire a range of novel methods improving on the efficiency aspect.	https://dl.acm.org/doi/abs/10.1145/3592421	Valentin Zénon Nigolian, Marcel Campen, David Bommes
Exploring Multiple-Display Interaction for Live XR Performance	Although VR concerts offer a unique experience for audience to watch a performance, people still tend to participate in a live performance physically for the co-presence and shared experience are difficult to perceive in VR. To address this issue, we propose Actualities, a live XR performance system that integrates onsite and online concerts to create a seamless experience across multiple displays. Our system utilizes various sensors to detect signals from musical instruments and onsite audiences, digitalizing onsite performance elements into a virtual world. We project the visuals onto screens and live-stream the content for audiences to watch through various devices, and we also designed several interactive elements for the audience to interact with the public display. To evaluate our system, we conducted exploratory research to help us refine our system and improve the cross-reality experience.	https://dl.acm.org/doi/abs/10.1145/3588028.3603661	Ke-Fan Lin, Yu-Chih Chou, Yu-Hsiang Weng, Yvone Tsai Chen, Tse-Yu Pan, Ping-Hsuan Han
Exterior Calculus in Graphics: Course Notes for a SIGGRAPH 2023 Course	The demand for a more advanced multivariable calculus has rapidly increased in computer graphics research, such as physical simulation, geometry synthesis, and differentiable rendering. Researchers in computer graphics often have to turn to references outside of graphics research to study identities such as the Reynolds Transport Theorem or the geometric relationship between stress and strain tensors. This course presents a comprehensive introduction to exterior calculus, which covers many of these advanced topics in a geometrically intuitive manner. The course targets anyone who knows undergraduate-level multivariable calculus and linear algebra and assumes no more prerequisites. Contrary to the existing references, which only serve the pure math or engineering communities, we use timely and relevant graphics examples to illustrate the theory of exterior calculus. We also provide accessible explanations to several advanced topics, including continuum mechanics, fluid dynamics, and geometric optimizations. The course is organized into two main sections: a lecture on the core exterior calculus notions and identities with short examples of graphics applications, and a series of mini-lectures on graphics topics using exterior calculus.	https://dl.acm.org/doi/abs/10.1145/3587423.3595525	Stephanie Wang, Mohammad Sina Nabizadeh, Albert Chern
ExudedVestibule: Enhancing Mid-air Haptics through Galvanic Vestibular Stimulation	This study presents a novel system that enhances air cannon tactile perception using synchronous galvanic vestibular stimulation (GVS). We conducted a user study with a within-subjects design to evaluate the enhancement effects of synchronous GVS on air cannon tactile sensations across multiple body locations. Results demonstrated significant improvements without affecting the magnitude of physical body sway, suggesting potential applications in virtual reality, particularly for augmenting existing air vortex ring haptics use cases.	https://dl.acm.org/doi/abs/10.1145/3588028.3603648	Shieru Suzuki, Kazuma Aoyama, Ryosei Kojima, Kazuya Izumi, Tatsuki Fushimi, Yoichi Ochiai
Fabrication of Edible lenticular lens	Lenticular lenses exhibit the color changing effect depending on the viewing angle and the vanishing effect in certain directions. In this study, we propose two fabrication methods for edible lenticular lenses. One is the mold forming method, and another is the knife cutting method using a knife with the inverse structure of a lenticular lens created by an SLA 3D printer. We also evaluate the properties of the end products. The IOR of material is optimized by using ray tracing simulation.	https://dl.acm.org/doi/abs/10.1145/3588028.3603675	Takegi Yoshimoto, Nobuhito Kasahara, Homei Miyashita
FactorMatte: Redefining Video Matting for Re-Composition Tasks	We propose , an alternative formulation of the video matting problem in terms of counterfactual video synthesis that is better suited for re-composition tasks. The goal of factor matting is to separate the contents of a video into independent components, each representing a counterfactual version of the scene where the contents of other components have been removed. We show that factor matting maps well to a more general Bayesian framing of the matting problem that accounts for complex conditional interactions between layers. Based on this observation, we present a method for solving the factor matting problem that learns augmented patch-based appearance priors to produce useful decompositions even for video with complex cross-layer interactions like splashes, shadows, and reflections. Our method is trained per-video and does not require external training data or any knowledge about the 3D structure of the scene. Through extensive experiments, we show that it is able to produce useful decompositions of scenes with such complex interactions while performing competitively on classical matting tasks as well. We also demonstrate the benefits of our approach on a wide range of downstream video editing tasks. Our project website is at: https://factormatte.github.io/.	https://dl.acm.org/doi/abs/10.1145/3592423	Zeqi Gu, Wenqi Xian, Noah Snavely, Abe Davis
Fast Complementary Dynamics via Skinning Eigenmodes	We propose a reduced-space elastodynamic solver that is well suited for augmenting rigged character animations with secondary motion. At the core of our method is a novel deformation subspace based on Linear Blend Skinning that overcomes many of the shortcomings prior subspace methods face. Our skinning subspace is parameterized entirely by a set of scalar weights, which we can obtain through a small, material-aware and rig-sensitive generalized eigenvalue problem. The resulting subspace can easily capture rotational motion and guarantees that the resulting simulation is rotation equivariant. We further propose a simple local-global solver for linear co-rotational elasticity and propose a clustering method to aggregate per-tetrahedra nonlinear energetic quantities. The result is a compact simulation that is fully decoupled from the complexity of the mesh.	https://dl.acm.org/doi/abs/10.1145/3592404	Otman Benchekroun, Jiayi Eris Zhang, Siddartha Chaudhuri, Eitan Grinspun, Yi Zhou, Alec Jacobson
Feather: 3D sketchbook light as a feather	"We present Feather, a 3D drawing system that allows users to create 3D curve artworks with a pen & touch interface on a tablet. We implemented multi-view 3D sketching by applying a concept of drawing & bending a ""3D guide"" and a view-oriented joystick widget that streamlines tedious 3D transformations—all of which are integrated into a single web application while maintaining the usability and look of conventional 2D drawing software."	https://dl.acm.org/doi/abs/10.1145/3588427.3595355	Yongkwan Kim, Kyuhyoung Hong, Junwon Yang
Film Grain Rendering and Parameter Estimation	We propose a realistic film grain rendering algorithm based on statistics derived analytically from a physics-based Boolean model that Newson et al. adopted for Monte Carlo simulations of film grain. We also propose formulas for estimation of the model parameters from scanned film grain images. The proposed rendering is computationally efficient and can be used for real-time film grain simulation for a wide range of film grain parameters when the individual film grains are not visible. Experimental results demonstrate the effectiveness of the proposed approach for both constant and real-world images, for a six orders of magnitude speed-up compared with the Monte Carlo simulations of the Newson approach.	https://dl.acm.org/doi/abs/10.1145/3592127	Kaixuan Zhang, Jingxian Wang, Daizong Tian, Thrasyvoulos N. Pappas
Finding Appeal: Creative Careers in Animation, Computer Graphics, and Interactive Techniques	Industry panelists share perspectives and insights for students and educators who are considering careers in animation, computer graphics, and interactive techniques. Creative industries continue to transform as a result of the global pandemic. Streaming media platforms, virtual production systems, faster network communications, and advances in machine learning are radically transforming creative industries and cultural production. Simultaneously, transformed workplace cultures and new technologies make room for alternative career paths, presenting a variety of opportunities and unforeseen challenges. Individual representatives discuss the general and specific state of affairs within their own industries, and provide insight into changing employment paradigms. Discussion includes advice for educators to help prepare students for changing workplace cultures, as well as the preparation, training, and personal attributes needed to enter related career fields, or make professional career transitions. Panelists consider what qualities make for desirable applicants in their respective fields, and elaborate upon changes in the transition from school to work resulting from the global pandemic. Represented industry segments include animation and VFX, virtual production, interactive design, and immersive themed entertainment. Questions considered include how pedagogy can help prepare and empower students for successful creative careers; what entry-level applicants should have (and should not have) on resumes, portfolios, and demo reels; and what can creative talents do to proactively acquire requisite credentials. Discussion will expose fresh outlooks on the futures of creative fields in animation, computer graphics, and interactive techniques	https://dl.acm.org/doi/abs/10.1145/3587424.3595586	Johannes Deyoung
Flamenco: The Simple Open Source Render Farm	Flamenco is the Open Source render farm software, developed by Blender Studio. It is aimed at performing tasks such as frame rendering and video encoding. This hands-on class will teach how to install and use it, and most importantly, how to adjust and extend it for your specific needs.	https://dl.acm.org/doi/abs/10.1145/3588029.3599738	Sybren Stüvel
Flexible Isosurface Extraction for Gradient-Based Mesh Optimization	This work considers gradient-based mesh optimization, where we iteratively optimize for a 3D surface mesh by representing it as the isosurface of a scalar field, an increasingly common paradigm in applications including photogrammetry, generative modeling, and inverse physics. Existing implementations adapt classic isosurface extraction algorithms like Marching Cubes or Dual Contouring; these techniques were designed to extract meshes from fixed, known fields, and in the optimization setting they lack the degrees of freedom to represent high-quality feature-preserving meshes, or suffer from numerical instabilities. We introduce FlexiCubes, an isosurface representation specifically designed for optimizing an unknown mesh with respect to geometric, visual, or even physical objectives. Our main insight is to introduce additional carefully-chosen parameters into the representation, which allow local adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task. We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.	https://dl.acm.org/doi/abs/10.1145/3592430	Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, Jun Gao
Fluid Cohomology	The formulation for incompressible inviscid fluids is the basis for many fluid simulation methods in computer graphics, including vortex methods, streamfunction solvers, spectral methods, and Monte Carlo methods. We point out that current setups in the vorticity-streamfunction formulation are insufficient at simulating fluids on general non-simply-connected domains. This issue is critical in practice, as obstacles, periodic boundaries, and nonzero genus can all make the fluid domain multiply connected. These scenarios introduce nontrivial cohomology components to the flow in the form of harmonic fields. The dynamics of these harmonic fields have been previously overlooked. In this paper, we derive the missing equations of motion for the fluid cohomology components. We elucidate the physical laws associated with the new equations, and show their importance in reproducing physically correct behaviors of fluid flows on domains with general topology.	https://dl.acm.org/doi/abs/10.1145/3592402	Hang Yin, Mohammad Sina Nabizadeh, Baichuan Wu, Stephanie Wang, Albert Chern
Fluid-Solid Coupling in Kinetic Two-Phase Flow Simulation	Real-life flows exhibit complex and visually appealing behaviors such as bubbling, splashing, glugging and wetting that simulation techniques in graphics have attempted to capture for years. While early approaches were not capable of reproducing multiphase flow phenomena due to their excessive numerical viscosity and low accuracy, kinetic solvers based on the lattice Boltzmann method have recently demonstrated the ability to simulate water-air interaction at high Reynolds numbers in a massively-parallel fashion. However, robust and accurate handling of fluid-solid coupling has remained elusive: be it for CG or CFD solvers, as soon as the motion of immersed objects is too fast or too sudden, pressures near boundaries and interfacial forces exhibit spurious oscillations leading to blowups. Built upon a phase-field and velocity-distribution based lattice-Boltzmann solver for multiphase flows, this paper spells out a series of numerical improvements in momentum exchange, interfacial forces, and two-way coupling to drastically reduce these typical artifacts, thus significantly expanding the types of fluid-solid coupling that we can efficiently simulate. We highlight the numerical benefits of our solver through various challenging simulation results, including comparisons to previous work and real footage.	https://dl.acm.org/doi/abs/10.1145/3592138	Wei Li, Mathieu Desbrun
Forager: Immersive Mycology Experiences and Time-lapse Photogrammetry	Forager is a multisensory virtual reality experience that immerses participants in the captivating world of fungi, delivering a profound connection with nature from various perspectives. Engaging sight, sound, touch, and scent, participants witness the complete life cycle of mushrooms, from spores to mycelium, fruiting body, and the inevitable decay. This transformative journey cultivates an appreciation for the interconnectedness of lifeforms and the wonders of the natural world on its own unique time scale.	https://dl.acm.org/doi/abs/10.1145/3588027.3603531	Winslow Porter, Elie Zananiri
Forming Terrains by Glacial Erosion	We introduce the first solution for simulating the formation and evolution of glaciers, together with their attendant erosive effects, for periods covering the combination of glacial and inter-glacial cycles. Our efficient solution includes both a fast yet accurate deep learning-based estimation of highorder ice flows and a new, multi-scale advection scheme enabling us to account for the distinct time scales at which glaciers reach equilibrium compared to eroding the terrain. We combine the resulting glacial erosion model with finer-scale erosive phenomena to account for the transport of debris flowing from cliffs. This enables us to model the formation of terrain shapes not previously adequately modeled in Computer Graphics, ranging from U-shaped and hanging valleys to fjords and glacial lakes.	https://dl.acm.org/doi/abs/10.1145/3592422	Guillaume Cordonnier, Guillaume Jouvet, Adrien Peytavie, Jean Braun, Marie-Paule Cani, Bedrich Benes, Eric Galin, Eric Guérin, James Gain
Fresh Memories: The Look	Bombed schools, hospitals, houses, and many others. War in Ukraine has taken away thousands of lives. In this VR experience, you look into the eyes of those who lost their homes or places closely connected to them. What can this look awake in us? Will it bring despair or hope?	https://dl.acm.org/doi/abs/10.1145/3577025.3584685	Ondřej Moravec
GREIL-Crowds: Crowd Simulation with Deep Reinforcement Learning and Examples	Simulating crowds with realistic behaviors is a difficult but very important task for a variety of applications. Quantifying how a person balances between different conflicting criteria such as goal seeking, collision avoidance and moving within a group is not intuitive, especially if we consider that behaviors differ largely between people. Inspired by recent advances in Deep Reinforcement Learning, we propose Guided REinforcement Learning (GREIL) Crowds, a method that learns a model for pedestrian behaviors which is guided by reference crowd data. The model successfully captures behaviors such as goal seeking, being part of consistent groups without the need to define explicit relationships and wandering around seemingly without a specific purpose. Two fundamental concepts are important in achieving these results: (a) the per agent state representation and (b) the reward function. The agent state is a temporal representation of the situation around each agent. The reward function is based on the idea that people try to move in situations/states in which they feel comfortable in. Therefore, in order for agents to stay in a comfortable state space, we first obtain a distribution of states extracted from real crowd data; then we evaluate states based on how much of an outlier they are compared to such a distribution. We demonstrate that our system can capture and simulate many complex and subtle crowd interactions in varied scenarios. Additionally, the proposed method generalizes to unseen situations, generates consistent behaviors and does not suffer from the limitations of other data-driven and reinforcement learning approaches.	https://dl.acm.org/doi/abs/10.1145/3592459	Panayiotis Charalambous, Julien Pettre, Vassilis Vassiliades, Yiorgos Chrysanthou, Nuria Pelechano
Galaxy Maps: Localized Foliations for Bijective Volumetric Mapping	A method is presented to compute volumetric maps and parametrizations of objects over 3D domains. As a key feature, continuity and bijectivity are ensured by construction. Arbitrary objects of ball topology, represented as tetrahedral meshes, are supported. Arbitrary convex as well as star-shaped domains are supported. Full control over the boundary mapping is provided. The method is based on the technique of simplicial foliations, generalized to a broader class of domain shapes and applied adaptively in a novel localized manner. This increases flexibility as well as efficiency over the state of the art, while maintaining reliability in guaranteeing map bijectivity.	https://dl.acm.org/doi/abs/10.1145/3592410	Steffen Hinderink, Marcel Campen
Generalizing Shallow Water Simulations with Dispersive Surface Waves	This paper introduces a novel method for simulating large bodies of water as a height field. At the start of each time step, we partition the waves into a (which approximately satisfies the assumptions of the shallow water equations) and (which approximately satisfy the assumptions of Airy wave theory). We then solve the two wave regimes separately using appropriate state-of-the-art techniques, and re-combine the resulting wave velocities at the end of each step. This strategy leads to the first heightfield wave model capable of simulating complex interactions between both deep and shallow water effects, like the waves from a boat wake sloshing up onto a beach, or a dam break producing wave interference patterns and eddies. We also analyze the numerical dispersion created by our method and derive an correction factor for waves at a constant water depth, giving us a numerically perfect re-creation of theoretical water wave dispersion patterns.	https://dl.acm.org/doi/abs/10.1145/3592098	Stefan Jeschke, Chris Wojtan
Generating Activity Snippets by Learning Human-Scene Interactions	We present an approach to generate virtual activity snippets, which comprise sequenced keyframes of multi-character, multi-object interaction scenarios in 3D environments, by learning from recordings of human-scene interactions. The generation consists of two stages. First, we use a sequential deep graph generative model with a temporal module to iteratively generate keyframe descriptions, which represent abstract interactions using graphs, while preserving spatial-temporal relations through the activities. Second, we devise an optimization framework to instantiate the activity snippets in virtual 3D environments guided by the generated keyframe descriptions. Our approach optimizes the poses of character and object instances encoded by the graph nodes to satisfy the relations and constraints encoded by the graph edges. The instantiation process includes a coarse 2D optimization followed by a fine 3D optimization to effectively explore the complex solution space for placing and posing the instances. Through experiments and a perceptual study, we applied our approach to generate plausible activity snippets under different settings.	https://dl.acm.org/doi/abs/10.1145/3592096	Changyang Li, Lap-Fai Yu
Generative Design of Sheet Metal Structures	Sheet Metal (SM) fabrication is perhaps one of the most common metalworking technique. Despite its prevalence, SM design is manual and costly, with rigorous practices that restrict the search space, yielding suboptimal results. In contrast, we present a framework for the first automatic design of SM parts. Focusing on load bearing applications, our novel system generates a high-performing manufacturable SM that adheres to the numerous constraints that SM design entails: The resulting part minimizes manufacturing costs while adhering to structural, spatial, and manufacturing constraints. In other words, the part should be strong enough, not disturb the environment, and adhere to the manufacturing process. These desiderata sum up to an elaborate, sparse, and expensive search space. Our generative approach is a carefully designed exploration process, comprising two steps. In connections from the input load to attachable regions are accumulated, and during the most performing valid combination is searched for. For Discovery, we define a slim grammar, and sample it for parts using a Markov-Chain Monte Carlo (MCMC) approach, ran in intercommunicating instances (i.e, chains) for diversity. This, followed by a short continuous optimization, enables building a diverse and high-quality library of substructures. During Composition, a valid and minimal cost combination of the curated substructures is selected. To improve compliance significantly without additional manufacturing costs, we reinforce candidate parts onto themselves --- a unique SM capability called we provide our code and data in https://github.com/amir90/AutoSheetMetal. We show our generative approach produces viable parts for numerous scenarios. We compare our system against a human expert and observe improvements in both part quality and design time. We further analyze our pipeline's steps with respect to resulting quality, and have fabricated some results for validation. We hope our system will stretch the field of SM design, replacing costly expert hours with minutes of standard CPU, making this cheap and reliable manufacturing method accessible to anyone.	https://dl.acm.org/doi/abs/10.1145/3592444	Amir Barda, Guy Tevet, Adriana Schulz, Amit Haim Bermano
GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents	The automatic generation of stylized co-speech gestures has recently received increasing attention. Previous systems typically allow style control via predefined text labels or example motion clips, which are often not flexible enough to convey user intent accurately. In this work, we present GestureDiffuCLIP, a neural network framework for synthesizing realistic, stylized co-speech gestures with flexible style control. We leverage the power of the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and present a novel CLIP-guided mechanism that extracts efficient style representations from multiple input modalities, such as a piece of text, an example motion clip, or a video. Our system learns a latent diffusion model to generate high-quality gestures and infuses the CLIP representations of style into the generator via an adaptive instance normalization (AdaIN) layer. We further devise a gesture-transcript alignment mechanism that ensures a semantically correct gesture generation based on contrastive learning. Our system can also be extended to allow fine-grained style control of individual body parts. We demonstrate an extensive set of examples showing the flexibility and generalizability of our model to a variety of style descriptions. In a user study, we show that our system outperforms the state-of-the-art approaches regarding human likeness, appropriateness, and style correctness.	https://dl.acm.org/doi/abs/10.1145/3592097	Tenglong Ao, Zeyi Zhang, Libin Liu
Give Life Back to Alternative Process: Exploring Handmade Photographic Printing Experiments towards Digital Nature Ecosystem	The proliferation of smartphones has made it easy for anyone to take digital photographs, and the recent popularization of text-to-image models has made it easy for anyone to create images. In this age, by combining digital technology with the tactile experience of handmade processes, we can rediscover the joy of creating with our own hands and the emotional connection that comes from physically interacting with our work. Previously, we proposed a new printing framework that integrated computer processing with full-color cyanotype printing. In this work, we demonstrate expanding the range of aesthetic expressions with computer processing for tone adjustment with several alternative processes such as salt print, platinum print, and cyanotype. In the installation, we present our printing framework with the user interface and exhibit works utilizing our proposed method. The use of new media developed after the digital age and the integration of computer processing in photo printing may be a way to create a new photographic life with the joy of materialising scenery.	https://dl.acm.org/doi/abs/10.1145/3588029.3599735	Chinatsu Ozawa, Kenta Yamamoto, Kazuya Izumi, Yoichi Ochiai
Globally Consistent Normal Orientation for Point Clouds by Regularizing the Winding-Number Field	Estimating normals with globally consistent orientations for a raw point cloud has many downstream geometry processing applications. Despite tremendous efforts in the past decades, it remains challenging to deal with an unoriented point cloud with various imperfections, particularly in the presence of data sparsity coupled with nearby gaps or thin-walled structures. In this paper, we propose a smooth objective function to characterize the requirements of an acceptable winding-number field, which allows one to find the globally consistent normal orientations starting from a set of completely random normals. By taking the vertices of the Voronoi diagram of the point cloud as examination points, we consider the following three requirements: (1) the winding number is either 0 or 1, (2) the occurrences of 1 and the occurrences of 0 are balanced around the point cloud, and (3) the normals align with the outside Voronoi poles as much as possible. Extensive experimental results show that our method outperforms the existing approaches, especially in handling sparse and noisy point clouds, as well as shapes with complex geometry/topology.	https://dl.acm.org/doi/abs/10.1145/3592129	Rui Xu, Zhiyang Dou, Ningna Wang, Shiqing Xin, Shuangmin Chen, Mingyan Jiang, Xiaohu Guo, Wenping Wang, Changhe Tu
Guided Linear Upsampling	Guided upsampling is an effective approach for accelerating high-resolution image processing. In this paper, we propose a simple yet effective guided upsampling method. Each pixel in the high-resolution image is represented as a linear interpolation of two low-resolution pixels, whose indices and weights are optimized to minimize the upsampling error. The downsampling can be jointly optimized in order to prevent missing small isolated regions. Our method can be derived from the and Compared to previous methods, our method can better preserve detail effects while suppressing artifacts such as bleeding and blurring. It is efficient, easy to implement, and free of sensitive parameters. We evaluate the proposed method with a wide range of image operators, and show its advantages through quantitative and qualitative analysis. We demonstrate the advantages of our method for both interactive image editing and real-time high-resolution video processing. In particular, for interactive editing, the joint optimization can be precomputed, thus allowing for instant feedback without hardware acceleration.	https://dl.acm.org/doi/abs/10.1145/3592453	Shuangbing Song, Fan Zhong, Tianju Wang, Xueying Qin, Changhe Tu
Guided Training of NeRFs for Medical Volume Rendering	Neural Radiance Fields (NeRF) trained on pre-rendered photorealistic images represent complex medical data in a fraction of the size, while interactive applications synthesize novel views directly from the neural networks. We demonstrate a practical implementation of NeRFs for high resolution CT volume data, using differentiable rendering for training view selection.	https://dl.acm.org/doi/abs/10.1145/3588028.3603657	Kaloian Petkov
Gumball Dreams: Live Theatre in VR	Since 2020 Ferryman Collective has been pushing the boundaries of what is possible in theater by using VR technology to give audiences the ability to become a part of the story in a fully immersive way. VR theatre offers the feeling of embodiment and immersion that closely mimics the feeling of going to a theater in person, with the added interactivity of gaming, and the cinematic qualities of film. Ferryman Collective productions have been well received at the most prestigious festivals in the world and garnered many awards along the way. Gumball Dreams centers around the story of an ancient alien on the precipice of transitioning from this life to the next. Experience an excerpt of Ferryman Collective and Screaming Color's award-winning production, Gumball Dreams. A one-on-one journey with an actor in VR for a 15-minute portion of this immersive theater play.	https://dl.acm.org/doi/abs/10.1145/3588027.3595593	Deirdre V. Lyons, Christopher Lane Davis, Stephen Butchko, Whitton Frank, Brian Tull, Braden Roy
HACK: Learning a Parametric Head and Neck Model for High-fidelity Animation	Significant advancements have been made in developing parametric models for digital humans, with various approaches concentrating on parts such as the human body, hand, or face. Nevertheless, connectors such as the neck have been overlooked in these models, with rich anatomical priors often unutilized. In this paper, we introduce HACK (Head-And-neCK), a novel parametric model for constructing the head and cervical region of digital humans. Our model seeks to disentangle the full spectrum of neck and larynx motions, facial expressions, and appearance variations, providing personalized and anatomically consistent controls, particularly for the neck regions. To build our HACK model, we acquire a comprehensive multi-modal dataset of the head and neck under various facial expressions. We employ a 3D ultrasound imaging scheme to extract the inner biomechanical structures, namely the precise 3D rotation information of the seven vertebrae of the cervical spine. We then adopt a multi-view photometric approach to capture the geometry and physically-based textures of diverse subjects, who exhibit a diverse range of static expressions as well as sequential head-and-neck movements. Using the multi-modal dataset, we train the parametric HACK model by separating the 3D head and neck depiction into various shape, pose, expression, and larynx blendshapes from the neutral expression and the rest skeletal pose. We adopt an anatomically-consistent skeletal design for the cervical region, and the expression is linked to facial action units for artist-friendly controls. We also propose to optimize the mapping from the identical shape space to the PCA spaces of personalized blendshapes to augment the pose and expression blendshapes, providing personalized properties within the framework of the generic model. Furthermore, we use larynx blendshapes to accurately control the larynx deformation and force the larynx slicing motions along the vertical direction in the UV-space for precise modeling of the larynx beneath the neck skin. HACK addresses the head and neck as a unified entity, offering more accurate and expressive controls, with a new level of realism, particularly for the neck regions. This approach has significant benefits for numerous applications, including geometric fitting and animation, and enables inter-correlation analysis between head and neck for fine-grained motion synthesis and transfer.	https://dl.acm.org/doi/abs/10.1145/3592093	Longwen Zhang, Zijun Zhao, Xinzhou Cong, Qixuan Zhang, Shuqi Gu, Yuchong Gao, Rui Zheng, Wei Yang, Lan Xu, Jingyi Yu
Haptics for Accessible Graphics in CS1	Tactile or haptic presentations of computer graphics visualizations can offer more inclusive experiences for persons who are blind or have low vision. However, teaching accessibility with haptics is challenging due to a lack of ready-to-use instructional resources. This Engaging Education Technique and Assignment (EETA) introduces accessibility with haptics and Turtle Graphics after only a few weeks into CS1. The sample assignments feature open-ended exploration to meaningfully map graphical attributes to haptic feedback that could benefit blind or low vision users. Supplementary materials include lesson guides and Python samples.	https://dl.acm.org/doi/abs/10.1145/3587424.3595584	William Bares, Jordan Wirfs-Brock
Heightened Empathy: A Multi-user Interactive Experience in a Bioresponsive Virtual Reality	We present Heightened Empathy, a multi-user interactive experience in bioresponsive virtual reality (VR) designed to visualize emotional states and stimulate different types of empathy between two players in various interactive modes. The experience immerses users in a VR representation of each other's emotional state, while also reflecting this to the audience using a table-top social robot and projection as they interact with each other. In competitive mode, the goal is to promote cognitive empathy where each user needs to understand the new emotional representation in VR to win. In communication mode, we have users act on one another verbally to promote emotional empathy. The experience aims to enhance empathy perception by placing the players in virtual environments that require them to understand, share, and act on each other's perspectives and emotions.	https://dl.acm.org/doi/abs/10.1145/3588027.3595599	Mark Armstrong, Kinga Skiers, Danyang Peng, Tamil Selvan Gunasekaran, Anish Kundu, Tanner Person, Yixin Wang, Kouta Minamizawa, Yun Suen Pai
Holy Fire Pilot Version	Holy Fire is an interactive documentary experience that explores humankind's difficult relationship with nuclear energy. It looks at the technology's tragic upscaling into risky nuclear power plants all across the world and examines the early days, when scientists were full of hope after discovering this genuine philosopher's stone.	https://dl.acm.org/doi/abs/10.1145/3577025.3584990	Ben Wahl, Georg Hobmeier, Lisley Viraphong
HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion	Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF , a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions . We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis.	https://dl.acm.org/doi/abs/10.1145/3592415	Mustafa Işık, Martin Rünz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, Matthias Nießner
Identity-Centered User-Generated Content to Create Occupational Identity with Visual Computing	This talk reports on an approach to computer science education for pre- and early-adolescents in which the goal is the formation of occupational identity with visual computing developed collaboratively between a large game studio, a research-intensive university, and a historically black college/university (HBCU). This ongoing project takes place in a rural public-school setting in the United States. Our project is structured around the idea that identity-centered user-content creation projects can positively influence student self-professed performance and interest in science, technology, engineering, art, and math (STEAM) related subjects and interest in STEAM careers. The projects in our curriculum engage students in real-time 3DCG coding and asset creation activities commonly associated with game development. We describe the process of working with school administrators and teachers to create a technology-infused environment in which remote external partners play a collaborative role in curriculum development and delivery. This work has significance for efforts to remotely engage with rural students at an age when occupational identity development is forming and thus develop a potential to expand the pathway for underrepresented minorities.	https://dl.acm.org/doi/abs/10.1145/3587424.3595583	Tim McLaughlin, Tracey L Moore, Aaron Peter Thibault, Kim Boddie Wright, Hira Tariq Roberts, Jinsil Hwaryoung Seo, Jason M. Orsatti
Image vectorization and editing via linear gradient layer decomposition	A key advantage of vector graphics over raster graphics is their editability. For example, linear gradients define a spatially varying color fill with a few intuitive parameters, which are ubiquitously supported in standard vector graphics formats and libraries. By layering regions filled with linear gradients, complex appearances can be created. We propose an automatic method to convert a raster image into layered regions of linear gradients. Given an input raster image segmented into regions, our approach decomposes the resulting regions into opaque and semi-transparent linear gradient fills. Our approach is fully automatic (e.g., users do not identify a background as in previous approaches) and exhaustively considers all possible decompositions that satisfy perceptual cues. Experiments on a variety of images demonstrate that our method is robust and effective.	https://dl.acm.org/doi/abs/10.1145/3592128	Zheng-Jun Du, Liang-Fu Kang, Jianchao Tan, Yotam Gingold, Kun Xu
Imperceptible Color Modulation for Power Saving in VR/AR	Untethered VR/AR HMDs can only last 2-3 hours on a single charge. Toward resolving this issue, we develop a real-time gaze-contingent power saving filter which modulates peripheral pixel color while preserving visual fidelity. At SIGGRAPH 2023, participants will be able to view a short panoramic video within a VR HMD with our perceptually-aware power saving filter turned on. Participants will also have the opportunity to view the power output of scenes through our power measurement setup.	https://dl.acm.org/doi/abs/10.1145/3588037.3595388	Kenneth Chen, Budmonde Duinkharjav, Nisarg Ujjainkar, Ethan Shahan, Abhishek Tyagi, Jiayi He, Yuhao Zhu, Qi Sun
Improved Automatic Colorization by Optimal Pre-colorization	"Automatic line-drawings colorization of anime characters is a challenging problem in computer graphics. The previous fully automatic colorization method suffers from colorization accuracy and costs colorization artists to validate and correct colorization errors. We propose to improve the colorization accuracy by introducing ""pre-colorization"" step into our production pipeline that requests user to manually colorize partial regions in line-drawings before doing automatic colorization. The pre-colorized regions work as clues to colorize the other regions and improve the colorization accuracy. We found an optimal region to be pre-colorized to obtain the best automatic colorization performance."	https://dl.acm.org/doi/abs/10.1145/3588028.3603669	Taiki Watanabe, Seitaro Shinagawa, Takuya Funatomi, Akinobu Maejima, Yasuhiro Mukaigawa, Satoshi Nakamura, Hiroyuki Kubo
Improved Water Sound Synthesis using Coupled Bubbles	We introduce a practical framework for synthesizing bubble-based water sounds that captures the rich inter-bubble coupling effects responsible for low-frequency acoustic emissions from bubble clouds. We propose coupled-bubble oscillator models with regularized singularities, and techniques to reduce the computational cost of time stepping with dense, time-varying mass matrices. Airborne acoustic emissions are estimated using finite-difference time-domain (FDTD) methods. We propose a simple, analytical surface-acceleration model, and a sample-and-hold GPU wavesolver that is simple and faster than prior CPU wavesolvers. Sound synthesis results are demonstrated using bubbly flows from incompressible, two-phase simulations, as well as procedurally generated examples using single-phase FLIP fluid animations. Our results demonstrate sound simulations with hundreds of thousands of bubbles, and perceptually significant frequency transformations with fuller low-frequency content.	https://dl.acm.org/doi/abs/10.1145/3592424	Kangrui Xue, Ryan M. Aronson, Jui-Hsien Wang, Timothy R. Langlois, Doug L. James
In-Timestep Remeshing for Contacting Elastodynamics	We propose In-Timestep Remeshing, a fully coupled, adaptive meshing algorithm for contacting elastodynamics where remeshing steps are tightly integrated, implicitly, within the timestep solve. Our algorithm refines and coarsens the domain automatically by measuring physical energy changes within each ongoing timestep solve. This provides consistent, degree-of-freedom-efficient, productive remeshing that, by construction, is physics-aware and so avoids the errors, over-refinements, artifacts, per-example hand-tuning, and instabilities commonly encountered when remeshing with timestepping methods. Our in-timestep computation then ensures that each simulation step's output is both a converged stable solution on the updated mesh and a temporally consistent trajectory with respect to the model and solution of the last timestep. At the same time, the output is guaranteed safe (intersection- and inversion-free) across all operations. We demonstrate applications across a wide range of extreme stress tests with challenging contacts, sharp geometries, extreme compressions, large timesteps, and wide material stiffness ranges - all scenarios well-appreciated to challenge existing remeshing methods.	https://dl.acm.org/doi/abs/10.1145/3592428	Zachary Ferguson, Teseo Schneider, Danny Kaufman, Daniele Panozzo
Inclusive Quiet Room -for building an inclusive society-	"A large percentage of people with autism or developmental disorders, which are mental disabilities, have sensory hypersensitivity. Therefore, the spread of ""quiet rooms"" in which they can feel at ease in social life is a necessary element in realizing a symbiotic society. However, the high cost of installing quiet rooms, which require highly soundproof rooms isolated from the outside, is an obstacle to their widespread use. The Inclusive Quiet Room is a new concept of portable quiet rooms that combines an easy-to-construct instant house, immersive videos, and relaxing sounds. In addition to enabling many people to experience the benefits of the room, the work proposes an image of the future quiet rooms that can be easily constructed anywhere. In this paper, we analyze the effectiveness of the Inclusive Quiet Room, exhibited in France, based on survey data from 372 respondents. Through the analysis, the relaxation effects and the demands for quiet rooms are substantiated. The room gives the feeling of being warmly embraced and secured. If all people including those without mental disorders could experience this embraced feeling, they would understand the need and benefits of relaxing environments for the people with sensory hypersensitivities."	https://dl.acm.org/doi/abs/10.1145/3588037.3603420	Shoko Kimura, Kenichi Ito, Ayaka Fujii, Rihito Tsuboi, Kazuki Okawa, Hibiki Kojima, Keisuke Kitagawa, Yoshinori Natsume
Inkjet 4D Print: Self-folding Tessellated Origami Objects by Inkjet UV Printing	We propose Inkjet 4D Print, a self-folding fabrication method of 3D origami tessellations by printing 2D patterns on both sides of a heat-shrinkable base sheet, using a commercialized inkjet ultraviolet (UV) printer. Compared to the previous folding-based 4D printing approach using fused deposition modeling (FDM) 3D printers [An et al. 2018], our method has merits in (1) more than 1200 times higher resolution in terms of the number of self-foldable facets, (2) 2.8 times faster printing speed, and (3) optional full-color decoration. This paper describes the material selection, the folding mechanism, the heating condition, and the printing patterns to self-fold both known and freeform tessellations. We also evaluated the self-folding resolution, the printing and transformation speed, and the shape accuracy of our method. Finally, we demonstrated applications enabled by our self-foldable tessellated objects.	https://dl.acm.org/doi/abs/10.1145/3592409	Koya Narumi, Kazuki Koyama, Kai Suto, Yuta Noma, Hiroki Sato, Tomohiro Tachi, Masaaki Sugimoto, Takeo Igarashi, Yoshihiro Kawahara
Interactive AI Material Generation and Editing in NVIDIA Omniverse	We present an AI-based tool for interactive material generation within the NVIDIA Omniverse environment. Our approach leverages a State-of-the-art Latent Diffusion model with some notable modifications to adapt it to the task of material generation. Specifically, we employ circular-padded convolution layers in place of standard convolution layers. This unique adaptation ensures the production of seamless tiling textures, as the circular padding facilitates seamless blending at image edges. Moreover, we extend the capabilities of our model by training additional decoders to generate various material properties such as surface normals, roughness, and ambient occlusions. Each decoder utilizes the same latent tensor generated by the de-noising UNet to produce a specific material channel. Furthermore, to enhance real-time performance and user interactivity, we optimize our model using NVIDIA TensorRT, resulting in improved inference speed for an efficient and responsive tool.	https://dl.acm.org/doi/abs/10.1145/3588430.3597248	Hassan Abu Alhaija, James Lucas, Alexander Zook, Michael Babcock, David Tyner, Rajeev Rao, Maria Shugrina
Intermediated Reality with an AI 3D Printed Character	We introduce live character conversational interactions in Intermediated Reality to bring real-world objects to life in Augmented Reality (AR) and Artificial Intelligence (AI). The AI recognizes live speech and generates short character responses, syncing the character's facial expressions with speech audio. The Intermediated Reality AR warping technique allows for a high degree of realism in the animated facial expressions and movements reusing live video optical appearance direct from the device's embedded camera. The proposed applications of Intermediated Reality with AI are exemplified through the captivating fusion of these technologies in toy interactive storytelling broadcasts and social telepresence scenarios. This innovative combination allows individuals to engage with AI characters in a natural and intuitive manner, creating new opportunities for social engagement and entertainment.	https://dl.acm.org/doi/abs/10.1145/3588430.3597251	Llogari Casas, Kenny Mitchell
Juxtaform: interactive visual summarization for exploratory shape design	We present , a novel approach to the interactive summarization of large shape collections for conceptual shape design. We conduct a formative study to ascertain design goals for creative shape exploration tools. Motivated by a mathematical formulation of these design goals, juxtaform integrates the exploration, analysis, selection, and refinement of large shape collections to support an interactive divergence-convergence shape design workflow. We exploit sparse, segmented sketch-stroke visual abstractions of shape and a novel visual summarization algorithm to balance the needs of shape understanding, shape juxtaposition, and visual clutter. Our evaluation is three-fold: we show that existing shape and stroke clustering algorithms do not address our design goals compared to our proposed shape corpus summarization algorithm; we compare juxtaform against a structured image gallery interface for various shape design and analysis tasks; and we present multiple compelling 2D/3D applications using juxtaform.	https://dl.acm.org/doi/abs/10.1145/3592436	Karran Pandey, Fanny Chevalier, Karan Singh
Kindred	Based on the true story of one aspirational LGBTQ+ parent's ground-breaking journey through the adoption process in the United Kingdom, KINDRED highlights unique challenges faced by non-binary parents in the U.K.'s outdated adoption process and tells the remarkable story of Syd and Ollie who helped redefine the meaning of family.	https://dl.acm.org/doi/abs/10.1145/3577025.3583567	Lee Harris
La Diplomatie de L'éclipse	At the moment of alignment, of total eclipse, humanity will vanish, this was the message sent by the Sun and the Moon. The World Council decides to send his best negociator to stop the end of the world.	https://dl.acm.org/doi/abs/10.1145/3577024.3588734	César Luton
Language-based Photo Color Adjustment for Graphic Designs	Adjusting the photo color to associate with some design elements is an essential way for a graphic design to effectively deliver its message and make it aesthetically pleasing. However, existing tools and previous works face a dilemma between the ease of use and level of expressiveness. To this end, we introduce an interactive language-based approach for photo recoloring, which provides an intuitive system that can assist both experts and novices on graphic design. Given a graphic design containing a photo that needs to be recolored, our model can predict the source colors and the target regions, and then recolor the target regions with the source colors based on the given language-based instruction. The multi-granularity of the instruction allows diverse user intentions. The proposed novel task faces several unique challenges, including: for recoloring with exactly the same color from the target design element as specified by the user; for parsing instructions correctly to generate a specific result or multiple plausible ones; and for recoloring in semantically meaningful local regions to preserve original image semantics. To address these challenges, we propose a model called with two main components: the language-based source color prediction module and the semantic-palette-based photo recoloring module. We also introduce an approach for generating a synthetic graphic design dataset with instructions to enable model training. We evaluate our model via extensive experiments and user studies. We also discuss several practical applications, showing the effectiveness and practicality of our approach. Please find the code and data at https://zhenwwang.github.io/langrecol.	https://dl.acm.org/doi/abs/10.1145/3592111	Zhenwei Wang, Nanxuan Zhao, Gerhard Hancke, Rynson W.H. Lau
Learning Human-like Locomotion Based on Biological Actuation and Rewards	We propose a method of learning a policy for human-like locomotion via deep reinforcement learning based on a human anatomical model, muscle actuation, and biologically inspired rewards, without any inherent control rules or reference motions. Our main ideas involve providing a dense reward using metabolic energy consumption at every step during the initial stages of learning and then transitioning to a sparse reward as learning progresses, and adjusting the initial posture of the human model to facilitate the exploration of locomotion. Additionally, we compared and analyzed differences in learning outcomes across various settings other than the proposed method.	https://dl.acm.org/doi/abs/10.1145/3588028.3603646	Minkwan Kim, Yoonsang Lee
Learning Physically Simulated Tennis Skills from Broadcast Videos	We present a system that learns diverse, physically simulated tennis skills from large-scale demonstrations of tennis play harvested from broadcast videos. Our approach is built upon hierarchical models, combining a low-level imitation policy and a high-level motion planning policy to steer the character in a motion embedding learned from broadcast videos. When deployed at scale on large video collections that encompass a vast set of examples of real-world tennis play, our approach can learn complex tennis shotmaking skills and realistically chain together multiple shots into extended rallies, using only simple rewards and without explicit annotations of stroke types. To address the low quality of motions extracted from broadcast videos, we correct estimated motion with physics-based imitation, and use a hybrid control policy that overrides erroneous aspects of the learned motion embedding with corrections predicted by the high-level policy. We demonstrate that our system produces controllers for physically-simulated tennis players that can hit the incoming ball to target positions accurately using a diverse array of strokes (serves, forehands, and backhands), spins (topspins and slices), and playing styles (one/two-handed backhands, left/right-handed play). Overall, our system can synthesize two physically simulated characters playing extended tennis rallies with simulated racket and ball dynamics. Code and data for this work is available at https://research.nvidia.com/labs/toronto-ai/vid2player3d/.	https://dl.acm.org/doi/abs/10.1145/3592408	Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, Kayvon Fatahalian
Learning to Simulate Crowds with Crowds	Controlling agent behaviors with Reinforcement Learning is of continuing interest in multiple areas. One major focus is to simulate multi-agent crowds that avoid collisions while locomoting to their goals. Although avoiding collisions is important, it is also necessary to capture realistic anticipatory navigation behaviors. We introduce a novel methodology that includes: 1) an RL method for learning an optimal navigational policy, 2) position-based constraints for correcting policy navigational decisions, and 3) a crowd-sourcing framework for selecting policy control parameters. Based on optimally selected parameters, we train a multi-agent navigation policy, which we demonstrate on crowd benchmarks. We compare our method to existing works, and demonstrate that our approach achieves superior multi-agent behaviors.	https://dl.acm.org/doi/abs/10.1145/3588028.3603670	Bilas Talukdar, Yunhao Zhang, Tomer Weiss
Les Pieds en Haut : Lou	There are as many ways to be autistic as there are autistic people. LOU --- Kid/Teen lets people experience what it's like to be in the body of an autistic child named Lou, to see and hear through his eyes and ears.	https://dl.acm.org/doi/abs/10.1145/3577025.3581770	Philippe Chrusten
Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models	Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-expert ensembles of diffusion models and demonstrate how these may be used for, e.g., style interpolation, a contribution we believe is of independent interest.	https://dl.acm.org/doi/abs/10.1145/3592458	Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter
LivEdge: Haptic Live Stream Interaction on a Smartphone by Electro-Tactile Sensation Through the Edges	We present LivEdge, a novel method for live stream interaction on smartphones utilizing electro-tactile sensation through the edges. Conventional interactions between users and a streamer on a smartphone are restricted to the streamer's response through user comments or effects. Our goal is to provide a more immersive interaction through the use of haptic technology. LivEdge can convey spatial tactile sensations through electrical stimulations from electrode arrays affixed to both edges of the smartphone. This spatial tactile stimulus represents the streamer's physical presence and movements in contact with the edge of the screen. Preliminary experiment showed LivEdge enhances the live stream experience.	https://dl.acm.org/doi/abs/10.1145/3588037.3595386	Taiki Takami, Taiga Saito, Takayuki Kameoka, Hiroyuki Kajimoto
Locally Attentional SDF Diffusion for Controllable 3D Shape Generation	Although the recent rapid evolution of 3D generative neural networks greatly improves 3D shape generation, it is still not convenient for ordinary users to create 3D shapes and control the local geometry of generated shapes. To address these challenges, we propose a diffusion-based 3D generation framework --- , to model plausible 3D shapes, via 2D sketch image input. Our method is built on a two-stage diffusion model. The first stage, named , aims to generate a low-resolution occupancy field to approximate the shape shell. The second stage, named , synthesizes a high-resolution signed distance field within the occupied voxels determined by the first stage to extract fine geometry. Our model is empowered by a novel mechanism for image-conditioned shape generation, which takes advantage of 2D image patch features to guide 3D voxel feature learning, greatly improving local controllability and model generalizability. Through extensive experiments in sketch-conditioned and category-conditioned 3D shape generation tasks, we validate and demonstrate the ability of our method to provide plausible and diverse 3D shapes, as well as its superior controllability and generalizability over existing work.	https://dl.acm.org/doi/abs/10.1145/3592103	Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, Heung-Yeung Shum
Locally Meshable Frame Fields	The main robustness issue of state-of-the-art frame field based hexahedral mesh generation algorithms originates from non-meshable topological configurations, which do not admit the construction of an integer-grid map but frequently occur in smooth frame fields. In this article, we investigate the topology of frame fields and derive conditions on their meshability, which are the basis for a novel algorithm to automatically turn a given non-meshable frame field into a similar but locally meshable one. Despite local meshability is only a necessary but not sufficient condition for the stronger requirement of meshability, our algorithm increases the 2% success rate of generating valid integer-grid maps with state-of-the-art methods to 58%, when compared on the challenging HexMe dataset [Beaufort et al. 2022]. The source code of our implementation and the data of our experiments are available at https://lib.algohex.eu.	https://dl.acm.org/doi/abs/10.1145/3592457	Heng Liu, David Bommes
Looking Back to Look Forward: Keeping SIGGRAPH Vibrant for Another 50 Years: SIGGRAPH 2023 Retrospective Panel	Panelists will discuss the effects of rapid technological change, global economic shifts, and conference site characteristics on the SIGGRAPH conference during its first 50 occurrences.	https://dl.acm.org/doi/abs/10.1145/3587422.3598009	David Kasik
Luna: Episode 1 --- Left Behind	LUNA: EPISODE 1 --- LEFT BEHIND is an interactive VR story about a robot and a little girl trying to survive an AI apocalypse. The experience uses voice recognition to let the player talk to the characters.	https://dl.acm.org/doi/abs/10.1145/3577025.3580493	Charuvit Wannissorn
Lustration	Lustration is a four-part animated series following a group of characters from both the real world and afterlife. As these characters' stories unfold and intersect, we discover the lengths that some will go to in the name of love while uncovering a vast conspiracy that will change everything.	https://dl.acm.org/doi/abs/10.1145/3577025.3579653	Nathan Anderson
MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes	Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.	https://dl.acm.org/doi/abs/10.1145/3592426	Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, Peter Hedman
Malacabra	What if, a goat ended the Mayan civilization?	https://dl.acm.org/doi/abs/10.1145/3577024.3588034	Marylou Bort
Masonry Shell Structures with Discrete Equivalence Classes	This paper proposes a method to model masonry shell structures where the shell elements fall into a set of discrete equivalence classes. Such shell structure can reduce the fabrication cost and simplify the physical construction due to reuse of a few template shell elements. Given a freeform surface, our goal is to generate a small set of template shell elements that can be reused to produce a seamless and buildable structure that closely resembles the surface. The major technical challenge in this process is balancing the desire for high reusability of template elements with the need for a seamless and buildable final structure. To address the challenge, we define three error metrics to measure the seamlessness and buildability of shell structures made from discrete equivalence classes and develop a hierarchical cluster-and-optimize approach to generate a small set of template elements that produce a structure closely approximating the surface with low error metrics. We demonstrate the feasibility of our approach on various freeform surfaces and geometric patterns, and validate buildability of our results with four physical prototypes. Code and data of this paper are at https://github.com/Linsanity81/TileableShell.	https://dl.acm.org/doi/abs/10.1145/3592095	Rulin Chen, Pengyun Qiu, Peng Song, Bailin Deng, Ziqi Wang, Ying He
Material Texture Design: Texture Representation System Utilizing Pseudo-Attraction Force Sensation	We propose Material Texture Design, a material texture representation system. This system presents a pseudo-attraction force sensation in response to the user's motion, and displays a shear sensation at the fingertips. The user perceives a change in the center of gravity from the shear sensation and feels the artificial material texture. Experimental results showed that the perceived texture could be changed by adjusting the frequency. Through demonstration, users can distinguish different textures such as water, jelly, or a rubber ball, depending on the frequency and latency. We propose this system as a small, lightweight, and simple implementation system for texture representation.	https://dl.acm.org/doi/abs/10.1145/3588037.3595397	Masaharu Hirose, Masahiko Inami
Materialistic: Selecting Similar Materials in Images	Separating an image into meaningful underlying components is a crucial first step for both editing and understanding images. We present a method capable of selecting the regions of a photograph exhibiting the same material as an artist-chosen area. Our proposed approach is robust to shading, specular highlights, and cast shadows, enabling selection in real images. As we do not rely on semantic segmentation (different woods or metal should not be selected together), we formulate the problem as a similarity-based grouping problem based on a user-provided image location. In particular, we propose to leverage the unsupervised DINO [Caron et al. 2021] features coupled with a proposed Cross-Similarity Feature Weighting module and an MLP head to extract material similarities in an image. We train our model on a new synthetic image dataset, that we release. We show that our method generalizes well to real-world images. We carefully analyze our model's behavior on varying material properties and lighting. Additionally, we evaluate it against a hand-annotated benchmark of 50 real photographs. We further demonstrate our model on a set of applications, including material editing, in-video selection, and retrieval of object photographs with similar materials.	https://dl.acm.org/doi/abs/10.1145/3592390	Prafull Sharma, Julien Philip, Michaël Gharbi, Bill Freeman, Fredo Durand, Valentin Deschaintre
Mechanical Brain Hacking : A VR Cybernetics Simulator for Body Editing	"Avatar technology has attracted attention, but research in the field of non-humanoid avatars has mainly focused on evaluating the operability and embodiment of new body systems. In addition to these points, we believe that users' design of their own avatar structure and body motion mapping is also important to improve the user experience. We present Mechanical Brain Hacking, a VR cybernetics simulator that allows for easy modification of both the mapping and the structure of the avatar. Participants take on the role of a damaged robot with limited mobility except for the right hand. They use a surgical machine linked to their avatar's hand movements to access their own brain and repair its circuits to regain control of their limbs. Haptic and electrical stimulation feedback, along with visual and auditory feedback through a VR headset, provide a fully immersive experience. Participants can even gain an extra limb by attaching additional limb parts. Through the experience of changing motion mapping and body structure in real time, this project offers a unique perspective on the concept of an ""editable body"", a body that is edited by oneself and in real time."	https://dl.acm.org/doi/abs/10.1145/3588027.3595595	Shuto Takashita, Naoki Tanaka, Taiga Suzuki
Melody Slot Machine HD	Melody Slot Machine HD is an application that allows users to experience generating melodies using musical structures while playing slot machines. To make it possible, we use the Generative Theory of Tonal Music (GTTM) to analyze the melody structure and generate variations of the melody. All variation melodies created on the basis of GTTM have a similar musical structure, so even if the melody switches to another variation in the middle, the overall structure will not collapse. It is possible to see the player on the display playing a new combination of a melody generated interactively by the user's manipulation.	https://dl.acm.org/doi/abs/10.1145/3588427.3595356	Masatoshi Hamanaka
Meso-Facets for Goniochromatic 3D Printing	Goniochromatic materials and objects appear to have different colors depending on viewing direction. This occurs in nature, such as in wood or minerals, and in human-made objects such as metal and effect pigments. In this paper, we propose algorithms to control multi-material 3D printers to produce goniochromatic effects on arbitrary surfaces by procedurally augmenting the input surface with meso-facets, which allow distinct colors to be assigned to different viewing directions of the input surface while introducing minimal changes to that surface. Previous works apply only to 2D or 2.5D surfaces, require multiple fabrication technologies, or make considerable changes to the input surface and require special post-processing, whereas our approach requires a single fabrication technology and no special post-processing. Our framework is general, allowing different generating functions for both the shape and color of the facets. Working with implicit representations allows us to generate geometric features at the limit of device resolution without tessellation. We evaluate our approach for performance, showing negligible overhead compared to baseline color 3D print processing, and for goniochromatic quality.	https://dl.acm.org/doi/abs/10.1145/3592137	Lubna Abu Rmaileh, Alan Brunton
Metro Re-illustrated: Incremental Generation of Stylized Paintings Using Neural Networks	Metro Re-illustrated is a project that explores incremental generation of stylized paintings of city metro maps using neural networks. It begins with an interactive system for labeling time-series data on city metro maps and generating reference images. These images are fed into a neural painter that incrementally generates oil painting-like strokes on virtual canvases. The generated paintings demonstrate blending and layering features of oil paintings while capturing the progressive nature of urban development.	https://dl.acm.org/doi/abs/10.1145/3588028.3603658	Bo Shui, Chufan Shi, Xiaomei Nie
Micro-Mesh Construction	Micro-meshes (μ-meshes) are a new structured graphics primitive supporting a large increase in geometric fidelity, without commensurate memory and run-time processing costs, consisting of a base mesh enriched by a displacement map. A new generation of GPUs supports this structure with native hardware μ-mesh ray-tracing, that leverages a self-bounding, compressed displacement mapping scheme to achieve these efficiencies. In this paper, we present anautomatic method to convert an existing multi-million triangle mesh into this compact format, unlocking the advantages of the data representation for a large number of scenarios. We identify the requirements for high-quality μ-meshes, and show how existing re-meshing and displacement-map baking tools are ill-suited for their generation. Our method is based on a simplification scheme tailored to the generation of high-quality , optimized for tessellation and displacement sampling, in conjunction with algorithms for determining to control the direction and range of displacements. We also explore the optimization of μ-meshes for texture maps and the representation of boundaries. We demonstrate our method with extensive batch processing, converting an existing collection of high-resolution scanned models to the micro-mesh representation, providing an open-source reference implementation, and, as additional material, the data and an inspection tool.	https://dl.acm.org/doi/abs/10.1145/3592440	Andrea Maggiordomo, Henry Moreton, Marco Tarini
Min-Deviation-Flow in Bi-directed Graphs for T-Mesh Quantization	Subdividing non-conforming T-mesh layouts into conforming quadrangular meshes is a core component of state-of-the-art (re-)meshing methods. Typically, the required constrained assignment of integer lengths to T-Mesh edges is left to generic branch-and-cut solvers, greedy heuristics, or a combination of the two. This either does not scale well with input complexity or delivers suboptimal result quality. We introduce the in bi-directed networks (Bi-MDF) and demonstrate its use in modeling and efficiently solving a variety of T-Mesh quantization problems. We develop a fast approximate solver as well as an iterative refinement algorithm based on matching in graphs that solves Bi-MDF exactly. Compared to the state-of-the-art QuadWild [Pietroni et al. 2021] implementation on the authors' dataset, our exact solver finishes after only 0.49% (total 17.06s) of their runtime (3491s) and achieves 11% lower energy while an approximation is computed after 0.09% (3.19s) of their runtime at the cost of 24% increased energy. A novel half-arc-based T-Mesh quantization formulation extends the feasible solution space to include previously unattainable quad meshes. The Bi-MDF problem is more general than our application in layout quantization, potentially enabling similar speedups for other optimization problems that fit into the scheme, such as quad mesh refinement.	https://dl.acm.org/doi/abs/10.1145/3592437	Martin Heistermann, Jethro Warnett, David Bommes
Missing 10 Hours VR	Missing 10 Hours makes you realize how much the decision and actions of bystanders can drastically influence the course of an evening. In this interactive VR piece, the viewer is led on a night out by Greg, a big-headed guy with bad intentions.	https://dl.acm.org/doi/abs/10.1145/3577025.3584977	Fanni Fazakas
Mixed Design in Reality: How to translate a VR Game to a MR World and What’s some design issues about it?	Adapting a VR game to MR posed challenges in 360º gameplay, UI, security, and thematic consistency. By prioritizing safety, readability, and engagement, we created an enjoyable experience. In this abstract, we explore the design challenges and decisions taken to address these issues.	https://dl.acm.org/doi/abs/10.1145/3588027.3595604	Marcelo Nery, Kako D'Angello
Mixed Reality Racing: Combining Real and Virtual Motorsport Racing	In this work, we proposed a proof-of-concept system that combines real and virtual racing, allowing professional racing drivers and e-racers to compete in real time. The real race car is imported into a racing game, and the virtual game car is exported into the real world in AR. This allows e-racers to compete against professional racing drivers and gives the audience an exciting experience from the grandstand with the use of AR. Our system was deployed during an actual racing event, and we gathered initial feedback from users. Both e-racers and audience expressed excitement about our system, indicating a strong potential for this new form of mixed reality racing.	https://dl.acm.org/doi/abs/10.1145/3588028.3603645	Hsuehhan Wu, Kelvin Cheng, Madoka Inoue, Soh Masuko
Mixed Reality Visualization of Room Impulse Response Map using Room Geometry and Physical Model of Sound Propagation	In this paper, an MR visualization method based on sound field modeling is proposed. Using a small quantity of measurement data, the sound field was modeled using equivalent sources and room shapes acquired via SLAM. From the modeled sound field, the estimated room impulse responses at the target grid points were then animated to visualize the sound field using MR technology. Consequently, the animation of the sound field in MR clearly represented how sound propagates, including reflections.	https://dl.acm.org/doi/abs/10.1145/3588028.3603693	Ayame Uchida, Izumi Tsunokuni, Yusuke Ikeda, Yasuhiro Oikawa
Motion from Shape Change	We consider motion effected by shape change. Such motions are ubiquitous in nature and the human made environment, ranging from single cells to platform divers and jellyfish. The shapes may be immersed in various media ranging from the very viscous to air and nearly inviscid fluids. In the absence of external forces these settings are characterized by constant momentum. We exploit this in an algorithm which takes a sequence of changing shapes, say, as modeled by an animator, as input and produces corresponding motion in world coordinates. Our method is based on the geometry of shape change and an appropriate variational principle. The corresponding Euler-Lagrange equations are first order ODEs in the unknown rotations and translations and the resulting time stepping algorithm applies to all these settings without modification as we demonstrate with a broad set of examples.	https://dl.acm.org/doi/abs/10.1145/3592417	Oliver Gross, Yousuf Soliman, Marcel Padilla, Felix Knöppel, Ulrich Pinkall, Peter Schröder
Music, Motion, and Mixed Reality: An Interdisciplinary, Problem-Based Educational Experience	This talk describes an interdisciplinary educational experience involving cohorts of students studying Computer Science, Dance, 3D Digital Design and Music. The experience takes a problem based learning approach towards a target goal of presentation of a set of live performances that combine music and dance with augmented reality. Our overall approach of integration of the varying disciples as well as impressions of students involved in the experience is discussed.	https://dl.acm.org/doi/abs/10.1145/3587424.3595581	Joe Geigel, Thomas Warfield, Yunn-Shan Ma, Dan Roach, Shaun Foster
Napkinmatic	Napkinmatic is a seamless spatial computing platform connecting AI to the real world – and back again; currently, it is an iOS and Android app that lets you (automagically) turn your napkin sketch into whatever it is you actually want it to be – from wireframe to vivid web design or sketch to masterpiece painting – or, using AppMode we can create our own mini-apps, for example, we can pipe our napkin idea into a 3D model creation endpoint [Chang 2023a].	https://dl.acm.org/doi/abs/10.1145/3588427.3595357	Yosun Chang
Napkinmatic 3D	We present a novel approach for 3D content creation via a platform that connects reality to AI, on paper.	https://dl.acm.org/doi/abs/10.1145/3588430.3597253	Yosun Chang
NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images	We present a neural rendering-based method called NeRO for reconstructing the geometry and the BRDF of reflective objects from multiview images captured in an unknown environment. Multiview reconstruction of reflective objects is extremely challenging because specular reflections are view-dependent and thus violate the multiview consistency, which is the cornerstone for most multiview reconstruction methods. Recent neural rendering techniques can model the interaction between environment lights and the object surfaces to fit the view-dependent reflections, thus making it possible to reconstruct reflective objects from multiview images. However, accurately modeling environment lights in the neural rendering is intractable, especially when the geometry is unknown. Most existing neural rendering methods, which can model environment lights, only consider direct lights and rely on object masks to reconstruct objects with weak specular reflections. Therefore, these methods fail to reconstruct reflective objects, especially when the object mask is not available and the object is illuminated by indirect lights. We propose a two-step approach to tackle this problem. First, by applying the split-sum approximation and the integrated directional encoding to approximate the shading effects of both direct and indirect lights, we are able to accurately reconstruct the geometry of reflective objects without any object masks. Then, with the object geometry fixed, we use more accurate sampling to recover the environment lights and the BRDF of the object. Extensive experiments demonstrate that our method is capable of accurately reconstructing the geometry and the BRDF of reflective objects from only posed RGB images without knowing the environment lights and the object masks. Codes and datasets are available at https://github.com/liuyuan-pal/NeRO.	https://dl.acm.org/doi/abs/10.1145/3592134	Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng Wang, Lingjie Liu, Taku Komura, Wenping Wang
NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads	We focus on reconstructing high-fidelity radiance fields of human heads, capturing their animations over time, and synthesizing re-renderings from novel viewpoints at arbitrary time steps. To this end, we propose a new multi-view capture setup composed of 16 calibrated machine vision cameras that record time-synchronized images at 7.1 MP resolution and 73 frames per second. With our setup, we collect a new dataset of over 4700 high-resolution, high-framerate sequences of more than 220 human heads, from which we introduce a new human head reconstruction benchmark . The recorded sequences cover a wide range of facial dynamics, including head motions, natural expressions, emotions, and spoken language. In order to reconstruct high-fidelity human heads, we propose Dynamic Neural Radiance Fields using Hash Ensembles (NeRSemble). We represent scene dynamics by combining a deformation field and an ensemble of 3D multi-resolution hash encodings. The deformation field allows for precise modeling of simple scene movements, while the ensemble of hash encodings helps to represent complex dynamics. As a result, we obtain radiance field representations of human heads that capture motion over time and facilitate re-rendering of arbitrary novel viewpoints. In a series of experiments, we explore the design choices of our method and demonstrate that our approach outperforms state-of-the-art dynamic radiance field approaches by a significant margin.	https://dl.acm.org/doi/abs/10.1145/3592455	Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, Matthias Nießner
Neural Fields for Visual Computing: SIGGRAPH 2023 Course	Neural fields---popularized by NeRFs ('neural radiance field')---seem to be everywhere in the popular press (e.g., Corridor Crew) for applications such as shape and image synthesis and human avatars. But, beyond writing research papers and fancy demos, what benefits might they bring to the broad SIGGRAPH community - to artists, game developers, or graphics engineers - through their inherent properties? Neural fields let us solve computer graphics problems by representing physical properties of scenes or objects across space and time using coordinate-based neural networks. Their key properties as continuous and compressed representations of shape and appearance are especially useful in tasks that reconstruct scenes from real-world images for content creation. These properties are so compelling that new graphics hardware architectures are being proposed to accelerate their use, and the acronym NeRF has now become generic. In sum, neural fields represent a wider inflection point within computer graphics, and people beyond researchers should know why this is. Over half a day, this course aims to provide an overview of neural fields techniques for visual computing, an understanding of the mathematical and computational properties that determine their practical uses, and examples of how we can use that understanding to solve many kinds of problems. We will identify the common components of neural field methods: their representations, their forward maps as differentiable renderers, the neural network architectures, their ability to generalize to different scenes and objects, and their ability to manipulate representations. The course features an invited industry speaker (Alex Yu of Luma AI) who will share how neural fields are used in practice, providing the audience with insights into how the latest developments can make practical tools for media production.	https://dl.acm.org/doi/abs/10.1145/3587423.3595477	Towaki Takikawa, Shunsuke Saito, James Tompkin, Vincent Sitzmann, Srinath Sridhar, Or Litany, Alex Yu
Neural Holographic Near-eye Displays for Virtual Reality	By manipulating light as a wavefront, holographic displays have the potential to revolutionize virtual reality (VR) and augmented reality (AR) systems. These displays support 3D focus cues for visual comfort, vision correcting capabilities, and high light efficiency. However, despite their incredible promise, holographic displays have consistently been hampered by poor image quality. Recently, artificial intelligence–driven computer-generated holography (CGH) algorithms have emerged as a solution to this obstacle. On a prototype holographic display, we demonstrate how the progress of recent state-of-the-art Neural Holography algorithms can produce high-quality dynamic 3D holograms with accurate focus cues. The advances demonstrated in this work aim to provide a glimpse into a future where our displays can fully reproduce three-dimensional virtual content.	https://dl.acm.org/doi/abs/10.1145/3588037.3595395	Suyeon Choi, Manu Gopakumar, Brian Chao, Gun-Yeal Lee, Jonghyun Kim, Gordon Wetzstein
Neural Prefiltering for Correlation-Aware Levels of Detail	We introduce a practical general-purpose neural appearance filtering pipeline for physically-based rendering. We tackle the previously difficult challenge of aggregating visibility across many levels of detail from local information only, without relying on learning visibility for the entire scene. The high adaptivity of neural representations allows us to retain geometric correlations along rays and thus avoid light leaks. Common approaches to prefiltering decompose the appearance of a scene into volumetric representations with physically-motivated parameters, where the inflexibility of the fitted models limits rendering accuracy. We avoid assumptions on particular types of geometry or materials, bypassing any special-case decompositions. Instead, we directly learn a compressed representation of the intra-voxel light transport. For such high-dimensional functions, neural networks have proven to be useful representations. To satisfy the opposing constraints of prefiltered appearance and correlation-preserving point-to-point visibility, we use two small independent networks on a sparse multi-level voxel grid. Each network requires 10--20 minutes of training to learn the appearance of an asset across levels of detail. Our method achieves 70--95% compression ratios and around 25% of quality improvements over previous work. We reach interactive to real-time framerates, depending on the level of detail.	https://dl.acm.org/doi/abs/10.1145/3592443	Philippe Weier, Tobias Zirr, Anton Kaplanyan, Ling-Qi Yan, Philipp Slusallek
Neural Volumetric Reconstruction for Coherent Synthetic Aperture Sonar	Synthetic aperture sonar (SAS) measures a scene from multiple views in order to increase the resolution of reconstructed imagery. Image reconstruction methods for SAS coherently combine measurements to focus acoustic energy onto the scene. However, image formation is typically under-constrained due to a limited number of measurements and bandlimited hardware, which limits the capabilities of existing reconstruction methods. To help meet these challenges, we design an analysis-by-synthesis optimization that leverages recent advances in neural rendering to perform coherent SAS imaging. Our optimization enables us to incorporate physics-based constraints and scene priors into the image formation process. We validate our method on simulation and experimental results captured in both air and water. We demonstrate both quantitatively and qualitatively that our method typically produces superior reconstructions than existing approaches. We share code and data for reproducibility.	https://dl.acm.org/doi/abs/10.1145/3592141	Albert Reed, Juhyeon Kim, Thomas Blanford, Adithya Pediredla, Daniel Brown, Suren Jayasuriya
OctFormer: Octree-based Transformers for 3D Point Clouds	We propose octree-based transformers, named OctFormer, for 3D point cloud learning. OctFormer can not only serve as a general and effective backbone for 3D point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. The key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. To combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. However, the point number in each window varies greatly, impeding the efficient execution on GPU. Observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. And we also introduce dilated octree attention to expand the receptive field further. Our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200 Built upon the octree attention, OctFormer can be easily scaled up and achieves state-of-the-art performances on a series of 3D semantic segmentation and 3D object detection benchmarks, surpassing previous sparse-voxel-based CNNs and point cloud transformers in terms of both efficiency and effectiveness. Notably, on the challenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in mIoU.	https://dl.acm.org/doi/abs/10.1145/3592131	Peng-Shuai Wang
Orientable Dense Cyclic Infill for Anisotropic Appearance Fabrication	We present a method to 3D print surfaces exhibiting a prescribed varying field of anisotropic appearance using only standard fused filament fabrication printers. This enables the fabrication of patterns triggering reflections similar to that of brushed metal with direct control over the directionality of the reflections. Our key insight, on which we ground the method, is that the direction of the deposition paths leads to a certain degree of surface roughness, which yields a visual anisotropic appearance. Therefore, generating dense cyclic infills aligned with a line field allows us to grade the anisotropic appearance of the printed surface. To achieve this, we introduce a highly parallelizable algorithm for optimizing oriented, cyclic paths. Our algorithm outperforms existing approaches regarding efficiency, robustness, and result quality. We demonstrate the effectiveness of our technique in conveying an anisotropic appearance on several challenging test cases, ranging from patterns to photographs reinterpreted as anisotropic appearances.	https://dl.acm.org/doi/abs/10.1145/3592412	Xavier Chermain, Cédric Zanni, Jonàs Martínez, Pierre-Alexandre Hugron, Sylvain Lefebvre
Out of the Cave	The story follows a young woman who seeks to overcome the emotional wounds of a past relationship. Blending the immersive realms of webcomics and intricately crafted VR environments, viewers are transported into the protagonist's world and intimately involved in her journey of self-discovery and healing.	https://dl.acm.org/doi/abs/10.1145/3577025.3585278	Neng-Hao Yu, Yu-han Li, Chia-yin Chang
Overwatch: Kiriko	The protector of Kanezaka strikes again. Discover the two sides of Kiriko, the loving daughter and the deadly protector.	https://dl.acm.org/doi/abs/10.1145/3577024.3588990	Jeremiah Johnson, Brian Horn
P2M: A Fast Solver for Querying Distance from Point to Mesh Surface	Most of the existing point-to-mesh distance query solvers, such as Proximity Query Package (PQP), Embree and Fast Closest Point Query (FCPW), are based on bounding volume hierarchy (BVH). The hierarchical organizational structure enables one to eliminate the vast majority of triangles that do not help find the closest point. In this paper, we develop a totally different algorithmic paradigm, named , to speed up point-to-mesh distance queries. Our original intention is to precompute a KD tree (KDT) of mesh vertices to approximately encode the geometry of a mesh surface containing vertices, edges and faces. However, it is very likely that the closest primitive to the query point is an edge (resp., a face ), but the KDT reports a mesh vertex υ instead. We call υ an of (resp., ). The main contribution of this paper is to invent a simple yet effective interception inspection rule and an efficient flooding interception inspection algorithm for quickly finding out all the interception pairs. Once the KDT and the interception table are precomputed, the query stage proceeds by first searching the KDT and then looking up the interception table to retrieve the closest geometric primitive. Statistics show that our query algorithm runs many times faster than the state-of-the-art solvers.	https://dl.acm.org/doi/abs/10.1145/3592439	Chen Zong, Jiacheng Xu, Jiantao Song, Shuangmin Chen, Shiqing Xin, Wenping Wang, Changhe Tu
PCBend: Light Up Your 3D Shapes With Foldable Circuit Boards	We propose a computational design approach for covering a surface with individually addressable RGB LEDs, effectively forming a low-resolution surface screen. To achieve a low-cost and scalable approach, we propose creating designs from flat PCB panels bent in-place along the surface of a 3D printed core. Working with standard rigid PCBs enables the use of established PCB manufacturing services, allowing the fabrication of designs with several hundred LEDs. Our approach optimizes the PCB geometry for folding, and then jointly optimizes the LED packing, circuit and routing, solving a challenging layout problem under strict manufacturing requirements. Unlike paper, PCBs cannot bend beyond a certain point without breaking. Therefore, we introduce parametric cut patterns acting as hinges, designed to allow bending while remaining compact. To tackle the joint optimization of placement, circuit and routing, we propose a specialized algorithm that splits the global problem into one sub-problem per triangle, which is then individually solved. Our technique generates PCB blueprints in a completely automated way. After being fabricated by a PCB manufacturing service, the boards are bent and glued by the user onto the 3D printed support. We demonstrate our technique on a range of physical models and virtual examples, creating intricate surface light patterns from hundreds of LEDs. The code and data for this paper are available at https://github.com/mfremer/pcbend.	https://dl.acm.org/doi/abs/10.1145/3592411	Marco Freire, Manas Bhargava, Camille Schreck, Pierre-Alexandre Hugron, Bernd Bickel, Sylvain Lefebvre
Path tracing in Production: The Path of Water	Physically-based light transport simulation has become a widely established standard to generate images in the movie industry. It promises various important practical advantages such as robustness, lighting consistency, progressive rendering and scalability. Through careful scene modelling it allows highly realistic and compelling digital versions of natural phenomena to be rendered very faithfully. The previous courses have documented some of the evolution and challenges along the journey of adopting this technology, yet even modern production path-tracers remain prone to costly rendering times in various classes of scenes, of which water shots remain among those most notoriously demanding. While this series in the past years covered a wide range of different topics within one course, this year we took the unusual step to focus on just one, the water-related challenges that we encountered during our work on Despite its seemingly simple nature, water causes a very multifaceted range of issues: specular surfaces cause spiky and sparse radiance distribution at various scales and in different forms, such as underwater caustics, godrays as well as fast-moving highlights and complex indirect on FX elements such as splashes, droplets and aeration bubbles. The purpose of this course is to share knowledge and experiences on the current state of the technology to stimulate active exchange in the academic and industrial research community that will advance the field on some of the challenging industrial benchmark problems. We will first give an overview of the nature of the singularities and its practical implications and then dive deeper into appearance and material aspects of water and the objects it interacts with. In the remaining sections, the course will focus on some specific aspects in more technical detail, providing both a solid mathematical background as well as practical strategies. Furthermore, we discuss some of the remaining unsolved problems that hopefully will inspire future research.	https://dl.acm.org/doi/abs/10.1145/3587423.3595519	Marc Droske, Johannes Hanika, Jiří Vorba, Andrea Weidlich, Manuele Sabbadin
Patternshop: Editing Point Patterns by Image Manipulation	Point patterns are characterized by their density and correlation. While spatial variation of density is well-understood, analysis and synthesis of spatially-varying correlation is an open challenge. No tools are available to intuitively edit such point patterns, primarily due to the lack of a compact representation for spatially varying correlation. We propose a low-dimensional perceptual embedding for point correlations. This embedding can map point patterns to common three-channel raster images, enabling manipulation with off-the-shelf image editing software. To synthesize back point patterns, we propose a novel edge-aware objective that carefully handles sharp variations in density and correlation. The resulting framework allows intuitive and backward-compatible manipulation of point patterns, such as recoloring, relighting to even texture synthesis that have not been available to 2D point pattern design before. Effectiveness of our approach is tested in several user experiments. Code is available at https://github.com/xchhuang/patternshop.	https://dl.acm.org/doi/abs/10.1145/3592418	Xingchang Huang, Tobias Ritschel, Hans-Peter Seidel, Pooran Memari, Gurprit Singh
Period Drama	When young Georgiana Crimsworth finds blood on her sheets and thinks she's dying, she spirals into fear imagining what's going to happen to her body.	https://dl.acm.org/doi/abs/10.1145/3577024.3588593	Anushka Nair, Lauryn Anthony
Photo-Realistic Streamable Free-Viewpoint Video	We present a novel free-viewpoint video(FVV) framework for capturing, processing and compressing the volumetric content for immersive VR/AR experience. Compared to previous FVV capture systems, we propose an easy-to-use multi-camera array consisting of mobile phones with time synchronization. In order to generate photo-realistic FVV results with sparse multi-camera input, we improve the novel view synthesis method by introducing visual hull guided neural representation, called VH-NeRF. Our VH-NeRF combines the advantages of both explicit models by traditional 3D reconstruction and the notable implicit representation of Neural Radiance Field. Each dynamic entity's VH-NeRF is learned and supervised by the visual hull reconstructed data, and can be further edited for complex and large-scale dynamic scenes. Moreover, our FVV solution can do both effective compression and transmission on multi-perspective videos, as well as real-time rendering on consumer-grade hardware. To the best of our knowledge, our work is the first solution for photo-realistic FVV captured by sparse multi-camera array, and allow real-time live streaming of large-scale dynamic scenes for immersive VR and AR applications on mobile devices.	https://dl.acm.org/doi/abs/10.1145/3588028.3603666	Shaohui Jiao, Yuzhong Chen, Zhaoliang Liu, Danying Wang, Wen Zhou, Li Zhang, Yue Wang
PhysicsAR: Problem Set Reality	PhysicsAR is an augmented reality (AR) application that transcends traditional teaching methods by enabling users to delve into the intricacies of physics through interactions with real-world objects. By seamlessly integrating AR technology with advanced physics concepts, PhysicsAR offers an unparalleled learning experience. This paper presents a comprehensive overview of PhysicsAR, emphasizing its ability to turn real world objects into interactive problems, and the potential it holds for physics education.	https://dl.acm.org/doi/abs/10.1145/3588427.3595359	Yosun Chang
Point Anywhere: Directed Object Estimation from Omnidirectional Images	One of the intuitive instruction methods in robot navigation is a pointing gesture. In this study, we propose a method using an omnidirectional camera to eliminate the user/object position constraint and the left/right constraint of the pointing arm. Although the accuracy of skeleton and object detection is low due to the high distortion of equirectangular images, the proposed method enables highly accurate estimation by repeatedly extracting regions of interest from the equirectangular image and projecting them onto perspective images. Furthermore, we found that training the likelihood of the target object in machine learning further improves the estimation accuracy.	https://dl.acm.org/doi/abs/10.1145/3588028.3603650	Nanami Kotani, Asako Kanezaki
PolyStokes: A Polynomial Model Reduction Method for Viscous Fluid Simulation	Standard liquid simulators apply operator splitting to independently solve for pressure and viscous stresses, a decoupling that induces incorrect free surface boundary conditions. Such methods are unable to simulate fluid phenomena reliant on the balance of pressure and viscous stresses, such as the liquid rope coil instability exhibited by honey. By contrast, unsteady Stokes solvers retain coupling between pressure and viscosity, thus resolving these phenomena, albeit using a much larger and thus more computationally expensive linear system compared to the decoupled approach. To accelerate solving the unsteady Stokes problem, we propose a reduced fluid model wherein interior regions are represented with incompressible polynomial vector fields. Sets of standard grid cells are consolidated into super-cells, each of which are modelled using a quadratic field of 26 degrees of freedom. We demonstrate that the reduced field must necessarily be at least quadratic, with the affine model being unable to correctly capture viscous forces. We reproduce the liquid rope coiling instability, as well as other simulated examples, to show that our reduced model is able to reproduce the same fluid phenomena at a smaller computational cost. Futhermore, we performed a crowdsourced user survey to verify that our method produces imperceptible differences compared to the full unsteady Stokes method.	https://dl.acm.org/doi/abs/10.1145/3592146	Jonathan Panuelos, Ryan Goldade, Eitan Grinspun, David Levin, Christopher Batty
Potentially Visible Hidden-Volume Rendering for Multi-View Warping	This paper presents the model and rendering algorithm of Potentially Visible Hidden Volumes (PVHVs) for multi-view image warping. PVHVs are 3D volumes that are occluded at a known source view, but potentially visible at novel views. Given a bound of novel views, we define PVHVs using the edges of foreground fragments from the known view and the bound of novel views. PVHVs can be used to batch-test the visibilities of source fragments without iterating individual novel views in multi-fragment rendering, and thereby, cull redundant fragments prior to warping. We realize the model of PVHVs in Depth Peeling (DP). Our Effective Depth Peeling (EDP) can reduce the number of completely hidden fragments, capture important fragments early, and reduce warping cost. We demonstrate the benefit of our PVHVs and EDP in terms of memory, quality, and performance in multi-view warping.	https://dl.acm.org/doi/abs/10.1145/3592108	Janghun Kim, Sungkil Lee
Preserving Virtual Reality	According to the Library of Congress, 75% of silent movies before the 1930s are lost forever. [Pierce, 2013] During the early 20th century, films were not considered to have much future value or lasting significance which lead to the majority of them not being properly archived or preserved. Most films were intentionally destroyed to avoid spending money on storage space and expensive upkeep of the materials. [Pierce, 2013] By the time studios and institutions started to establish film archives, it was already too late for most of early cinema. We are seeing a similar scenario take place with preserving interactive media projects as the majority of VR experiences have been ignored by institutional archives. Preserving virtual reality has a particularly unique challenge because it is designed with interface, hardware, and software in mind. Unlike films and games that can easily be archived in formats that can be universally played on screens, the majority of virtual reality experiences are designed to run on very specific hardware. Once a headset is no longer supported, the dedicated VR experiences also disappear with it, unless they are continuously updated to move to newer systems. Unfortunately, the majority of independent developers and creators do not have the resources, time, or budget to keep up with the constant turnover rate of new hardware. Not to mention the lack of infrastructure that prevents content creators from ensuring the longevity of titles that are not positioned with a financial gain. The panel will explore and discuss the challenges of establishing a standard of best practices for VR preservation and how to support the futurity of these studies. The discussion will further delve into the need for collaboration between artists, institutions, and above all the technology companies that the medium so heavily relies on. Panelists will provide real-world examples of why it's important to consider the potential long-term value of archiving virtual reality content, both for researchers and for the general public. The panel will identify key challenges and risks to bring clarity to the complex nature of this undertaking, like the intricate ecosystem of hardware and software and how the rapid obsolescence cycles continuously challenge efforts for conservation. We know that archives are not neutral. They are a product of their culture, oftentimes the dominant culture. In this case, big tech is the gatekeeper when it comes to deciding what gets to be kept and what gets lost within the virtual reality community. As companies like Meta take further control of the industry, people will surely not get a clear understanding of the systems of oppression within the history of immersive media, let alone a virtual reality archive for future students to reference from the past.	https://dl.acm.org/doi/abs/10.1145/3587422.3595554	Zeynep Abes
Puss in Boots: The Last Wish	"Behind the scenes look at creating ""Puss in Boots: The Last Wish."""	https://dl.acm.org/doi/abs/10.1145/3577024.3588883	Mark Edwards
Pyramid Texture Filtering	We present a simple but effective technique to smooth out textures while preserving the prominent structures. Our method is built upon a key observation---the coarsest level in a Gaussian pyramid often naturally eliminates textures and summarizes the main image structures. This inspires our central idea for texture filtering, which is to progressively upsample the very low-resolution coarsest Gaussian pyramid level to a full-resolution texture smoothing result with well-preserved structures, under the guidance of each fine-scale Gaussian pyramid level and its associated Laplacian pyramid level. We show that our approach is effective to separate structure from texture of different scales, local contrasts, and forms, without degrading structures or introducing visual artifacts. We also demonstrate the applicability of our method on various applications including detail enhancement, image abstraction, HDR tone mapping, inverse halftoning, and LDR image enhancement. Code is available at https://rewindl.github.io/pyramid_texture_filtering/.	https://dl.acm.org/doi/abs/10.1145/3592120	Qing Zhang, Hao Jiang, Yongwei Nie, Wei-Shi Zheng
Random-Access Neural Compression of Material Textures	The continuous advancement of photorealism in rendering is accompanied by a growth in texture data and, consequently, increasing storage and memory demands. To address this issue, we propose a novel neural compression technique specifically designed for material textures. We unlock two more levels of detail, i.e., 16× more texels, using low bitrate compression, with image quality that is better than advanced image compression techniques, such as AVIF and JPEG XL. At the same time, our method allows on-demand, real-time decompression with random access similar to block texture compression on GPUs, enabling compression on disk and memory. The key idea behind our approach is compressing multiple material textures and their mipmap chains together, and using a small neural network, that is optimized for each material, to decompress them. Finally, we use a custom training implementation to achieve practical compression speeds, whose performance surpasses that of general frameworks, like PyTorch, by an order of magnitude.	https://dl.acm.org/doi/abs/10.1145/3592407	Karthik Vaidyanathan, Marco Salvi, Bartlomiej Wronski, Tomas Akenine-Moller, Pontus Ebelin, Aaron Lefohn
Reactive Visuals in P5.js with Custom Analog and Digital Inputs	This SIGGRAPH lab is an introduction to creating reactive graphics in p5.js that respond to inputs from external sensors connected through a hand-built circuit via the Raspberry Pi Pico microcontroller. Participants are guided through the process of building a working hardware and software template that can be further customised for their own creative designs.	https://dl.acm.org/doi/abs/10.1145/3588029.3599742	Kieran Nolan
Real-Time Radiance Fields for Single-Image Portrait View Synthesis	We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ), but our algorithm can also be applied in the future to other categories with a 3D-aware image generator.	https://dl.acm.org/doi/abs/10.1145/3592460	Alex Trevithick, Matthew Chan, Michael Stengel, Eric Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano
Real-Time Ray-Tracing with Vulkan for the Impatient	The author accepts no responsibility for the accuracy, completeness or quality of the information provided, nor for ensuring that it is up to date. Liability claims against the author relating to material or non-material damages arising from the information provided being used or not being used or from the use of inaccurate and incomplete information are excluded if there was no intentional or gross negligence on the part of the author. The author expressly retains the right to change, add to or delete parts of the text or the whole text without prior notice or to withdraw the information temporarily or permanently.	https://dl.acm.org/doi/abs/10.1145/3587423.3595476	Benjamin Kenwright
Real-time Collision using AI	Traditional narrow collision pipelines for rigid body dynamics typically consist of calculation of collisions between primitives with a dedicated deepest point function for each primitive pair and a much slower general collision calculation between a pair of convex shapes. The mixture of different collision functions makes these methods not CPU-cache friendly and unsuitable for modern GPU hardware. For many reinforcement learning applications, data transfers become the bottle-neck of the whole pipeline. To get a stable behavior of rigid bodies even on a flat ground without jittering, it is necessary to use multiple contact points for each pair of objects. This is achieved by either keeping track of old collision points, or by adding noise to the collision pair. Using the first approach we get a full set of collision points only after multiple frames, using the second, the computational cost largely increases by a factor equal to the number of samples. In this project, we propose to tackle these issues by introducing: a) the quadric decomposition - a data representation for both primitives and general shapes, describing each object as a union of the intersection of planar and quadric inequalities, b) calculation of the deepest point as a maximization of depth over a set of quadric inequalities, c) prediction of the desired number of contact points as a maximization of the surface of the intersection polygon, d) Recurrent neural network (RNN) based solver to speed up the optimization process.	https://dl.acm.org/doi/abs/10.1145/3588430.3597254	Lubor Ladicky, Sohyeon Jeong
Real-time Stage Modelling and Visual Effects for Live Performances	We present a novel live platform enhancing stage performances with real-time visual effects. Our demo showcases real-time 3D modeling, rendering and blending of assets, and interaction between real and virtual performers. We demonstrate our platform's capabilities with a mixed reality performance featuring virtual and real actors engaged with in-person audiences.	https://dl.acm.org/doi/abs/10.1145/3588430.3597245	Taehyun Rhee, Andrew Chalmers, Faisal Zaman, Anna Stangnes, Vic Roberts
Realistic Dexterous Manipulation of Virtual Objects with Physics-Based Haptic Rendering	This paper introduces a system that focuses on physics-based manipulation and haptic rendering to achieve realistic dexterous manipulation of virtual objects in VR environments. The system uses a coreless motor with wire as the haptic actuator and physics engine in the software to create a virtual hand that provides haptic feedback through multi-channel audio signals. The device simulates contact collision, pressure, and friction, including stick-slip, to provide users with a realistic and immersive experience. Our device is lightweight and does not interfere with real-world operations or the performance of vision-based hand-tracking technology.	https://dl.acm.org/doi/abs/10.1145/3588037.3595400	Yunxiu Xu, Siyu Wang, Shoichi Hasegawa
Recursive Control Variates for Inverse Rendering	We present a method for reducing errors---variance and bias---in physically based differentiable rendering (PBDR). Typical applications of PBDR repeatedly render a scene as part of an optimization loop involving gradient descent. The actual change introduced by each gradient descent step is often relatively small, causing a significant degree of redundancy in this computation. We exploit this redundancy by formulating a gradient estimator that employs a , which leverages information from previous optimization steps. The control variate reduces variance in gradients, and, perhaps more importantly, alleviates issues that arise from differentiating loss functions with respect to noisy inputs, a common cause of drift to bad local minima or divergent optimizations. We experimentally evaluate our approach on a variety of path-traced scenes containing surfaces and volumes and observe that primal rendering efficiency improves by a factor of up to 10.	https://dl.acm.org/doi/abs/10.1145/3592139	Baptiste Nicolet, Fabrice Rousselle, Jan Novak, Alexander Keller, Wenzel Jakob, Thomas Müller
Redirected Walking in Overlapping Rooms	Walking in larger virtual environments than the physical one can lead to collisions with physical boundaries. Multiple locomotion techniques like Redirected Walking (RDW) and Overlapping Architecture (OA) aim to overcome this limitation. Combining these two has yet to be investigated in large physical spaces with resets. In this work, a hybrid locomotion method was implemented that combines RDW and OA. A user study was conducted where participants collected items in a virtual environment with multiple rooms. The study showed that the distance walked between resets was increased substantially, thus showing the solid advantages of combining OA and RDW.	https://dl.acm.org/doi/abs/10.1145/3588028.3603672	Mathieu Lutfallah, Christian Hirt, Valentina Gorobets, Manuel Gregor, Andreas Kunz
Reflections on Socio-political, Cultural and Environmental Challenges through Computational Media Arts and Design	The field of emerging computational media arts and design is rapidly evolving as an interdisciplinary and high-impact domain. Beyond its technological affordances, it could also serve as a lens for addressing the pressing environmental, social, cultural, and political challenges. Within this context, this panel highlights the contributions of four women that shed light on the nuanced topics of inclusivity, transparency, accessibility, awareness, and action. These experts bring diverse perspectives and engage with technical advancements critically through their practice and research.	https://dl.acm.org/doi/abs/10.1145/3587422.3595566	Behnaz Farahi
Reimagined Volume I: Nyssa	After losing her best friend/broom, a precocious young witch is guided by her familiar spirit through the depths of the dark migration where she must learn about fear and intuition in order to protect her village from the wrath of the evil Teemencaag.	https://dl.acm.org/doi/abs/10.1145/3577025.3580395	Julie Cavaliere
Reprojection-Free VR Passthrough	Virtual reality (VR) passthrough uses external cameras on the front of a headset to allow the user to see their environment. However, passthrough cameras cannot physically be co-located with the user's eyes, so the passthrough images have a different perspective than what the user would see without the headset. Although the images can be computationally reprojected into the desired view, errors in depth estimation and missing information at occlusion boundaries can lead to undesirable artifacts. We propose a novel computational camera that directly samples the rays that would have gone into the user's eye, several centimeters behind the sensor. Our design contains an array of lenses with an aperture behind each lens, and the apertures are strategically placed to allow through only the desired rays. The resulting thin, flat architecture has suitable form factor for VR, and the image reconstruction is computationally lightweight, enabling low-latency passthrough. We demonstrate our approach experimentally in a fully functional binocular passthrough prototype with practical calibration and real-time image reconstruction.	https://dl.acm.org/doi/abs/10.1145/3588037.3595391	Grace Kuo, Eric Penner, Seth Moczydlowski, Alex Ching, Douglas Lanman, Nathan Matsuda
Retinal-Resolution Varifocal VR	We develop a virtual reality (VR) head-mounted display (HMD) that achieves near retinal resolution with an angular pixel density up to 56 pixels per degree (PPD), supporting a wide range of eye accommodation from 0 to 4 diopter (i.e. infinity to 25 cm), and matching the dynamics of eye accommodation with at least 10 diopter/s peak velocity and 100 diopter/s2 acceleration. This system includes a high-resolution optical design, a mechanically actuated, eye-tracked varifocal display that follows the user's vergence point, and a closed-loop display distortion rendering pipeline that ensures VR content remains correct in perspective despite the varying display magnification. To our knowledge, this work is the first VR HMD prototype that approaches retinal resolution and fully supports human eye accommodation in range and dynamics. We present this installation to exhibit the visual benefits of varifocal displays, particularly for high-resolution, near-field interaction tasks, such as reading text and working with 3D models in VR.	https://dl.acm.org/doi/abs/10.1145/3588037.3595389	Yang Zhao, Dave Lindberg, Bruce Cleary, Olivier Mercier, Ryan Mcclelland, Eric Penner, Yu-Jen Lin, Julia Majors, Douglas Lanman
Reverse Projection: Real-Time Local Space Texture Mapping	We present Reverse Projection, a novel projective texture mapping technique for painting a decal directly to the texture of a 3D object. Designed to be used in games, this technique works in real-time. By using projection techniques that are computed in local space textures and outward-looking, users using low-end android devices to high-end gaming desktops are able to enjoy the personalization of their assets. We believe our proposed pipeline is a step in improving the speed and versatility of model painting.	https://dl.acm.org/doi/abs/10.1145/3588028.3603653	Adrian Xuan Wei Lim, Lynnette Hui Xian Ng, Conor Griffin, Nicholas Kryer, Faraz Baghernezhad
Revisiting controlled mixture sampling for rendering applications	Monte Carlo rendering makes heavy use of mixture sampling and multiple importance sampling (MIS). Previous work has shown that control variates can be used to make such mixtures more efficient and more robust. However, the existing approaches failed to yield practical applications, chiefly because their underlying theory is based on the unrealistic assumption that a single mixture is optimized for a single integral. This is in stark contrast with rendering reality, where millions of integrals are computed---one per pixel---and each is infinitely recursive. We adapt and extend the theory introduced by previous work to tackle the challenges of real-world rendering applications. We achieve robust mixture sampling and (approximately) optimal MIS weighting for common applications such as light selection, BSDF sampling, and path guiding.	https://dl.acm.org/doi/abs/10.1145/3592435	Qingqin Hua, Pascal Grittmann, Philipp Slusallek
Rhizomorph: The Coordinated Function of Shoots and Roots	Computer graphics has dedicated a considerable amount of effort to generating realistic models of trees and plants. Many existing methods leverage procedural modeling algorithms - that often consider biological findings - to generate branching structures of individual trees. While the realism of tree models generated by these algorithms steadily increases, most approaches neglect to model the root system of trees. However, the root system not only adds to the visual realism of tree models but also plays an important role in the development of trees. In this paper, we advance tree modeling in the following ways: First, we define a physically-plausible soil model to simulate resource gradients, such as water and nutrients. Second, we propose a novel developmental procedural model for tree roots that enables us to emergently develop root systems that adapt to various soil types. Third, we define long-distance signaling to coordinate the development of shoots and roots. We show that our advanced procedural model of tree development enables - for the first time - the generation of trees with their root systems.	https://dl.acm.org/doi/abs/10.1145/3592145	Bosheng Li, Jonathan Klein, Dominik L. Michels, Bedrich Benes, Sören Pirk, Wojtek Pałubicki
Roald	A birthday party escalate when an toad who perceives life in a cute and colorful way meets a paranoiac fly.	https://dl.acm.org/doi/abs/10.1145/3577024.3587948	Julie Chêne
Roblox Generative AI in action	Roblox is investing in generative AI techniques to revolutionize the creation process on its platform. By leveraging natural language and other intuitive expressions of intent, creators can build interactive objects and scenes without complex modeling or coding. The use of AI image generation services and large language models aim to make creation faster and easier for every user on the platform.	https://dl.acm.org/doi/abs/10.1145/3588430.3597250	Brent Vincent, Kartik Ayyar
Robust Low-Poly Meshing for General 3D Models	We propose a robust re-meshing approach that can automatically generate visual-preserving low-poly meshes for any high-poly models found in the wild. Our method can be seamlessly integrated into current mesh-based 3D asset production pipelines. Given an input high-poly, our method proceeds in two stages: 1) Robustly extracting an offset surface mesh that is feature-preserving, and guaranteed to be watertight, manifold, and self-intersection free; 2) Progressively simplifying and flowing the offset mesh to bring it close to the input. The simplicity and the visual-preservation of the generated low-poly is controlled by a user-required target screen size of the input: decreasing the screen size reduces the element count of the low-poly but enlarges its visual difference from the input. We have evaluated our method on a subset of the Thingi10K dataset that contains models created by practitioners in different domains, with varying topological and geometric complexities. Compared to state-of-the-art approaches and widely used software, our method demonstrates its superiority in terms of the element count, visual preservation, geometry, and topology guarantees of the generated low-polys.	https://dl.acm.org/doi/abs/10.1145/3592396	Zhen Chen, Zherong Pan, Kui Wu, Etienne Vouga, Xifeng Gao
Rock, Paper, Scissors	Step into teenager Priya's world and influence her story by playing the game Rock Paper Scissors against single-mum Lina in this interactive virtual reality coming-of-age drama funded by the BFI Network.	https://dl.acm.org/doi/abs/10.1145/3577025.3584914	Juliette Chabrier
Rockets, Robots, and AI: Lessons Learned from Science-Fiction Movies and TV for HCI/UX	"This tutorial present examples from notable science-fiction films and videos that incorporate human-computer interaction (HCI) and user-experience (UX) design and shows what lessons can be learned. The course begins with the advent of movies in the early 1900s ( , Melies' ""A Trip to the Moon,"" which was later referenced in the movie ""Hugo"", 2011) and concludes with the latest sci-fi movies/videos. Originally, many science-fiction movies, taking their cue from pulp science-fiction, focused on rocket ships and interplanetary travel. Later the scope of the stories broadened and deepened to future consumer products, psychological/social issues, and new technologies such as exoskeletons, robots, and artificial intelligence. Once a rarified genre and primarily products of Hollywood (with notable films from Germany, the United Kingdom, and Japan), these films/videos now occupy a primary place in modern international popular media, and originate in China, India, or South Korea, and Japan, as well as North America and Europe."	https://dl.acm.org/doi/abs/10.1145/3587423.3595550	Aaron Marcus
SIGGRAPH 2023 - Course Notes What We Talk About, When We Talk About Story	"Today's digital media is defined by not only the latest, greatest technical innovations but also how original story solutions are adapting to these new technological changes. No longer are the story/narrative solutions the exclusive responsibility of directors and writers but increasingly fall on the shoulders of technical directors, software engineers, animators, VFX creators, and game/XR designers whose work is essential in making ""the content"" come to life. Understanding story is particularly useful when communicating with screenwriters, directors, and producers. This course answers the question, ""What is Story?"" (and you don't even have to take a course in screenwriting). Knowing the basics of story enables the crew to become collaborators with the producer and director. As a director talks about their story goals; and the crew will know what specific story benchmarks they are trying to meet. This information is more than a story being ""a sequence of events (acts) to a ""dramatic"" story that that builds from Setup through Resolution. Having an understanding of story structure allows one to understand a story's elements in context (i.e., theme, character, setting, conflict etc.) and their relationship to classic story structure (i.e., setup, inciting incident, rising action, climax, resolution, etc.). This information is for all whose work makes the story better, but their job is not creating the story. The following course notes have been adapted from CRC Publishers, a division of Taylor and Francis."	https://dl.acm.org/doi/abs/10.1145/3587423.3595494	Craig Caldwell
SIGGRAPH 2023 Course on Diffusion Models	Diffusion models have been successfully used in various applications such as text-to-image generation, 3D assets generation, controllable image editing, video generation, natural language generation, audio synthesis, and motion generation. The rate of progress on diffusion models is astonishing. In the year 2022 alone, diffusion models have been applied to many large-scale text-to-image foundation models, such as DALL-E 2 [Ramesh et al. 2022], Imagen [Saharia et al. 2022], Stable Diffusion [Rombach et al. 2022], and eDiff-I [Balaji et al. 2022]; video generation models such as Imagen Video [Ho et al. 2022] and Make-a-video [Singer et al. 2022]; 3D asset generation models such as Magic3D [Lin et al. 2022] and DreamFusion [Poole et al. 2022]. This course covers the advances in diffusion models over the last few years and will be tailored to the computer graphics community. We will first cover the fundamental machine learning and deep learning techniques relevant to diffusion models. Next, we will present state-of-the-art techniques for the application of diffusion models to high-fidelity image synthesis, controllable image generation, compositional representation learning, and 3D asset generation. Finally, we will conclude with a discussion on the future application of this technology, societal impact and open research problems. After the course, the attendees will learn basic knowledge about diffusion models and how such models can be applied to different applications such as image generation, image editing, and 3D asset generation.	https://dl.acm.org/doi/abs/10.1145/3587423.3595503	Chenlin Meng, Jiaming Song, Shuang Li, Jun-Yan Zhu, Stefano Ermon, Tsung-Yi Lin, Chen-Hsuan Lin, Karsten Kreis
Sag-Free Initialization for Strand-Based Hybrid Hair Simulation	Lagrangian/Eulerian hybrid strand-based hair simulation techniques have quickly become a popular approach in VFX and real-time graphics applications. With Lagrangian hair dynamics, the inter-hair contacts are resolved in the Eulerian grid using the continuum method, i.e., the MPM scheme with the granular Drucker-Prager rheology, to avoid expensive collision detection and handling. This fuzzy collision handling makes the authoring process significantly easier. However, although current hair grooming tools provide a wide range of strand-based modeling tools for this simulation approach, the crucial sag-free initialization functionality remains often ignored. Thus, when the simulation starts, gravity would cause any artistic hairstyle to sag and deform into unintended and undesirable shapes. This paper proposes a novel four-stage sag-free initialization framework to solve stable quasistatic configurations for hybrid strand-based hair dynamic systems. These four stages are split into two global-local pairs. The first one ensures static equilibrium at every Eulerian grid node with additional inequality constraints to prevent stress from exiting the yielding surface. We then derive several associated closed-form solutions in the local stage to compute segment rest lengths, orientations, and particle deformation gradients in parallel. The second global-local step solves along each hair strand to ensure all the bend and twist constraints produce zero net torque on every hair segment, followed by a local step to adjust the rest Darboux vectors to a unit quaternion. We also introduce an essential modification for the Darboux vector to eliminate the ambiguity of the Cosserat rod rest pose in both initialization and simulation. We evaluate our method on a wide range of hairstyles, and our approach can only take a few seconds to minutes to get the rest quasistatic configurations for hundreds of hair strands. Our results show that our method successfully prevents sagging and has minimal impact on the hair motion during simulation.	https://dl.acm.org/doi/abs/10.1145/3592143	Jerry Hsu, Tongtong Wang, Zherong Pan, Xifeng Gao, Cem Yuksel, Kui Wu
ScanBot: Autonomous Reconstruction via Deep Reinforcement Learning	Autoscanning of an unknown environment is the key to many AR/VR and robotic applications. However, autonomous reconstruction with both high efficiency and quality remains a challenging problem. In this work, we propose a reconstruction-oriented autoscanning approach, called ScanBot, which utilizes hierarchical deep reinforcement learning techniques for global (ROI) planning to improve the scanning efficiency and local (NBV) planning to enhance the reconstruction quality. Given the partially reconstructed scene, the global policy designates an ROI with insufficient exploration or reconstruction. The local policy is then applied to refine the reconstruction quality of objects in this region by planning and scanning a series of NBVs. A novel mixed 2D-3D representation is designed for these policies, where a 2D quality map with tailored quality channels encoding the scanning progress is consumed by the global policy, and a coarse-to-fine 3D volumetric representation that embodies both local environment and object completeness is fed to the local policy. These two policies iterate until the whole scene has been completely explored and scanned. To speed up the learning of complex environmental dynamics and enhance the agent's memory for spatial-temporal inference, we further introduce two novel auxiliary learning tasks to guide the training of our global policy. Thorough evaluations and comparisons are carried out to show the feasibility of our proposed approach and its advantages over previous methods. Code and data are available at https://github.com/HezhiCao/Scanbot.	https://dl.acm.org/doi/abs/10.1145/3592113	Hezhi Cao, Xi Xia, Guan Wu, Ruizhen Hu, Ligang Liu
Scratch-based Reflection Art via Differentiable Rendering	The 3D visual optical arts create fascinating special effects by carefully designing interactions between objects and light sources. One of the essential types is 3D reflection art, which aims to create reflectors that can display different images when viewed from different directions. Existing works produce impressive visual effects. Unfortunately, previous works discretize the reflector surface with regular grids/facets, leading to a large parameter space and a high optimization time cost. In this paper, we introduce a new type of 3D reflection art - , which allows for a more compact parameter space, easier fabrication, and computationally efficient optimization. To design a 3D reflection art with scratches, we formulate it as a multi-view optimization problem and introduce differentiable rendering to enable efficient gradient-based optimizers. For that, we propose an analytical scratch rendering approach, together with a high-performance rendering pipeline, allowing efficient differentiable rendering. As a consequence, we could display multiple images on a single metallic board with only several minutes for optimization. We demonstrate our work by showing virtual objects and manufacturing our designed reflectors with a carving machine.	https://dl.acm.org/doi/abs/10.1145/3592142	Pengfei Shen, Ruizeng Li, Beibei Wang, Ligang Liu
Seated-Walking: A Walking-in-Place Technique for Seated VR	We present Seated-Walking, a footstep locomotion technique designed for use when sitting on a chair. Seated-Walking is driven by users' forefoot or rearfoot stepping, which is meant to embody the foot motions of real walking while mitigating potential fatigue. We demonstrate Seated-Walking in two applications. The first application delivers a casual virtual showroom experience, where users sitting iOptoma Corporationn a chair are allowed to walk navigating artworks in the virtual environment. The second application is an intense survival shooter experience where users track and fight enemies while avoiding their attacks, all while seated walking.	https://dl.acm.org/doi/abs/10.1145/3588027.3595603	Liwei Chan, Tzu-Wei Mi, Zung-Hao Hsueh, Ming-Yun Hsu
Second-order Stencil Descent for Interior-point Hyperelasticity	In this paper, we present a GPU algorithm for finite element hyperelastic simulation. We show that the interior-point method, known to be effective for robust collision resolution, can be coupled with non-Newton procedures and be massively sped up on the GPU. Newton's method has been widely chosen for the interior-point family, which fully solves a linear system at each step. After that, the active set associated with collision/contact constraints is updated. Mimicking this routine using a non-Newton optimization (like gradient descent or ADMM) unfortunately does not deliver expected accelerations. This is because the barrier functions employed in an interior-point method need to be updated at every iteration to strictly confine the search to the feasible region. The associated cost (e.g., per-iteration CCD) quickly overweights the benefit brought by the GPU, and a new parallelism modality is needed. Our algorithm is inspired by the domain decomposition method and designed to move interior-point-related computations to local domains as much as possible. We minimize the size of each domain (i.e., a stencil) by restricting it to a single element, so as to fully exploit the capacity of modern GPUs. The stencil-level results are integrated into a global update using a novel hybrid sweep scheme. Our algorithm is locally second-order offering better convergence. It enables simulation acceleration of up to two orders over its CPU counterpart. We demonstrate the scalability, robustness, efficiency, and quality of our algorithm in a variety of simulation scenarios with complex and detailed collision geometries.	https://dl.acm.org/doi/abs/10.1145/3592104	Lei Lan, Minchen Li, Chenfanfu Jiang, Huamin Wang, Yin Yang
Seeing Photons in Color	Megapixel single-photon avalanche diode (SPAD) arrays have been developed recently, opening up the possibility of deploying SPADs as generalpurpose passive cameras for photography and computer vision. However, most previous work on SPADs has been limited to monochrome imaging. We propose a computational photography technique that reconstructs high-quality color images from mosaicked binary frames captured by a SPAD array, even for high-dyanamic-range (HDR) scenes with complex and rapid motion. Inspired by conventional burst photography approaches, we design algorithms that jointly denoise and demosaick single-photon image sequences. Based on the observation that motion effectively increases the color sample rate, we design a blue-noise pseudorandom RGBW color filter array for SPADs, which is tailored for imaging dark, dynamic scenes. Results on simulated data, as well as real data captured with a fabricated color SPAD hardware prototype shows that the proposed method can reconstruct high-quality images with minimal color artifacts even for challenging low-light, HDR and fast-moving scenes. We hope that this paper, by adding color to computational single-photon imaging, spurs rapid adoption of SPADs for real-world passive imaging applications.	https://dl.acm.org/doi/abs/10.1145/3592438	Sizhuo Ma, Varun Sundar, Paul Mos, Claudio Bruschini, Edoardo Charbon, Mohit Gupta
SegAnimeChara: Segmenting Anime Characters Generated by AI	This work introduces SegAnimeChara, a novel system of transforming AI-generated anime images into game characters while retaining unique features. Using volume-based body pose segmentation, SegAnimeChara can efficiently, zero-shot, segment body parts from generative images based on OpenPose human skeleton. Furthermore, this system integrates a semantic segmentation pipeline based on the text prompts of the existing Text2Image workflow. The system conserves the game character's unique outfit and reduces the redundant duplicate text prompts for semantic segmentation.	https://dl.acm.org/doi/abs/10.1145/3588028.3603685	Andy Yu-Hsiang Tseng, Wen-Fan Wang, Bing-Yu Chen
Semantics and Scheduling for Machine Knitting Compilers	Machine knitting is a well-established fabrication technique for complex soft objects, and both companies and researchers have developed tools for generating machine knitting patterns. However, existing representations for machine knitted objects are incomplete (do not cover the complete domain of machine knittable objects) or overly specific (do not account for symmetries and equivalences among knitting instruction sequences). This makes it difficult to define correctness in machine knitting, let alone verify the correctness of a given program or program transformation. The major contribution of this work is a formal semantics for knitout, a low-level Domain Specific Language for knitting machines. We accomplish this by using what we call the , which extends concepts from knot theory to allow for a mathematical definition of knitting program equivalence that matches the intuition behind knit objects. Finally, using this formal representation, we prove the correctness of a sequence of rewrite rules; and demonstrate how these rewrite rules can form the foundation for higher-level tasks such as compiling a program for a specific machine and optimizing for time/reliability, all while provably generating the same knit object under our proposed semantics. By establishing formal definitions of correctness, this work provides a strong foundation for compiling and optimizing knit programs.	https://dl.acm.org/doi/abs/10.1145/3592449	Jenny Lin, Vidya Narayanan, Yuka Ikarashi, Jonathan Ragan-Kelley, Gilbert Bernstein, James McCann
Semi-supervised reference-based sketch extraction using a contrastive learning framework	Sketches reflect the drawing style of individual artists; therefore, it is important to consider their unique styles when extracting sketches from color images for various applications. Unfortunately, most existing sketch extraction methods are designed to extract sketches of a single style. Although there have been some attempts to generate various style sketches, the methods generally suffer from two limitations: low quality results and difficulty in training the model due to the requirement of a paired dataset. In this paper, we propose a novel multi-modal sketch extraction method that can imitate the style of a given reference sketch with unpaired data training in a semi-supervised manner. Our method outperforms state-of-the-art sketch extraction methods and unpaired image translation methods in both quantitative and qualitative evaluations.	https://dl.acm.org/doi/abs/10.1145/3592392	Chang Wook Seo, Amirsaman Ashtari, Junyong Noh
Shadow	21st of March 1945. You're the second wave of British bombers headed to Copenhagen to bomb Gestapo's headquarters. But as you approach the city with over 500 km an hour, you realize something isn't right. Nevertheless, your decision has to be made --- now.	https://dl.acm.org/doi/abs/10.1145/3577025.3584915	Juliette Chabrier
ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives	We introduce ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across a dataset, so that programs rewritten with these abstractions are more compact, and suppress spurious degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stringent input assumptions. This is principally made possible by two methodological advancements: (a) a shape-to-program recognition network that learns to solve sub-problems and (b) the use of e-graphs, augmented with a conditional rewrite scheme, to determine when abstractions with complex parametric expressions can be applied, in a tractable manner. We evaluate ShapeCoder on multiple datasets of 3D shapes, where primitive decompositions are either parsed from manual annotations or produced by an unsupervised cuboid abstraction method. In all domains, ShapeCoder discovers a library of abstractions that captures high-level relationships, removes extraneous degrees of freedom, and achieves better dataset compression compared with alternative approaches. Finally, we investigate how programs rewritten to use discovered abstractions prove useful for downstream tasks.	https://dl.acm.org/doi/abs/10.1145/3592416	R. Kenny Jones, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie
Shortest Path to Boundary for Self-Intersecting Meshes	We introduce a method for efficiently computing the exact shortest path to the boundary of a mesh from a given internal point in the presence of self-intersections. We provide a formal definition of shortest boundary paths for self-intersecting objects and present a robust algorithm for computing the actual shortest boundary path. The resulting method offers an effective solution for collision and self-collision handling while simulating deformable volumetric objects, using fast simulation techniques that provide no guarantees on collision resolution. Our evaluation includes complex self-collision scenarios with a large number of active contacts, showing that our method can successfully handle them by introducing a relatively minor computational overhead.	https://dl.acm.org/doi/abs/10.1145/3592136	He Chen, Elie Diaz, Cem Yuksel
Single-Shot VR	The physical world has contents at varying depths, allowing our eye to squish or relax to focus at different distances; this is commonly referred to as the accommodation cue for human eyes. To allow a realistic 3D viewing experience, it is crucial to support the accommodation cue—the 3D display needs to show contents at different depths. However, supporting the native focusing of the eye has been an immense challenge to 3D displays. Commercial near-eye VR displays, which use binocular disparity as the primary cue for inducing depth perception, fail this challenge since all contents they show arise from a fixed depth—ignoring the focusing of the eye. Many research prototypes of VR displays do account for the accommodation cue; however, supporting accommodation cues invariably comes with performance loss among other typically assessed criteria for 3D displays. To tackle these challenges, we present a novel kind of near-eye 3D display that can create 3D scenes supporting realistic accommodation cues in a single shot, i.e., without using time multiplexing or eye tracking. This display, which we present in our demo, can stream 3D content over a large depth range, at 4K spatial resolution, and in real-time. Our display offers an exciting step forward towards a truly immersive real-time 3D experience. Participants will get to enjoy 3D movies and play interactive games in their demo experience.	https://dl.acm.org/doi/abs/10.1145/3588037.3595396	Yingsi Qin, Wei-Yu Chen, Matthew O'Toole, Aswin C. Sankaranarayanan
SinkInSync: A Brainwave Synchronization Experience in VR Towards Enhancing Social Connectedness: A Brainwave Synchronization Experience in VR Towards Enhancing Social Connectedness	SinkInSync presents the design and prototype of a VR-based cross-person EEG neurofeedback platform. This generative VR platform uses one user's brainwave data to procedurally render 3D scenes and passively displays visual cues that are synchronized with the real-time brainwave frequency to another user. With the platform, we aim to explore the potential of VR as an avenue for augmenting cognitive and emotional social connectedness in remote interactions via externally-induced brainwave synchronization between pairs of individuals.	https://dl.acm.org/doi/abs/10.1145/3588027.3595597	Tiange Wang, Xin Feng
SketchFaceNeRF: Sketch-based Facial Generation and Editing in Neural Radiance Fields	Realistic 3D facial generation based on Neural Radiance Fields (NeRFs) from 2D sketches benefits various applications. Despite the high realism of free-view rendering results of NeRFs, it is tedious and difficult for artists to achieve detailed 3D control and manipulation. Meanwhile, due to its conciseness and expressiveness, sketching has been widely used for 2D facial image generation and editing. Applying sketching to NeRFs is challenging due to the inherent uncertainty for 3D generation with 2D constraints, a significant gap in content richness when generating faces from sparse sketches, and potential inconsistencies for sequential multi-view editing given only 2D sketch inputs. To address these challenges, we present SketchFaceNeRF, a novel sketch-based 3D facial NeRF generation and editing method, to produce free-view photo-realistic images. To solve the challenge of sketch sparsity, we introduce a Sketch Tri-plane Prediction net to first inject the appearance into sketches, thus generating features given reference images to allow color and texture control. Such features are then lifted into compact 3D tri-planes to supplement the absent 3D information, which is important for improving robustness and faithfulness. However, during editing, consistency for unseen or unedited 3D regions is difficult to maintain due to limited spatial hints in sketches. We thus adopt a Mask Fusion module to transform free-view 2D masks (inferred from sketch editing operations) into the tri-plane space as 3D masks, which guide the fusion of the original and sketch-based generated faces to synthesize edited faces. We further design an optimization approach with a novel space loss to improve identity retention and editing faithfulness. Our pipeline enables users to flexibly manipulate faces from different viewpoints in 3D space, easily designing desirable facial models. Extensive experiments validate that our approach is superior to the state-of-the-art 2D sketch-based image generation and editing approaches in realism and faithfulness.	https://dl.acm.org/doi/abs/10.1145/3592100	Lin Gao, Feng-Lin Liu, Shu-Yu Chen, Kaiwen Jiang, Chun-Peng Li, Yu-Kun Lai, Hongbo Fu
Sketching Pipelines for Ephemeral Immersive Spaces	This hands-on class will allow artists to use open-source tools to create interactive and immersive experiences. These tools have been created and incubated at the Society for Arts and Technology (SAT), a unique non-profit organization in Canada whose mission is to democratize technologies to enable people to experience and author multisensory immersions. During the class we invite participants to use their favorite software on platforms they are already familiar with, to interface with our tools. The toolset will include transmission protocols, video mapping tools, sound spatialization software, and gestural control using pose detection. The class will be organized in two parts: a presentation of the tools and context involving the development and applications, and a hands-on session with an ephemeral immersive space. This event is designed for art researchers, artists, designers, content creators, and other creatives interested in creating immersive spaces using research-developed tools. Participants will learn how to employ open-source tools for different artistic tasks so that they will be able to deploy their own immersive spaces after the class.	https://dl.acm.org/doi/abs/10.1145/3588029.3599740	Michał Seta, Eduardo A. L. Meneses, Emmanuel Durand, Christian Frisson
Skin-Screen: A Computational Fabrication Framework for Color Tattoos	Tattoos are a highly popular medium, with both artistic and medical applications. Although the mechanical process of tattoo application has evolved historically, the results are reliant on the artisanal skill of the artist. This can be especially challenging for some skin tones, or in cases where artists lack experience. We provide the first systematic overview of tattooing as a computational fabrication technique. We built an automated tattooing rig and a recipe for the creation of silicone sheets mimicking realistic skin tones, which allowed us to create an accurate model predicting tattoo appearance. This enables several exciting applications including tattoo previewing, color retargeting, novel ink spectra optimization, color-accurate prosthetics, and more.	https://dl.acm.org/doi/abs/10.1145/3592432	Michal Piovarci, Alexandre Chapiro, Bernd Bickel
Social VR: How Academics Can Use Social VR for Research and Education	While many VR demos involve two users or one using a virtual agent, few contemplate using social VR. Many live demos at industry conferences do not adequately address the social aspect of VR and will not explore its possibilities. Given that Immersive experiences are moving more and more to social VR experiences, it is crucial to discuss its development and implications in various applications, such as arts, games, entertainment, health, and education.	https://dl.acm.org/doi/abs/10.1145/3588027.3605556	Brian Beams
SomatoShift: A Wearable Haptic Display for Somatomotor Reconfiguration via Modifying Acceleration of Body Movement	This paper proposes a wearable haptic device that utilizes control moment gyroscopes and a motion sensor to achieve somatomotor reconfiguration, altering the user's somatic perception of their body. The device can manipulate sensations, making body parts feel heavier or lighter, and modify the ease of movement during interactions with objects. Given its potential applications in avatar technology, sports, and assistive technology, this proposed device represents a promising avenue for enriching the user's bodily experiences.	https://dl.acm.org/doi/abs/10.1145/3588037.3595390	Takeru Hashimoto, Shigeo Yoshida, Takuji Narumi
Split-Lohmann Multifocal Displays	This work provides the design of a multifocal display that can create a dense stack of focal planes in a single shot. We achieve this using a novel computational lens that provides spatial selectivity in its focal length, i.e, the lens appears to have different focal lengths across points on a display behind it. This enables a multifocal display via an appropriate selection of the spatially-varying focal length, thereby avoiding time multiplexing techniques that are associated with traditional focus tunable lenses. The idea central to this design is a modification of a Lohmann lens, a focus tunable lens created with two cubic phase plates that translate relative to each other. Using optical relays and a phase spatial light modulator, we replace the physical translation of the cubic plates with an optical one, while simultaneously allowing for different pixels on the display to undergo different amounts of translations and, consequently, different focal lengths. We refer to this design as a Split-Lohmann multifocal display. Split-Lohmann displays provide a large étendue as well as high spatial and depth resolutions; the absence of time multiplexing and the extremely light computational footprint for content processing makes it suitable for video and interactive experiences. Using a lab prototype, we show results over a wide range of static, dynamic, and interactive 3D scenes, showcasing high visual quality over a large working range.	https://dl.acm.org/doi/abs/10.1145/3592110	Yingsi Qin, Wei-Yu Chen, Matthew O'Toole, Aswin C. Sankaranarayanan
State of the Art in Telepresence (Part 1)	the use of virtual reality technology, especially for remote control of machinery or for apparent participation in distant events	https://dl.acm.org/doi/abs/10.1145/3587423.3595484	Jason Lawrence, Ye Pan, Dan B Goldman, Rachel McDonnell, Carol O'Sullivan, Dave Luebke, Koki Nagano, Michael Zollhoefer, Jason Saragih
Stay Alive, My Son Chapter 1 & 2	"This abstract provides an overview of the immersive and interactive experience titled ""Stay Alive, My Son Chapter 1 & 2"" [Bousis 2021] which is based on the memoirs of Pin Yathay [Pin 2000], a survivor of the Cambodian genocide. The abstract summarizes the narrative structure, technological aspects, and thematic significance of the project. Additionally, it highlights the transformative journey of the audience, the utilization of virtual human creation technology, and the director's vision to evoke empathy and inspire change."	https://dl.acm.org/doi/abs/10.1145/3588027.3595601	Victoria Bousis
StripMaker: Perception-driven Learned Vector Sketch Consolidation	Artist sketches often use multiple strokes to depict a single intended curve. Humans effortlessly mentally such sketches by detecting groups of overdrawn strokes and replacing them with the corresponding intended curves. While this mental process is near instantaneous, manually annotating or retracing sketches to communicate this intended mental image is highly time consuming; yet most sketch applications are not designed to handle overdrawing and can only operate on overdrawing-free, consolidated sketches. We propose , a new and robust learning based method for automatic consolidation of raw vector sketches. We avoid the need for an unsustainably large manually annotated learning corpus by leveraging observations about artist workflow and perceptual cues viewers employ when mentally consolidating sketches. We train two perception-aware classifiers that assess the likelihood that a pair of stroke groups jointly depicts the same intended curve: our first classifier is purely local and only accounts for the properties of the evaluated strokes; our second classifier incorporates global context and is designed to operate on approximately consolidated sketches. We embed these classifiers within a consolidation framework that leverages artist workflow: we first process strokes in the order they were drawn and use our local classifier to arrive at an approximate consolidation output, then use the contextual classifier to refine this output and finalize the consolidated result. We validate StripMaker by comparing its results to manual consolidation outputs and algorithmic alternatives. StripMaker achieves comparable performance to manual consolidation. In a comparative study participants preferred our results by a 53% margin over those of the closest algorithmic alternative (67% versus 14%, other/neither 19%).	https://dl.acm.org/doi/abs/10.1145/3592130	Chenxi Liu, Toshiki Aoki, Mikhail Bessmeltsev, Alla Sheffer
Suit Up: AI MoCap	We present a novel marker-based motion capture (MoCap) technology. Instead of leveraging initialization and tracking for marker labeling as traditional solutions do, the present system is built upon real-time and low-latency data-driven models and optimization techniques, offering new possibilities and overcoming limitations currently present in the MoCap landscape. Even though we similarly begin with unlabeled markers captured with optical sensing within a capturing area, our approach diverges as we follow a data-driven and optimization pipeline to simultaneously denoise the markers and robustly and accurately solve the skeleton per frame. Similarly to traditional marker-based options, our work demonstrates higher stability and accuracy than inertial and/or markerless optical MoCap. Inertial MoCap lacks absolute positioning and suffers from drifting, therefore, it is almost impossible to achieve comparable positional accuracy. Markerless solutions lack the existence of a strong prior (i.e., markers) to increase the capturing precision, while, due to the heavier workload, the capturing frequency cannot easily scale, resulting in inaccuracies in fast movements. On the other hand, traditional marker-based motion capture heavily relies on high-quality marker data, assuming precise localization, outlier elimination and consistent marker tracking. In contrast, our innovative approach operates without such assumptions, effectively mitigating input noise, including ghost markers, occlusions, marker swaps, misplacement and mispositioning. This noise tolerance enables our system to function seamlessly with cameras with lower cost and specifications. Our method introduces body structure invariance, empowering automatic marker layout configuration by selecting from a diverse pool of models trained with different marker layouts. Our proposed MoCap technology integrates various consumer-grade optical sensors, leverages efficient data acquisition, succeeds in precise marker position estimation and allows for spatio-temporal alignment of multi-view streams. Sequentially, by incorporating data-driven models, our system achieves low latency and real-time rate performances. Finally, efficient body optimization techniques further improve the final MoCap solving, enabling seamless integration into various applications requiring real-time, accurate and robust motion capture. Concluding, real-time communities can be benefited from our MoCap which is a) affordable; with the use of low-cost equipment, b) scalable; with processing on the edge, c) portable; with easy setup and spatial calibration, d) robust; on heavy occlusions, marker removal and camera coverage and e) flexible; no need for super precise marker placement, super precise camera calibration, body calibration per actor or manual marker configuration.	https://dl.acm.org/doi/abs/10.1145/3588430.3597249	Anargyros Chatzitofis, Georgios Albanis, Nikolaos Zioulis, Spyridon Thermos
Surface Simplification using Intrinsic Error Metrics	"This paper describes a method for fast simplification of surface meshes. Whereas past methods focus on visual appearance, our goal is to solve equations on the surface. Hence, rather than approximate the extrinsic geometry, we construct a coarse of the input domain. In the spirit of the , we perform greedy decimation while agglomerating global information about approximation error. In lieu of extrinsic quadrics, however, we store intrinsic tangent vectors that track how far curvature ""drifts"" during simplification. This process also yields a bijective map between the fine and coarse mesh, and prolongation operators for both scalar- and vector-valued data. Moreover, we obtain hard guarantees on element quality via intrinsic retriangulation---a feature unique to the intrinsic setting. The overall payoff is a ""black box"" approach to geometry processing, which decouples mesh resolution from the size of matrices used to solve equations. We show how our method benefits several fundamental tasks, including geometric multigrid, all-pairs geodesic distance, mean curvature flow, geodesic Voronoi diagrams, and the discrete exponential map."	https://dl.acm.org/doi/abs/10.1145/3592403	Hsueh-Ti Derek Liu, Mark Gillespie, Benjamin Chislett, Nicholas Sharp, Alec Jacobson, Keenan Crane
Swing to the Moon	Living in the forest, little spider Temi dreams of catching the Moon. For that, she will do anything.	https://dl.acm.org/doi/abs/10.1145/3577024.3587949	Marie Bordessoule, Adriana Bouissié, Nadine De Boer, Elisa Drique, Chloé Lauzu, Vincent Levrero, Solenne Moreau
SyncArms: Gaze-Driven Target Object-oriented Manipulation for Parallel Operation of Robot Arms in Distributed Physical Environments	Enhancing human capabilities through the use of multiple bodies has been a significant research agenda. When multiple bodies are synchronously operated in different environments, the differences in environment placement make it difficult to interact with objects simultaneously. In contrast, if automatic control is performed to complement the differences and to perform a parallel task, the mismatch between the user and robotic arm movements generates visuomotor incongruence, leading to a decline in embodiment across the body. This can lead to difficulty completing tasks or achieving goals, and may even cause frustration or anxiety. To address this issue, we have developed a system that allows a parallel operation of synchronized multiple robotic arms by assisting the arm towards which the user's gaze is not directed while maintaining the sense of embodiment over the robotic arms.	https://dl.acm.org/doi/abs/10.1145/3588037.3595401	Koki Kawamura, Shunichi Kasahara, Masaaki Fukuoka, Katsutoshi Masai, Ryota Kondo, Maki Sugimoto
THE CALLING VR: A Musical Virtual Reality Experience	"In the three plus years since the COVID-19 pandemic shut down Broadway and regional theaters across the United States, many performing arts institutions are slowly working their way back to pre-pandemic levels in terms of annual revenue [Weinert, 2023]. During this period, many theaters turned to sliding scale ticket models [Mangi, 2022] and hybrid in-person/streamed theater seasons to combat increased inflation related production costs [Vincentelli, 2021]. As we use the lessons learned from the pandemic shutdown to shape the future of American theater, there is a golden opportunity for musical theater producers to embrace virtual reality (VR) technology to reduce development and production related expenses ahead of staging their next musical for a live audience. The Calling: A Musical VR Experience serves as an immersive historical fantasy inspired by the events that transpired during the infamous Memphis Sanitation Strike of 1968. Created in Unity for Oculus Quest I, II, and The Rift S, The Calling was developed as a ""guided experience"" serving as the prologue for a larger historical musical currently under development with the Tony Award winning producing team at Apples & Oranges Studios."	https://dl.acm.org/doi/abs/10.1145/3588027.3595598	Alani Ilongwe, Christopher Sepulveda, Tim Kashani
Tear Off	A young bee must overcome her condition if she is to survive the DESTRUCTIVE HORNET that has invaded her hive. This adventure allows us to discover the dark side of a colony that is utopian at first glance. The film also explores the world of bees in more depth, with a story that highlights the fear of the unknown, of the dark, of claustrophobia, pushed in that by a macro camera which follows the character, in an alternative and frightening documented universe.	https://dl.acm.org/doi/abs/10.1145/3577024.3583775	Clément Del Negro, Charlotte Fargier, Héloïse Neveu, Camille Souchard, Nalini Bashin, Mikko Petremand, Matthias Bourgeuil
Tell Me, Inge…: An Interactive Interview with a Holocaust Survivor	Tell Me, Inge… is a cross-platform VR/XR experience that allows the public to interact with Holocaust survivor Inge Auerbacher. Users are immersed in Inge's memories as a child survivor of the Theresienstadt ghetto, asking questions in their own words and listening to her video responses combined with hand-drawn 3D animations within a 360-degree canvas. All reconstructions of Inge's memories were based on her personal testimony, historical documentation, and location surveys. We created a novel pipeline for authoring 3D Quill animations in the PlayCanvas WebXR engine. The system is driven by StoryFile's Conversa platform for AI training and conversational video streaming.	https://dl.acm.org/doi/abs/10.1145/3588430.3597256	Val Jones, Weston Gaylord, Ari Palitz, Inge Auerbacher, Andrea Csabai, Dan Franke, Moira Hamilton, Kia Hays, Andrew Lush, Laiylaly Mandujano, Ruxandra Popescu, Francis Rockwell, Zoe Roellin, Ronen Shaham, Stephen Smith
Temporal Set Inversion for Animated Implicits	We exploit the temporal coherence of closed-form animated implicit surfaces by locally re-evaluating an octree-like discretization of the implicit field only as and where is necessary to rigorously maintain a global error invariant over time, thereby saving resources in static or slowly-evolving areas far from the motion where per-frame updates are not necessary. We treat implicit surface rendering as a special case of the continuous constraint satisfaction problem of set inversion, which seeks preimages of arbitrary sets under vector-valued functions. From this perspective, we formalize a temporally-coherent set inversion algorithm that localizes changes in the field by range-bounding its time derivatives using interval arithmetic. We implement our algorithm on the GPU using persistent thread scheduling and apply it to the scalar case of implicit surface and swept volume rendering where we achieve significant speedups in complex scenes with localized deformations like those found in games and modelling applications where interactivity is required and bounded-error approximation is acceptable.	https://dl.acm.org/doi/abs/10.1145/3592448	Kavosh Jazar, Paul G. Kry
Text to Haptics: Method and Case Studies of Designing Tactile Graphics for Inclusive Tactile Picture Books by Digital Fabrication and Generative AI	In this case study, we explore the possibilities between Generative AI and tactile graphics for inclusivity in computer graphics communities. The use of Generative AI in the design of tactile graphics has made it possible to support the processes used by publishers and tactile graphics designers. In addition, the idea of printing tactile graphics on transparent sheets with a 3D printer through digital fabrication technology allows the creation of inclusive tactile picture books that can be read and enjoyed together by sighted and visually impaired people in a single picture book.	https://dl.acm.org/doi/abs/10.1145/3588029.3595471	Kengo Tanaka, Tastuki Fushimi, Ayaka Tsutsui, Yoichi Ochiai
The Anticipation of Rain	The Anticipation of Rain is fine artist Naima Karim's memoir and plea for global collaboration in a time of climate change. It utilises Naima's brush strokes, immersive sound, and bespoke scents. Having grown up in Bangladesh, the monsoon is a joyous and exciting experience for Naima, which became unpredictable now.	https://dl.acm.org/doi/abs/10.1145/3577025.3582684	Naima Karim
The Seneca Animation Model: Cross-disciplinary animation education program design for a changing industry	"Seneca Polytechnique, located in Toronto, Canada, first launched its three-year animation advanced diploma program in 2002 and regularly ranks as a global top-five program, producing or partnering on groundbreaking films like Chris Landreth's Ryan (2005, Oscar Winner), Subconscious Password (2013, Anney Grand Prix), and Aubry Mintz's Nothing to Say (2018, multiple awards). Graduates have been in high demand in production studios around the world. At the height of this success, the animation faculty proposed the unimaginable: scrap the program in its current form and start over. Originally, the program was based on a familiar template that emphasized foundational drawing skills with increasing specialization leading to a capstone project in which students demonstrated skills aligned with production studios' needs. All students took two years of foundational instruction that focused on 2D animation production with classes in character animation, storyboarding, layout, character design, life drawing, acting and animation history. Toon Boom Harmony was introduced in second year, after a focus on paper flipping in year one. Students moving into the third year had the option choosing one of three specialization streams: 2D animation, 3D animation, and game art & animation. While regular curriculum maintenance enabled graduates to be competitive in the labour market, specialization pathways became less clear because more methods were becoming cross-disciplinary between 2D, 3D, gaming, visual development, and visual effects. The goal of the new program was to create a wholly new cross-disciplinary animation education model in recognition of the changing conditions of the industry. Our new program begins with a foundational year that develops student ability in animation art for any specialization. An example of this approach is a new first-year storyboard class, now named Film Language, which focuses on analytical assignments as opposed to creating storyboards for production. The related skills of boarding/previz are expanded upon in years two and three. We also increased digital workflows in our studios with specially designed animation desks that fit professional drawing tablets. In the new program, students choose a specialization stream in year two, selecting one of four options: 2D animation, 3D animation, game art animation, and visual development. ""Horizontal learning"", in which students across all streams take common courses, promotes cross-disciplinary learning to enable practical awareness of other production modes. For example, students across all four streams continue to mix for life drawing classes. In year three, students continue in their specialization stream but will work on capstone projects that include team members across all streams. In this scenario, cross-disciplinary learning takes place in the collaboration of film projects rather than shared classes. Imagine a team of 2D students preparing their final-year film working with 3D students on some elements, game students to prepare real-time delivery of the final film in a game engine, and visual development students working concept art for the films of the 3D students. A ""production weeklies"" class in the final year enable the sharing of project concepts across all streams to stimulate self-organized collaboration. To support these changes, Seneca invested approximately $1.7 million to make significant enhancements in technology and facilities. This was required to accommodate many more students as the enrolment of the program will double from its original size. Upon full rollout of the new program in 2024, it will accommodate 450 students. Besides solid production and artistic skills, the faculty pre-defined the goals for student and graduate professional expectations. These soft skills, including ability to work in teams, were foundational guides for the development of all new curriculum. To support student success, they are enrolled in the following professional practices courses to strengthen soft skills in their first semester. This process was almost entirely faculty-driven and supported by college administration in the investment of new facilities that mirrored contemporary production studios."	https://dl.acm.org/doi/abs/10.1145/3587424.3595579	Mark Jones, Sean Craig
The Talk: Speculative Conversation with Everyday Objects	Communication between humans and everyday objects is often implicit. In this paper, we create speculative scenarios where users could have a conversation, as explicit communication, with everyday objects that usually may not have conversational artificial intelligence (AI) installed, such as a book. We present a design fiction narrative and a conversational system about conversations with everyday objects. Our user test showed a positive acceptance of conversation with everyday objects, and users tried to build a human-like relationship with them.	https://dl.acm.org/doi/abs/10.1145/3588028.3603643	Keijiroh Nagano, Maya Georgieva, Harpreet Sareen, Clarinda Mac Low
The Visual Language of Fabrics	We introduce text2fabric, a novel dataset that links free-text descriptions to various fabric materials. The dataset comprises 15,000 natural language descriptions associated to 3,000 corresponding images of fabric materials. Traditionally, material descriptions come in the form of tags/keywords, which limits their expressivity, induces pre-existing knowledge of the appropriate vocabulary, and ultimately leads to a chopped description system. Therefore, we study the use of free-text as a more appropriate way to describe material appearance, taking the use case of fabrics as a common item that non-experts may often deal with. Based on the analysis of the dataset, we identify a compact lexicon, set of attributes and key structure that emerge from the descriptions. This allows us to accurately understand how people describe fabrics and draw directions for generalization to other types of materials. We also show that our dataset enables specializing large vision-language models such as CLIP, creating a meaningful latent space for fabric appearance, and significantly improving applications such as fine-grained material retrieval and automatic captioning.	https://dl.acm.org/doi/abs/10.1145/3592391	Valentin Deschaintre, Julia Guerrero-Viu, Diego Gutierrez, Tamy Boubekeur, Belen Masia
The Werewolf Experience	An immersive branching narrative, in TWE the user is a werewolf in an 1850s mining town hunting the sacred white stag in order to cure themselves of their lycanthropy; however, the town is hunting them. Using their voice, they must choose, howl or stay silent. Every choice effects the outcome.	https://dl.acm.org/doi/abs/10.1145/3577025.3585286	Christopher Morrison
The use of Containers in OpenGL, ML and HPC for Teaching and Research Support	We share our experience of using containers (Docker and Singularity) in computer graphics teaching and research support, as well as the use of the same containers in HPC. We use OpenISS sample containers for this purpose, containers that are open-source and publicly available.	https://dl.acm.org/doi/abs/10.1145/3588028.3603676	Serguei Mokhov, Jonathan Llewellyn, Carlos Alarcon Meza, Tariq Daradkeh, Gillian Roper
The voice in the hollow	An African fable of sisterhood, envy, and ancient evil.	https://dl.acm.org/doi/abs/10.1145/3577024.3582072	Miguel Ortega
Tidd: Augmented Tabletop Interaction Supports Children with Autism to Train Daily Living Skills	Children with autism may have difficulties in learning daily living skills due to repetitive behavior, which poses a challenge to their independent living training. Previous studies have shown the potential of using interactive technology to help children with autism train daily living skills. In this poster, we present Tidd, an interactive device based on desktop augmented reality projection, designed to support children with autism in daily living skills training. The system combines storytelling with Applied Behavior Analysis (ABA) therapy to scaffold the training process. A pilot study was conducted on 13 children with autism in two autism rehabilitation centers. The results showed that Tidd helped children with autism learn bed-making and dressing skills while engaging in the training process.	https://dl.acm.org/doi/abs/10.1145/3588028.3603674	Qin Wu, Wenlu Wang, Qianru Liu, Jiashuo Cao, Duo Xu, Suranga Nanayakkara
Today's Holiday Moments are Tomorrow's Memories	"Food has an incredible power to ensnare our senses and punch through time, bestowing upon us a remarkable window through which we can experience sharing past meals with loved ones once again. This enchanting, magical force is behind Hornet's most ambitious holiday film for Kroger yet where co-directors Yves Geleyn and Michael Thurmeier reinforce the campaign's core message that ""Today's Holiday Moments are Tomorrow's Memories""."	https://dl.acm.org/doi/abs/10.1145/3577024.3588992	Yves Geleyn
Topology driven approximation to rational surface-surface intersection via interval algebraic topology analysis	Computing the intersection between two parametric surfaces (SSI) is one of the most fundamental problems in geometric and solid modeling. Maintaining the SSI topology is critical to its computation robustness. We propose a topology-driven hybrid symbolic-numeric framework to approximate rational parametric surface-surface intersection (SSI) based on a concept of , which configures within a 4D interval box the SSI topology. We map the SSI topology to an algebraic system's solutions within the framework, classify and enumerate all topological cases as a mixture of four fundamental cases (or their specific sub-cases). Various complicated topological situations are covered, such as cusp points or curves, tangent points (isolated or not) or curves, tiny loops, self-intersections, or their mixtures. The theoretical formulation is also implemented numerically using advanced real solution isolation techniques, and computed within a topology-driven framework which maximally utilizes the advantages of the topology maintenance of algebraic analysis, the robustness of iterative subdivision, and the efficiency of forward marching. The approach demonstrates improved robustness under benchmark topological cases when compared with available open-source and commercial solutions, including IRIT, SISL, and Parasolid.	https://dl.acm.org/doi/abs/10.1145/3592452	Jin-San Cheng, Bingwei Zhang, Yikun Xiao, Ming Li
Toward Efficient Capture of Spatially Varying Material Properties	Improvements in the science and art of computer-graphics rendering, particularly with a shift in recent decades toward more physically driven models in both real-time and offline rendering, have motivated improvements in material models. However, real-world materials are often still significantly more complex in their observable light scattering than current shading models used to represent them in renderers. In order to represent these complexities at higher visible fidelity, improved methods for material acquisition and representation are desired, and one important area of continued study is capture and representation of properties of spatially varying physical materials. We present developing efforts toward acquiring and representing those spatially varying properties that build on recent work concerning parameterization techniques to improve the efficiency of material acquisition.	https://dl.acm.org/doi/abs/10.1145/3588028.3603677	Jessica Baron, Eric Patterson, Jonathan Dupuy
Towards Attention–aware Foveated Rendering	Foveated graphics is a promising approach to solving the bandwidth challenges of immersive virtual and augmented reality displays by exploiting the falloff in spatial acuity in the periphery of the visual field. However, the perceptual models used in these applications neglect the effects of higherlevel cognitive processing, namely the allocation of visual attention, and are thus overestimating sensitivity in the periphery in many scenarios. Here, we introduce the first attention-aware model of contrast sensitivity. We conduct user studies to measure contrast sensitivity under different attention distributions and show that sensitivity in the periphery drops significantly when the user is required to allocate attention to the fovea. We motivate the development of future foveation models with another user study and demonstrate that tolerance for foveation in the periphery is significantly higher when the user is concentrating on a task in the fovea. Analysis of our model predicts significant bandwidth savings over those afforded by current models. As such, our work forms the foundation for attention-aware foveated graphics techniques.	https://dl.acm.org/doi/abs/10.1145/3592406	Brooke Krajancich, Petr Kellnhofer, Gordon Wetzstein
Towards Material Digitization with a Dual-scale Optical System	Existing devices for measuring material appearance in spatially-varying samples are limited to a single scale, either micro or mesoscopic. This is a practical limitation when the material has a complex multi-scale structure. In this paper, we present a system and methods to digitize materials at two scales, designed to include high-resolution data in spatially-varying representations at larger scales. We design and build a hemispherical light dome able to digitize flat material samples up to 11x11cm. We estimate geometric properties, anisotropic reflectance and transmittance at the microscopic level using polarized directional lighting with a single orthogonal camera. Then, we propagate this structured information to the mesoscale, using a neural network trained with the data acquired by the device and image-to-image translation methods. To maximize the compatibility of our digitization, we leverage standard BSDF models commonly adopted in the industry. Through extensive experiments, we demonstrate the precision of our device and the quality of our digitization process using a set of challenging real-world material samples and validation scenes. Further, we demonstrate the optical resolution and potential of our device for acquiring more complex material representations by capturing microscopic attributes which affect the global appearance: we characterize the properties of textile materials such as the yarn twist or the shape of individual fly-out fibers. We also release the SEDDIDOME dataset of materials, including raw data captured by the machine and optimized parameteres.	https://dl.acm.org/doi/abs/10.1145/3592147	Elena Garces, Victor Arellano, Carlos Rodriguez-Pardo, David Pascual-Hernandez, Sergio Suja, Jorge Lopez-Moreno
Town Hall Square	Bernard works behind the windows of a ticket booth at the Town Hall Square subway station and is waiting for something he seems to have forgotten. One day, however, a little tiger shows up and turns Bernard's orderly life upside down. A short film about an unexpected visitor, annoying radio songs and the lust for life!	https://dl.acm.org/doi/abs/10.1145/3577024.3587994	Christian Kaufmann
Transtiff: Haptic Interaction with a Stick Interface with Various Stiffness	We propose Transtiff, a stick-shaped device that can display various stiffness for stick-based haptic interaction. The device has a stiffness-changing joint replicating an artificial muscle mechanism in the relay portion of the stick to change its stiffness. Transtiff can be applied to touch interaction of the screen, augmenting the haptic experience of operating with a stylus pen, which is usually felt uniform. As applications, users can experience the sensation of pen and brush writing on a single device. In addition, it is possible to change the stiffness of the device for each object on the screen to reproduce the tactile feel of that object.	https://dl.acm.org/doi/abs/10.1145/3588037.3595402	Ayumu Ogura, Kodai Ito, Shigeo Yoshida, Kazutoshi Tanaka, Yuichi Itoh
TriSalus: SD-101 and SmartValve Technology	TriSalus's investigational therapeutic candidate SD-101 has the potential to reprogram or eliminate immunosuppressive cells and enable immunotherapy in liver tumors.	https://dl.acm.org/doi/abs/10.1145/3577024.3588877	David Ehlert, Joshua Bird, Dani Bergey, Valerie Bentivegna, Brandon Keehner, Regina Milner, Rachel Simons
Trim Regions for Online Computation of From-Region Potentially Visible Sets	Visibility computation is a key element in computer graphics applications. More specifically, a from-region potentially visible set (PVS) is an established tool in rendering acceleration, but its high computational cost means a from-region PVS is almost always precomputed. Precomputation restricts the use of PVS to static scenes and leads to high storage cost, in particular, if we need fine-grained regions. For dynamic applications, such as streaming content over a variable-bandwidth network, online PVS computation with configurable region size is required. We address this need with trim regions, a new method for generating from-region PVS for arbitrary scenes in real time. Trim regions perform controlled erosion of object silhouettes in image space, implicitly applying the shrinking theorem known from previous work. Our algorithm is the first that applies automatic shrinking to unconstrained 3D scenes, including non-manifold meshes, and does so in real time using an efficient GPU execution model. We demonstrate that our algorithm generates a tight PVS for complex scenes and outperforms previous online methods for from-viewpoint and from-region PVS. It runs at 60 Hz for realistic game scenes consisting of millions of triangles and computes PVS with a tightness matching or surpassing existing approaches.	https://dl.acm.org/doi/abs/10.1145/3592434	Philip Voglreiter, Bernhard Kerbl, Alexander Weinrauch, Joerg Hermann Mueller, Thomas Neff, Markus Steinberger, Dieter Schmalstieg
Ultrasonic Embossment of Acrylic Sheets with Transparency Control	This paper discusses a new processing technology for acrylic sheets using high-intensity focused ultrasound (HIFU). Acrylic are widely used due to their transparency and weather resistance, but current processing methods cannot create embossing features without reductive manufacturing methods. HIFU emits ultrasonic waves that melt the surface of the acrylic sheet, creating bumps that can be controlled by adjusting the irradiation time, speed, distance, and amplitude. This technology can control transparency, line thickness, and tactility and can create bumps and depressions instantaneously, making it potentially useful for embossing, etching, and braille generation. The paper demonstrates the controllable variables of the HIFU approach and highlights its potential for industry as a high-precision acrylic plate processing technology.	https://dl.acm.org/doi/abs/10.1145/3588029.3595475	Ayaka Tsutsui, Tatsuki Fushimi, Kengo Tanaka, Takahito Murakami, Yoichi Ochiai
UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image	Text-driven image generation methods have shown impressive results recently, allowing casual users to generate high quality images by providing textual descriptions. However, similar capabilities for editing existing images are still out of reach. Text-driven image editing methods usually need edit masks, struggle with edits that require significant visual changes and cannot easily keep specific details of the edited portion. In this paper we make the observation that image-generation models can be converted to image-editing models simply by fine-tuning them on a single image. We also show that initializing the stochastic sampler with a noised version of the base image before the sampling and interpolating relevant details from the base image after sampling further increase the quality of the edit operation. Combining these observations, we propose UniTune, a novel image editing method. UniTune gets as input an arbitrary image and a textual edit description, and carries out the edit while maintaining high fidelity to the input image. UniTune does not require additional inputs, like masks or sketches, and can perform multiple edits on the same image without retraining. We test our method using the Imagen model in a range of different use cases. We demonstrate that it is broadly applicable and can perform a surprisingly wide range of expressive editing operations, including those requiring significant visual changes that were previously impossible.	https://dl.acm.org/doi/abs/10.1145/3592451	Dani Valevski, Matan Kalman, Eyal Molad, Eyal Segalis, Yossi Matias, Yaniv Leviathan
Unleashing the Power of Python in Autodesk Maya	Coding empowers automation. Scripts can handle mundane and repetitive tasks in an efficient and precise manner. This course will offer will use an hands-on interactive format to walk attendees through representative scripting projects, selected to be useful for everyday workflows. It is intended to be an intermediate course. The goal is to cover provide enough information for attendees to build on later. Python scripting can automate many tasks in Maya, from running simple commands to developing plug-ins. Attendees will learn how to automate a simple task using the magic of scripting, through a hands-on projects. The course will placing objects in a scene through scripting MASH (motion graphic) networks. Attendees should walk away with a solid understanding of the power Python scripting and Maya commands provide, and the the ability to conceive their own advance projects for Maya. Attendees should have programming experience, preferably in Python, but a solid grasp of the foundational programming constructs should suffice. Attendees should have Autodesk Maya, Python, and Visual Studio Code pre-loaded on their devices if they intend to follow along.	https://dl.acm.org/doi/abs/10.1145/3588029.3599745	Ann McNamara
Unsupervised Learning of Robust Spectral Shape Matching	We propose a novel learning-based approach for robust 3D shape matching. Our method builds upon deep functional maps and can be trained in a fully unsupervised manner. Previous deep functional map methods mainly focus on predicting optimised functional maps alone, and then rely on off-the-shelf post-processing to obtain accurate point-wise maps during inference. However, this two-stage procedure for obtaining point-wise maps often yields sub-optimal performance. In contrast, building upon recent insights about the relation between functional maps and point-wise maps, we propose a novel unsupervised loss to couple the functional maps and point-wise maps, and thereby directly obtain point-wise maps without any post-processing. Our approach obtains accurate correspondences not only for near-isometric shapes, but also for more challenging non-isometric shapes and partial shapes, as well as shapes with different discretisation or topological noise. Using a total of nine diverse datasets, we extensively evaluate the performance and demonstrate that our method substantially outperforms previous state-of-the-art methods, even compared to recent supervised methods. Our code is available at https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.	https://dl.acm.org/doi/abs/10.1145/3592107	Dongliang Cao, Paul Roetzer, Florian Bernard
Utilizing LiDAR Data for 3D Sound Source Localization	This paper introduces a visualization system of 3D sound pressure distribution using a minimum variance distortionless response (MVDR) beamformer with Light Detection and Ranging (LiDAR) technology to estimate the sound source localization. By using LiDAR to capture 3D data, the proposed system calculates the time-averaged output power of the MVDR beamformer at the virtual source position for each point in the point cloud data. The results are then superimposed onto the 3D data to estimate sound sources. The proposed system provides a more visually comprehensible display of the sound pressure distribution in 3D.	https://dl.acm.org/doi/abs/10.1145/3588028.3603682	Masahiko Goto, Yasuhiro Oikawa, Atsuto Inoue, Wataru Teraoka, Takahiro Sato, Yasuyuki Iwane, Masahito Kobayashi
Variational quasi-harmonic maps for computing diffeomorphisms	Computation of injective (or inversion-free) maps is a key task in geometry processing, physical simulation, and shape optimization. Despite being a longstanding problem, it remains challenging due to its highly nonconvex and combinatoric nature. We propose computation of to obtain smooth inversion-free maps. Our work is built on a key observation about inversion-free maps: A planar map is a diffeomorphism if and only if it is quasi-harmonic and satisfies a special Cauchy boundary condition. We hence equate the inversion-free mapping problem to an optimal control problem derived from our theoretical result, in which we search in the space of parameters that define an elliptic PDE. We show that this problem can be solved by minimizing within a family of functionals. Similarly, our discretized functionals admit exactly injective maps as the minimizers, empirically producing inversion-free discrete maps of triangle meshes. We design efficient numerical procedures for our problem that prioritize robust convergence paths. Experiments show that on challenging examples our methods can achieve up to orders of magnitude improvement over state-of-the-art, in terms of speed or quality. Moreover, we demonstrate how to optimize a generic energy in our framework while restricting to quasi-harmonic maps.	https://dl.acm.org/doi/abs/10.1145/3592105	Yu Wang, Minghao Guo, Justin Solomon
VideoDoodles: Hand-Drawn Animations on Videos with Scene-Aware Canvases	We present an interactive system to ease the creation of so-called - videos on which artists insert hand-drawn animations for entertainment or educational purposes. Video doodles are challenging to create because to be convincing, the inserted drawings must appear as if they were part of the captured scene. In particular, the drawings should undergo tracking, perspective deformations and occlusions as they move with respect to the camera and to other objects in the scene - visual effects that are difficult to reproduce with existing 2D video editing software. Our system supports these effects by relying on planar canvases that users position in a 3D scene reconstructed from the video. Furthermore, we present a custom tracking algorithm that allows users to anchor canvases to static or dynamic objects in the scene, such that the canvases move and rotate to follow the position and direction of these objects. When testing our system, novices could create a variety of short animated clips in a dozen of minutes, while professionals praised its speed and ease of use compared to existing tools.	https://dl.acm.org/doi/abs/10.1145/3592413	Emilie Yu, Kevin Blackburn-Matzen, Cuong Nguyen, Oliver Wang, Rubaiat Habib Kazi, Adrien Bousseau
Virtual Cinematography: The Embodied Camera ✱	The Embodied Camera is an analog exercise deployed within my Virtual Cinematography course, designed for students to explore and enact key virtual/CGI camera concepts through playful real world activities.	https://dl.acm.org/doi/abs/10.1145/3587424.3595585	C K Olsen
Virtual Manipulation of Cultural Assets: An Initial Case Study with Single-Joint Articulated Models	Virtual space can eliminate the physical constraints of real space. Three-dimensional (3D) digital twins enable us to experience manipulation of cultural assets that are inaccessible in real space. However, most 3D models obtained using current reconstruction techniques are static; they cannot be manipulated dynamically. In this study, we reproduce a dynamic 3D model of a simple articulated object with a rotating joint from a static point cloud only with reference to a video of the motion and a manually added rotation axis. The reconstructed actual cultural asset is used for first-person experiences in an augmented reality environment.	https://dl.acm.org/doi/abs/10.1145/3588028.3603649	Sakura Shinji, Issei Fujishiro
Virtual Mirrors: Non-Line-of-Sight Imaging Beyond the Third Bounce	Non-line-of-sight (NLOS) imaging methods are capable of reconstructing complex scenes that are not visible to an observer using indirect illumination. However, they assume only third-bounce illumination, so they are currently limited to single-corner configurations, and present limited visibility when imaging surfaces at certain orientations. To reason about and tackle these limitations, we make the key observation that planar diffuse surfaces behave specularly at wavelengths used in the computational wave-based NLOS imaging domain. We call such surfaces We leverage this observation to expand the capabilities of NLOS imaging using illumination beyond the third bounce, addressing two problems: imaging single-corner objects at limited visibility angles, and imaging objects hidden behind two corners. To image objects at limited visibility angles, we first analyze the reflections of the known illuminated point on surfaces of the scene as an estimator of the position and orientation of objects with limited visibility. We then image those limited visibility objects by computationally building secondary apertures at other surfaces that observe the target object from a direct visibility perspective. Beyond single-corner NLOS imaging, we exploit the specular behavior of virtual mirrors to image objects hidden behind a second corner by imaging the space behind such virtual mirrors, where the mirror image of objects hidden around two corners is formed. No specular surfaces were involved in the making of this paper.	https://dl.acm.org/doi/abs/10.1145/3592429	Diego Royo, Talha Sultan, Adolfo Muñoz, Khadijeh Masumnia-Bisheh, Eric Brandt, Diego Gutierrez, Andreas Velten, Julio Marco
Virtual reality: the gateway for non-professionals to solve complex design problems	Leopoly is working on a 3D design software for VR called Shapelab with the aim to provide tools for professional and hobby 3D artists. During development, we have been approached by users coming from various professional backgrounds other than artistic 3D design, which made us want to explore the possibilities virtual reality has to offer for these use cases. We argue that VR could open doors for people with no background in 3D modeling to solve complex design problems needed to digitize their traditional workflows.	https://dl.acm.org/doi/abs/10.1145/3587424.3595582	Dorottya Esztergalos, Daniel Andrassy, Marcell Pal, Zoltan Karpati
VirtualVoxel: Real-Time Large Scale Scene Visualization and Modification	This short paper introduces VirtualVoxel, a novel rasterization-based voxel renderer that enables real-time rendering of 4M3 resolution voxel scenes. VirtualVoxel combines the benefits of virtual texture and virtual geometry, allowing for efficient storage and rendering of texture and geometry information simultaneously. Similar to the VirtualTexture approach, VirtualVoxel streams the appropriate level of detail into GPU memory, ensuring optimal performance. However, unlike previous graph-based voxel visualization methods, VirtualVoxel stores data linearly and employs the rasterization rendering pipeline, resulting in highly efficient modification of large scenes. VirtualVoxel's high-performance capabilities make it ideal for a wide range of applications, from video games to scientific visualizations. It can render scenes at resolutions of up to 4M3 voxels in real-time, providing artists and designers with a powerful new tool for creating stunning graphics.	https://dl.acm.org/doi/abs/10.1145/3588028.3603664	Jinyuan Yang, Abraham Campbell
Walk on Stars: A Grid-Free Monte Carlo Method for PDEs with Neumann Boundary Conditions	Grid-free Monte Carlo methods based on the algorithm solve fundamental partial differential equations (PDEs) like the Poisson equation without discretizing the problem domain or approximating functions in a finite basis. Such methods hence avoid aliasing in the solution, and evade the many challenges of mesh generation. Yet for problems with complex geometry, practical grid-free methods have been largely limited to basic Dirichlet boundary conditions. We introduce the algorithm, which solves linear elliptic PDEs with arbitrary mixed Neumann and Dirichlet boundary conditions. The key insight is that one can efficiently simulate reflecting Brownian motion (which models Neumann conditions) by replacing the balls used by WoS with domains. We identify such domains via the closest point on the visibility silhouette, by simply augmenting a standard bounding volume hierarchy with normal information. Overall, WoSt is an easy modification of WoS, and retains the many attractive features of grid-free Monte Carlo methods such as progressive and view-dependent evaluation, trivial parallelization, and sublinear scaling to increasing geometric detail.	https://dl.acm.org/doi/abs/10.1145/3592398	Rohan Sawhney, Bailey Miller, Ioannis Gkioulekas, Keenan Crane
Web Programming Using the WebGPU API	Today's web-based programming environments has become more multifaceted for accomplishing tasks that go beyond 'browsing' web-pages. The process of developing efficient web-based programs for such a wide array of applications poses a number of challenges to the programming community. Applications possess a number of workload behaviors, ranging from control intensive (e.g., searching, sorting, and parsing) to data intensive (e.g., image processing, simulation and modeling, and data mining). Web-based applications can also be characterized as compute intensive (e.g., iterative methods, numerical methods, and financial modeling), where the overall throughput of the web application is heavily dependent on the computational efficiency of the underlying hardware. Of course, no single architecture is best for running all classes of workloads, and most applications possess a mix of the workload characteristics. For instance, control-intensive applications tend to run faster on the CPU, whereas data-intensive applications tend to run fast on massively parallel architectures (like the GPU), where the same operation is applied to multiple data items concurrently. To extend and support these various workload classes so that browser-based applications wouldn't be hindered, a new generation of API needed to be developed (open the door for developers so that they can access the power of new hardware/technologies). One example of this, is the WebGPU API which exposes the capabilities of GPU hardware for the Web. The course is intended to help you get started with the WebGPU API while understanding both the HOW and WHY behind it works, so you can create your own solutions. This course is designed to teach you the new WebGPU API for graphics and compute techniques without any prior knowledge. All you need is some JavaScript experience and preferably an understanding of basic trigonometry. Whether you're new to graphics and compute development or an old pro, everyone has to start somewhere. Generally, that means starting with the basics which is the focus of this course. You'll learn through simple, easy-to-learn hands-on exercises that help you master the subject. It does this by using multiple task-based activities and discussions which complement and build upon one another.	https://dl.acm.org/doi/abs/10.1145/3587423.3595543	Benjamin Kenwright
What is Github? Brand Film	Through a blend of styles, some comedic charm, and the aid of a rubber duck, we set out to help the world's biggest software development company, GitHub, unleash their raison d'être to the masses.	https://dl.acm.org/doi/abs/10.1145/3577024.3589016	Vincent Lammers, Justin Cone
Winding Numbers on Discrete Surfaces	In the plane, the is the number of times a curve wraps around a given point. Winding numbers are a basic component of geometric algorithms such as point-in-polygon tests, and their generalization to data with noise or topological errors has proven valuable for geometry processing tasks ranging from surface reconstruction to mesh booleans. However, standard definitions do not immediately apply on surfaces, where not all curves bound regions. We develop a meaningful generalization, starting with the well-known relationship between winding numbers and harmonic functions. By processing the derivatives of such functions, we can robustly filter out components of the input that do not bound any region. Ultimately, our algorithm yields (i) a closed, completed version of the input curves, (ii) integer labels for regions that are meaningfully bounded by these curves, and (iii) the complementary curves that do not bound any region. The main computational cost is solving a standard Poisson equation, or for surfaces with nontrivial topology, a sparse linear program. We also introduce special basis functions to represent singularities that naturally occur at endpoints of open curves.	https://dl.acm.org/doi/abs/10.1145/3592401	Nicole Feng, Mark Gillespie, Keenan Crane
Women in the Story of Immersive Technology	The contributions of women to the history of computing are under recognized, and computer graphics is no exception. This anniversary of SIGGRAPH is particularly suitable for attending to the role of women in the diverse story of immersive technology. This panel gathers three creative contributors – one from the academy, one from the artworld, and one from industry – to discuss where immersive technology has been and where its headed.	https://dl.acm.org/doi/abs/10.1145/3587422.3603105	David C. Brock
Wonder of Life	What is life? What is life? It's the transient resting and then flyaway of a dragonfly. It's a faint fragrance dissipating in the air. It's ripples waving through the water surface and disappearing. Life lies within every little wonder and mystery of nature in the lotus pond at dawn.	https://dl.acm.org/doi/abs/10.1145/3577025.3584991	Wen Chieh Chang
Word-As-Image for Semantic Typography	A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. This task is highly challenging as it requires semantic understanding of the word and a creative idea of where and how to depict these semantics in a visually pleasing and legible manner. We rely on the remarkable ability of recent large pretrained language-vision models to distill textual concepts visually. We target simple, concise, black-and-white designs that convey the semantics clearly. We deliberately do not change the color or texture of the letters and do not use embellishments. Our method optimizes the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model. We incorporate additional loss terms to ensure the legibility of the text and the preservation of the style of the font. We show high quality and engaging results on numerous examples and compare to alternative techniques. Code and demo will be available at our project page.	https://dl.acm.org/doi/abs/10.1145/3592123	Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, Ariel Shamir
You Destroy. We Create | The War on Ukraine's Culture	What happens when museums can no longer show their collections, instead needing to hide them? How is it to be a street artist or create music during war? Witness how Ukrainian art and culture have become targets of the war, and meet the inspiring people on the frontlines protecting it.	https://dl.acm.org/doi/abs/10.1145/3577025.3585312	Gayatri Parameswaran, Felix Gaedtke
[in]florescence – a tangible audio-visual installation	This hands-on exhibition, based upon a physical computing artwork, will allow conference-goers to participate in the creation of an ongoing audio-visual composition.	https://dl.acm.org/doi/abs/10.1145/3588029.3595472	Ryan Buyssens
earrings.ai	earrings.ai is an AI design app platform that lets you create your dream earrings simply from describing it – and AR try them on [Chang 2018] and even participate in a virtual and real world marketplace [Chang 2016] to have artisan creators make them, or a network of 3D printers fab them!	https://dl.acm.org/doi/abs/10.1145/3588427.3595358	Yosun Chang
sPellorama: An Immersive Prototyping Tool using Generative Panorama and Voice-to-Prompts	We proposed sPellorama, an immersive tool that enables Virtual Reality (VR) content creators to quickly prototype scenes based on verbal input. The system first converts voice input to text, then utilizes a text-guided panorama generation model to produce the described scene. The panorama is later applied to Skybox in Unity. Previously generated panorama will be preserved in the photosphere and ready to be viewed. The pilot study shows that our tool can enhance the process of discussion and prototyping for VR content creators.	https://dl.acm.org/doi/abs/10.1145/3588028.3603667	Timothy Chen, Miguel Ying Jie Then, Jing-Yuan Huang, Yang-Sheng Chen, Ping-Hsuan Han, Yi-Ping Hung
Étendue Expansion in Holographic Near Eye Displays through Sparse Eye-box Generation Using Lens Array Eyepiece	In this paper, we present a novel method the étendue expansion of near-eye holographic displays through the generation of a sparse eye-box. Conventional holographic near-eye displays have suffered from narrow field of view or narrow eye-box due to the limited étendue supported by a spatial light modulator. We focus on the fact that these displays typically form a dense eye-box, which could be an excessive investment of the limited étendue. By rearranging the eye-box in a sparse manner, the practical étendue can be extended. With a properly designed sparse eye-box shape, it can provide the 3D holographic images and ensure continuous light entrance to the pupil. To create a sparse eye-box, we utilize a lens array as an eyepiece lens. We optimize the spatial light modulator's phase profile for the proposed system and analyze the impact of the use of the lens array eyepiece on the holographic image quality. In particular, we focus on the lens array specification of the lenslet pitch and the focal length, deriving feasible specifications based on our analysis. We experimentally demonstrate a near eye see-through display using the proposed system and verify the étendue expansion.	https://dl.acm.org/doi/abs/10.1145/3592441	Minseok Chae, Kiseung Bang, Dongheon Yoo, Yoonchan Jeong
∇-Prox: Differentiable Proximal Algorithm Modeling for Large-Scale Optimization	Tasks across diverse application domains can be posed as large-scale optimization problems, these include graphics, vision, machine learning, imaging, health, scheduling, planning, and energy system forecasting. Independently of the application domain, proximal algorithms have emerged as a formal optimization method that successfully solves a wide array of existing problems, often exploiting problem-specific structures in the optimization. Although model-based formal optimization provides a principled approach to problem modeling with convergence guarantees, at first glance, this seems to be at odds with black-box deep learning methods. A recent line of work shows that, when combined with learning-based ingredients, model-based optimization methods are effective, interpretable, and allow for generalization to a wide spectrum of applications with little or no extra training data. However, experimenting with such hybrid approaches for different tasks by hand requires domain expertise in both proximal optimization and deep learning, which is often error-prone and time-consuming. Moreover, naively unrolling these iterative methods produces lengthy compute graphs, which when differentiated via autograd techniques results in exploding memory consumption, making batch-based training challenging. In this work, we introduce ∇-Prox, a domain-specific modeling language and compiler for large-scale optimization problems using differentiable proximal algorithms. ∇-Prox allows users to specify optimization objective functions of unknowns concisely at a high level, and intelligently compiles the problem into compute and memory-efficient differentiable solvers. One of the core features of ∇-Prox is its full differentiability, which supports hybrid model- and learning-based solvers integrating proximal optimization with neural network pipelines. Example applications of this methodology include learning-based priors and/or sample-dependent inner-loop optimization schedulers, learned with deep equilibrium learning or deep reinforcement learning. With a few lines of code, we show ∇-Prox can generate performant solvers for a range of image optimization problems, including end-to-end computational optics, image deraining, and compressive magnetic resonance imaging. We also demonstrate ∇-Prox can be used in a completely orthogonal application domain of energy system planning, an essential task in the energy crisis and the clean energy transition, where it outperforms state-of-the-art CVXPY and commercial Gurobi solvers.	https://dl.acm.org/doi/abs/10.1145/3592144	Zeqiang Lai, Kaixuan Wei, Ying Fu, Philipp Härtel, Felix Heide
