title	abstract	url	authors
'South of Midnight': Crafting a Stop Motion Southern Gothic Game	This talk will break down the animation process in South of Midnight across gameplay and cutscenes. And how it is a mix of art and tech that brings the stop motion, southern gothic world and characters alive.	https://dl.acm.org/doi/abs/10.1145/3698897.3730573	Vincent Schneider, Remi Edmond, Sebastien Dussault
3D Stylization via Large Reconstruction Model	With the growing success of text or image guided 3D generators, users demand more control over the generation process, appearance stylization being one of them. Given a reference image, this requires adapting the appearance of a generated 3D asset to reflect the visual style of the reference while maintaining visual consistency from multiple viewpoints. To tackle this problem, we draw inspiration from the success of 2D stylization methods that leverage the attention mechanisms in large image generation models to capture and transfer visual style. In particular, we probe if large reconstruction models, commonly used in the context of 3D generation, has a similar capability. We discover that the certain attention blocks in these models capture the appearance specific features. By injecting features from a visual style image to such blocks, we develop a simple yet effective 3D appearance stylization method. Our method does not require training or test time optimization. Through both quantitative and qualitative evaluations, we demonstrate that our approach achieves superior results in terms of 3D appearance stylization, significantly improving efficiency while maintaining high-quality visual outcomes. Code and models are available via our project website: https://github.com/ipekoztas/3D-Stylization-LRM.	https://dl.acm.org/doi/abs/10.1145/3721238.3730636	Ipek Oztas, Duygu Ceylan, Aysegul Dundar
3D-Fixup: Advancing Photo Editing with 3D Priors	Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730695	Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liang-Yan Gui, Matheus Gadelha, Paul Guerrero, Nanxuan Zhao
3DGS2: Near Second-order Converging 3D Gaussian Splatting	3D Gaussian Splatting (3DGS) has emerged as a mainstream solution for novel view synthesis and 3D reconstruction. By explicitly encoding a 3D scene using a collection of Gaussian kernels, 3DGS achieves high-quality rendering with superior efficiency. As a learning-based approach, 3DGS training has been dealt with the standard stochastic gradient descent (SGD) method, which offers at most linear convergence. Consequently, training often requires tens of minutes, even with GPU acceleration. This paper introduces a (near) second-order convergent training algorithm for 3DGS, leveraging its unique properties. Our approach is inspired by two key observations. First, the attributes of a Gaussian kernel contribute independently to the image-space loss, which endorses isolated and local optimization algorithms. We exploit this by splitting the optimization at the level of individual kernel attributes, analytically constructing small-size Newton systems for each parameter group, and efficiently solving these systems on GPU threads. This achieves Newton-like convergence per training image without relying on the global Hessian. Second, kernels exhibit sparse and structured coupling across input images. This property allows us to effectively utilize spatial information to mitigate overshoot during stochastic training. Our method converges an order faster than standard GPU-based 3DGS training, requiring over 10 × fewer iterations while maintaining or surpassing the quality of the compared with the SGD-based 3DGS reconstructions.	https://dl.acm.org/doi/abs/10.1145/3721238.3730687	Lei Lan, Tianjia Shao, Zixuan Lu, Yu Zhang, Chenfanfu Jiang, Yin Yang
60,000nits Full-color Native RGB Single Junction 3,386PPI Micro-OLED	In this study, we independently developed a knowledge system named uNEEDXR™, successfully realizing a full-color micro-OLED device with a brightness of 60,000 nits on a silicon-based backplane. This knowledge system was developed over nine years and innovatively know-how and integrates expertise across design, processes, manufacturing, equipment, and material, overcoming the performance limitations of traditional micro-OLED architectures. The system enables high brightness, high pixel density, low power consumption, high contrast ratio, high color saturation, and tunable energy distribution (including viewing angle, wavelength, and bandwidth). Additionally, it meets customer requirements for reliability and lifespan. This technology provides a scalable production solution for near-eye display applications in augmented reality.	https://dl.acm.org/doi/abs/10.1145/3721250.3742970	Kuo-Cheng Hsu, Li-Min Huang, Yujia Qiu
A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling	Median filtering is a non-linear smoothing technique widely used in digital image processing to remove noise while retaining sharp edges. It is particularly well suited to removing outliers (impulse noise) or granular artifacts (speckle noise). However, the high computational cost of median filtering can be prohibitive. Sorting-based algorithms excel with small kernels but scale poorly with increasing kernel diameter, in contrast to constant-time methods characterized by higher constant factors but better scalability, such as histogram-based approaches or the 2D wavelet matrix. This paper introduces a novel algorithm, leveraging the separability of the sorting problem through hierarchical tiling to minimize redundant computations. We propose two variants: a data-oblivious selection network that can operate entirely within registers, and a data-aware version utilizing random-access memory. These achieve per-pixel complexities of ( log ( )) and ( ), respectively, for a × kernel — unprecedented for sorting-based methods. Our CUDA implementation is up to 5 times faster than the current state of the art on a modern GPU and is the fastest median filter in most cases for 8-, 16-, and 32-bit data types and kernels from 3 × 3 to 75 × 75.	https://dl.acm.org/doi/abs/10.1145/3721238.3730709	Louis Sugy
A Fluorescent Material Model for Non-Spectral Editing & Rendering	Fluorescent materials are characterized by a spectral reradiation toward longer wavelengths. Recent work [Fichet et al. ] has shown that the rendering of fluorescence in a non-spectral engine is possible through the use of appropriate reduced reradiation matrices. But the approach has limited expressivity, as it requires the storage of one reduced matrix per fluorescent material, and only works with measured fluorescent assets. In this work, we introduce an analytical approach to the editing and rendering of fluorescence in a non-spectral engine. It is based on a decomposition of the reduced reradiation matrix, and an analytically-integrable Gaussian-based model of the fluorescent component. The model reproduces the appearance of fluorescent materials accurately, especially with the addition of a UV basis. Most importantly, it grants variations of fluorescent material parameters in real-time, either for the editing of fluorescent materials, or for the dynamic spatial variation of fluorescence properties across object surfaces. A simplified one-Gaussian fluorescence model even allows for the artist-friendly creation of plausible fluorescent materials from scratch, requiring only a few reflectance colors as input.	https://dl.acm.org/doi/abs/10.1145/3721238.3730721	Laurent Belcour, Alban Fichet, Pascal Barla
A Large-Étendue Direct-View Holographic Display System	Direct-view 3D displays enable immersive experiences but often cause visual discomfort from relying on binocular disparity alone. Holographic displays offer an ultimate solution by reconstructing the full light wavefront, but face scalability and viewing freedom limitations due to the spatial-bandwidth product and high cost of fine-pitch phase-only SLMs. We present a system combining an amplitude-only display with ultra-high pixel count and dynamic optical steering for a fully 3D eye box. By axially translating a lens, we expand the eye box in depth beyond the capabilities of conventional pupil-steering methods. We further extend SGD-based hologram optimization to support dual light sources and an amplitude-only SLM, enabling stereoscopic delivery with suppressed crosstalk. Our prototype shows accurate depth cues, paving the way for scalable, high-quality holographic displays with expanded viewing freedom.	https://dl.acm.org/doi/abs/10.1145/3721250.3742975	Ryota Koiso, Suyeon Choi, Manu Gopakumar, Brian Chao, Jacqueline Yang, Gordon Wetzstein
A No-Code Introduction to Creating Geospatial Worlds with 3D Tiles and Cesium	Digital twins increasingly require integrating, viewing, and interacting with ever larger amounts of disparate data. 3D Tiles enables users to visualize geospatial data on a massive scale while balancing performance across devices. 3D Tiles likewise provides a method to aggregate diverse data, such as 3D models, photogrammetry, LiDAR, BIM/CAD models, and more, into an interoperable ecosystem based on open-source standards. Cesium is a leading platform for 3D geospatial data suitable for developers, technical artists, data visualizers, and non-technical stakeholders. Cesium provides users with a number of free tools to convert 3D models into 3D Tiles and to host 3D Tiles for streaming. In this hands-on lab, we'll leverage the Cesium platform to create global-scale digital twins by creating 3D Tiles, uploading them to a server, and interacting with the data in various runtimes such as the web and game engines.	https://dl.acm.org/doi/abs/10.1145/3721251.3742865	Cory Barr, Ben Jewett
A Novel Maxwellian Optics Combining Spherical Multi Pinholes and TMD for Enhanced Field of View	In this study, we propose a novel Maxwellian optics that achieves a wide field of view and evaluate its effectiveness through 2D and 3D simulations. The fundamental principle of the proposed system is based on Maxwellian view, a form of retinal projection that can mitigate the vergence–accommodation conflict. Conventional Maxwellian optics typically employ a pinhole to focus light on a single point within the pupil, enabling the projection of sharp images onto the retina with a wide field of view, independent of the eye's accommodation. However, a major limitation of such systems is the disappearance of the image when the eye rotates and the convergence point shifts outside the pupil. To address this issue, we propose a novel Maxwellian optical system that combines a spherical multi-pinhole (SMP) with a transmissive mirror device (TMD).	https://dl.acm.org/doi/abs/10.1145/3721250.3743022	Shuri Futamura, Ryuichi Inui, Tomoki Matsumoto, Yasuhisa Nakano, Tatsuji Tokiwa
A Platform for Interactive AI Character Experiences	From movie characters to modern science fiction — bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce , which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.	https://dl.acm.org/doi/abs/10.1145/3721238.3730762	Rafael Wampfler, Chen Yang, Dillon Elste, Nikola Kovacevic, Philine Witzig, Markus Gross
A Polyhedral Construction of Empty Spheres in Discrete Distance Fields	Lie sphere geometry provides a unified representation of points, oriented spheres and hyperplanes in Euclidean -space as the subset of lines in that are contained in a certain quadric. The natural scalar product in this construction is zero if two elements are in oriented contact. We show how the sign of this product can be used to decide if spheres are disjoint. This allows us to model the space of spheres that are not intersecting a given union of spheres as the intersection of half-spaces (and the quadric). The maximal spheres are on the boundary of this set and can be computed by first constructing the intersection of half-spaces, which is a convex hull problem, and then intersecting edges of the hull against the quadric, which are the roots of a univariate quadratic. We demonstrate the method at the example of contouring a discrete signed distance field: every sample of the signed distance field represents an empty spheres and the zero-level contour has to be disjoint from the union of these spheres. Maximal spheres outside the empty spheres provide samples on the zero-level contour. The quality of this sample set is comparable to existing methods relying on optimization, while being deterministic and faster in practice.	https://dl.acm.org/doi/abs/10.1145/3721238.3730748	Maximilian Kohlbrenner, Marc Alexa
A Robust and Generalized Gauss-Seidel Solver for Physically-Correct Simultaneous Collisions	Simulating multi-object collisions in real-time environments remains a significant challenge, particularly when handling simultaneous collisions in a physically accurate manner. Traditional Gauss-Seidel solvers, widely employed in physics engines, often fail to preserve the symmetry and consistency of multi-object interactions that are often observed in physics. In this paper, we present a generalized and robust Gauss-Seidel solver to overcome the difficulties in simultaneous collisions. Using spatial and temporal collision states to classify and resolve constraints, our algorithm ensures correct collision propagation and symmetry, addressing the limitations commonly found in existing solvers. Moreover, our algorithm can mitigate jitters caused by floating-point errors, ensuring smooth and stable collision responses. Our approach demonstrates fast convergence and improved accuracy in scenarios involving complex multi-object collisions.	https://dl.acm.org/doi/abs/10.1145/3728291	Huanbo Zhou, Zhenyu Xu, Xijun Liu, Xinyu Zhang
A Sparrow's Song	Inspired by a true story, an elderly widow in the midst of World War II struggles to overcome grief and rediscover joy in her life. Day by day, she serves as an air raid warden in the crowded shelters, witnessing the suffering of children and others. One morning, she finds a dying sparrow and hopes to save its fragile life. As the sparrow gradually heals, a bond grows between them, and the bird begins to respond to her piano playing—a shared language that builds a bridge. During bombing raids, she carries the sparrow to the shelters, where she plays the piano, and the sparrow sings its song to comfort the children and offer hope to those around her. Through this newfound purpose and unexpected alliance, her life begins to change.	https://dl.acm.org/doi/abs/10.1145/3698896.3721357	Tobias Eckerlin
A Texture Streaming Pipeline for Real-Time GPU Ray Tracing	Disney Animation makes heavy use of Ptex [Burley and Lacewell ] across our assets [Burley et al. ], which required a new texture streaming pipeline for our new real-time ray tracer. Our goal was to create a scalable system which could provide a real-time, zero-stall experience to users at all times, even as the number of Ptex files expands into the tens of thousands. We cap the maximum size of the GPU cache to a relatively small footprint, and employ a fast LRU eviction scheme when we hit the limit.	https://dl.acm.org/doi/abs/10.1145/3721239.3734098	Mark Lee, Nathan Zeichner, Yining Karl Li
A Whirlwind Introduction to Computer Graphics	"For a beginner, walking into a SIGGRAPH conference is a very intimidating experience. There is so much to see and so much to do, and everyone seems to be speaking an unfamiliar language, and they seem to be ooh'ing and ahh'ing over things that they appreciate but you don't know how to. This course is for you! Rather than kicking new attendees off the dock and expecting them to swim at SIGGRAPH, the Whirlwind Introduction course's purpose is to give anyone who wants it a basic background in the concepts and terminology that they need to get more from the different venues in the conference. It is like a ""pre-course"" in that it is more fundamental than any other introductory activities and should be attended before anything else in the conference program."	https://dl.acm.org/doi/abs/10.1145/3721241.3733985	Mike Bailey
AI-Powered Real-Time VFX for Mobile	Cinematic visual effects have long been the domain of high-end hardware, reserved for movie studios and powerful gaming rigs. But what if we could bring that same level of quality to mobile devices—without sacrificing performance? AI VFX was born out of this challenge: to push the boundaries of real-time visual effects on mobile by merging Generative Adversarial Networks (GANs)[Goodfellow et al. ] with the VFX Graph. In this talk, we'll take you behind the scenes of AI VFX, a technology that enables stunning, real-time fire and water effects that run seamlessly across the full spectrum of mobile devices—from six-year-old Android phones to the latest iPhones. By integrating AI-powered enhancements into the VFX Graph (GPU particle system), we've developed an approach that balances visual fidelity with efficiency, ensuring cinematic-quality effects can exist in mobile gaming, social media, and beyond. We'll dive into the technical breakthroughs that made this possible, including synthetic data generation, real-time inference optimization, and workflows for combining GAN[Goodfellow et al. ] and VFX Graph—all while overcoming the hardware constraints of mobile platforms. Through recorded demos and real-world examples from TikTok, you'll see firsthand how AI is revolutionizing mobile VFX, unlocking new creative possibilities for developers, artists, and storytellers.	https://dl.acm.org/doi/abs/10.1145/3721239.3734099	Domin Lee, Anda Li, Kexin Lin
AI-Powered Visual Generation for Video Mapping	This hands-on session introduces a practical and accessible method for AI-powered visual generation in projection mapping. Participants will explore how artificial intelligence, real-time computer graphics, and interactive mapping can converge to empower creators—regardless of coding or technical background—to produce immersive visuals. The class focuses on generating dynamic visual content using AI models trained on diverse artistic styles, and adapting it in real time to various 2D and 3D projection surfaces. Attendees will learn how to use intuitive interfaces to customize visual outputs and control shader-based rendering parameters for optimized performance and fluid interaction. Technical insights will be shared on how real-time shader optimizations reduce computational load without compromising visual quality. Beyond technical execution, the session will include an open discussion on ethical AI use, transparency in dataset sourcing, and energy-conscious design practices. Over the course of 90 minutes, participants will: This class transforms a complex creative pipeline into an accessible learning experience, offering concrete skills and critical insights into the future of AI-assisted digital art.	https://dl.acm.org/doi/abs/10.1145/3721251.3737367	Etienne Mathé
AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning	Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. However, methods typically rely on a weighted sum of conflicting reward functions, requiring extensive tuning to achieve a desired behavior. Due to the computational cost of RL, this iterative process is a tedious, time-intensive task. Furthermore, for robotics applications, the weights need to be chosen such that the policy performs well in the real world, despite inevitable sim-to-real gaps. To address these challenges, we propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character. Moreover, we explore how weight-conditioned policies can be leveraged in hierarchical settings, using a high-level policy to dynamically select weights according to the current task. We show that the multi-objective policy encodes a diverse spectrum of behaviors, facilitating efficient adaptation to novel tasks.	https://dl.acm.org/doi/abs/10.1145/3721238.3730656	Lucas N. Alegre, Agon Serifi, Ruben Grandia, David Müller, Espen Knoop, Moritz Bächer
Accelerated Gamut Discovery via Massive Parallelization	This paper presents a scalable framework for efficiently discovering the performance gamut of different processes. Gamut boundaries comprise the set of highest-performing solutions within a design space. While sampling methods are often inefficient or prone to premature convergence, Bayesian optimization struggles with taking advantage of existing large-scale parallel computation or experimentation. To address these challenges, we utilize Bayesian neural networks as scalable surrogates for performance prediction and uncertainty estimation. We further introduce a novel acquisition function that combines the diversity-driven exploration of stochastic optimization with the information-efficient exploitation of Bayesian optimization. This enables generating large, high-quality batches of samples. Our approach leverages large batch sizes to reduce the number of iterations needed for optimization. We demonstrate its effectiveness on real-world engineering and robotic problems, achieving faster and more extensive discovery of the performance gamut. Code and data are available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730712	Navid Ansari, Hans-Peter Seidel, Vahid Babaei
Accelerating AEC Workflows with 3D Geospatial Context in Runtime Engines	HNTB, a global architectural and infrastructure engineering firm, has partnered with Cesium, a leader in open and interoperable 3D geospatial platforms, to enhance the design, visualization, and contextual analysis of large-scale infrastructure projects. By adopting Cesium's technology, HNTB has significantly reduced the manual effort required to model complex projects—from small toll plazas to extensive 26-mile roadway alignments—by leveraging Cesium's 3D Tiles and geospatial streaming capabilities. To further streamline workflows, HNTB developed custom tools within Unity and Unreal Engine for editing Cesium 3D Tiles, including volumetric masking, edge blending, curvature adjustments, and the addition of natural elements like trees and foliage. Cesium supports AEC (Architecture, Engineering, and Construction) workflows by providing out-of-the-box geospatial context, such as Google Photorealistic 3D Tiles, and by optimizing and hosting user data for performant runtime streaming. Enhancing this ecosystem, Cesium continues to innovate with tools like the Cesium Design Tiler for efficient BIM model streaming and Cesium ion for Autodesk Revit, enabling seamless export and publication of Revit designs as 3D Tiles.	https://dl.acm.org/doi/abs/10.1145/3721239.3736526	Philip Luhn, Alex Paulson, Adam Liss, David Willard
Adaptive Tetrahedral Grids for Volumetric Path-Tracing	We advertise the use of tetrahedral grids constructed via the longest edge bisection algorithm for rendering volumetric data with path tracing. The key benefits of such grids is two-fold. First, they provide a highly adaptive space-partitioning representation that limits the memory footprint of volumetric assets. Second, each (tetrahedral) cell has exactly 4 neighbors within the volume (one per face of each tetrahedron) or less at boundaries. We leverage these properties to devise optimized algorithms and data-structures to compute and path-trace adaptive tetrahedral grids on the GPU. In practice, our GPU implementation outperforms regular grids by up to × 30 and renders production assets in realtime at 32 samples per pixel.	https://dl.acm.org/doi/abs/10.1145/3721239.3734093	Anis Benyoub, Jonathan Dupuy
Advancements in Non-Procedural Groom Workflows for Mufasa: The Lion King	At MPC, we have recently transitioned from a proprietary, procedural solution to a new, non-procedural technology called . was one of the first projects chosen to use in a production setting, primarily with the goal of creating higher fidelity grooms needed by the 20+ hero characters on the project without the performance limitations of a procedural system. However, non-procedural workflows come with their own limitations so, in this paper, we will discuss some of the problems we faced during production and the solutions we developed to navigate them.	https://dl.acm.org/doi/abs/10.1145/3721239.3734072	Matthew Traynar, Marco Romeo, Naveena Baleguli, Solene Morvan, Alex Richardson, Murali Chandran
Advancing VFX Workflows with Houdini Solaris in House of the Dragon Season 2	This work presents a successful real-world transition of a major VFX studio, Rodeo FX, from a multi-software pipeline to a unif ied Solaris and USD-based workflow. Motivated by the growing complexity of episodic and feature film productions, such as Red One and House of the Dragon Season 2, the shift aimed to consolidate disparate departments and eliminate redundant tasks by using Houdini Solaris as the central platform for asset development, Crowd, CFX, FX, lighting, and rendering. The core of this pipeline transformation involved standardizing USD layer stack composition, introducing reusable HDAs and Rodeo Chunks, and automating per-shot workflows using Shotgun event triggers and dispatch graphs. We detail implementation specifics such as the USD Payload Package concept, layer stack auto-generation, and robust automation systems that allowed for efficient iteration, shot synchronization, and cross-departmental collaboration. Finally, we demonstrate production-level results using case studies from House of the Dragon Season 2, including environment builds of King's Landing, Silverwing dragon shading, FX instancing workflows, and automated shot renders. This abstract outlines practical strategies for large studios aiming to migrate to a scalable, artist-friendly, and automation-driven USD pipeline.	https://dl.acm.org/doi/abs/10.1145/3721239.3734132	Peter Dominik
Aerial 3D Display with Ultra-Wide Viewing Zone Using Polarization Characteristics of LCDs	We propose a naked-eye stereoscopic display with an ultra-wide viewing zone by applying the display principle of general LCDs. By replacing the polarizer of an LCD with a reflective polarizer and arranging them three-dimensionally, this technology refracts light rays freely and enables an expansion of the viewing zone. In this study, we created a prototype and confirmed that the viewing zone expanded.	https://dl.acm.org/doi/abs/10.1145/3721250.3742990	Haruki Kato, Naoki Hashimoto
Aerial Path Online Planning for Urban Scene Updation	We present the first aerial path planning algorithm specifically designed for detecting and updating change areas in urban environments. While existing methods for large-scale 3D urban scene reconstruction focus on achieving high accuracy and completeness, they are inefficient for scenarios requiring periodic updates, as they often re-explore and reconstruct entire scenes, wasting significant time and resources on unchanged areas. To address this limitation, our method leverages prior reconstructions and change probability statistics to guide UAVs in detecting and focusing on areas likely to have changed. Our approach introduces a novel to evaluate the likelihood of scene changes, driving the planning of two flight paths: a informed by static priors and a dynamic that adapts to newly detected changes. Extensive experiments on real-world urban datasets demonstrate that our method significantly reduces flight time and computational overhead while maintaining high-quality updates comparable to full-scene re-exploration and reconstruction. These contributions pave the way for efficient, scalable, and adaptive UAV-based scene updates in complex urban environments.	https://dl.acm.org/doi/abs/10.1145/3721238.3730639	Mingfeng Tang, Ningna Wang, Ziyuan Xie, Jianwei Hu, Ke Xie, Xiaohu Guo, Hui Huang
Amen	A group of pigs are raised peacefully in a monastery when, one day, one of them finds out the truth behind their existence. Thus he decides to free his friends.	https://dl.acm.org/doi/abs/10.1145/3698896.3719323	Orphée Coutier, Bettina Demarty, Kimie Maingonnat, Laurène Perego, Louise Poulain, Avril Zundel
An Incompressible Crack Model for Volume Preserving MPM Fracture	This paper proposes a novel method to simulate the dynamic fracture effect of elastoplastic objects. Our method is based on the continuum damage mechanics (CDM) theory and uses the material point method (MPM) to discretize the governing equations. Our proposed approach distinguishes itself from previous works by incorporating a novel method for modeling decohesion, which preserves the incompressibility of the cracks. In contrast to existing methods that rely on material stiffness degradation, our approach leverages carefully crafted constitutive models for both fully and partially damaged materials. We further introduce a novel granular material model that effectively captures the physical behavior of fully damaged debris. This is augmented by a volume-aware deformation gradient tensor designed to evaluate and stabilize material expansion. We conduct a comprehensive evaluation of our proposed method on multiple dynamic fracturing scenarios and demonstrate its effectiveness in producing visually richer and more realistic behaviors compared to previous state-of-the-art MPM approaches.	https://dl.acm.org/doi/abs/10.1145/3728298	Shiguang Liu, Maolin Wu, Chenfanfu Jiang, Yisheng Zhang
An Infinity Mirror Without Apparent Mirroring	An is an optical novelty that uses facing mirrors – at least one of which is partially transparent to allow viewing – to present the appearance of an infinite tunnel of copies of a scene. One limitation of infinity mirrors is that alternate reflections of the scene are – by necessity – reflected, which means one cannot create, e.g., effects where lights chase into or out of the apparent tunnel. I present a prototype infinity mirror that uses light cells to overcome this limitation. These cells have a different appearance when viewed from the front and back, apparently breaking the symmetry between the primary and reflected versions of the scene. The cells are made from a 3D-printed baffle and diffuser and lit with off-the-shelf programmable LED strips, resulting in an overall inexpensive-to-produce design. In this poster I discuss the construction of my prototype infinity mirror, demonstrate some simple speed tunnel effects, and discuss the design trade-offs in my simple light-cell design.	https://dl.acm.org/doi/abs/10.1145/3721250.3743015	James McCann
An Introduction to Neural Shading	"The past five years has seen rapid growth in techniques that replace physically based shading algorithms with learned neural approximations. These techniques compress data and code to provide compact, high-quality approximations to complex algorithms. As of recently, these algorithms can now be hardware accelerated using cross-platform APIs in Vulkan and Direct3D to access AI acceleration hardware. Combined with modern differentiable shading languages, there is now a complete development toolchain for building and deploying neural shaders. The course will take attendees on a theory-to-practice journey through the neural shading space, starting with a survey of the types of techniques and their applications. We will then teach the math behind neural shading, starting small and adding detail from there. After that, the course will cover hardware acceleration concepts and share tips on bringing models from training into a production C++ environment. We close by reviewing a few ""full"" neural graphics models and discuss how they advance the state-of-the-art in real-time graphics. After completing this course, participants will understand neural shading fundamentals, and be able to build and deploy hardware-accelerated neural shading models in modern renderers."	https://dl.acm.org/doi/abs/10.1145/3721241.3733999	Nat Duca, Yong He, Benedikt Bitterli, Chris Cummings, Alexey Bekin, Alexey Panteleev, Aaron Lefohn
An Introduction to Quantum Computing	Quantum computing is a radically new and exciting approach to programming. By exploiting the unusual behavior of quantum objects, this new technology invites us to re-imagine the computer graphics methods we know and love in revolutionary new ways. This course is math-free and requires no technical background.	https://dl.acm.org/doi/abs/10.1145/3721241.3733983	Andrew Glassner
AniDepth : Anime In-between Diffusion using Depth-guided Warped Line-art	We propose AniDepth, a novel anime in-betweening method using a video diffusion model. Even if the model is fine-tuned on an anime dataset, it still suffers from the domain gap between anime and the natural image domain, due to the strong priors of the base model. Therefore, we convert anime illustration into depth map, which is a modality filling the gap between anime and realistic domains to fully exploit the model's prior knowledge. In addition, by using line-arts as guide during the in-betweening, we enhance the fidelity of the generated line-arts details. Our approach first interpolates converted depth maps, warps line-arts based on the depth maps, then interpolates colored images with using the line-arts as the conditions, preserving complex line-art and flat coloring even under large movements. Experiments show that our method outperforms the existing diffusion method in quantitative evaluation, improves temporal smoothness, and reduces line-art distortion. Moreover, because it requires no extra training, it can be easily integrated into current production pipelines at low operational cost.	https://dl.acm.org/doi/abs/10.1145/3721239.3734091	Sosui Koga, Hiroyuki Kubo, Seitaro Shinagawa, Yuki Fujimura, Kazuya Kitano, Akinobu Maejima, Takuya Funatomi, Yasuhiro Mukaigawa
Animation Theater In Conversation: Inkwo for When the Starving Return	Two lifetimes from now the world hangs in the balance. Dove, a young, enigmatic, gender-shifting warrior, discovers the gifts and burdens of their Inkwo (medicine) to defend against an army of hungry, ferocious monsters. Dove's courage, resilience and alliance with the Earth culminates in a battle against these flesh-consuming creatures, who become stronger with each body and soul they devour. Inkwo for When the Starving Return is a call to action to fight and protect against the forces of greed around us.	https://dl.acm.org/doi/abs/10.1145/3698896.3722209	Amanda Strong, Eloi Champagne, Maya McKibbin
Anime Colorization Using Segment Matching with Candidate Colors	We propose a new automatic colorization method for anime line drawings using segment matching with a few reference images. To address the limitations of existing segment matching methods in handling large motion gaps or small regions, the authors introduce patch-based few-shot colorization and a color shuffling process to estimate candidate colors for subsequent segment matching. This addresses the nonlinear movements that is unique to anime, and optical flow estimation struggles with. The paper demonstrates that the proposed method improves accuracy compared to the state-of-the-art segment matching method.	https://dl.acm.org/doi/abs/10.1145/3721250.3743016	Yu Takano, Akinobu Maejima, Shugo Yamaguchi, Shigeo Morishima
Any Character, Any Movement	presents a real-time character animation system powered by generative AI. Our pipeline enables fully automated motion synthesis across arbitrary 3D characters using four novel components: one-click auto-retargeting, natural language-to-motion generation, interactive diffusion-based motion control, and AI-powered motion stitching. Retargeting is achieved through a learned joint mapping algorithm that adapts source skeletons to target rigs in sub-second time. Motion generation is guided by plain English prompts, producing expressive full-body animations in 6 seconds. Real-time motion control is enabled via a diffusion model optimized for 30 ms responsiveness in the browser using WebGPU. Finally, motion stitching module computes transitions between clips using latent space interpolation. These tools operate with no manual retargeting or keyframing, allowing users an accessible approach to animating characters.	https://dl.acm.org/doi/abs/10.1145/3721243.3735985	Dave Savostyanov, Tony Thibault, Ryan Woodard, Chen Goldberg, Viren Tellis
AnyTop: Character Animation Diffusion with Any Topology	Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data. In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input. Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism. Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons. Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well. Furthermore, our model's latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation, and motion editing. Our webpage, , includes links to videos and code.	https://dl.acm.org/doi/abs/10.1145/3721238.3730621	Inbar Gat, Sigal Raab, Guy Tevet, Yuval Reshef, Amit Haim Bermano, Daniel Cohen-Or
Anymate: A Dataset and Baselines for Learning 3D Object Rigging	Rigging and skinning are essential steps to create realistic 3D animations, often requiring significant expertise and manual effort. Traditional attempts at automating these processes rely heavily on geometric heuristics and often struggle with objects of complex geometry. Recent data-driven approaches show potential for better generality, but are often constrained by limited training data. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets paired with expert-crafted rigging and skinning information—70 times larger than existing datasets. Using this dataset, we propose a learning-based auto-rigging framework with three sequential modules for joint, connectivity, and skinning weight prediction. We systematically design and experiment with various architectures as baselines for each module and conduct comprehensive evaluations on our dataset to compare their performance. Our models significantly outperform existing methods, providing a foundation for comparing future methods in automated rigging and skinning. Code and dataset can be found at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730743	Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, Jiajun Wu
Anywhere Door Experience: Projection Mapping for Enhancing Entertainment and Immersion	In this study, we propose an experience inspired by the Anywhere Door concept, in which users transition between multiple life-sized projected virtual spaces by opening, closing, and passing through a physical door. We demonstrate that this approach not only enhances the entertainment value of the visual experience but also increases the sense of immersion in the destination virtual space.	https://dl.acm.org/doi/abs/10.1145/3721250.3742967	Naoki Hashimoto, Yuki Inada
Aokana: A GPU-Driven Voxel Rendering Framework for Open World Games	Voxels are among the most popular 3D geometric representations today. Due to their intuitiveness and ease-of-editing, voxels have been widely adopted in stylized games and low-cost independent games. However, the high storage cost of voxels, along with the significant time overhead associated with large-scale voxel rendering, limits the further development of open-world voxel games. In this paper, we introduce Aokana, a GPU-Driven Voxel Rendering Framework for Open World Games. Aokana is based on a Sparse Voxel Directed Acyclic Graph (SVDAG). It incorporates a Level-of-Details (LOD) mechanism and a streaming system, enabling seamless map loading as players traverse the open-world game environment. We also designed a corresponding high-performance GPU-driven voxel rendering pipeline to support real-time rendering of the voxel scenes that contain tens of billions of voxels. Aokana can be directly applied to existing game engines and easily integrated with mesh-based rendering methods, demonstrating its practical applicability in game development. Experimental evaluations show that, with increasing voxel scene resolution, Aokana can reduce memory usage by up to ninefold and achieves rendering speeds up to 4.8 times faster than those of previous state-of-the-art approaches.	https://dl.acm.org/doi/abs/10.1145/3728299	Yingrong Fang, Qitong Wang, Wei Wang
Appearance-aware Multi-view SVBRDF Reconstruction via Deep Reinforcement Learning	Recent advancements in deep learning have revolutionized the reconstruction of spatially-varying surface reflectance of real-world objects. Many existing methods have successfully recovered high-quality reflectance maps using a remarkably limited number of images captured by a lightweight handheld camera and a flash-like light source. As the samples become sparse, the choice of the sampling set has a significant impact on the results. To determine the best sampling set for each material while ensuring minimal capture costs, we introduce an appearance-aware adaptive sampling method in this paper. We model the sampling process as a sequential decision-making problem, and employ a deep reinforcement learning (DRL) framework to solve it. At each step, an agent (NBVL Planner), after trained on a specially designed dataset, plans the next best view-lighting (NBVL) pair based on the appearance of the material recognized so far. Once stopped, the sequence of the NBVLs constitutes the best sampling set for the material. We show, through extensive experiments on both synthetic materials and real-world cases, that the best sampling set extracted by our method outperforms other sampling sets, especially for challenging materials featuring globally-varying specular reflectance.	https://dl.acm.org/doi/abs/10.1145/3721238.3730718	Pengfei Zhu, Jie Guo, Yifan Liu, Qi Sun, Yanxiang Wang, Keheng Xu, Ligang Liu, Yanwen Guo
Applying AI Weather Models with NVIDIA Earth-2	As deep learning has matured into a driver of unprecedented insight, its application has expanded across a wide range of industries and scientific domains. One area of growing interest is the use of data-driven models in digital twin systems, with the Earth emerging as a natural and consequential subject. Earth system modeling is more than a scientific curiosity — it plays a critical role in addressing global safety, resilience, and environmental planning — and NVIDIA's Earth-2 platform represents one such effort. While the Earth-2 initiative focuses on Earth's macro-scale physical processes, it draws on techniques that are broadly applicable across domains and provide a strong foundation for those interested in physical system modeling and machine learning in the natural sciences. This workshop offers a hands-on exploration of the Earth-2 ecosystem, centered on the Earth2Studio Python library for AI-driven global weather forecasting. Participants will progress through three interactive exercises that take them directly into real-world forecasting workflows. Through the workshop, attendees will learn to generate ensemble forecasts, evaluate them with established metrics, and produce detailed, high-resolution predictions tailored to specific regional needs. The course emphasizes the use of deep learning models and showcases their advantages over traditional Numerical Weather Prediction (NWP) methods. By working through examples like hurricane tracking and heatwave analysis, participants will build foundational knowledge around scalable, accurate, and cost-effective inference solutions. By the end of this workshop, participants will have the necessary tools and skills to apply cutting-edge weather modeling techniques and understand how to structure and validate applications surrounding them.	https://dl.acm.org/doi/abs/10.1145/3721251.3734068	Vadim Kudlay, Kevin Lee
Areia	Venturing through a distant desert, we wander the terrain of our intimate desires and fears, seeking answers. Yet, life never fails to deliver a surprise. When the opportunity arises, do you free yourself?	https://dl.acm.org/doi/abs/10.1145/3698896.3725521	Ive Machado, Gustavo Ribeiro
Armored Core: Asset Management	In a frostbitten frontier world, a legendary mech pilot learns his latest mission might hold the key to the demons that have haunted him for decades.	https://dl.acm.org/doi/abs/10.1145/3698896.3725442	Alex Rabb
Art-directing Realistic Looking Procedural Wind on Curves	Curves are used to represent assets such as fur, hair, and grass in CG and can be used for rigs that drive vegetation. Applying just noise fields to create procedural wind on curves has been an attractive method. However, this makes the wind appear artificial, lacking in both its dynamic nature that can be art-directed and its realistic interaction with objects. In this talk, techniques are presented to art-direct as well as enhance the realism of procedural curve wind in shots with the addition of collisions, shielding, gusts, and recovery. These provide the artists with significant control over the wind.	https://dl.acm.org/doi/abs/10.1145/3721239.3734087	Arunachalam Somasundaram
Assessing Learned Models for Phase-only Hologram Compression	We evaluate the performance of four common learned models utilizing INR and VAE structures for compressing phase-only holograms in holographic displays. The evaluated models include a , [Sitzmann et al. ], and [Chan et al. ], with [Bohan ] as the representative VAE model. Our experiments reveal that a pretrained image VAE, , with 2.2 parameters struggles with phase-only hologram compression, revealing the need for task-specific adaptations. Among the INR s, with 4.9 parameters achieves compression with high quality in the reconstructed 3D images (PSNR = 34.54 dB). These results emphasize the effectiveness of INR s and identify the limitations of pretrained image compression VAE s for hologram compression task.	https://dl.acm.org/doi/abs/10.1145/3721250.3742993	Zicong Peng, Yicheng Zhan, Josef Spjut, Kaan Akşit
AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization	Recent research on generative models has primarily focused on creating product-ready visual outputs; however, designers often favor access to standardized asset libraries, a domain that has yet to be significantly enhanced by generative capabilities. Although open-world scenes provide ample raw materials for designers, efficiently extracting high-quality, standardized assets remains a challenge. To address this, we introduce AssetDropper, the first generative framework designed to extract any asset from reference images, providing artists with an open-world asset palette. Our model adeptly extracts a front view of selected subjects from input images, effectively handling complex scenarios such as perspective distortion and subject occlusion. We establish a synthetic dataset of more than 200,000 image-subject pairs and a real-world benchmark with thousands more for evaluation, facilitating the exploration of future research in downstream tasks. Furthermore, to ensure precise asset extraction that aligns well with the image prompts, we employ a pre-trained reward model to achieve a closed loop with feedback. We design the reward model to perform an inverse task that pastes the extracted assets back into the reference sources, which assists training with additional consistency and mitigates hallucination. Extensive experiments show that, with the aid of reward-driven optimization, AssetDropper achieves the state-of-the-art results in asset extraction. Our code and dataset are available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730731	Lanjiong Li, Guanhua Zhao, Lingting Zhu, Zeyu Cai, Lequan Yu, Jian Zhang, Zeyu Wang
AutoKeyframe: Autoregressive Keyframe Generation for Human Motion Synthesis and Editing	Keyframing has long been the cornerstone of standard character animation pipelines, offering precise control over detailed postures and dynamics. However, this approach is labor-intensive, necessitating significant manual effort. Automating this process while balancing the trade-off between minimizing manual input and maintaining full motion control has therefore been a central research challenge. In this work, we introduce AutoKeyframe, a novel framework that simultaneously accepts dense and sparse control signals for motion generation by generating keyframes directly. Dense signals govern the overall motion trajectory, while sparse signals define critical key postures at specific timings. This approach substantially reduces manual input requirements while preserving precise control over motion. The generated keyframes can be easily edited to serve as detailed control signals. AutoKeyframe operates by automatically generating keyframes from dense root positions, which can be determined through arc-length parameterization of the trajectory curve. This process is powered by an autoregressive diffusion model, which facilitates keyframe generation and incorporates a skeleton-based gradient guidance technique for sparse spatial constraints and frame editing. Extensive experiments demonstrate the efficacy of AutoKeyframe, achieving high-quality motion synthesis with precise and intuitive control.	https://dl.acm.org/doi/abs/10.1145/3721238.3730664	Bowen Zheng, Ke Chen, Yuxin Yao, Zijiao Zeng, Xinwei Jiang, He Wang, Joan Lasenby, Xiaogang Jin
Automated Task Scheduling for Cloth and Deformable Body Simulations in Heterogeneous Computing Environments	The concept of the Internet of Things (IoT) has driven the development of system-on-a-chip (SoC) technology for embedded and mobile systems, which may define the future of next-generation computation. In SoC devices, efficient cloth and deformable body simulations require parallelized, heterogeneous computation across multiple processing units. The key challenge in heterogeneous computation lies in task distribution, which must account for varying inter-task dependencies and communication costs. This paper proposes a novel framework for automated task scheduling to optimize simulation performance by minimizing communication overhead and aligning tasks with the specific strengths of each device. To achieve this, we introduce an efficient scheduling method based on the Heterogeneous Earliest Finish Time (HEFT) algorithm, adapted for hybrid systems. We model simulation tasks—such as those in iterative methods like Jacobi and Gauss-Seidel—as a Directed Acyclic Graph (DAG). To maximize the parallelism of nonlinear Gauss-Seidel simulation tasks, we present an innovative asynchronous Gauss-Seidel method with specialized data synchronization across units. Additionally, we employ task merging and tailored task-sorting strategies for Gauss-Seidel tasks to achieve an optimal balance between convergence and efficiency. We validate the effectiveness of our framework across various simulations, including XPBD, vertex block descent, and second-order stencil descent, using Apple M-series processors with both CPU and GPU cores. By maximizing computational efficiency and reducing processing times, our method achieves superior simulation frame rates compared to approaches that rely on individual devices in isolation. The source code with hybrid Metal/C++ implementation is available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730625	Chengzhu He, Zhendong Wang, Zhaorui Meng, Junfeng Yao, Shihui Guo, Huamin Wang
Automatic Interpretation of Ancient Egyptian Texts for Education and Research	We introduce a pipeline for interpreting Ancient Egyptian hieroglyphic texts combining OCR, transliteration, and translation. Designed for the low-resource data, our system improves accessibility for learners and efficiency for researchers. We evaluate its performance on a new diverse dataset reflective of real-world conditions.	https://dl.acm.org/doi/abs/10.1145/3721250.3743025	Maksim Golyadkin, Innokentiy Humonen, Yanis Plevokas, Ekaterina Bureeva, Ekaterina Alexandrova, Ilya Makarov
Backseat Gaming Permitted: Meet the AI Co-Streamer from Streamlabs	Interfacing with artificial intelligence has evolved from the domain of computer science into an everyday consumer activity. From the advent of virtual assistants to the mass adoption of LLMs for both professional and recreational use, we have welcomed a new population of automated, synthetic personalities into society that is actively transforming how we live. Advancements in artificial intelligence personalities are of particular utility to the digital creator class, whose livelihood depends largely on online interactions. Creators' audiences have already crossed the threshold of virtual engagement; these users possess a level of comfort with digital personas that lays the groundwork for the effective AI augmentation of audience interactivity. Beyond consumers' growing familiarity with digital personalities, there is also a perceived need for additional automation of streaming content production. The creator economy is becoming increasingly lucrative and competitive, attracting swathes of new talent that may not have the production resources or faculties to create content on par with audiences' expectations. Simultaneously, more experienced creators are seeking more efficient production practices. Here, we see an opportunity for artificial intelligence to serve a technical utility with its social and entertainment uses, acting as a virtual crewmember for streamers. Streamlabs has seen the following as the trajectory of future digital creation: a world where AI automation greases the wheels of creativity, assisting creators behind the scenes while also becoming an integral part of the show. This vision informs our work with NVIDIA and Inworld AI to create the Streamlabs Intelligent Streaming Agent: an AI-powered co-host, producer, and technical assistant for digital creators. After debuting the Agent at CES 2025, we have now prepared an updated technical demonstration to showcase the Agent's depth of functionality. In this demonstration, we begin by customizing the Agent's avatar, showcasing the process of selecting its appearance, style, and personality traits. We examine the relationship between the user interface (UI) design and fine-tuning the Agent's personality. We then configure automations through the Streamlabs app, programming the following game-reactive and production assistant functions: comment reactions to the streamer getting 'eliminated' in a game; showing an image; unhiding a source; and switching scenes. Throughout, we emphasize flexibility and ease of use, given our focus on tool-building for streamers of all experience levels.	https://dl.acm.org/doi/abs/10.1145/3721243.3735992	Ashray Urs, Sean Kaiser
Bacterial Molecular Landscape: Visualizing Earth's Simplest Life Form	This scientific visualization depicts a bacterial molecular landscape and explores the speed of diffusion and biochemical reactions that power life at the molecular scale.	https://dl.acm.org/doi/abs/10.1145/3698896.3725524	Gael McGill, Jonathan Khao, Hans Busstra
Balloonerism	A mysterious journey illustrates Mac Miller's previously unreleased album.	https://dl.acm.org/doi/abs/10.1145/3698896.3724390	Samuel Mason
Be Decisive: Noise-Induced Layouts for Multi-Subject Generation	Generating multiple distinct subjects remains a challenge for existing text-to-image diffusion models. Complex prompts often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. Preventing leakage among subjects necessitates knowledge of each subject's spatial location. Recent methods provide these spatial locations via an external layout control. However, enforcing such a prescribed layout often conflicts with the innate layout dictated by the sampled initial noise, leading to misalignment with the model's prior. In this work, we introduce a new approach that predicts a spatial layout aligned with the prompt, derived from the initial noise, and refines it throughout the denoising process. By relying on this noise-induced layout, we avoid conflicts with externally imposed layouts and better preserve the model's prior. Our method employs a small neural network to predict and refine the evolving noise-induced layout at each denoising step, ensuring clear boundaries between subjects while maintaining consistency. Experimental results show that this noise-aligned strategy achieves improved text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the rich diversity of the model's original distribution.	https://dl.acm.org/doi/abs/10.1145/3721238.3730631	Omer Dahary, Yehonathan Cohen, Or Patashnik, Kfir Aberman, Daniel Cohen-Or
Beyond Automation: Fostering Agency Between Humans, Algorithms, and Machines in Computational Design for Digital Fabrication	This project explores the role of the designer in digital fabrication workflows as digitization leads to higher levels of design automation. As digital technologies are adopted to streamline design to manufacturing workflows, elements of the creative process can become standardized to improve production efficiency at the cost of designer autonomy and product customization. In order to ensure designers' agency and increase product variation, the Carrara project presents a collaborative tool utilizing agent-based modeling (ABM) to represent designers, fabrication machines, and algorithms as active co-participants in the design process. This co-participatory workflow enables a generative, scalable product line that takes advantage of digital efficiencies while providing the designer with autonomy and control in the creative process.	https://dl.acm.org/doi/abs/10.1145/3721250.3742992	Cody Tucker
Beyond Digital Twins: 3D Gaussian Splatting, Game Engines and Crossmedia Cultural Heritage Representations	This paper explores the use of 3D Gaussian Splatting (3DGS) for cultural heritage representation, integrating real-time game engines to create immersive cross-media experiences. Using a historic Hakka mansion in Hong Kong as a case study, the research examines the technical and aesthetic challenges of 3DGS, particularly its limitations in lighting, geometry accuracy, and artifact generation. By embracing these constraints, the project investigates new modes of representing historical spaces beyond traditional digital twins. The study highlights how 3DGS, combined with SLAM-based scanning and game engine workflows, can offer novel approaches to interactive storytelling, site documentation, and artistic reinterpretation in digital heritage preservation and other use cases.	https://dl.acm.org/doi/abs/10.1145/3721239.3734094	Lukasz Mirocha
Beyond the Lens: Tools Fueling VFX Creativity on 'The Sandman'	We present a different approach in modelizing lens characteristics such as vignetting, defocus and chromatic aberrations and how to integrate them in a visual effect pipeline with artistic constraints.	https://dl.acm.org/doi/abs/10.1145/3721239.3736527	John Montegut, Vincent Poitras, Cedric Tremblay, Valerie Clement
Blinded By The Light: A Case Study Of HDR Integration In Animation Production	This talk explores the challenges faced by Netflix Animation Studios in implementing HDR (High Dynamic Range) technology in artist workflows, through an internal animated short film project serving as a case study. We discuss overcoming software and hardware integration constraints, specifically the lack of native HDR support in Linux-based DCC applications when not using dedicated video output, as well as the limited availability of suitable display hardware. Our approach involved developing technical solutions, namely SDR simulation for UI elements and tailored ACES display transforms. These efforts supported creative workflows, enabling artists to work interactively with HDR imagery.	https://dl.acm.org/doi/abs/10.1145/3721239.3734090	Michael De Caria, Manuel Macha, Iain Northcott, Steve Agland
BodyOpt 2.0 Advancements in Character Deformations at WetaFX	Achieving lifelike character deformations in high-end visual effects is both an artistic and technical challenge. This paper presents BodyOpt [Sprenger et al. ] 2.0, WetaFX's advanced character deformation pipeline, focusing on generalization beyond bipeds, improved ease of use, and higher-quality reconstructions and dynamics. We highlight technical advancements, including a more efficient model transfer process, and discuss how BodyOpt 2.0 enables scalable production workflows across small and large projects.	https://dl.acm.org/doi/abs/10.1145/3721239.3734097	Tobias Mack, Niall Ryan, Christoph Sprenger
BrepDiff: Single-Stage B-rep Diffusion Model	The Boundary Representation (B-rep) is a widely used 3D model representation of most consumer products designed with CAD software. However, its highly irregular and sparse set of relationships poses significant challenges for designing a generative model tailored to B-reps. Existing approaches use multi-stage approaches to satisfy the complex constraints sequentially. As a result, the final geometry cannot incorporate user edits due to the non-deterministic dependencies between cascaded stages. In contrast, we propose , a single-stage diffusion model for B-rep generation. We present a masked UV grid representation consisting of structured point samples from faces, serving as input for a diffusion transformer. By introducing an asynchronous and shifted noise schedule, we improve the training signal, enabling the diffusion model to better capture the distribution of UV grids. The explicitness of our masked UV grid representation enables users to intuitively understand and freely design surface geometry without being constrained by topological validity. The interconnectivity can be derived from the face layout, which is later processed into a valid solid volume during post-processing. Our approach achieves performance on par with state-of-the-art cascaded models while offering complex and diverse manipulations of geometry and topology, such as shape completion, merging, and interpolation.	https://dl.acm.org/doi/abs/10.1145/3721238.3730698	Mingi Lee, Dongsu Zhang, Clément Jambon, Young Min Kim
Bringing Life to the Communiverse: Procedural Look Development in Pixar's Elio	In Pixar's (2025), Elio visits the Communiverse, a protopian space station populated by various species from throughout the universe. To convey a vivacious city inhabited by a diverse population, a small team of environment artists amplified the traditional pipeline with procedural techniques used in unique ways to develop a vast quantity of various biomes of alien terrains and architectures that pulsed with their own internal energy.	https://dl.acm.org/doi/abs/10.1145/3721239.3734120	Chris Burrows, Mallory Kohut, Laura Murphy, Hosuk Chang, Luke Elwood, Ting Zhang
Bringing NASA Data to Life - One Story at a Time	This production session presents how NASA Earth science reaches global audiences through compelling data-driven visualizations. As scientific data grows increasingly complex and voluminous, the challenge lies in transforming data into meaningful, accessible knowledge. NASA team members bridge this gap by working closely with scientists and mission teams to create innovative visualizations that make intricate Earth phenomena universally understandable. These visualizations transform complex datasets into captivating narratives that advance both science and public understanding.	https://dl.acm.org/doi/abs/10.1145/3698897.3719216	Helen-Nicole Kostis
Bringing The Wild Robot's Environment to Life	The environment in is rich in variety, dense in layout, painterly in look, and a character in itself alive with motion. This talk describes the different techniques, tools, and pipeline used to breathe life into this wild environment, along with the challenges faced while dealing with the environment's complexity.	https://dl.acm.org/doi/abs/10.1145/3721239.3734089	Damon Riesberg, Jason Dengler, Chris Michael, Levi Biasco, Arunachalam Somasundaram
Building a Blender pipeline in 30 Minutes	This class focuses on how the artist-centered solutions developed at Blender Studio over more than a decade of filmmaking can help non-technical artists work together seamlessly. The only tools required are Blender, Python and Docker.	https://dl.acm.org/doi/abs/10.1145/3721251.3742867	Francesco Siddi, Nick Alberelli
BuildingBlock: A Hybrid Approach for Structured Building Generation	Three-dimensional building generation is vital for applications in gaming, virtual reality, and digital twins, yet current methods face challenges in producing diverse, structured, and hierarchically coherent buildings. We propose , a hybrid approach that integrates generative models, procedural content generation (PCG), and large language models (LLMs) to address these limitations. Specifically, our method introduces a two-phase pipeline: the Layout Generation Phase (LGP) and the Building Construction Phase (BCP). LGP reframes box-based layout generation as a point-cloud generation task, utilizing a newly constructed architectural dataset and a Transformer-based diffusion model to create globally consistent layouts. With LLMs, these layouts are extended into rule-based hierarchical designs, seamlessly incorporating component styles and spatial structures. The BCP leverages these layouts to guide PCG, enabling local-customizable, high-quality structured building generation. Experimental results demonstrate 's effectiveness in generating diverse and hierarchically structured buildings, achieving state-of-the-art results on multiple benchmarks, and paving the way for scalable and intuitive architectural workflows.	https://dl.acm.org/doi/abs/10.1145/3721238.3730705	Junming Huang, Chi Wang, Letian Li, Changxin Huang, Qiang Dai, Weiwei Xu
CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation	We introduce CLR-Wire, a novel framework for 3D curve-based wireframe generation that integrates geometry and topology into a unified . Unlike conventional methods that decouple vertices, edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (VAE). This unified approach facilitates joint learning and generation of both geometry and topology. To generate wireframes, we employ a flow matching model to progressively map Gaussian noise to these latents, which are subsequently decoded into complete 3D wireframes. Our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. Experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for CAD design, geometric reconstruction, and 3D content creation.	https://dl.acm.org/doi/abs/10.1145/3721238.3730638	Xueqi Ma, Yilin Liu, Tianlong Gao, Qirui Huang, Hui Huang
CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation	Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.	https://dl.acm.org/doi/abs/10.1145/3721238.3730722	Peng Li, Suizhi Ma, Jialiang Chen, Yuan Liu, Congyi Zhang, Wei Xue, Wenhan Luo, Alla Sheffer, Wenping Wang, Yike Guo
CNESST - Hanging By A Thread	"An office worker unravels as he struggles to deal with bullying, psychological violence and harassment at work. At work, psychological health can sometimes be so fragile that it ""hangs by a thread."" The campaign aims to spark open conversations and encourage proactive solutions."	https://dl.acm.org/doi/abs/10.1145/3698896.3720512	Dale Hayward
CageNet: A Meta-Framework for Learning on Wild Meshes	Learning on triangle meshes has recently proven to be instrumental to a myriad of tasks, from shape classification, to segmentation, to deformation and animation, to mention just a few. While some of these applications are tackled through neural network architectures which are tailored to the application at hand, many others use generic frameworks for triangle meshes where the only customization required is the modification of the input features and the loss function. Our goal in this paper is to broaden the applicability of these generic frameworks to , i.e. meshes in-the-wild which often have multiple components, non-manifold elements, disrupted connectivity, or a combination of these. We propose a configurable meta-framework based on the concept of : Given a mesh, a is a single component manifold triangle mesh that envelopes it closely. map between functions on the cage, and functions on the mesh, allowing us to learn and test on a variety of data, in different applications. We demonstrate this concept by learning segmentation and skinning weights on difficult data, achieving better performance to state of the art techniques on wild meshes.	https://dl.acm.org/doi/abs/10.1145/3721238.3730654	Michal Edelstein, Hsueh-Ti Derek Liu, Mirela Ben-Chen
Candice	Candice, a young girl is bullied by the other children. One day she found a dead cat which ask her to fix it. She is then going to search for every dead animals possible to create some new friends.	https://dl.acm.org/doi/abs/10.1145/3698896.3717358	Swann Valenza
Candy Crush: Simulating Crystal Growth in Avatar: The Last Airbender: How Image Engine Grew Magical Rock Candy Crystals-Fast Beautiful and Believable using their Houdini Pipeline	Crystals erupt from the ground, surrounding Aang in seconds. To bring this moment to life in Avatar: The Last Airbender, we developed a custom, art-directable crystal growth system built in Houdini that could integrate seamlessly with actors and plates. Designed to support extreme speed, flexible timing, and creative direction, the system enabled us to emphasize the scene's intensity while adhering to the look and logic of real-world crystals.	https://dl.acm.org/doi/abs/10.1145/3721239.3735550	Anders Elmholdt, Christoph Hennig, Matthew Kiefer, Greg Massie
Capsule: Efficient Player Isolation for Datacenters	Cloud gaming is increasingly popular. A challenge for cloud provider is to efficiently operate their datacenters, i.e., keep datacenter utilization high: a non-trivial task due to application variety. Cloud datacenter resources are also diverse, e.g., CPUs, GPUs, NPUs. We propose player-level isolation to address this challenge. We implemented such an isolation mechanism in Open 3D Engine (O3DE) with Capsule. Capsule allows multiple players to efficiently share one GPU. It is efficient because computation can be reused across players. Our evaluations show that Capsule can increase datacenter resource utilization by accommodating up to 2.25 × more players, without degrading player gaming experience. Capsule is also application agnostic. We ran four applications on Capsule-based O3DE with no application changes. Our experiences show that Capsule design can be adopted by other game engines to increase datacenter utilization across cloud providers.	https://dl.acm.org/doi/abs/10.1145/3721250.3742988	Zhouheng Du, Nima Davari, Li Li, Nodir Kodirov
"Choreography of Hair and Cloth in Disney's ""Moana 2"""	"In Walt Disney Animation Studios' ""Moana 2"", the main characters undergo significant evolution from the original film, displaying a broad range of emotions while interacting with diverse environmental conditions. Crafting the hair and cloth motion to consistently support and enhance the complex character performances presented challenges for the character team, particularly when combined with nuanced art direction. To this end, we developed a multi-tiered and strategic approach for the choreography of the hair and cloth which includes three main areas of emphasis: Performance Categorization, Continuity through Visual Planning, and Iterative Refinement. Integral to this process were extensive and detailed drawovers, to ensure that hairstyles and costumes reacted consistently. This strategy allowed us to frontload the work through careful planning, enabling technical animation to execute on art direction of shots efficiently. We illustrate the overall approach with specifics for hair with Moana and Maui, and with Matangi for cloth."	https://dl.acm.org/doi/abs/10.1145/3721239.3734105	Avneet Kaur, Hubert Leo, Nachiket Pujari, Bret Bays
CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation	In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware control signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals—comprising rendered depth maps, camera trajectories, and object class labels—serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation.	https://dl.acm.org/doi/abs/10.1145/3721238.3730755	Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai
Co-Speech Gesture and Facial Expression Generation for Non-Photorealistic 3D Characters	With the advancement of conversational AI, research on bodily expressions, including gestures and facial expressions, has also progressed. However, many existing studies focus on photorealistic avatars, making them unsuitable for non-photorealistic characters, such as those found in anime. This study proposes methods for expressing emotions, including exaggerated expressions unique to non-photorealistic characters, by utilizing expression data extracted from comics and dialogue-specific semantic gestures. A user study demonstrated significant improvements across multiple aspects when compared to existing research.	https://dl.acm.org/doi/abs/10.1145/3721250.3742976	Taisei Omine, Naoyuki Kawabata, Fuminori Homma
Cobra: Efficient Line Art COlorization with BRoAder References	The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control.A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce , an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730660	Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan
Code To The Cosmos: Animation Outside the Box, in Pixar's Elio	"Amongst a variety of alien species in Pixar's , one of the more unusual characters in the film is the Universal Users Manual (or UUM for short) - a sentient alien book harboring all the answers to the universe. The UUM is a ""character"" consisting of a stack of pages with the ability to zoom along distinctly shaped paths and is rigged and animated in our traditional pipeline within Presto with the additional hookup of Houdini Engine (HE) to create the distinct look of the character with traditional FX layered on top. By hooking up a base shape to Houdini Engine we have the ability to combine native ease of manipulating a character in presto with the power and sophistication of our FX pipeline."	https://dl.acm.org/doi/abs/10.1145/3721239.3734108	Trent Crow, Anna-Christine Lykkegaard, Eric Anderson
Color Matching and Biomimicry for Multi-Material Dental 3D Printing	The growing global demand for removable partial and full dentures, driven by an aging population and the high prevalence of edentulism, emphasizes the importance of advancing manufacturing solutions. Multi-material jetting, with newly regulatory-approved dental resins, facilitates the production of monolithic, full-color dentures, reducing manual labor and enabling advanced aesthetic customization. We propose a practical method for dental layer biomimicry and multi-spot shade matching using multi-material 3D printing. It integrates seamlessly into workflows combining dental CAD tools, which compute the outer shape of dentures, and industrial multi-material slicers. We introduce a morphable star-shape descriptor to embed enamel, dentin, and root layers with controlled thicknesses into outer tooth geometries. To compute per-layer material ratios, we first present a forward model to predict the color of printed layered slabs mimicking inner tooth structures with variable translucencies. The slab design enables reasonable color predictions on layered tooth shapes. Based on the forward model, we propose a backward model to compute per-layer material ratios for given layer-translucencies to achieve color matches at multiple points on the tooth surface. The method's effectiveness is demonstrated by fabricating various dentures with custom layers that accurately replicate VITA classical shades, showcasing its practical and versatile application in denture manufacturing.	https://dl.acm.org/doi/abs/10.1145/3721238.3730708	András Simon, Danwu Chen, Philipp Urban, Vincent Duveiller, Henning Lübbe
ColorSurge: Bringing Vibrancy and Efficiency to Automatic Video Colorization via Dual-Branch Fusion	Automatic video colorization poses challenges, requiring efficient generation of results that ensure frame and color consistency. Previous video colorization works often suffer from issues such as color flickering, bleeding, artifacts, and low color richness due to the inherent ambiguity and limitations of the models. While diffusion-based video-to-video approaches can produce customized colorization models through fine-tuning, their high inference costs limit their suitability for real-time scenarios. To address these challenges, we propose ColorSurge, a lightweight network for efficient end-to-end video colorization. ColorSurge employs a dual-branch structure, consisting of a grayscale branch and a color branch. In the grayscale branch, we extract the semantic content of grayscale videos and reconstruct and output features at different spatial scales. In the color branch, we introduce learnable color tokens and fuse these multi-scale semantic features through stacked Color Alchemy Blocks (CABs). Within each CAB, we incorporate Color Spatial Transformer Blocks (CSTB) and Color Temporal Transformer Blocks (CTTB) to constrain the spatial harmony and temporal consistency of colors. Finally, we use a Color Mapper to unify the grayscale and color features, mapping them to obtain the final colorized video result. Extensive experiments demonstrate that our method significantly outperforms previous state-of-the-art models in both qualitative and quantitative evaluations. Our code and model are available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730736	Hongbo Zhao, Jiaxing Li, Peiyi Zhang, Peng Xiao, Jianxin Lin, Yijun Wang
Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting	Online reconstruction of dynamic scenes is significant as it enables learning scenes from live-streaming video inputs, while existing offline dynamic reconstruction methods rely on recorded video inputs. However, previous online reconstruction approaches have primarily focused on efficiency and rendering quality, overlooking the temporal consistency of their results, which often contain noticeable artifacts in static regions. This paper identifies that errors such as noise in real-world recordings affect temporal inconsistency in online reconstruction. We propose a method that enhances temporal consistency in online reconstruction from observations with temporal inconsistency which is inevitable in cameras. We show that our method restores the ideal observation by subtracting the learned error. We demonstrate that applying our method to various baselines significantly enhances both temporal consistency and rendering quality across datasets. Code, video results, and checkpoints are available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730678	Youngsik Yun, Jeongmin Bae, Hyunseung Son, Seoha Kim, Hahyun Lee, Gun Bang, Youngjung Uh
Computational Craft: Computational Fabrication Methods for Enabling Craft Production in Textiles and Ceramics	This course will introduce attendees to foundational concepts and methods in applying computational design and digital fabrication techniques to craft production. Instructors will cover methods from ceramics and textiles. Attendees will learn about craft materials and manual methods for fabrication of these materials, 2) compatible machining methods, and general computational design and optimization techniques for computational fabrication. We will cover general approaches to developing domain-specific computational representations for craft production processes, suitable applications of material simulation and visualization, and methods for enforcing craft-specific constraints in computational design tools without limiting the exercise of craft skills and creative decision-making. In addition, we will introduce computational approaches to dynamically control digital fabrication machine behaviors in ways that align with manual craft production. Overall, we aim to illustrate the connections between methods from graphics research and computational fabrication while providing concrete examples of how the physical realities of craft production require flexible computational methods directly informed by material practice.	https://dl.acm.org/doi/abs/10.1145/3721241.3733998	Jennifer Jacobs, Emilie Yu, Mackenzie Leake, Nadya Peek, Hannah Twigg-Smith, Emily Whiting
Computational Modeling of Gothic Microarchitecture	"Gothic microarchitecture is a design phenomenon widely observed in late medieval European art, comprising sculptural works that emulate the forms and structural composition of monumental Gothic architecture. Despite its prevalence in preserved artifacts, the design and construction methods of Gothic microarchitecture used by artisans remain a mystery, as these processes were orally transmitted and rarely documented. The Basel goldsmith drawings (""Basler Goldschmiedrisse""), a collection of over 200 late 15th-century design drawings from the Upper Rhine region, provide a rare glimpse into the workshop practices of Gothic artisans. This collection consists of unpaired 2D drawings, including top-view and side-view projections of Gothic microarchitecture, featuring nested curve networks without annotations or explicitly articulated design principles. Understanding these 2D drawings and reconstructing the 3D objects they represent has long posed a significant challenge due to the lack of documentation and the complexity of the designs. In this work, we propose a framework of simple yet expressive geometric principles to model Gothic microarchitecture as 3D curve networks, using limited input such as historical 2D drawings. Our approach formalizes a historically informed design space, constrained to tools traditionally available to artisans–namely compass and straightedge–and enables faithful reproduction of Gothic microarchitecture that conforms to physical artifacts. Our framework is intuitive and efficient, allowing users to interactively create 3D Gothic microarchitecture with minimal effort. It bridges the gap between historical artistry and modern computational design, while also shedding light on a lost chapter of Gothic craftsmanship."	https://dl.acm.org/doi/abs/10.1145/3721238.3730649	Aviv Segall, Jing Ren, Martin Schwarz, Olga Sorkine-Hornung
Confidence Estimation of Few-shot Patch-based Learning for Anime-style Colorization	In hand-drawn anime production, automatic colorization is used to boost productivity, where line drawings are automatically colored based on reference frames. However, the results sometimes include wrong color estimations, requiring artists to carefully inspect each region and correct colors—a time-consuming and labor-intensive task. To support this process, we propose a confidence estimation method that indicates the confidence level of colorization for each region of the image. Our method compares local patches in the colorized result and the reference frame.	https://dl.acm.org/doi/abs/10.1145/3721250.3742964	Yuexiang Ji, Akinobu Maejima, Yotam Sechayk, Yuki Koyama, Takeo Igarashi
Controllable Tracking-Based Video Frame Interpolation	Temporal video frame interpolation has been an active area of research in recent years, with a primary focus on motion estimation, compensation, and synthesis of the final frame. While recent methods have shown good quality results in many cases, they can still fail in challenging scenarios. Moreover, they typically produce fixed outputs with no means of control, further limiting their application in film production pipelines. In this work, we address the less explored problem of user-assisted frame interpolation to improve quality and enable control over the appearance and motion of interpolated frames. To this end, we introduce a tracking-based video frame interpolation method that utilizes sparse point tracks, first estimated and interpolated with existing point tracking methods and then optionally refined by the user. Additionally, we propose a mechanism for controlling the levels of hallucination in interpolated frames through inference-time model weight adaptation, allowing a continuous trade-off between hallucination and blurriness. Even without any user input, our model achieves state-of-the-art results in challenging test cases. By using points tracked over the whole sequence, we can use better motion trajectory interpolation methods, such as cubic splines, to more accurately represent the true motion and achieve significant improvements in results. Our experiments demonstrate that refining tracks and their trajectories through user interactions significantly improves the quality of interpolated frames.	https://dl.acm.org/doi/abs/10.1145/3721238.3730598	Karlis Martins Briedis, Abdelaziz Djelouah, Raphaël Ortiz, Markus Gross, Christopher Schroers
Cora: Correspondence-aware image editing using few step diffusion	Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora , a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives. Project Page & source code: cora-edit.github.io	https://dl.acm.org/doi/abs/10.1145/3721238.3730650	Amirhossein Alimohammadi, Aryan Mikaeili, Sauradip Nag, Negar Hassanpour, Andrea Tagliasacchi, Ali Mahdavi-Amiri
Corpus and the Wandering	"One dancer, one body, one phone. Modern society lives on the grid. Phone screens, conference calls, cubicles, apartment windows. We are systemically divided by a culture that profits from our collective alienation. When the COVID pandemic first hit, director and dancer Jo Roy was inspired to create a world transcending this insidious framework of consumerism and patriarchy. The result: Corpus and the Wandering, an experimental self-portrait that follows one woman's journey as she rediscovers her inner compass and kinship with nature. Using only her own body and her iPhone, Roy transforms a 100-screen ""videomosaic"" grid into a wonderland of the Earth and beyond. A mix of technical innovation and creative vulnerability, Corpus and the Wandering dares to ask how we can remove the walls of a fragmented society and repurpose technology to align with an interconnected future."	https://dl.acm.org/doi/abs/10.1145/3698896.3722211	Jo Roy
Courage	When Anna, an Olympic athlete, finds herself behind in the race, she redoubles her efforts with the aim of winning the competition and never disappoint again. But as she pushes herself beyond her limits, she burns out. She then has to face the wave of judgment that comes with dropping out.	https://dl.acm.org/doi/abs/10.1145/3698896.3719123	Margot Jacquet, Nathan Baudry, Marion Choudin, Jeanne Desplanques, Lise Delcroix, Salomé Cognon
Crafting Expressive, Non-Humanoid Alien Characters	This paper presents the challenges and innovations behind rigging non-humanoid alien characters with unconventional anatomies—such as limbless, multi-segmented, or fluid forms. Using collaborative workflows, modular rigs, and procedural tools, the team enabled expressive, efficient animation while balancing creative ambition with technical scalability.	https://dl.acm.org/doi/abs/10.1145/3721239.3734115	Kevin Singleton, Daniela Dwek, Ozgur Aydogdu, Anthony Muscarella
Crafting Malgosha, The Mean Queen 'Gosh'	Creating a believable digital character that has never existed requires a large team of dedicated people. We often have real-world source material to reference, but in the case of Malgosha, there was nothing but a concept and a charmingly low-resolution game world. That's when we rely on Wētā FX's recipe of collaboration working towards a common goal of making audiences believe that something unbelievable is real. Malgosha, the main villainess, evolved from a simple concept into a fully realized digital character. We will showcase the intricate and collaborative process that brought her to life, highlighting Wētā FX's unique approach and deep history in character creation.	https://dl.acm.org/doi/abs/10.1145/3721239.3734109	Kevin Estey, Sheldon Stopsack
"Creating the Mudskipper Pile in Disney's ""Moana 2"": A Slippery Problem Space"	"Collisions are a key problem in generating complex crowds animation. The mudskippers in Walt Disney Animation Studios' ""Moana 2"" presented a particularly challenging scenario where throngs of floppy amphibious fish pack tightly together to form a towering pile. Our solution introduces an additional simulation step to deform the skinned character meshes using Houdini's Vellum solver to resolve body-to-body contact. This departure from our standard deferred mesh generation workflow provided an effective solution to this problem, and opens up new avenues for achieving character deformations beyond the capabilities of character rigging."	https://dl.acm.org/doi/abs/10.1145/3721239.3734086	Alberto J Luceño Ros, Cecilia Berriz
CueTip: An Interactive and Explainable Physics-aware Pool Assistant	We present an interactive and explainable automated coaching assistant called CueTip for a variant of pool/billiards. CueTip's novelty lies in its combination of three features: a natural-language interface, an ability to perform contextual, physics-aware reasoning, and that its explanations are rooted in a set of predetermined guidelines developed by domain experts. We instrument a physics simulator so that it generates event traces in natural language alongside traditional state traces. Event traces lend themselves to interpretation by language models, which serve as the interface to our assistant. We design and train a neural adaptor that decouples tactical choices made by CueTip from its interactivity and explainability allowing it to be reconfigured to mimic any pool playing agent. Our experiments show that CueTip enables contextual query-based assistance and explanations while maintaining the strength of the agent in terms of win rate (improving it in some situations). The explanations generated by CueTip are physically-aware and grounded in the expert rules and are therefore more reliable.	https://dl.acm.org/doi/abs/10.1145/3721238.3730742	Sean Memery, Kevin Denamganaï, Jiaxin Zhang, Zehai Tu, Yiwen Guo, Kartic Subr
Curl Quantization for Automatic Placement of Knit Singularities	We develop a method for automatic placement of knit singularities based on curl quantization, extending the knit-planning frameworks of Mitra et al. [ ; ]. Stripe patterns are generated that closely follow the isolines of an underlying knitting time function, and has course and wale singularities in regions of high curl for the normalized time function gradient and its 90° rotated field, respectively. Singularities are placed in an iterative fashion, and we show that this strategy allows us to easily maintain the structural constraints necessary for machine-knitting, e.g., the helix-free constraint, and to satisfy user constraints such as stripe alignment and singularity placement. Our more performant approach obviates the need for a mixed-integer solve [Mitra et al. ], manual fixing of singularity positions, or the running of a singularity matching procedure in post-processing [Mitra et al. ]. Our global optimization also produces smooth knit graphs that provide quick simulation-free previews of rendered knits without the surface artifacts of competing methods. Furthermore, we extend our method to the popular cut-and-sew garment design paradigm. We validate our method by machine-knitting and rendering yarn-based visualizations of prototypical models in the 3D and cut-and-sew settings.	https://dl.acm.org/doi/abs/10.1145/3721238.3730715	Rahul Mitra, Mattéo Couplet, Tongtong Wang, Megan Hoffman, Kui Wu, Edward Chien
DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution	Real-world video super-resolution (VSR) presents significant challenges due to complex and unpredictable degradations. Although some recent methods utilize image diffusion models for VSR and have shown improved detail generation capabilities, they still struggle to produce temporally consistent frames. We attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address this issue. However, due to the intrinsic image-animation characteristics of SVD, it is challenging to generate fine details using only low-quality videos. To tackle this problem, we propose DAM-VSR, an appearance and motion disentanglement framework for VSR. This framework disentangles VSR into appearance enhancement and motion control problems. Specifically, appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. This disentanglement fully leverages the generative prior of video diffusion models and the detail generation capabilities of image super-resolution models. Furthermore, equipped with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art performance on real-world data and AIGC data, demonstrating its powerful detail generation capabilities.	https://dl.acm.org/doi/abs/10.1145/3721238.3730699	Zhe Kong, Le Li, Yong Zhang, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming Wei, Guanying Chen, Wenhan Luo
DC-VSR: Spatially and Temporally Consistent Video Super-Resolution with Video Diffusion Prior	Video super-resolution (VSR) aims to reconstruct a high-resolution (HR) video from a low-resolution (LR) counterpart. Achieving successful VSR requires producing realistic HR details and ensuring both spatial and temporal consistency. To restore realistic details, diffusion-based VSR approaches have recently been proposed. However, the inherent randomness of diffusion, combined with their tile-based approach, often leads to spatio-temporal inconsistencies. In this paper, we propose DC-VSR, a novel VSR approach to produce spatially and temporally consistent VSR results with realistic textures. To achieve spatial and temporal consistency, DC-VSR adopts a novel Spatial Attention Propagation (SAP) scheme and a Temporal Attention Propagation (TAP) scheme that propagate information across spatio-temporal tiles based on the self-attention mechanism. To enhance high-frequency details, we also introduce Detail-Suppression Self-Attention Guidance (DSSAG), a novel diffusion guidance scheme. Comprehensive experiments demonstrate that DC-VSR achieves spatially and temporally consistent, high-quality VSR results, outperforming previous approaches.	https://dl.acm.org/doi/abs/10.1145/3721238.3730719	Janghyeok Han, Gyujin Sim, Geonung Kim, Hyun-Seung Lee, Kyuha Choi, Youngseok Han, Sunghyun Cho
DJESTHESIA: Tangible Multimedia for DJs	DJESTHESIA uses tangible interaction to craft real-time audiovisual multimedia, blending sound, visuals, and gestures into a unified live performance. The project supports four interaction modes: I) Knob changes music, where standard DJing is performed. II) Music changes visuals, where changes in the audio parameters done through the mixer have a direct impact in the visualizations representing the music (e.g., color palette). III) Gesture changes visuals, where gestures and body movements give the possibility to interact physically with the visual representation of the music (e.g., grab, release, throw). IV) Gesture changes music, where, gestures can convey information to an audio composition software to alter aspects of the music being played (e.g., EQs). The aim of DJESTHESIA is to transform the DJ into both a performer and a performance.	https://dl.acm.org/doi/abs/10.1145/3721243.3735980	Eduardo Castelló Ferrer
Dare to be Fabulous	As to prove himself to his gangster father, Alessandro decides to rob a bar. What he wouldn't expect is to meet another side of his father: Lady Victoria the drag queen.	https://dl.acm.org/doi/abs/10.1145/3698896.3719327	João Buosi, Yangjia Chen, Adam Meziane Philipps, Carla Sampaio Da Silva, Qin Xinxin, Zhen Zhou
Data-Efficient Discovery of Hyperelastic TPMS Metamaterials with Extreme Energy Dissipation	Triply periodic minimal surfaces (TPMS) are a class of metamaterials with a variety of applications and well-known primitive morphologies. We present a new method for discovering novel microscale TPMS structures with exceptional energy-dissipation capabilities, achieving double the energy absorption of the best existing TPMS primitive structure. Our approach employs a parametric representation, allowing seamless interpolation between structures and representing a rich TPMS design space. As simulations are intractable for efficiently optimizing microscale hyperelastic structures, we propose a sample-efficient computational strategy for rapid discovery with limited empirical data from 3D-printed and tested samples that ensures high-fidelity results. We achieve this by leveraging a predictive uncertainty-aware Deep Ensembles model to identify which structures to fabricate and test next. We iteratively refine our model through batch Bayesian optimization, selecting structures for fabrication that maximize exploration of the performance space and exploitation of our energy-dissipation objective. Using our method, we produce the first open-source dataset of hyperelastic microscale TPMS structures, including a set of novel structures that demonstrate extreme energy dissipation capabilities, and show several potential applications of these structures.	https://dl.acm.org/doi/abs/10.1145/3721238.3730759	Maxine Perroni-Scharf, Zachary Ferguson, Thomas Butruille, Carlos Portela, Mina Konaković Luković
Dawn	A baby sea turtle needs to reach the ocean. To achieve that, she has to overcome lots of obstacles and predators on her way.	https://dl.acm.org/doi/abs/10.1145/3698896.3719631	Apolline Royer, Maxime Forestier, Lorys Stora, Matthieu Dejoux, YuFang Chang, Lucas Jonckheere, Noah Mercier, Marie Pradeilles
DeepMill: Neural Accessibility Learning for Subtractive Manufacturing	Manufacturability is vital for product design and production, with accessibility being a key element, especially in subtractive manufacturing. Traditional methods for geometric accessibility analysis are time-consuming and struggle with scalability, while existing deep learning approaches in manufacturability analysis often neglect geometric challenges in accessibility and are limited to specific model types. In this paper, we introduce DeepMill, the first neural framework designed to accurately and efficiently predict inaccessible and occlusion regions under varying machining tool parameters, applicable to both CAD and freeform models. To address the challenges posed by cutter collisions and the lack of extensive training datasets, we construct a cutter-aware dual-head octree-based convolutional neural network (O-CNN) and generate an inaccessible and occlusion regions analysis dataset with a variety of cutter sizes for network training. Experiments demonstrate that DeepMill achieves 94.7% accuracy in predicting inaccessible regions and 88.7% accuracy in identifying occlusion regions, with an average processing time of 0.04 seconds for geometries. Based on the outcomes, DeepMill implicitly captures both local and global geometric features, as well as the complex interactions between cutters and intricate 3D models.	https://dl.acm.org/doi/abs/10.1145/3721238.3730657	Fanchao Zhong, Yang Wang, Peng-Shuai Wang, Lin Lu, Haisen Zhao
Deformable Beta Splatting	3D Gaussian Splatting (3DGS) has advanced radiance field reconstruction by enabling real-time rendering. However, its reliance on Gaussian kernels for geometry and low-order Spherical Harmonics (SH) for color encoding limits its ability to capture complex geometries and diverse colors. We introduce Deformable Beta Splatting (DBS), a deformable and compact approach that enhances both geometry and color representation. DBS replaces Gaussian kernels with deformable Beta Kernels, which offer bounded support and adaptive frequency control to capture fine geometric details with higher fidelity while achieving better memory efficiency. In addition, we extended the Beta Kernel to color encoding, which facilitates improved representation of diffuse and specular components, yielding superior results compared to SH-based methods. Furthermore, Unlike prior densification techniques that depend on Gaussian properties, we mathematically prove that adjusting regularized opacity alone ensures distribution-preserved Markov chain Monte Carlo (MCMC), independent of the splatting kernel type. Experimental results demonstrate that DBS achieves state-of-the-art visual quality while utilizing only 45% of the parameters and rendering 1.5x faster than 3DGS-MCMC, highlighting the superior performance of DBS for real-time radiance field rendering. Interactive demonstrations and source code are available on our project website: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730716	Rong Liu, Dylan Sun, Meida Chen, Yue Wang, Andrew Feng
Demystifying noise: Role of randomness in generative AI: Demystifying randomness in generative AI	This course offers a thorough exploration of the role of randomness in generative AI, leveraging foundational knowledge from statistical physics, stochastic differential equations, and computer graphics. By connecting these disciplines, the course aims to provide participants with a deep understanding of how noise impacts generative modeling and introduce state-of-the-art techniques and applications of noise in AI. First, we revisit the mathematical concepts essential for understanding diffusion and the integral role of noise in diffusion-based generative modeling. In the second part of the course, we introduce the various types of noises studied within the computer graphics community and present their impact on rendering, texture synthesis and content creation. In the last part, we will look at how different noise correlations and noise schedulers impact the expressive power of image and video generation models. By the end of the course, participants will gain an in-depth understanding of the mathematical constructs for diffusion models and how noise correlations can play an important role in enhancing the diversity and expressiveness of these models. The audience will also learn to code these noises developed in the graphics literature and their impact on generative modeling. The course is aimed for students, researchers and practitioners. All the related material for the course can be found on https://diffusion-noise.mpi-inf.mpg.de/.	https://dl.acm.org/doi/abs/10.1145/3721241.3734002	Gurprit Singh, Xingchang Huang, Jente Vandersanden, Cengiz Oztireli, Niloy Mitra
Descendant: Re-imagining Guangdong Embroidery and Folk Rituals through Digitization: Workflow Innovation in Heritage Conservation: Cross-Platform Real-Time Tool Integration for Cultural Memory Digitization	Descendant advances cultural heritage digital conservation through a Guangdong embroidery case study. establishing a workflow that digitizes endangered the embroidery artifacts and related rituals via cross-platform integration of real-time tools: Blender, OpenXR-based VR, and WeChat. This integration bridges preservation and cultural storytelling—democratizing heritage revival for designers, enabling community memory-making, and establishing scalable digital-legacy frameworks. By reconfiguring ritual as computational material, Descendant demonstrates real-time systems' potential to sustain cultural continuity.	https://dl.acm.org/doi/abs/10.1145/3721243.3735984	Wei He, Naishu Hu, Yangxingyue Wang
Designing Balancing Toys Through Mass and Shape Optimization	We propose an algorithm to interactively design freeform balancing toys that stably balance on a single point of contact. We achieve this by positioning the center of mass outside the model's surface while deforming the external surface. Our approach relies on a simple energy function that is fast to evaluate and optimize, allowing an interactive design process. The results confirm the feasibility of creating stable balancing toys via standard 3D printing, expanding the possibilities for mechanical design.	https://dl.acm.org/doi/abs/10.1145/3721250.3743002	Shunsuke Hirata, Yuta Noma, Koya Narumi, Yoshihiro Kawahara
Designing Hammock Tower: Digital Twin Methods for Climate Resilient Architecture	Climate simulation has become a critical tool for shaping architectural performance in response to rising global temperatures. This Labs hands-on session presents the digital twin workflow behind , an architectural design for Paris in 2100, which leverages NVIDIA Omniverse, SimScale, Cesium, and Autodesk Forma to inform climate-resilient strategies for a projected +4 °C future. Participants will learn how to use environmental simulation to guide passive cooling strategies, validate design hypotheses, and visualize airflow at both building and urban scales. The session emphasizes translating site-specific data into actionable spatial interventions for sustainable architectural design. Project documentation is available at .	https://dl.acm.org/doi/abs/10.1145/3721251.3742862	Sharel Liu
Designing Oscar: Bespoke Tablet Workflows in ILM StageCraft	StageCraft, the end-to-end virtual production platform created by Industrial Light & Magic (ILM), is the culmination of decades of experience in the field. Advancements in real-time rendering, combined with motion tracking, have provided an opportunity for a new paradigm: on-set, in-camera visual effects (ICVFX), with a level of spontaneity and flexibility not previously possible in visual effects creation. Filmmakers can now expect near-instanteous responses to their creative direction on set. In order to support StageCraft Operators as they work to achieve a filmmaker's on-demand vision, we developed : a suite of tablet-based user interfaces which provide intuitive access to the complexities of the underlying StageCraft technology.	https://dl.acm.org/doi/abs/10.1145/3721239.3734075	David Hirschfield, Mike Jutan
Detlev	A constantly freezing street sweeper drives to a remote gas station every evening and orders a microwave-warmed Toast Hawaii. He secretly indulges himself into this in a bizarre ritual - the only thing that warms him in his daily life. One night, however, a broadly grinning construction worker is observing him while doing that. Detlev flees in embarrassment and his world begins to fall apart as the desperate search for warmth drags him deeper and deeper into his inner abyss.	https://dl.acm.org/doi/abs/10.1145/3698896.3721361	Ferdinand Ehrhardt
Developing Industrial Digital Twin Applications for Apple Vision Pro	Develop front-end experiences for the Apple Vision Pro and NVIDIA Omniverse as the back-end server for captivating spatial experiences. This innovative lab explores the integration of Apple Vision Pro with NVIDIA Omniverse to create immersive industrial digital twin experiences. Participants will learn to develop spatial applications that leverage the power of both platforms, combining Swift and SwiftUI for front-end development with Omniverse's robust backend capabilities. The lab guides attendees through setting up the development environment, including configuring Xcode for Vision Pro development and preparing Omniverse as a backend server. Participants will build a SwiftUI-based client application, implementing CloudXR framework components for Omniverse communication. They'll then establish connectivity between Vision Pro and Omniverse, setting up Omniverse Kit for streaming 3D content. Interactive features are a key focus, with participants developing a message bus system for client-server communication and creating functions for camera control and animation playback. The lab also covers enhancing user experience by adding metadata viewing capabilities for 3D objects and optimizing performance for real-time interaction. Through hands-on exercises, attendees will build a functional Vision Pro client that connects to Omniverse, implement interactive features like bay camera selection and animation control, and customize the application to display metadata for selected 3D objects. This lab represents a significant contribution to the SIGGRAPH community by bridging cutting-edge AR technology with powerful 3D content creation and streaming capabilities.	https://dl.acm.org/doi/abs/10.1145/3721251.3734060	Jen Borucki, Anthony Cope
Developing an OpenUSD Configurator Experience for Apple Vision Pro	This lab introduces a framework for developing multi-asset spatial configuration systems on Apple Vision Pro, integrating OpenUSD's compositional workflows with NVIDIA Omniverse's real-time simulation capabilities. Participants learn to build gesture-controlled SwiftUI interfaces that dynamically synchronize with physics-aware Omniverse environments, enabling real-time customization of 3D products across categories such as consumer goods and industrial components. The system employs a hybrid renderi...	https://dl.acm.org/doi/abs/10.1145/3721251.3734061	Max Bickley, Roy C Anthony
Developing the Stylized World of The Wild Robot	The Wild Robot follows the journey of Roz, a high-tech robot stranded on a remote island. In the film, we accentuate this juxtaposition of nature and technology stylistically—a futuristic machined precision amongst a deconstructed, painterly world. The style of the film serves the story—Roz does not initially belong on the island, stylistically. To that goal, we reimagined every workflow to retain the immediacy and fluidity of an artist's hand.	https://dl.acm.org/doi/abs/10.1145/3698897.3718365	Jeff Budsberg, Baptiste Van Opstal, Lisa Connors, Mike Necci, James Jackson
Diablo IV Vessel of Hatred Official Cinematic Trailer	The battle against Hatred has only just begun. Journey into the new region of Nahantu in search of Neyrelle, who is both suffering the fate of her choice to imprison the Prime Evil Mephisto, and seeking a means to destroy him.	https://dl.acm.org/doi/abs/10.1145/3698896.3723887	Doug Gregory
Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control	Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process—such as camera manipulation or content editing—remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation. More results are available in the supplementary materials.	https://dl.acm.org/doi/abs/10.1145/3721238.3730607	Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, Yuan Liu
Digital Meets Handmade: Can VP enhance stop-motion?: An exploration of virtual production on and off the set	An R&D initiative within Aardman Animations exploring VP technologies for stop-motion film making. Taking a holistic view of VP from story development through to delivery. Utilising real-time tools, digital twins, and a cross platform XR sandbox. Striving to evolve traditional processes, enhancing creativity, efficiency, and integration across the production pipeline.	https://dl.acm.org/doi/abs/10.1145/3721239.3734092	David Gray, Eve Bolotova, Jonah Ling, Leon Cresdee
Digital Twin Robotics: Immersive Software-in-the-Loop Testing with OpenUSD, Isaac Sim, and ROS	This hands-on lab introduces digital twin technology for robotics, focusing on software-in-the-loop testing using OpenUSD, NVIDIA Isaac Sim, and the Robot Operating System (ROS) with a locomotion policy trained by NVIDIA Isaac Lab. As the robotics industry continues to evolve, the need for efficient and cost-effective testing methodologies becomes crucial. Digital twins offer a powerful solution, allowing developers to simulate and validate robotic systems in virtual environments before physical deployment.	https://dl.acm.org/doi/abs/10.1145/3721251.3734066	Ji Yuan Feng, Rishabh Chadha
Digitizing Devotion: Virtual Religious Spaces for Cultural Preservation and Transmission	"Digitizing Devotion"" utilizes advanced oblique photography and AI to create immersive virtual reconstructions of sacred spaces, preserving traditional worship practices for the global diaspora while ensuring cultural continuity across generations and geographical boundaries."	https://dl.acm.org/doi/abs/10.1145/3721250.3742966	Kuo Zhang, Zhiqi Gao, Shuai Zhang, Mingrui He, Shuochen Zhao, Mengyao Guo
Directing Cloth Draping through Blended UVs	Cloth draping is a prevalent tailoring process that gives 3D form to sewn 2D panels of fabric. However, when dressing animated characters, artists often prefer to model garments with delineated spatial structures and clean silhouettes at the cost of diminishing the presence of folds and wrinkles produced by draping. To reconcile stylization and realism, this work describes a new approach for directing cloth draping that accommodates 3D shaping and 2D pattern making simultaneously. Our key contribution is a method that generates custom UVs blending the distortion induced by 3D shapes into 2D fabric panels. As a result, we can retarget cloth simulations to compute physically plausible draping deformations that smoothly transition to prescribed 3D forms. To assist the garment design, we also propose a flattening tool that constructs low-resolution UV panels amenable to 2D manipulation. We showcase our results with a series of garment assets and cloth animations from Pixar feature films (2024) and (2025).	https://dl.acm.org/doi/abs/10.1145/3721239.3734074	Juan Carlos Olmos Guerra, Fernando de Goes, Christine Waggoner, David Eberle
Disentangled Phoneme-Prosody Mapping for Controllable 3D Facial Animation	We introduce a two-stage pipeline that gives artists fine-grained, input-level control of audio-driven 3D facial animation. Stage 1 learns a latent relative-motion prior from neutral/offset position maps, confining deformations to realistic shapes. Stage 2 projects an explainable phoneme–prosody vector into this space, so visemes and expressions are editable in feature space. Early experiments show preserved lip-sync and natural motion, narrowing the gap between fidelity and control.	https://dl.acm.org/doi/abs/10.1145/3721250.3743011	Danzel Serrano, Przemyslaw Musialski
DiversePuppetry: An Immersive Multi-User Puppetry System Based on Asymmetric Interaction	This work presents DiversePuppetry, an immersive and asymmetric puppetry interaction system that integrates a virtual reality head-mounted display (VR-HMD), a mixed reality head-mounted display (MR-HMD), and a CAVE Automatic Virtual Environment (VR-CAVE). In this project, traditional Taiwanese Budaixi puppets were digitized and incorporated into diverse forms of immersive experiences. This study explores an interactive and immersive platform for puppetry through multiple modes of control. The findings highlight the potential of asymmetric immersive interaction, offering puppetry culture a novel way of creating a complete and immersive digital experience.	https://dl.acm.org/doi/abs/10.1145/3721250.3742973	Chun-Cheng Hsu, Wei-Chen Yen, Jen-Kai Liu, Ping-Hsuan Han
Divide-and-Conquer Embedding	We propose an exact method for embedding a disk-topology triangular mesh onto any convex polygon. The method employs a divide-and-conquer approach, iteratively decomposing the embedding problem into smaller sub-problems that map sub-meshes to convex sub-polygons. The process continues until each triangle in the mesh is naturally embedded into a corresponding 3-sided polygon. The approach is supported by a constructive proof, ensuring its theoretical validity. We translate this proof into a practical algorithm, incorporating various dividing strategies and interpolation weights. Unlike previous methods, our approach preserves the connectivity of the input mesh throughout the embedding process. Extensive experiments demonstrate the efficiency and effectiveness of the proposed method.	https://dl.acm.org/doi/abs/10.1145/3721238.3730693	Yuan-Yuan Cheng, Qing Fang, Ligang Liu, Xiao-Ming Fu
Doodle: A Modular Toolset for 2D Artistic Control in the 3D Effects of The Wild Robot and The Bad Guys 2	Recent films at DreamWorks Animation, such as and , have pursued ambitiously illustrative styles, requiring FX with an emphasis on painterly simplification and 2D inspired silhouette and timing, that also maintain a 3D sophistication of scale, detail and rendering. To meet the demanding art direction of these films, while contending with the challenges of 3D Lighting integration and ever-changing cameras in a 3D feature animation pipeline, we developed a suite of tools to let artists creatively mix simulation with drawing all from within a modern node-based FX software package. The collection of tools, called , enable artists to use traditional painting and sculpting techniques in 3D space, procedurally manipulate and simulate the resulting geometry, and compose these hand-drawn elements into reusable 3D sprites for aggregate effects.	https://dl.acm.org/doi/abs/10.1145/3721239.3734096	Michael Losure
Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation for 3D Gaussian Splatting	Recent advancements in generative models have significantly propelled 3D scene editing. While existing methods excel at text-guided texture modifications for 3D representations like 3D Gaussian Splatting (3DGS), they struggle with geometric transformations (e.g., rotating a character's head) and lack precise spatial control over edits due to the inherent ambiguity of language-driven guidance. To address these limitations, we introduce DYG, a 3D drag-based editing framework for 3DGS. Users intuitively define editing regions using 3D masks and specify desired transformations through pairs of control points. DYG integrates the implicit triplane representation to establish the geometric scaffold of editing results, effectively overcoming suboptimal editing outcomes caused by the sparsity of 3DGS in the desired editing regions. Additionally, we incorporate a drag-based Latent Diffusion Model through the proposed Drag-SDS loss, enabling flexible, multi-view consistent, and fine-grained editing. Extensive experiments demonstrate that DYG enables effective drag-based editing, outperforming other baselines in terms of editing effect and quality. Additional results are available on our project page: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730600	Yansong Qu, Dian Chen, Xinyang Li, Xiaofan Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji
Drawing with Light: AI-Driven Visual Synthesis for Real-Time Laser Installations	This paper presents a real-time system combining AI image generation with laser installations for live audiovisual performance. Using StreamDiffusion integrated with TouchDesigner, we generate imagery and convert it to laser-traceable paths while working within graphics hardware and laser display constraints. The system includes feedback loops between AI generation, performer control, and physical laser output, creating an interactive visual instrument for live musical performance.	https://dl.acm.org/doi/abs/10.1145/3721243.3735988	Spencer Sterling
DreamCraft: Interactive 3D Scene Creation from Editable Panorama in Virtual Reality	Creating interactive 3D scenes often requires technical expertise and significant time, limiting accessibility for non-experts. To address this, we present DreamCraft, a VR system enabling users to intuitively generate and edit interactive 3D environments from panoramas without professional skills. DreamCraft supports panorama generation, interactive object selection, panorama editing, and 3D reconstruction. By combining techniques like 3D Gaussian Splatting (3DGS), object segmentation, and 2D-to-3D conversion, it streamlines immersive scene creation. A user study confirmed its usability, ease of learning, and creative potential, positioning DreamCraft as a step toward accessible 3D content creation.	https://dl.acm.org/doi/abs/10.1145/3721250.3743026	Cheng-Chih Tsai, Tse-Yu Pan
DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data	Open-vocabulary panoptic segmentation has received significant attention due to its applicability in the real world. Despite claims of robust generalization, we find that the advancements of previous works are attributed mainly to trained categories, exposing a lack of generalization to novel classes. In this paper, we explore boosting existing models from a data-centric perspective. We propose DreamMask, which systematically explores how to generate training data in the open-vocabulary setting, and how to train the model with both real and synthetic data. For the first part, we propose an automatic data generation pipeline with off-the-shelf models. We propose crucial designs for vocabulary expansion, layout arrangement, data filtering, etc. Equipped with these techniques, our generated data could significantly outperform the manually collected web data. To train the model with generated data, a synthetic-real alignment loss is designed to bridge the representation gap, bringing noticeable improvements across multiple benchmarks. In general, DreamMask significantly simplifies the collection of large-scale training data, serving as a plug-and-play enhancement for existing methods. For instance, when trained on COCO and tested on ADE20K, the model equipped with DreamMask outperforms the previous state-of-the-art by a substantial margin of 2.1% mIoU.	https://dl.acm.org/doi/abs/10.1145/3721238.3730684	Yuanpeng Tu, Xi Chen, Ser-Nam Lim, Hengshuang Zhao
Dual-Band Feature Fusion for Neural Global Illumination with Multi-Frequency Reflections	In this paper, we present a novel neural global illumination approach that enables multi-frequency reflections in dynamic scenes. Our method utilizes object-centric, spatial feature grids as the core framework to model rendering effects implicitly. A lightweight scene query, based on single-bounce ray tracing, is then performed on these feature grids to extract principal and secondary features separately. The principal features can capture a wide range of relatively low-frequency global illumination effects, such as diffuse indirect lighting and reflections on rough surfaces. In contrast, the secondary features can provide sparse scene-specific reflection details, typically with much higher frequencies than the final observed radiance. Inspired by the physical processes of light propagation, we introduce a novel dual-band feature fusion module that seamlessly blends these two types of features, generating fused features capable of modeling multi-frequency rendering effects. Additionally, we propose a two-stage training strategy tailored to accommodate the distinct characteristics of each feature type, significantly enhancing the overall quality and reducing artifacts in the rendered results. Experimental results demonstrate that our method delivers high-quality, multi-frequency dynamic reflections, outperforming state-of-the-art baselines, including path tracing with screen-space neural denoising and other neural global illumination methods.	https://dl.acm.org/doi/abs/10.1145/3721238.3730733	Shaohua Mo, Chuankun Zheng, Zihao Lin, Dianbing Xi, Qi Ye, Rui Wang, Hujun Bao, Yuchi Huo
DualMS: Implicit Dual-Channel Minimal Surface Optimization for Heat Exchanger Design	Heat exchangers are critical components in a wide range of engineering applications, from energy systems to chemical processing, where efficient thermal management is essential. The design objectives for heat exchangers include maximizing the heat exchange rate while minimizing the pressure drop, requiring both a large interface area and a smooth internal structure. State-of-the-art designs, such as triply periodic minimal surfaces (TPMS), have proven effective in optimizing heat exchange efficiency. However, TPMS designs are constrained by predefined mathematical equations, limiting their adaptability to freeform boundary shapes. Additionally, TPMS structures do not inherently control flow directions, which can lead to flow stagnation and undesirable pressure drops. This paper presents , a novel computational framework for optimizing dual-channel minimal surfaces specifically for heat exchanger designs in freeform shapes. To the best of our knowledge, this is the first attempt to directly optimize minimal surfaces for two-fluid heat exchangers, rather than relying on TPMS. Our approach formulates the heat exchange maximization problem as a constrained connected maximum cut problem on a graph, with flow constraints guiding the optimization process. To address undesirable pressure drops, we model the minimal surface as a classification boundary separating the two fluids, incorporating an additional regularization term for area minimization. We employ a neural network that maps spatial points to binary flow types, enabling it to classify flow skeletons and automatically determine the surface boundary. DualMS demonstrates greater flexibility in surface topology compared to TPMS and achieves superior thermal performance, with lower pressure drops while maintaining a similar heat exchange rate under the same material cost. The project is open-sourced at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730700	Weizheng Zhang, Hao Pan, Lin Lu, Xiaowei Duan, Xin Yan, Ruonan Wang, Qiang Du
DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling	We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination. Code and model weights are available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730741	Anindita Ghosh, Bing Zhou, Rishabh Dabral, Jian Wang, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek, Chuan Guo
Dust in Time: Exploring Embodied Experience of Time via an Interactive Installation	This paper introduces , a tangible and embodied art installation that allows the audience to interact with physical hourglasses and virtual particles through embodied gestures and motions. We describe the design concept and technical details of this installation. Through this conceptual tangible interactive installation, we aim to explore how tangible and embodied interaction can be used to represent the concepts of time and its relationship with human beings and promote both explicit and implicit interaction.	https://dl.acm.org/doi/abs/10.1145/3721250.3743021	Zhonghe Ruan, Junwei Liu, Min Fan, Haiyan Li
Dynamic Concepts Personalization from Single Videos	Personalizing generative text-to-image models has seen remarkable progress, but extending this personalization to text-to-video models presents unique challenges. Unlike static concepts, personalizing text-to-video models has the potential to capture dynamic concepts – entities defined not only by their appearance but also by their motion.In this paper, we introduce Set-and-Sequence, a novel framework for personalizing Diffusion Transformers (DiTs)–based generative video models with dynamic concepts. Our approach imposes a spatio-temporal weight space within an architecture that does not explicitly separate spatial and temporal features. This is achieved in two key stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an unordered set of frames from the video to learn an appearance LoRA basis that represents the appearance, free from temporal interference. In the second stage, with the appearance LoRAs frozen, we augment their coefficients with Motion Residuals and fine-tune them on the full video sequence, capturing motion dynamics. Our Set-and-Sequence framework resulting in a spatio-temporal weight space effectively embeds dynamic concepts into the video model's output domain, enabling unprecedented editability and compositionality, and setting a new benchmark for personalizing dynamic concepts.	https://dl.acm.org/doi/abs/10.1145/3721238.3730644	Rameen Abdal, Or Patashnik, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, Kfir Aberman
Dynamic Skinning: Kinematics-Driven Cartoon Effects for Articulated Characters	We propose Dynamic Skinning (DS), an extension of rig skinning which exhibits the appearance of a physical phenomena without the need for simulation. Our approach applies offsets from traditional skinning to produce these effects based on time-delayed, filtered joint motion. We showcase a number of effects including 1) time-varying oscillation and 2) time delay across skeletal bones to produce what we call delayed linear blend skinning (dLBS) directly on skinning computation. Our approach is easy to control by artists with simple input parameters and the method is compatible with standard rigged characters.	https://dl.acm.org/doi/abs/10.1145/3721250.3743004	Karim Salem, Damien Rohmer, Niranjan Kalyanasundaram, Victor Zordan
ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition	The art of instrument performance stands as a vivid manifestation of human creativity and emotion. Nonetheless, generating instrument performance motions is a highly challenging task, as it requires not only capturing intricate movements but also reconstructing the complex dynamics of the performer-instrument interaction. While existing works primarily focus on modeling partial body motions, we propose Expressive ceLlo performance motion Generation for Audio Rendition (ELGAR), a state-of-the-art diffusion-based framework for whole-body fine-grained instrument performance motion generation solely from audio. To emphasize the interactive nature of the instrument performance, we introduce Hand Interactive Contact Loss (HICL) and Bow Interactive Contact Loss (BICL), which effectively guarantee the authenticity of the interplay. Moreover, to better evaluate whether the generated motions align with the semantic context of the music audio, we design novel metrics specifically for string instrument performance motion generation, including finger-contact distance, bow-string distance, and bowing score. Extensive evaluations and ablation studies are conducted to validate the efficacy of the proposed methods. In addition, we put forward a motion generation dataset SPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated, ELGAR has shown great potential in generating instrument performance motions with complicated and fast interactions, which will promote further development in areas such as animation, music education, interactive art creation, etc. Our code and SPD-GEN dataset are available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730756	Zhiping Qiu, Yitong Jin, Yuan Wang, Yi Shi, Chao Tan, Chongwu Wang, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai
ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content Generation with Gaussian Splatting	In recent years, significant advancements have been made in text-driven 3D content generation. However, several challenges remain. In practical applications, users often provide extremely simple text inputs while expecting high-quality 3D content. Generating optimal results from such minimal text is a difficult task due to the strong dependency of text-to-3D models on the quality of input prompts. Moreover, the generation process exhibits high variability, making it difficult to control. Consequently, multiple iterations are typically required to produce content that meets user expectations, reducing generation efficiency. To address this issue, we propose GPT-4V for self-optimization, which significantly enhances the efficiency of generating satisfactory content in a single attempt. Furthermore, the controllability of text-to-3D generation methods has not been fully explored. Our approach enables users to not only provide textual descriptions but also specify additional conditions, such as style, edges, scribbles, poses, or combinations of multiple conditions, allowing for more precise control over the generated 3D content. Additionally, during training, we effectively integrate multi-view information, including multi-view depth, masks, features, and images, to address the common Janus problem in 3D content generation. Extensive experiments demonstrate that our method achieves robust generalization, facilitating the efficient and controllable generation of high-quality 3D content.	https://dl.acm.org/doi/abs/10.1145/3728305	Huiqi Wu, Li Yao, Jianbo Mei, Yingjie Huang, Yining Xu, Jingjiao You, Yilong Liu
EVA: Expressive Virtual Avatars from Multi-view Videos	With recent advancements in neural rendering and motion capture algorithms, remarkable progress has been made in photorealistic human avatar modeling, unlocking immense potential for applications in virtual reality, augmented reality, remote communication, and industries such as gaming, film, and medicine. However, existing methods fail to provide complete, faithful, and expressive control over human avatars due to their entangled representation of facial expressions and body movements. In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real time while enabling independent control of facial expressions, body movements, and hand gestures. Specifically, our approach designs the human avatar as a two-layer model: an and a . First, we present an expressive template tracking algorithm that leverages coarse-to-fine optimization to accurately recover body motions, facial expressions, and non-rigid deformation parameters from multi-view videos. Next, we propose a novel decoupled 3D Gaussian appearance model designed to effectively disentangle body and facial appearance. Unlike unified Gaussian estimation approaches, our method employs two specialized and independent modules to model the body and face separately. Experimental results demonstrate that EVA surpasses state-of-the-art methods in terms of rendering quality and expressiveness, validating its effectiveness in creating full-body avatars. This work represents a significant advancement towards fully drivable digital human models, enabling the creation of lifelike digital avatars that faithfully replicate human geometry and appearance.	https://dl.acm.org/doi/abs/10.1145/3721238.3730677	Hendrik Junkawitsch, Guoxing Sun, Heming Zhu, Christian Theobalt, Marc Habermann
EditDuet: A Multi-Agent System for Video Non-Linear Editing	Automated tools for video editing and assembly have applications ranging from filmmaking and advertisement to content creation for social media. Previous video editing work has mainly focused on either retrieval or user interfaces, leaving actual editing to the user. In contrast, we propose to automate the core task of video editing, formulating it as sequential decision making process. Ours is a multi-agent approach. We design an Editor agent and a Critic agent. The Editor takes as input a collection of video clips together with natural language instructions and uses tools commonly found in video editing software to produce an edited sequence. On the other hand, the Critic gives natural language feedback to the editor based on the produced sequence or renders it if it is satisfactory. We introduce a learning-based approach for enabling effective communication across specialized agents to address the language-driven video editing task. Finally, we explore an LLM-as-a-judge metric for evaluating the quality of video editing system and compare it with general human preference. We evaluate our system's output video sequences qualitatively and quantitatively through a user study and find that our system vastly outperforms existing approaches in terms of coverage, time constraint satisfaction, and human preference. Please see our companion supplemental video for qualitative results.	https://dl.acm.org/doi/abs/10.1145/3721238.3730761	Marcelo Sandoval-Castañeda, Bryan Russell, Josef Sivic, Gregory Shakhnarovich, Fabian Caba Heilbron
Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations	Ray tracing is a widely used technique for modeling optical systems, involving sequential surface-by-surface computations which can be computationally intensive. We propose , a novel method that leverages implicit neural representations to model optical systems with greater efficiency, eliminating the need for surface-by-surface computations in a single pass end-to-end model. learns the mapping between rays emitted from a given source and their corresponding rays after passing through a given optical system in a physically accurate manner. We train on nine off-the-shelf optical systems, acheiving positional errors on the order of 1 m and angular deviations on the order 0.01 degrees in the estimated output rays. Our work highlights the potentials of neural representations as a proxy optical raytracer.	https://dl.acm.org/doi/abs/10.1145/3721250.3742994	Shiva Sinaei, Chuanjun Zheng, Kaan Akşit, Daisuke Iwai
Eigenanalysis in Computer Graphics	Like a semester long graduate seminar on Eigenanalysis, Singular Value Decompositions, and Principal Component Analysis in Computer Graphics and Interactive Techniques, this course looks at matrix diagonalization and analysis through the lens of 13 technical papers selected by the lecturers. The lecturers will highlight trends, similarities, differences, and historical threads through the papers. Applications will range from image segmentation, to fast, robust phsyics simulation, to geometric modeling, to BRDF modeling. Note that we slightly abuse the term Eigenanalysis to include its generalizations, Singular Value Decomposition and Principal Component Analysis, as all three techniques rely on matrix diagonalization. The course will also serve as a retrospective on the selected papers, placing them in historical context and highlighting significant contributions, as well as forgotten gems. The lecturers have broad expertise across computer graphics and interactive techniques and have co-led the VANGOGH lab meeting at UMBC since 2015. They delivered a similarly structured course on at SIGGRAPH 2024.	https://dl.acm.org/doi/abs/10.1145/3721241.3733993	Adam Bargteil, Marc Olano
Elastic Locomotion with Mixed Second-order Differentiation	We present a framework of elastic locomotion, which allows users to enliven an elastic body to produce interesting locomotion by prescribing its high-level kinematics. We formulate this problem as an inverse simulation problem and seek the optimal muscle activations to drive the body to complete the desired actions. We employ the interior-point method to model wide-area contacts between the body and the environment with logarithmic barrier penalties. The core of our framework is a mixed second-order differentiation algorithm. By combining both analytic differentiation and numerical differentiation modalities, a general-purpose second-order differentiation scheme is made possible. Specifically, we augment complex-step finite difference (CSFD) with reverse automatic differentiation (AD). We treat AD as a generic function, mapping a computing procedure to its derivative w.r.t. output loss, and promote CSFD along the AD computation. To this end, we carefully implement all the arithmetics used in elastic locomotion, from elementary functions to linear algebra and matrix operation for CSFD promotion. With this novel differentiation tool, elastic locomotion can directly exploit Newton's method and use its strong second-order convergence to find the needed activations at muscle fibers. This is not possible with existing first-order inverse or differentiable simulation techniques. We showcase a wide range of interesting locomotions of soft bodies and creatures to validate our method.	https://dl.acm.org/doi/abs/10.1145/3721238.3730685	Siyuan Shen, Tianjia Shao, Kun Zhou, Chenfanfu Jiang, Sheldon Andrews, Victor Zordan, Yin Yang
Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model	High-quality 3D assets are essential for various applications in computer graphics and 3D vision but remain scarce due to significant acquisition costs. To address this shortage, we introduce Elevate3D, a novel framework that transforms readily accessible low-quality 3D assets into higher quality. At the core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that significantly improves texture quality while preserving the appearance and geometry while fixing its degradations. Furthermore, Elevate3D operates in a view-by-view manner, alternating between texture and geometry refinement. Unlike previous methods that have largely overlooked geometry refinement, our framework leverages geometric cues from images refined with HFS-SDEdit by employing state-of-the-art monocular geometry predictors. This approach ensures detailed and accurate geometry that aligns seamlessly with the enhanced texture. Elevate3D outperforms recent competitors by achieving state-of-the-art quality in 3D model refinement, effectively addressing the scarcity of high-quality open-source 3D assets.	https://dl.acm.org/doi/abs/10.1145/3721238.3730701	Nuri Ryu, Jiyun Won, Jooeun Son, Minsu Gong, Joo-Haeng Lee, Sunghyun Cho
Embracing Greener Graphics Practices in Call of Duty®	This work emphasizes the environmental impact of gaming, highlighting the energy consumption of retail devices and the carbon footprint of gameplay across several titles, with a focus on the Call of Duty ® gaming franchise. It presents practical steps that real-time graphics developers can take to reduce energy consumption in a device-agnostic manner. Care is taken to ensure this effort does not have an adverse effect on gameplay standards or business requirements. Each step taken is backed with appropriate metrics, including feedback validation across the gaming retail landscape.	https://dl.acm.org/doi/abs/10.1145/3721239.3734071	Rulon Raymond
End Of Summer	A boy navigates fleeting a childhood friendship and discovers his own queerness across three pivotal summers in 1990s southern China.	https://dl.acm.org/doi/abs/10.1145/3698896.3725425	Haojun Huang, Xiang Wang, Lisa-Marie Preston, Ben Xubin Liang
Enhancing the Precision of a User's Hand When Sliding on Objects in VR	Interaction in virtual reality (VR) remains a major research challenge, due to the difficulty of accurately reproducing a user's position, the differences that exist between the virtual and real worlds, and the need to ensure a faster response than in traditional computer systems. Our work focuses on software-based approaches that support the user's hand running over VR objects. Existing constraint-based methods implemented in game engines result in visible glitches which negatively affect the user's immersion. To mitigate this, we have developed an easy-to-implement human-object interface based on auxiliary projection shapes. Our method, like other software-centered solutions, does not require any additional advanced haptic hardware; it relies solely on a standard VR headset (HMD) with controllers. The evaluation performed confirmed that our approach allows smooth motion to be attained in a 3D scene without adding additional latency to the VR system. Furthermore, for different projection shapes, different hand behavior on the same 3D object is obtained. As a result, scene designers are offered a simple visual mechanism to manipulate hand motion on 3D objects' surfaces, enhancing its precision while sliding to create more desirable hand trajectories.	https://dl.acm.org/doi/abs/10.1145/3728307	Anna Kozłowska, Mikołaj Wójcik, Maciej Spychała, Joanna Porter-Sobieraj, Jakub Ciecierski
Escher Tile Deformation via Closed-Form Solution	We present a real-time deformation method for Escher tiles—interlocking organic forms that seamlessly tessellate the plane following symmetry rules. We formulate the problem as determining a periodic displacement field. The goal is to deform Escher tiles without introducing gaps or overlaps. The resulting displacement field is obtained in closed form by an analytical solution. Our method processes tiles of 17 wallpaper groups across various representations such as images and meshes. Rather than treating tiles as mere boundaries, we consider them as textured shapes, ensuring that both the boundary and interior deform simultaneously. To enable fine-grained artistic input, our interactive tool features a user-controllable adaptive fall-off parameter, allowing precise adjustment of locality and supporting deformations with meaningful semantic control. We demonstrate the effectiveness of our method through various examples, including photo editing and shape sculpting, showing its use in applications such as fabrication and animation.	https://dl.acm.org/doi/abs/10.1145/3721238.3730681	Crane He Chen, Vladimir Kim
Evaluating Skin Tone Biases in Virtual Human Rendering	We evaluate skin tone bias in a real-time rendering engine using 80 MetaHumans covering all 10 levels of the Monk Skin Tone (MST) scale. Two color pipelines are compared: MST-RS, which uses standard RGB reference swatches, and MST-CS, based on cheek-sampled RGB values from real photographs. We apply patch-based metrics, median RGB intensity. MST-RS exhibits a smooth, monotonic RGB decline from MST 1 to 10, while MST-CS reveals geometry-sensitive, non-linear variations and gamut compression in darker tones. These differences highlight potential rendering biases and support the need for tone-aware shader validation.	https://dl.acm.org/doi/abs/10.1145/3721250.3743031	Erick Menezes, Helena Leal, JoÃO VÍTor Moura, Victor Araujo, Soraia Raupp Musse
Evaluating the Effectiveness of Configurable Virtual Reality System for Multi-sensory Spatial Audio Training	Enhanced Auditory Reality Simulation for Improved Mapping (EARSIM) is a stand-alone, virtual reality (VR) application that procedurally configures a multi-sensory cue system to deliver adaptive auditory localization tasks. In a pilot study, twenty-one participants completed three 40-second sessions with progressively increasing sensory cues. Median localization accuracy decreased monotonically as the number of cues increased, suggesting that the dynamic cue system was effective in modulating task difficulty. These results validate EARSIM as a configurable platform and its potential for future clinical applications in auditory rehabilitation.	https://dl.acm.org/doi/abs/10.1145/3721250.3743041	Sinnie Choi, Delsther James Edralin, Eric Tang, Mark Harmon, Julien Roy
Exploring AI Frame Interpolation Techniques for Watercolour Animation	The animated short film explores a novel workflow for hand-painted watercolor animation, blending traditional artistic methods with AI-based frame interpolation techniques. By combining compositing with the Real-Time Intermediate Flow Estimation (RIFE) image interpolation network, we significantly reduced production time while maintaining the unique hand-painted aesthetic.	https://dl.acm.org/doi/abs/10.1145/3721250.3742971	Tanja Nuijten, Hannes Sturm, Vincent Maurer, Avina Graefe
Exploring Distance Management in Immersive Combat Sports Training with Encountered-type Haptic Feedback	Distance management is a crucial component of immersive combat sports training. However, limited research has explored the distance management in virtual reality (VR) combat training. Our preliminary study invited professional boxers to engage with a VR combat training system, aiming to evaluate the effects of encountered-type haptic feedback via tracking system. The results indicate that haptic feedback led to shorter punch distances and a lower movement ratio. However, no significant differences were observed in step count or the average distance to the opponent. These findings suggest that haptic feedback supports more efficient distance management, allowing users to move less while maintaining effective positioning.	https://dl.acm.org/doi/abs/10.1145/3721250.3743005	Yen-Hua Lai, Chieh-Hsin Liu, Yu-Hsiang Weng, Ping-Hsuan Han, Chien-Hsing Chou, Wen-Hsin Chiu
Exploring Real-Time Water Surface Simulation for Immersive Virtual Reality Using Marker-Based Tracking	Although real-time fluid simulation in virtual environments has been widely explored, existing systems often rely on virtual models and predefined parameters, limiting their ability to capture the complexity of physical water flow. To address this, we propose a marker-based VR system that simulates water surface dynamics by tracking real-world water flow using ArUco markers. The system analyzes floating marker trajectories to generate a FlowMap, which is applied to a virtual water surface in Unity for real-time flow simulation. A controllable circular pool with water-jet units was used to create varying flow conditions, and computer vision techniques converted the data into directional vector fields. The FlowMap is continuously updated and interpolated to reduce visual lag. We implemented the prototype in a VR environment and verified the accuracy of the generated flow patterns. Results demonstrate the potential of this sensor-driven approach for realistic water simulations in immersive VR.	https://dl.acm.org/doi/abs/10.1145/3721250.3742995	Li-En Lai, Chi-Yu Lin, Tse-Yu Pan, Ping-Hsuan Han
Exploring the Novel Real-Time Pipeline of Nickelodeon's Max & the Midknights	Come hear about the creation of Nickelodeon's hit new animated action/adventure series Max & the Midknights! The show's Supervising Producer and CG Supervisor, along with production partner Xentrix's Head of Pipeline and Creative Director, will share a behind-the-scenes look at the challenges they faced creating this ambitious cinematic CG show with its hand-made and stop-motion look and feel.	https://dl.acm.org/doi/abs/10.1145/3698897.3718367	Chris Perry, Sica Von Medicus, Abhishek Kumar, Ushma Pandya
"Fabricating Stop Motion Creatures Using CG Grooms: A Hybrid Approach for LAIKA's ""Wildwood"": A Hybrid Approach for LAIKA's ""Wildwood"""	"Our groundbreaking groom pipeline for LAIKA's ""Wildwood"" revolutionizes stop-motion puppet fabrication through CG-assisted silicone casting. By leveraging the VFX team's 3D models and digital grooms, we were able to scale to the needs of this epic feature film, while providing anisotropic characteristics and enabling seamless integration between practical and digital elements."	https://dl.acm.org/doi/abs/10.1145/3721239.3734073	Micah Henrie
FaceExpressions-70k: A Dataset of Perceived Expression Differences	Facial expressions are key to human communication, conveying emotions and intentions. Given the rising popularity of digital humans and avatars, the ability to accurately represent facial expressions in real time has become an important topic. However, quantifying perceived differences between pairs of expressions is difficult, and no comprehensive subjective datasets are available for testing. This work introduces a new dataset targeting this problem: FaceExpressions-70k. Obtained via crowdsourcing, our dataset contains 70,500 subjective expression comparisons rated by over 1,000 study participants We demonstrate the applicability of the dataset for training perceptual expression difference models and guiding decisions on acceptable latency and sampling rates for facial expressions when driving a face avatar.	https://dl.acm.org/doi/abs/10.1145/3721238.3730653	Avinab Saha, Yu-Chih Chen, Jean-Charles Bazin, Christian Häne, Ioannis Katsavounidis, Alexandre Chapiro, Alan Bovik
Facial Microscopic Structures Synthesis from a Single Unconstrained Image	Obtaining 3D faces with microscopic structures from a single unconstrained image is challenging. The complexities of wrinkles and pores at a microscopic level, coupled with the blurriness of the input image, raise the difficulty. However, the distribution of wrinkles and pores tends to follow a specialized pattern, which can provide a strong prior for synthesizing them. Therefore, a key to microstructure synthesis is a parametric wrinkles and pore model with controllable semantic parameters. Additionally, ensuring differentiability is essential for enabling optimization through gradient descent methods. To this end, we propose a novel framework designed to reconstruct facial micro-wrinkles and pores from naturally captured images efficiently. At the core of our framework is a differentiable representation of wrinkles and pores via a graph neural network (GNN), which can simulate the complex interactions between adjacent wrinkles by multiple graph convolutions. Furthermore, to overcome the problem of inconsistency between the blurry input and clear wrinkles during optimization, we proposed a Direction Distribution Similarity that ensures that the wrinkle-directional features remain consistent. Consequently, our framework can synthesize facial micro-structures from a blurry skin image patch, which is cropped from a natural-captured facial image, in around an average of 2 seconds. Our framework can seamlessly integrate with existing macroscopic facial detail reconstruction methods to enhance their detailed appearance. We showcase this capability on several works, including DECA, HRN, and FaceScape.	https://dl.acm.org/doi/abs/10.1145/3721238.3730760	Youyang Du, Lu Wang, Beibei Wang
Farewell	Larry, a man in his thirties, makes one final attempt to save his father from alcoholism, even at the risk of his own downfall.	https://dl.acm.org/doi/abs/10.1145/3698896.3719633	Juliette Berthe, Samuel Perouse de Montclos, Viviane Sollacaro, Maud Cuenot, Anne-Sarah Reiniche
FashionComposer: Compositional Fashion Image Generation	"We present FashionComposer for compositional fashion image generation. Unlike previous methods, FashionComposer is highly flexible. It takes multi-modal input ( text prompt, parametric human model, garment image, and face image) and supports personalizing the appearance, pose, and figure of the human and assigning multiple garments in one pass. To achieve this, we first develop a universal framework capable of handling diverse input modalities. We construct scaled training data to enhance the model's robust compositional capabilities. To accommodate multiple reference images (garments and faces) seamlessly, we organize these references in a single image as an ""asset library"" and employ a reference UNet [Hu et al. ] to extract appearance features. To inject the appearance features into the correct pixels in the generated result, we propose subject-binding attention. It binds the appearance features from different ""assets"" with the corresponding text features. In this way, the model could understand each asset according to their semantics, supporting arbitrary numbers and types of reference images. As a comprehensive solution, FashionComposer also supports many other applications like human album generation, diverse virtual try-on tasks, etc."	https://dl.acm.org/doi/abs/10.1145/3721238.3730663	Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, Hengshuang Zhao
Fast Fluid Up-Res	We present a production-proven method for up-resing volumetric fluid simulations, allowing creation of high-resolution results efficiently from fast-to-compute, low resolution sims. Existing approaches often rely on secondary simulation and supplementary turbulence to inject missing details at higher resolutions. At Industrial Light and Magic (ILM), we have devised a simpler approach, one that is frame independent and therefore parallelisable across frames. Our combination of gridless advection, anti-aliased sampling and camera-frustum aligned grids has been used with great success on many recent productions and is a mainstay of our fluids toolkit.	https://dl.acm.org/doi/abs/10.1145/3721239.3734123	Xavier Martin Ramirez, Richard Jones
Fast Isotropic Median Filtering	Median filtering is a cornerstone of computational image processing. It provides an effective means of image smoothing, with minimal blurring or softening of edges, invariance to monotonic transformations such as gamma adjustment, and robustness to noise and outliers. However, known algorithms have all suffered from practical limitations: the bit depth of the image data, the size of the filter kernel, or the kernel shape itself. Square-kernel implementations tend to produce streaky cross-hatching artifacts, and nearly all known efficient algorithms are in practice limited to square kernels. We present for the first time a method that overcomes all of these limitations. Our method operates efficiently on arbitrary bit-depth data, arbitrary kernel sizes, and arbitrary convex kernel shapes, including circular shapes.	https://dl.acm.org/doi/abs/10.1145/3721238.3730763	Ben Weiss
Fast Physics-Based Modeling of Knots and Ties using Templates	Knots and ties are captivating elements of digital garments and accessories, but they have been notoriously challenging and computationally expensive to model manually. In this paper, we propose a physics-based modeling system for knots and ties using templates. The primary challenge lies in transforming cloth pieces into desired knot and tie configurations in a controllable, penetration-free manner, particularly when interacting with surrounding meshes. To address this, we introduce a pipe-like parametric knot template representation, defined by a Bézier curve as its medial axis and an adaptively adjustable radius for enhanced flexibility and variation. This representation enables customizable knot sizes, shapes, and styles while ensuring intersection-free results through robust collision detection techniques. Using the defined knot template, we present a mapping and penetration-free initialization method to transform selected cloth regions from UV space into the initial 3D knot shape. We further enable quasistatic simulation of knots and their surrounding meshes through a fast and reliable collision handling and simulation scheme. Our experiments demonstrate the system's effectiveness and efficiency in modeling a wide range of digital knots and ties with diverse styles and shapes, including configurations that were previously impractical to create manually.	https://dl.acm.org/doi/abs/10.1145/3721238.3730622	Dewen Guo, Zhendong Wang, Zegao Liu, Sheng Li, Guoping Wang, Yin Yang, Huamin Wang
Feature-Preserving Mesh Repair via Restricted Power Diagram	Mesh repair is a critical process in 3D geometry processing aimed at correcting errors and imperfections in polygonal meshes to produce watertight, manifold, and feature-preserving meshes suitable for downstream tasks. While errors such as degeneracies, duplication, holes, and overlaps can be addressed through standard repair processes, cracks along trimmed curves require special attention and should ideally be repaired to align with sharp feature lines. In this paper, we present a unified framework for repairing diverse mesh imperfections by leveraging a manifold wrap surface as a mediating agent. The primary role of the wrap surface is to define spatial connections between points on the original surface, thereby decoupling the challenges of edge connectivity and point relocation during repair. Throughout the process, our algorithm operates on the dual objects: the original defective mesh and the manifold wrap surface. The implementation begins by extracting a set of samples from the wrap surface and projecting them onto the original surface. These projected samples are optimized by minimizing the quadratic error relative to the tangent planes of neighboring points on the original surface. Notably, samples far from feature lines remain unchanged, while samples near feature lines converge to those lines even when the input surface lacks correct mesh topology. We then assign an adaptive weight to each sample based on the squared moving distance. By introducing this weight setting, we observe that the restricted power diagram prioritizes connectivity along feature lines, thereby effectively preserving sharp features. Through extensive experiments, we demonstrate the superiority of our proposed algorithm over existing methodologies in terms of manifoldness, watertightness, topological correctness, triangle quality, and feature preservation.	https://dl.acm.org/doi/abs/10.1145/3721238.3730671	Huibiao Wen, Guilong He, Rui Xu, Shuangmin Chen, Shiqing Xin, Zhenyu Shu, Taku Komura, Jieqing Feng, Wenping Wang, Changhe Tu
Feeling Blue or Seeing Red? Investigating the effect of light color, shadow and realism on the perception of emotion of real and virtual humans	Cinematic lighting is a powerful tool used in film-making to create a mood or atmosphere and to influence the audience's perception and emotional response to a scene. For example, red can be used to increase feelings of anxiety or excitement, while blue might have a more calming effect. These responses can be harnessed to enhance the storytelling. Previous studies in Psychology have shown that light color has a direct impact on the perception of emotions and feelings. However, there is a lack of controlled empirical studies for understanding if lighting alone can alter the interpretation of emotion. Realistic virtual humans are an underused tool to study these effects in a controlled manner as they retain the same emotional expression across lighting conditions, and can display the same emotion across different genders and races. In this paper, we focus on studying the effect of light temperature, color, and shadow on the interpretation of emotions of realistic virtual humans, and compare to a human photo baseline. We are particularly interested in recognition of emotion, emotion intensity, and genuineness of the emotion. Our findings can be used by developers to increase the emotional intensity and genuineness of their virtual humans.	https://dl.acm.org/doi/abs/10.1145/3721238.3730728	Rachel McDonnell, Bharat Vyas, Uros Sikimic, Pisut Wisessing
Fine Tuning Every Grain on the Beach	For the interactive beaches featured on Pixar's Elio, we developed an innovative method which improved upon past approaches. Traditional interactive sand pipelines are made of three parts: a render-optimized surface with limited grain interaction for most of the set, a limited number of interactive particles, and a compositing integration step to bridge a notorious gap between static set sand and interactive sand. Our approach introduces a render-optimized sand asset entirely made of points with reading and writing capability for every point, ready to modify for interaction without look alteration. This drastic workflow simplification provided a much more capable pipeline leading to the efficient authoring of individually tweakable sand grains above half a billion. By leveraging a perceptual model that uses the camera frustums of all frames, we plausibly represented expansive areas which in reality would be composed of many trillions of grains.	https://dl.acm.org/doi/abs/10.1145/3721239.3734116	Alexis Angelidis, Lan Tang, Gary Bruins, Jae Jun Yi, David Quirus, David Luoh
FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios	Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730683	Shiyi Zhang, Junhao Zhuang, Zhaoyang Zhang, Ying Shan, Yansong Tang
Flexible 3D Cage-based Deformation via Green Coordinates on Bézier Patches	Cage-based deformation is a fundamental problem in geometry processing, where a cage, a user-specified boundary of a region, is used to deform the ambient space of a given mesh. Traditional 3D cages are typically composed of triangles and quads. While quads can represent non-planar regions when their four corners are not coplanar, they form ruled surfaces with straight isoparametric curves, which limits their ability to handle curved and high-curvature deformations. In this work, we extend the cage for curved boundaries using Bézier patches, enabling flexible and high-curvature deformations with only a few control points. The higher-order structure of the Bézier patch also allows for the creation of a more compact and precise curved cage for the input model. Based on Green's third identity, we derive the Green coordinates for the Bézier cage, achieving shape-preserving deformation with smooth surface boundaries. These coordinates are defined based on the vertex positions and normals of the Bézier control net. Given that the coordinates are approximately calculated through the Riemann summation, we propose a global projection technique to ensure that the coordinates accurately conform to the linear reproduction property. Experimental results show that our method achieves high performance in handling curved and high-curvature deformations.	https://dl.acm.org/doi/abs/10.1145/3721238.3730630	Dong Xiao, Renjie Chen
Flink's Pigeon Problems	Flink sets out with a bit of magic to help rescue the messenger pigeons that have turned to stone.	https://dl.acm.org/doi/abs/10.1145/3698896.3725612	Susan Fitzer, Brian Pimental
ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation	Realistic Hand manipulation is a key component of immersive virtual reality (VR), yet existing methods often rely on a kinematic approach or motion-capture datasets that omit crucial physical attributes such as contact forces and finger torques. Consequently, these approaches prioritize tight, one-size-fits-all grips rather than reflecting users' intended force levels. We present , a deep learning agent that synthesizes realistic hand manipulation motions, faithfully reflecting the user's grip force intention. Instead of mimicking predefined motion datasets, ForceGrip uses generated training scenarios—randomizing object shapes, wrist movements, and trigger input flows—to challenge the agent with a broad spectrum of physical interactions. To effectively learn from these complex tasks, we employ a three-phase curriculum learning framework comprising , , and . This progressive strategy ensures stable hand-object contact, adaptive force control based on user inputs, and robust handling under dynamic conditions. Additionally, a proximity reward function enhances natural finger motions and accelerates training convergence. Quantitative and qualitative evaluations reveal ForceGrip's superior force controllability and plausibility compared to state-of-the-art methods. Demo videos are available as supplementary material and the code is provided at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730738	DongHeun Han, Byungmin Kim, RoUn Lee, KyeongMin Kim, Hyoseok Hwang, HyeongYeop Kang
Forevergreen	An orphaned bear cub finds a home with a fatherly evergreen tree, until his hunger for trash leads him to danger.	https://dl.acm.org/doi/abs/10.1145/3698896.3725483	Nathan Engelhardt, Jeremy Spears
Foveated Animations for Efficient Crowd Simulation	Foveated rendering techniques have seen recent development with the advent of commercial head-mounted displays with eye-tracking capabilities. The main drive is to exploit particular features of our peripheral vision that allow optimizing rendering pipelines, which allows using less computational effort where the human visual system may be unaware of differences. Most efforts have been focused on simplifying spatial visual detail on areas not being focused on by adjusting acuity of shading models, sharpness of images, and pixel density. However, other perception pipeline areas are also influential, particularly in certain purpose-specific applications. In this paper, we demonstrate it is possible to reduce animation rates in crowd simulations up to a complete stop for agents in our peripheral vision without users noticing the effect. We implemented a prototype Unity3D application with typical crowd simulation scenarios and carried out user experiments to study subjects' perception to changes in animation rates. We find that in the best case we were able to reduce the number of operations by 99.3% compared to an unfoveated scenario, with opportunities for developments combined with other acceleration techniques. This paper also includes an in-depth discussion about human perception of movement in peripheral vision with novel ideas that will have applications beyond crowd simulation.	https://dl.acm.org/doi/abs/10.1145/3728306	Florin-Vladimir Stancu, Tomer Weiss, Rafael Kuffner dos Anjos
From Loom to Code and Back Again	This talk explores the integration of traditional Jacquard weaving techniques into our existing workflow at Netflix Animation Studios. We discuss the merits of fibre-level construction and the lessons learned as we laboured to place the power of a Jacquard loom into the hands of digital artists.	https://dl.acm.org/doi/abs/10.1145/3721239.3734126	Mike Davison, Guillaume Pernin, Curtis Andrus
From Style to Identity: AI Pipelines for Visual and Character Coherence in Film	This study contrasts two generative AI (GenAI) workflows (Figure ) addressing visual and character consistency and introduces a filmmaker-oriented framework for AI-assisted production, grounded in two practice-based short films.	https://dl.acm.org/doi/abs/10.1145/3721250.3742968	Zhiyu Zhang
From storytelling to story making: Interactive techniques to creating immersion in public spaces.	Immersive and interactive media have long sought to engage audiences through sensory-rich experiences. However, designing for public spaces presents unique challenges—such as co-presence (sharing space with others), cognitive overload (excessive sensory input that impairs decision-making), and opaque interfaces (unintuitive or invisible interaction cues), which often hinder engagement. Unlike games or personal screen-based media with clear conventions, experiences in public spaces require intuitive, shared interactions that foster emotional connection between participants to create satisfying immersion.	https://dl.acm.org/doi/abs/10.1145/3721239.3734133	Christopher Panzetta
Full-color natural light holographic video camera	This paper presents a compact handheld type holographic video camera system capable of capturing real-time, full-color complex hologram videos under natural lighting conditions. By integrating a geometric phase lens with a polarization image sensor, our system captures interference patterns without requiring specialized lighting or bulky equipment. We successfully apply conventional 2D video super-resolution techniques to the complex holograms, significantly enhancing both resolution and visibility while preserving digital refocusing capabilities. Our experimental results demonstrate that this approach satisfies three critical requirements for practical modern cameras: operation under incoherent lighting, robustness to mobile shooting condition, and compact design. This work represents an advancement toward practical holographic media applications, particularly for broadcast content production in extended reality and mixed reality environments.	https://dl.acm.org/doi/abs/10.1145/3721250.3742987	Kihong Choi, Daeyoul Park, Keehoon Hong
G-FED: G-Buffer Guided Frame Extrapolation in Video Diffusion Models	Rendering near-final quality previews requires a great number of samples per pixel. Recently, diffusion models have shown superior denoising capabilities, but suffer from large variance. This is further amplified by the spatial and temporal inconsistencies introduced by diffusion models. In our pipeline, we propose the use of multiple control features and forward projections to denoise 1 sample per pixel frames & extrapolate a high-quality frame to generate a consistent and controllable sequence of high-quality frames.	https://dl.acm.org/doi/abs/10.1145/3721250.3742972	Pedro Antonio Pena, Karthik Mohan Kumar, Damian Andrysiak, Kunal Tyagi, Rama Harihara
GAIA: Generative Animatable Interactive Avatars with Expression-conditioned Gaussians	3D generative models of faces trained on in-the-wild image collections have improved greatly in recent times, offering better visual fidelity and view consistency. Making such generative models animatable is a hard yet rewarding task, with applications in virtual AI agents, character animation, and telepresence. However, it is not trivial to learn a well-behaved animation model with the generative setting, as the learned latent space aims to best capture the data distribution, often omitting details such as dynamic appearance and entangling animation with other factors that affect controllability. We present GAIA: Generative Animatable Interactive Avatars, which is able to generate high-fidelity 3D head avatars for both realistic animation and rendering. To achieve consistency during animation, we learn to generate Gaussians embedded in an underlying morphable model for human heads via a shared UV parameterization. For modeling realistic animation, we further design the generator to learn expression-conditioned details for both geometric deformation and dynamic appearance. Finally, facing an inevitable entanglement problem between facial identity and expression, we propose a novel two-branch architecture that encourages the generator to disentangle identity and expression. On existing benchmarks, GAIA achieves state-of-the-art performance in visual quality as well as realistic animation. The generated Gaussian-based avatar supports highly efficient animation and rendering, making it readily available for interactive animation and appearance editing.	https://dl.acm.org/doi/abs/10.1145/3721238.3730737	Zhengming Yu, Tianye Li, Jingxiang Sun, Omer Shapira, Seonwook Park, Michael Stengel, Matthew Chan, Xin Li, Wenping Wang, Koki Nagano, Shalini De Mello
GBake: Baking 3D Gaussian Splats into Reflection Probes	The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce , a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.	https://dl.acm.org/doi/abs/10.1145/3721250.3742978	Stephen Pasch, Joel Salzman, Changxi Zheng
GPU Work Graphs	Work Graphs is a Direct3D12 feature added in 2024 and enables GPU compute shaders to dispatch other shaders directly without CPU intervention. Thus, they can help improve GPU memory usage and runtime speed. In this course, we teach Work Graphs concepts at practical and relevant hands-on examples with a focus on the non-trivial HLSL Work Graphs additions. After this class, participants should be able to assess when Work Graphs is useful for their tasks and be able to understand, explain, and apply Work Graphs.	https://dl.acm.org/doi/abs/10.1145/3721241.3734003	Bastian Kuth, Max Oberberger, Quirin Meyer
GSHeadRelight: Fast Relightability for 3D Gaussian Head Synthesis	Relighting and novel view synthesis of human portraits are essential in applications such as portrait photography, virtual reality (VR), and augmented reality (AR). Despite recent progress, 3D-aware portrait relighting remains challenging due to the demands for photorealistic rendering, real-time performance, and generalization to unseen subjects. Existing works either rely on supervision from limited and expensive light stage captured data or produce suboptimal results. Moreover, many works are based on generative NeRFs, which suffer from poor 3D consistency and low real-time performance. We resort to recent progress on generative 3D Gaussians and design a lighting model based on a unified neural radiance transfer representation, which responds linearly to incident light. Using only in-the-wild images, our method achieves state-of-the-art relighting results and a significantly faster rendering speed (× 12) compared to previous 3D-aware portrait relighting research.	https://dl.acm.org/doi/abs/10.1145/3721238.3730614	Henglei Lv, Bailin Deng, Jianzhu Guo, Xiaoqiang Liu, Pengfei Wan, Di Zhang, Lin Gao
GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering	Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce GaVS, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent 'local reconstruction and rendering' paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study. Project Page: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730757	Zinuo You, Stamatios Georgoulis, Anpei Chen, Siyu Tang, Dengxin Dai
GarmentImage: Raster Encoding of Garment Sewing Patterns with Diverse Topologies	Garment sewing patterns are the design language behind clothing, yet their current vector-based digital representations weren't built with machine learning in mind. Vector-based representation encodes a sewing pattern as a discrete set of panels, each defined as a sequence of lines and curves, stitching information between panels and the placement of each panel around a body. However, this representation causes two major challenges for neural networks: discontinuity in latent space between patterns with different topologies and limited generalization to garments with unseen topologies in the training data. In this work, we introduce GarmentImage, a unified raster-based sewing pattern representation. GarmentImage encodes a garment sewing pattern's geometry, topology and placement into multi-channel regular grids. Machine learning models trained on GarmentImage achieve seamless transitions between patterns with different topologies and show better generalization capabilities compared to models trained on vector-based representation. We demonstrate the effectiveness of GarmentImage across three applications: pattern exploration in latent space, text-based pattern editing, and image-to-pattern prediction. The results show that GarmentImage achieves superior performance on these applications using only simple convolutional networks.	https://dl.acm.org/doi/abs/10.1145/3721238.3730632	Yuki Tatsukawa, Anran Qi, I-Chao Shen, Takeo Igarashi
Gaussian Compression for Precomputed Indirect Illumination	Precomputed global illumination (GI) techniques, such as light probes, particularly focus on capturing indirect illumination and have gained widespread adoption. However, as the scale of the scenes continues to expand, the demand for storage space and runtime memory for light probes also increases substantially. To address this issue, we propose a novel Gaussian fitting compression technique specifically designed for light field probes, which enables the use of denser samples to describe illumination in complex scenes. The core idea of our method is utilizing low-bit adaptive Gaussian functions to store the latent representation of light probes, enabling parallel and high-speed decompression on the GPU. Additionally, we implement a custom gradient propagation process to replace conventional inference frameworks, like PyTorch, ensuring an exceptional compression speed. At the same time, by constructing a cascaded light field texture in real-time, we avoid the need for baking and storing a large number of redundant light field probes arranged in the form of 3D textures. This approach allows us to achieve further compression of the memory while maintaining high visual quality and rendering speed. Compared to traditional methods based on Principal Component Analysis (PCA), our approach consistently yields superb results across various test scenarios, achieving compression ratios of up to 1:50.	https://dl.acm.org/doi/abs/10.1145/3721238.3730758	Zhi Zhou, Chao Li, Zhenyuan Zhang, Mingcong Tang, Zibin Li, Shuhang Luan, Zhangjin Huang
Gaussian Fluids: A Grid-Free Fluid Solver based on Gaussian Spatial Representation	We present a grid-free fluid solver featuring a novel Gaussian representation. Drawing inspiration from the expressive capabilities of 3D Gaussian Splatting in multi-view image reconstruction, we model the continuous flow velocity as a weighted sum of multiple Gaussian functions. the of fluid solvers , We evaluate the proposed solver across a broad range of 2D and 3D fluid phenomena, The source code for our fluid solver is publicly available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730620	Jingrui Xing, Bin Wang, Mengyu Chu, Baoquan Chen
Gaze Entropy and Driver Safety: Understanding Cognitive Failure and Situational Response Before Take-over	Understanding the driver's cognitive state is critical in conditionally autonomous driving, particularly when responding to Take-Over Requests (TOR). However, existing approaches rely primarily on visual attention and are limited in capturing fundamental cognitive failures. This study proposes a quantifiable framework that identifies such failures through gaze entropy analysis and links the driver's gaze behavior to accident risk.	https://dl.acm.org/doi/abs/10.1145/3721250.3742974	Mi Chang, Eun Hye Jang, Woojin Kim, Daesub Yoon
GenAI Demystified	Generative artificial intelligence is taking the world by storm. It's a new technology that enables computers learn from historical data to generate new content that feels original and human-like. Although generative AI has become a part of everyday life, many people still lack a clear understanding of how it works. Without a solid understanding of generative AI, it can be difficult for individuals to fully embrace this emerging technology. This course offers a comprehensive introduction to the concepts, technologies, and real-world impact of generative AI. The course begins with a historical overview of artificial intelligence that traces its evolution from early rules-based logic to the deep learning breakthroughs that have enabled today's most powerful generative AI models. Participants will explore foundational concepts that include illustrations of neural network models to gain intuition behind how machines learn to generate text, images, and other types of content. The course delves into practical applications of generative AI across multiple modalities, including natural language, computer vision, audio, and more. Participants will also study high-level system architectures of an AI chatbot, the poster child of generative AI, to understand how these systems operate. Finally, the course will examine future development of generative AI, including the rise of agentic applications that can reason, plan, and interact autonomously.	https://dl.acm.org/doi/abs/10.1145/3721241.3737653	Kevin Lee, Vadim Kudlay
GenAI and Immersive Theater - Moment Factory x Third Rail Projects	This research explores the integration of Generative Artificial Intelligence (GenAI) into immersive, site-specific theater through a collaborative initiative between Moment Factory and Third Rail Projects. The objective was to investigate how GenAI technologies can augment dramaturgical practices by enabling co-creation among authors, performers, and audience members. Using an iterative research-creation methodology, the project unfolded in two phases: a remote prototyping cycle and an on-site laboratory. Across three use cases: generative scenography, physical space extension, and improvised narration—AI tools were integrated into live performance contexts to dynamically respond to human presence and input. Techniques included the use of real-time generative visuals, silhouette-based transformations, and voice-activated storytelling systems. The findings highlight both the creative opportunities and technical constraints of employing GenAI in live performance. Critical insights emerged around timing, audience perception, system orchestration, and the need for adaptable, artist-centric AI workflows. This study emphasizes the potential of GenAI to function not merely as a technical instrument, but as a dramaturgical collaborator in the creation of emotionally resonant, participatory experiences.	https://dl.acm.org/doi/abs/10.1145/3721239.3734125	Julien Bigeault, Nelly Roxane Karimpour-Harvey, Conner Tozier, Alexis Doyon-Lacroix
Generalized, Dynamic Multi-agent Torso Crowds	In crowd simulation algorithms, discs are commonly used due to their geometrical simplicity. However, discs overestimate the body's projected shape on the ground plane, and do not naturally embody rotational information for each crowd agent. We propose a crowd simulation algorithm that represents crowd agents as 2D capsules. Our algorithm is based on position-based dynamics (PBD), for which we design novel constraints for capturing dynamics of capsule-shaped agents. We demonstrate our algorithm in several scenarios, showing superior performance compared to a disc-based representation on settings that range from sparse to dense.	https://dl.acm.org/doi/abs/10.1145/3728303	Bilas Talukdar, Tomer Weiss
Generative Neural Materials	Advancements in neural rendering techniques have sparked renewed interest in neural materials, which are capable of representing bidirectional texture functions (BTFs) cheaply and with high quality. However, content creation in the neural material format is not straightforward. To address this limitation, we present the first image-conditioned diffusion model for neural materials, and show an extension to text conditioning. To achieve this, we make two main contributions: (1) we introduce a universal MLP variant of the NeuMIP architecture, defining a universal basis for neural materials as 16-channel feature textures, and (2) we train a conditional diffusion model for generating neural materials in this basis from flash images, natural images and text prompts. To achieve this, we also construct a new dataset of 150k neural materials in 16 categories, since no large-scale neural material data exists. To our knowledge, our work is the first to enable single-shot neural material generation from arbitrary text or image prompts.	https://dl.acm.org/doi/abs/10.1145/3721238.3730746	Nithin Raghavan, Krishna Mullia, Alexander Trevithick, Fujun Luan, Miloš Hašan, Ravi Ramamoorthi
Generative Video Matting	Video matting has traditionally been limited by the lack of high-quality ground-truth data. Most existing video matting datasets provide only human-annotated imperfect alpha and foreground annotations, which must be composited to background images or videos during the training stage. Thus, the generalization capability of previous methods in real-world scenarios is typically poor. In this work, we propose to solve the problem from two perspectives. First, we emphasize the importance of large-scale pre-training by pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also develop a scalable synthetic data generation pipeline that can render diverse human bodies and fine-grained hairs, yielding around 200 video clips with a 3-second duration for fine-tuning. Second, we introduce a novel video matting approach that can effectively leverage the rich priors from pre-trained video diffusion models. This architecture offers two key advantages. First, strong priors play a critical role in bridging the domain gap between synthetic and real-world scenes. Second, unlike most existing methods that process video matting frame-by-frame and use an independent decoder to aggregate temporal information, our model is inherently designed for video, ensuring strong temporal consistency. We provide a comprehensive quantitative evaluation across three benchmark datasets, demonstrating our approach's superior performance, and present comprehensive qualitative results in diverse real-world scenes, illustrating the strong generalization capability of our method. Code is available at https://github.com/aim-uofa/GVM	https://dl.acm.org/doi/abs/10.1145/3721238.3730642	Yongtao Ge, Kangyang Xie, Guangkai Xu, Li Ke, Mingyu Liu, Longtao Huang, Hui Xue, Hao Chen, Chunhua Shen
Generative detail enhancement for physically based materials	We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to increase the visual fidelity of existing materials by adding, for instance, signs of wear, aging, and weathering that are tedious to author. To obtain realistic appearance with minimal user effort, we leverage a generative image model trained on a large dataset of natural images. Given the geometry, UV mapping, and basic appearance of an object, we proceed as follows: We render multiple views of the object and use them, together with an appearance-defining text prompt, to condition a diffusion model. The generated details are then backpropagated from the enhanced images to the material parameters via inverse rendering. For inverse rendering to be successful, the generated appearance has to be consistent across all the images. We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space. Second, we enforce spatial consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. Our approach does not require any training or finetuning of the diffusion model, is agnostic to the used material model, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists. We demonstrate prompt-based material edits exhibiting high levels of realism and detail. This project is available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730751	Saeed Hadadan, Benedikt Bitterli, Tizian Zeltner, Jan Novák, Fabrice Rousselle, Jacob Munkberg, Jon Hasselgren, Bartlomiej Wronski, Matthias Zwicker
Graphics4Science: Computer Graphics for Scientific Impacts	Computer graphics, often associated with films, games, and visual effects, has long been a powerful tool for addressing scientific challenges—from its origins in 3D visualization for medical imaging to its role in modern computational modeling and simulation. This course explores the deep and evolving relationship between computer graphics and science, highlighting past achievements, ongoing contributions, and open questions that remain. We show how core methods, such as geometric reasoning and physical modeling, provide inductive biases that help address challenges in both fields, especially in data-scarce settings. To that end, we aim to reframe graphics as a modeling language for science by bridging vocabulary gaps between the two communities. Designed for both newcomers and experts, invites the graphics community to engage with science, tackle high-impact problems where graphics expertise can make a difference, and contribute to the future of scientific discovery. Additional details are available on the course website: https://graphics4science.github.io.	https://dl.acm.org/doi/abs/10.1145/3721241.3733986	Peter Yichen Chen, Minghao Guo, Hanspeter Pfister, Ming Lin, William Freeman, Qixing Huang, Han-Wei Shen, Wojciech Matusik
Greetings from p5.js 2.0: Animation, Interaction, and Typography in 2D and 3D	"p5.js is a free and open-source JavaScript library for creative coding that prioritizes access. As a ""batteries-included"" toolkit, p5.js is used worldwide for teaching, performance, installation, collaboration, and experimentation. In this lab, we introduce participants to graphics in p5.js by creating an interactive postcard with a mix of 2D and 3D elements. This walkthrough includes an introduction to code-based animation, parameterized visuals, mouse and touch interactivity, and screen reader support in p5.js. Additionally, we will demonstrate new features in the latest p5.js 2.0 release, including improvements to typography and support for authoring shaders in JavaScript. We use an interactive postcard as an example because it shows how easy it is to bring these different parts of the creative coding toolkit together to create not only sketches, but complete web-based interactive artworks and high-resolution exports. For technical artists and creative technologists, p5.js offers a unique level of variety and control."	https://dl.acm.org/doi/abs/10.1145/3721251.3742859	Dave Pagurek van Mossel, Kit Kuksenok
Guided Lens Sampling for Efficient Monte Carlo Circle-of-Confusion Rendering	We introduce a guided lens sampling method for efficient rendering of circles of confusion (CoCs). While traditional Monte Carlo techniques simulate depth-of-field (DoF) effects by perturbing camera rays at the lens, uniform lens sampling often results in significant noise by failing to prioritize rays toward highlight regions in the scene. Although path guiding has proven effective for global illumination by learning importance distributions for incoming radiance, no comparable guiding technique for CoCs exists, primarily due to the strong parallax between adjacent pixels. We model highlight spots in world space using a globally shared radiance field, which is then transformed into lens space through a bipolar-cone projection to guide camera ray generation. We implement this theory using 3D Gaussians, achieving fast, robust guiding with minimal computational and storage overhead, making it suitable for production rendering. We also propose two extensions to further enhance local adaptation. Our experiments show that this approach significantly improves the sampling efficiency for CoC rendering.	https://dl.acm.org/doi/abs/10.1145/3721238.3730608	Jiawei Huang, Shaokun Zheng, Kun Xu, Yoshifumi Kitamura, Jiaping Wang
Guiding-Based Importance Sampling for Walk on Stars	has shown its power in being applied to Monte Carlo methods for solving partial differential equations, but the sampling techniques in WoSt are not satisfactory, leading to high variance. We propose a guiding-based importance sampling method to reduce the variance of WoSt. Drawing inspiration from path guiding in rendering, we approximate the directional distribution of the recursive term of WoSt using online-learned parametric mixture distributions, decoded by a lightweight neural field. This adaptive approach enables importance sampling the recursive term, which lacks shape information before computation. We introduce a reflection technique to represent guiding distributions at Neumann boundaries and incorporate multiple importance sampling with learnable selection probabilities to further reduce variance. We also present a practical GPU implementation of our method. Experiments show that our method effectively reduces variance compared to the original WoSt, given the same time or the same sample budget. Code and data for this paper are at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730593	Tianyu Huang, Jingwang Ling, Shuang Zhao, Feng Xu
HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination	We present HOIGaze – a novel learning-based approach for gaze estimation during hand-object interactions (HOI) in extended reality (XR). HOIGaze addresses the challenging HOI setting by building on one key insight: Eye, hand, and head movements are closely coordinated during HOIs and this coordination can be exploited to identify samples that are most useful for gaze estimator training – as such, effectively denoising the training data. This denoising approach is in stark contrast to previous gaze estimation methods that treated all training samples as equal. Specifically, we propose: 1) a novel that first recognises the hand currently visually attended to and then estimates gaze direction based on the attended hand; 2) a new that uses cross-modal Transformers to fuse head and hand-object features extracted using a convolutional neural network and a spatio-temporal graph convolutional network; and 3) a novel that upgrades training samples belonging to the coordinated eye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin (ADT) datasets and show that it significantly outperforms state-of-the-art methods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in mean angular error. To demonstrate the potential of our method, we further report significant performance improvements for the sample downstream task of eye-based activity recognition on ADT. Taken together, our results underline the significant information content available in eye-hand-head coordination and, as such, open up an exciting new direction for learning-based gaze estimation.	https://dl.acm.org/doi/abs/10.1145/3721238.3730692	Zhiming Hu, Daniel Haeufle, Syn Schmitt, Andreas Bulling
Hand Gesture-Driven Vertical Teleportation: Navigating Complex Height Differences in VR	This paper presents two novel teleportation methods for VR environments that address limitations of conventional parabola-based approaches when navigating varying heights. The SphereBackcast and Penetration methods utilize straight-line specification for intuitive movement to elevated locations. Experiments with 22 participants showed our methods significantly outperformed parabola-based teleportation for height differences above 2m, while maintaining comparable performance on flat terrain. NASA-TLX and SUS evaluations confirmed improved usability and reduced cognitive load, indicating these methods can be readily integrated into existing VR applications.	https://dl.acm.org/doi/abs/10.1145/3721250.3743020	Hibiki Kirihata, Tomokazu Ishikawa
Hands-On Class: Introduction to Slang: The Next Generation Shading Language	This hands-on lab introduces Slang, an open-source, open governance shading language hosted by Khronos that simplifies graphics development across platforms. Designed to tackle the growing complexity of shader code, Slang offers modern programming constructs while maintaining top performance on current GPUs. Participants will get practical experience with Slang's stand-out features: cross-platform capabilities for write-once, run-anywhere development, a module system that streamlines code organization, and generics and interfaces that eliminate preprocessor headaches. We'll cover integrating Slang with industry tools like RenderDoc and Visual Studio Code, and demonstrate converting existing GLSL and HLSL code to Slang. We'll also briefly introduce advanced features like automatic differentiation, reflection, and bindless capabilities. Whether you're developing games, working in visualization, or researching new rendering techniques, you'll gain practical skills to immediately improve your graphics workflow and prepare for next-generation rendering challenges.	https://dl.acm.org/doi/abs/10.1145/3721251.3742860	Nia Bickford, Tristan Lorach, Chris Hebert
Hands-on Vulkan® Ray Tracing With Dynamic Rendering	This Lab invites participants to explore how today's Vulkan ecosystem tools and resources allow everyone to write real-time ray-traced effects with far less boilerplate than in the past. Building on a trimmed version of the open-source Khronos Vulkan Tutorial [Khronos ] and the vk-bootstrap [Giessen ] and Vulkan-RAII [Khronos ] helper libraries, we will incrementally assemble a renderer that combines rasterization with ray tracing to implement accurate shadows. The session balances bite-sized coding intervals with short concept discussions, so that every participant leaves with a runnable project and a mental map of where to dig deeper afterward.	https://dl.acm.org/doi/abs/10.1145/3721251.3742861	Jose Emilio Muñoz Lopez, Steven Winston
Haunting Horizons: Human–AI Auto-Cartography of Tasmanian Island Experience	Lutruwita/Tasmania's island conditions are often misperceived as isolated and unchanging. Building on Giada Peterle's concept of auto-cartography, this paper explores Tasmania's dynamic island identity through human-AI mapping. This is achieved through the creation of , an interactive installation powered by a customised generative AI model. By translating my island experience into a training dataset, this work positions human-AI auto-cartography as an embodied, affective process, enabling artists and participants to engage with maps and reflect on their relations to place in new ways.	https://dl.acm.org/doi/abs/10.1145/3721250.3743010	Adam Hsieh
Hierarchical Neural Skinning Deformation with Self-supervised Training for Character Animation	We propose a self-supervised neural deformation method for character skin deformation animations, incorporating musculoskeletal and adipose tissue deformations driven by joint movements. This hierarchical approach combines two self-supervised neural emulators: the musculoskeletal neural emulator, based on a biomaterial musculoskeletal model, and the soft-tissue neural emulator, capturing soft secondary dynamics. The neural emulators achieve self-supervised learning through a loss function derived from physical principles, eliminating the need for ground-truth datasets. The musculoskeletal neural emulator enhances deformation effects, including musculoskeletal bulging, contraction, and dynamic responses, and generates inner-layer animations. Building on this, the soft-tissue neural emulator adds enhanced deformation effects, such as secondary motion, driven by the inner-layer animations. During inference, the method efficiently simulates realistic skin deformation animations for arbitrary character structures and joint motion sequences. The resulting animations demonstrate visual fidelity comparable to, or surpassing, state-of-the-art data-driven methods in the industry.	https://dl.acm.org/doi/abs/10.1145/3728300	Tianyi Wang, Shiguang Liu
High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion	Despite recent advances in Novel View Synthesis (NVS), generating high-fidelity views from single or sparse observations remains challenging. Existing splatting-based approaches often produce distorted geometry due to splatting errors. While diffusion-based methods leverage rich 3D priors to achieve improved geometry, they often suffer from texture hallucination. In this paper, we introduce , a pixel-splatting-guided video diffusion model designed to synthesize high-fidelity novel views from a single image. Specifically, we propose an aligned synthesis strategy for precise control of target viewpoints and geometry-consistent view synthesis. To mitigate texture hallucination, we design a texture bridge module that enables high-fidelity texture generation through adaptive feature fusion. In this manner, SplatDiff leverages the strengths of splatting and diffusion for geometrically consistent, high-fidelity view synthesis. Extensive experiments verify the state-of-the-art performance of SplatDiff in single-view NVS. Additionally, without extra training, SplatDiff shows remarkable zero-shot performance across diverse tasks, including sparse-view NVS and stereo video conversion.	https://dl.acm.org/doi/abs/10.1145/3721238.3730669	Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, Christopher Schroers
Highly Accurate GPU-Accelerated Animation Rigs for Toothless	Real-time feedback of character rigs is an important feature for every animator [Kahwaty et al. ]. While improvements to Maya's execution engine has helped to run character rigs faster in parallel, it has been challenging to use high-end rigs directly in animation. Instead, proxy rigs are utilized to give animators instant feedback, but with the drawback of providing a low-quality preview. Deformations can also be approximated using a machine learning network, but this requires a time-consuming training process [Öhrström et al. ]. This paper describes how we have implemented complex deformers as OpenCL kernels that work with Maya's GPU evaluator. We will explain how we have ported less GPU-friendly algorithms, such as BVH construction, to the GPU while achieving accurate results. This has drastically reduced the need to create proxy rigs and provides animators with an interactive user experience without sacrificing visual fidelity.	https://dl.acm.org/doi/abs/10.1145/3721239.3734131	Leon Sooi, Christoph Genzwuerker
Histogram Stratification for Spatio-Temporal Reservoir Sampling	Monte Carlo (MC) rendering is a widely used approach for photorealistic image synthesis, yet real-time applications often limit sampling to one path per pixel, resulting in high noise levels. To mitigate this, resampled importance sampling (RIS) has shown promise by approximating ideal sample distributions through a discrete set of candidates, avoiding the complexity of neural models or data-intensive structures. However, current RIS techniques often rely on random sampling, which fails to maximize the potential of the candidate pool. We propose a two step approach that first organizes samples candidates into local histograms and then sample the histogram using Quasi Monte Carlo and antithetic patterns. This can be done with minimal overhead and allows to reduce error in rendering to increase visual quality. Additionally, we show how it can be combined with blue noise error distribution to perceptually reduce noise artifacts. Our approach yields a higher-quality resampling estimator with enhanced noise reduction, demonstrating significant improvements in real-time rendering tasks.	https://dl.acm.org/doi/abs/10.1145/3721238.3730723	Corentin Salaün, Martin Balint, Laurent Belcour, Eric Heitz, Gurprit Singh, Karol Myszkowski
Holograms, Battlefields & Genetic Dynasties: Image Engine's VFX for Dune: Prophecy: Go inside Image Engine's creative and technical process for Dune: Prophecy and find out how we leveraged an efficient, design-driven VFX approach.	Enter the world of Dune: Prophecy and discover how Image Engine helped bring this iconic universe to life through visual effects.	https://dl.acm.org/doi/abs/10.1145/3698897.3717827	Martyn Culpitt, Viktoria Rucker, Adrien Vallecilla, Xander Kennedy, Daniel Bigaj
How We Aped It: Thousands of Shots and Character Simulations with Only a Handful of TDs	We present a procedural, modular, and asset-centric approach to character effects and simulation (CFX) workflow particularly regarding costumes and hair/fur. This differs from many other approaches by focusing on the simulation setups at individual asset level, and then using costume collection configuration, procedurally constructing the simulation scene by merging the assets and having them solved together. Shot specific modifications and overrides such as custom cloth poses and parameter tweaks can be applied in a hierarchical and procedural manner so that when there are upstream changes, the subsequent job iterations will pick up the changes without requiring manual rework.	https://dl.acm.org/doi/abs/10.1145/3721239.3736271	Jefri Haryono, Andrea Merlo, Gios Johnston, Tim Hawker
How to Break Crowd Simulation Algorithms	Virtual crowds are prevalent in entertainment media, including movies, games, and educational or training experiences. In this work, we analyze algorithms for simulating such crowds. Specifically, we demonstrate that widely popular state-of-the-art algorithms fail in several basic benchmark cases. With the goal of designing more robust algorithms, we discuss potential solutions, which can be easily integrated into the specified simulation crowds.	https://dl.acm.org/doi/abs/10.1145/3721239.3734112	Alexander Chen, Tomer Weiss
HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers	3D human reconstruction and animation are long-standing topics in computer graphics and vision. However, existing methods typically rely on sophisticated dense-view capture and/or time-consuming per-subject optimization procedures. To address these limitations, we propose HumanRAM, a novel feed-forward approach for generalizable human reconstruction and animation from monocular or sparse human images. Our approach integrates human reconstruction and animation into a unified framework by introducing explicit pose conditions, parameterized by a shared SMPL-X neural texture, into transformer-based large reconstruction models (LRM). Given monocular or sparse input images with associated camera parameters and SMPL-X poses, our model employs scalable transformers and a DPT-based decoder to synthesize realistic human renderings under novel viewpoints and novel poses. By leveraging the explicit pose conditions, our model simultaneously enables high-quality human reconstruction and high-fidelity pose-controlled animation. Experiments show that HumanRAM significantly surpasses previous methods in terms of reconstruction accuracy, animation fidelity, and generalization performance on real-world datasets.	https://dl.acm.org/doi/abs/10.1145/3721238.3730605	Zhiyuan Yu, Zhe Li, Hujun Bao, Can Yang, Xiaowei Zhou
Hyper-Dimensional Deformation Simulation	We present a method for simulating deformable bodies in four spatial dimensions. To accomplish this, we generalize several pieces of the traditional simulation pipeline. Starting from the meshing stage, we propose a simple method for generating a mesh, the 4D analog of a tetrahedral mesh. Next, we show how to generalize the deformation invariants, allowing us to construct 4D hyperelastic energies that lead directly to hyper-dimensional deformation forces. Finally, we formulate collision detection and response in 4D. Our eigenanalyses of the resulting deformation and collision energies generalize to arbitrarily higher dimensions. The resulting simulations display a variety of previously unseen visual phenomena.	https://dl.acm.org/doi/abs/10.1145/3721238.3730730	Alvin Shi, Haomiao Wu, Theodore Kim
HyperParamBRDF: Fast Parametric Reflectance via Hypernetworks and Physics-Based Simulation	Simulating and fabricating plasmonic nanostructures for specific colors is slow and costly. exploits a hypernetwork to learn a parametric reflectance model from physical parameters. Trained on sparse FDTD data, it infers BRDFs in milliseconds, achieving > 10 × speedup with high fidelity, enabling real-time appearance exploration for complex simulated materials.	https://dl.acm.org/doi/abs/10.1145/3721250.3743039	Abraham Beauferris, Wei Sen Loi
IP-Composer: Semantic Composition of Visual Concepts	Content creators often draw inspiration from multiple visual sources, combining distinct elements to craft new compositions. Modern computational approaches now aim to emulate this fundamental creative process. Although recent diffusion models excel at text-guided compositional synthesis, text as a medium often lacks precise control over visual details. Image-based composition approaches can capture more nuanced features, but existing methods are typically limited in the range of concepts they can capture, and require expensive training procedures or specialized data. We present IP-Composer, a novel training-free approach for compositional image generation that leverages multiple image references simultaneously, while using natural language to describe the concept to be extracted from each image. Our method builds on IP-Adapter, which synthesizes novel images conditioned on an input image's CLIP embedding. We extend this approach to multiple visual inputs by crafting composite embeddings, stitched from the projections of multiple input images onto concept-specific CLIP-subspaces identified through text. Through comprehensive evaluation, we show that our approach enables more precise control over a larger range of visual concept compositions.	https://dl.acm.org/doi/abs/10.1145/3721238.3730624	Sara Dorfman, Dana Cohen-Bar, Rinon Gal, Daniel Cohen-Or
IP-Prompter: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting	The stories and characters that captivate us as we grow up shape unique fantasy worlds, with images serving as the primary medium for visually experiencing these realms. Personalizing generative models through fine-tuning with theme-specific data has become a prevalent approach in text-to-image generation. However, unlike object customization, which focuses on learning specific objects, theme-specific generation encompasses diverse elements such as characters, scenes, and objects. Such diversity also introduces a key challenge: how to adaptively generate multi-character, multi-concept, and continuous theme-specific images (TSI). Moreover, fine-tuning approaches often come with significant computational overhead, time costs, and risks of overfitting. This paper explores a fundamental question: Can image generation models directly leverage images as contextual input, similarly to how large language models use text as context? To address this, we present , a novel training-free TSI generation method. introduces visual prompting, a mechanism that integrates reference images into generative models, allowing users to seamlessly specify the target theme without requiring additional training. To further enhance this process, we propose a Dynamic Visual Prompting (DVP) mechanism, which iteratively optimizes visual prompts to improve the accuracy and quality of generated images. Our approach enables diverse applications, including consistent story generation, character design, realistic character generation, and style-guided image generation. Comparative evaluations against state-of-the-art personalization methods demonstrate that achieves significantly better results and excels in maintaining character identity preserving, style consistency and text alignment, offering a robust and flexible solution for theme-specific image generation. Our project page: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730670	Yuxin Zhang, Minyan Luo, Weiming Dong, Xiao Yang, Haibin Huang, Chongyang Ma, Oliver Deussen, Tong-Yee Lee, Changsheng Xu
Image-GS: Content-Adaptive Image Representation via 2D Gaussians	Neural image representations have emerged as a promising approach for encoding and rendering visual data. Combined with learning-based workflows, they demonstrate impressive trade-offs between visual fidelity and memory footprint. Existing methods in this domain, however, often rely on fixed data structures that suboptimally allocate memory or compute-intensive implicit models, hindering their practicality for real-time graphics applications. Inspired by recent advancements in radiance field rendering, we introduce Image-GS, a content-adaptive image representation based on 2D Gaussians. Leveraging a custom differentiable renderer, Image-GS reconstructs images by adaptively allocating and progressively optimizing a group of anisotropic, colored 2D Gaussians. It achieves a favorable balance between visual fidelity and memory efficiency across a variety of stylized images frequently seen in graphics workflows, especially for those showing non-uniformly distributed features and in low-bitrate regimes. Moreover, it supports hardware-friendly rapid random access for real-time usage, requiring only 0.3K MACs to decode a pixel. Through error-guided progressive optimization, Image-GS naturally constructs a smooth level-of-detail hierarchy. We demonstrate its versatility with several applications, including texture compression, semantics-aware compression, and joint image compression and restoration.	https://dl.acm.org/doi/abs/10.1145/3721238.3730596	Yunxiang Zhang, Bingxuan Li, Alexandr Kuznetsov, Akshay Jindal, Stavros Diolatzis, Kenneth Chen, Anton Sochenov, Anton Kaplanyan, Qi Sun
Image-Space Collage and Packing with Differentiable Rendering	Collage and packing techniques are widely used to organize geometric shapes into cohesive visual representations, facilitating the representation of visual features holistically, as seen in image collages and word clouds. Traditional methods often rely on object-space optimization, requiring intricate geometric descriptors and energy functions to handle complex shapes. In this paper, we introduce a versatile image-space collage technique. Leveraging a differentiable renderer, our method effectively optimizes the object layout with image-space losses, bringing the benefit of fixed complexity and easy accommodation of various shapes. Applying a hierarchical resolution strategy in image space, our method efficiently optimizes the collage with fast convergence, large coarse steps first and then small precise steps. The diverse visual expressiveness of our approach is demonstrated through various examples. Experimental results show that our method achieves an order of magnitude speedup performance compared to state-of-the-art techniques.	https://dl.acm.org/doi/abs/10.1145/3721238.3730690	Zhenyu Wang, Min Lu
Image-space Adaptive Sampling for Fast Inverse Rendering	Inverse rendering is crucial for many scientific and engineering disciplines. Recent progress in differentiable rendering has led to efficient differentiation of the full image formation process with respect to scene parameters, enabling gradient-based optimization. However, computational demands pose a significant challenge for differentiable rendering, particularly when rendering all pixels during inverse rendering from high-resolution/multi-view images. This computational cost leads to slow performance in each iteration of inverse rendering. Meanwhile, naively reducing the sampling budget by uniformly sampling pixels to render in each iteration can result in high gradient variance during inverse rendering, ultimately degrading overall performance. Our goal is to accelerate inverse rendering by reducing the sampling budget without sacrificing overall performance. In this paper, we introduce a novel image-space adaptive sampling framework to accelerate inverse rendering by dynamically adjusting pixel sampling probabilities based on gradient variance and contribution to the loss function. Our approach efficiently handles high-resolution images and complex scenes, with faster convergence and improved performance compared to uniform sampling, making it a robust solution for efficient inverse rendering.	https://dl.acm.org/doi/abs/10.1145/3721238.3730627	Kai Yan, Cheng Zhang, Sébastien Speierer, Guangyan Cai, Yufeng Zhu, Zhao Dong, Shuang Zhao
Implementing SOTA Generative AI Pipelines in Your 3D Application	Generative AI is transforming how we create and interact with 3D content—yet for many developers, integrating state-of-the-art (SOTA) models into existing 3D pipelines remains a significant hurdle. This 90-minute hands-on lab bridges that gap, offering a streamlined theoretical foundation followed by practical, open-source solutions for embedding generative AI into real-time or offline 3D applications. We focus on two highly extensible workflows: • Using ComfyUI to convert concept art into 3D assets, accelerating previsualization and asset generation. • Leveraging video-to-video generative models to produce enhanced renders and motion-driven animation sequences. Participants will walk away with modular, reproducible pipelines that are adaptable across engines and frameworks. From model discovery to local inference integration, this session provides the tools and insight to deploy generative AI wherever your 3D workflows lives - from Blender/Maya to Unity/Unreal and beyond.	https://dl.acm.org/doi/abs/10.1145/3721251.3734070	Yosun Chang, Michael Gold, Caramel Corgi.cam Chang
Import bpy: Modern add-on development with Blender	This class focuses on extending Blender's functionality through its powerful Python API. Starting from fundamental concepts such as operators, we will gradually increase the complexity of our solution, to a small but useful development tool. Attendees should come prepared with Blender, Python knowledge, and a code editor to follow along and experiment in real time.	https://dl.acm.org/doi/abs/10.1145/3721251.3734065	Sybren Stüvel
Improved Stochastic Texture Filtering Through Sample Reuse	Stochastic texture filtering (STF) has re-emerged as a technique that can bring down the cost of texture filtering of advanced texture compression methods, e.g., neural texture compression. However, during texture magnification, the swapped order of filtering and shading with STF can result in aliasing. The inability to smoothly interpolate material properties stored in textures, such as surface normals, leads to potentially undesirable appearance changes. We present a novel method to improve the quality of stochastically-filtered magnified textures and reduce the image difference compared to traditional texture filtering. When textures are magnified, nearby pixels filter similar sets of texels and we introduce techniques for sharing texel values among pixels with only a small increase in cost (0.04–0.14 ms per frame). We propose an improvement to weighted importance sampling that guarantees that our method never increases error beyond single-sample stochastic texture filtering. Under high magnification, our method has >10 dB higher PSNR than single-sample STF. Our results show greatly improved image quality both with and without spatiotemporal denoising.	https://dl.acm.org/doi/abs/10.1145/3728292	Bartlomiej Wronski, Matt Pharr, Tomas Akenine-Möller
In Between	When little Nora's parents split up, the Earth splits in two. She now has to juggle between both hemispheres to visit them. Problem is, the backpack she's carrying is getting heavier and heavier...	https://dl.acm.org/doi/abs/10.1145/3698896.3717033	Roxane David, Nour El Achkar, Morgane Chauvet, Flora Rouvel, Lucie Perales, Sacha Moreau, Cindy Fanchonna
In Your Dreams	A comedy adventure about Stevie and her brother Elliot who journey into the landscape of their own dreams.	https://dl.acm.org/doi/abs/10.1145/3698896.3725618	Alex Woo, Erik Benson
InfiniteStudio: 4D Volumetric Capture for Film Making and Beyond	We present InfiniteStudio, the first 4D volumetric capture system that meets the visual fidelity requirements for professional-grade video production. Building upon innovations in 4D Gaussian Splatting, InfiniteStudio reduces production time while unlocking unprecedented creative freedom during post-production. The system harnesses 4D Gaussian Splatting to maintain production-level visual quality, support long-duration content, and achieve high compression rates of 80–120 Mbps. InfiniteStudio revolutionizes film production by eliminating 20-30% of shooting time traditionally spent capturing multiple angles and reducing retakes by 5-10%. It empowers filmmakers with complete creative freedom for virtual camera movements, slow-motion effects, and advanced background replacement, paving the way for next-generation interactive media and immersive spatial storytelling.	https://dl.acm.org/doi/abs/10.1145/3721243.3735979	Jiaming Sun, Siyu Zhang, Yu Zhang, Zhen Xu, Xiaowei Zhou
Innovating with Generative AI: A Hands-On ComfyUI Workshop	This hands-on, three-hour workshop introduces participants to ComfyUI, a visual, node-based interface for generative AI workflows built for Stable Diffusion. As an open-source Python and JavaScript tool, ComfyUI can run both locally and in cloud environments, making it ideal for flexible deployment and experimentation. Through guided exercises and demonstrations, attendees gain practical experience with advanced generative models for image, video, and 3D creation, and learn how to integrate both open and proprietary tools into modular creative pipelines.	https://dl.acm.org/doi/abs/10.1145/3721241.3734004	Michael Gold, Yosun Chang
Instance Segmentation of Scene Sketches Using Natural Image Priors	Sketch segmentation involves grouping pixels within a sketch that belong to the same object or instance. It serves as a valuable tool for sketch editing tasks, such as moving, scaling, or removing specific components. While image segmentation models have demonstrated remarkable capabilities in recent years, sketches present unique challenges for these models due to their sparse nature and wide variation in styles. We introduce InkLayer, a method for instance segmentation of raster scene sketches. Our approach adapts state-of-the-art image segmentation and object detection models to the sketch domain by employing class-agnostic fine-tuning and refining segmentation masks using depth cues. Furthermore, our method organizes sketches into sorted layers, where occluded instances are inpainted, enabling advanced sketch editing applications. As existing datasets in this domain lack variation in sketch styles, we construct a synthetic scene sketch segmentation dataset, InkScenes, featuring sketches with diverse brush strokes and varying levels of detail. We use this dataset to demonstrate the robustness of our approach. Code and data for this paper are released at project page: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730606	Mia Tang, Yael Vinker, Chuan Yan, Lvmin Zhang, Maneesh Agrawala
InstanceGen: Image Generation with Instance-level Instructions	Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances. Additionally, we contribute CompoundPrompts, a benchmark composed of complex prompts with three difficulty levels in which object instances are progressively compounded with attribute descriptions and spatial relations. Extensive experiments demonstrate that our method significantly surpasses the performance of prior models, particularly over complex multi-object and multi-attribute use cases.	https://dl.acm.org/doi/abs/10.1145/3721238.3730613	Etai Sella, Yanir Kleiman, Hadar Averbuch-Elor
InstantRestore: Single-Step Personalized Face Restoration with Shared-Image Attention	Face image restoration aims to enhance degraded facial images while addressing challenges such as diverse degradation types, real-time processing demands, and, most crucially, the preservation of identity-specific features. Existing methods often struggle with slow processing times and suboptimal restoration, especially under severe degradation, failing to accurately reconstruct finer-level identity details. To address these issues, we introduce InstantRestore, a novel framework that leverages a single-step image diffusion model and an attention-sharing mechanism for fast and personalized face restoration. Additionally, InstantRestore incorporates a novel landmark attention loss, aligning key facial landmarks to refine the attention maps, enhancing identity preservation. At inference time, given a degraded input and a small (∼ 4) set of reference images, InstantRestore performs a single forward pass through the network to achieve near real-time performance. Unlike prior approaches that rely on full diffusion processes or per-identity model tuning, InstantRestore offers a scalable solution suitable for large-scale applications. Extensive experiments demonstrate that InstantRestore outperforms existing methods in quality and speed, making it an appealing choice for identity-preserving face restoration.	https://dl.acm.org/doi/abs/10.1145/3721238.3730628	Howard Zhang, Yuval Alaluf, Sizhuo Ma, Achuta Kadambi, Jian Wang, Kfir Aberman
Interactive Optimization of Scaffolded Procedural Patterns	A procedural program is the representation of a family of assets that share the same structural or semantic properties, whose final appearance is determined by different parameter assignments. Identifying the parameter values that define a desired asset is usually a time-consuming operation, since it requires manually tuning parameters separately and in a non-intuitive manner. In the domain of procedural patterns, recent works focused on estimating parameter values to match a target render or sketch, using parameter optimization or inference via neural networks. However, these approaches are neither fast enough for interactive design nor precise enough to give direct control. In this work, we propose an interactive method for procedural parameter estimation based on the idea of scaffolded procedural patterns. A scaffolded procedural pattern is a sequence of procedural programs that model a pattern in a coarse-to-fine manner, in which the desired pattern appearance is reached step-by-step by inheriting previously optimized parameters. Through scaffolding, patterns are more straightforward to sketch for users and easier to optimize for most algorithms. In our implementation, patterns are represented as procedural signed distance functions whose parameters are estimated with a gradient-free optimization method that runs in real-time on the GPU. We show that scaffolded patterns can be created with a node-based interface familiar to artists. We validate our approach by creating and interactively editing several scaffolded patterns. We show the effectiveness of scaffolding through a user study, where scaffolding enhances both the output quality and the editing experience with respect to approaches that optimize the procedural parameters all at once. We also perform a comparison with previous strategies and provide several recordings of real-time editing sessions in the accompanying materials.	https://dl.acm.org/doi/abs/10.1145/3721238.3730667	Davide Sforza, Marzia Riso, Filippo Muzzini, Nicola Capodieci, Fabio Pellacini
Interactive Trailers and Posters that Enhance Viewing Intentions through 'Breaking the Fourth Wall'	"The rise of video streaming has shifted video consumption from traditional venues like theaters to mobile and social media platforms. However, promotional strategies have not kept pace—posters and trailers are still used on mobile devices without leveraging their unique capabilities. This paper presents a new approach to boosting viewing intent through interactive engagement. It introduces interactive posters and trailers that break the ""Fourth Wall,"" allowing characters to communicate directly with users. A prototype enabling dialog-based interaction was tested with 33 participants in their 20s and 30s. Results showed that these interactive experiences significantly increased anticipation and intent to watch the film."	https://dl.acm.org/doi/abs/10.1145/3721250.3743001	Boyoung Lim, Jusub Kim
Intersection-Free Garment Retargeting	Manual design of garments for avatars requires a large effort. Garment retargeting methods can save manual efforts by automatically deforming an existing garment design from one avatar to another. Previous methods are limited to human avatars with small variations in body shapes, while non-human avatars with unrealistic characteristics widely appear in games and animations. In this paper, the goal is to retarget artist-designed garments on a standard mannequin to a more general class of avatars. While there is a lack of training data of various avatars wearing garments, we propose a training-free method that performs optimizations on the mesh representation of the garments, with a combination of loss functions that preserve the geometrical features in the original design, guarantee intersection-free, and fit the garment adaptively to the avatars. Our method produces simulation-ready garment models that can be used later in avatar animations.	https://dl.acm.org/doi/abs/10.1145/3721238.3730590	Zizhou Huang, Chrystiano Araújo, Andrew Kunz, Denis Zorin, Daniele Panozzo, Victor Zordan
Introduction To Generative Machine Learning	This is an intermediate level course for attendees to gain a strong understanding of the basic principles of generative AI. The course will help build intuition around several topics with easy-to-understand explanations and examples from some of the prevalent algorithms and models including Autoencoders, CNN, Diffusion Models, Transformers, and NeRFs.	https://dl.acm.org/doi/abs/10.1145/3721251.3736530	Rajesh Sharma, Mia Tang
Introduction to the Fourier Transform	The Fourier Transform is fundamental to computer graphics, explaining topics from aliasing and sampling to image compression and filtering. This friendly course explains the principles in words, pictures, and animation, rather than math. The concepts are the important thing. We show that they are comprehensible, useful, and beautiful.	https://dl.acm.org/doi/abs/10.1145/3721241.3733984	Andrew Glassner
Inverse Design of Discrete Interlocking Materials with Desired Mechanical Behavior	We present a computational approach for designing Discrete Interlocking Materials (DIMs) with desired mechanical properties. Unlike conventional elastic materials, DIMs are kinematic materials governed by internal contacts among elements. These contacts induce anisotropic deformation limits that depend on the shape and topology of the elements. To enable gradient-based design optimization of DIMs with desired deformation limits, we introduce an implicit representation of interlocking elements based on unions of tori. Using this low-dimensional representation, we simulate DIMs with smoothly evolving contacts, allowing us to predict changes in deformation limits as a function of shape parameters. With this toolset in hand, we optimize for element shape parameters to design heterogeneous DIMs that best approximate prescribed limits. We demonstrate the effectiveness of our method by designing discrete interlocking materials with diverse limit profiles for in- and out-of-plane deformation and validate our method on fabricated physical prototypes.	https://dl.acm.org/doi/abs/10.1145/3721238.3730675	Pengbin Tang, Bernhard Thomaszewski, Stelian Coros, Bernd Bickel
James-Stein Gradient Combiner for Inverse Monte Carlo Rendering	Inferring scene parameters such as BSDFs and volume densities from user-provided target images has been achieved using a gradient-based optimization framework, which iteratively updates the parameters using the gradient of a loss function defined by the differences between rendered and target images. The gradient can be unbiasedly estimated via a physics-based rendering, i.e., differentiable Monte Carlo rendering. However, the estimated gradient can become noisy unless a large number of samples are used for gradient estimation, and relying on this noisy gradient often slows optimization convergence. An alternative is to exploit a biased version of the gradient, e.g., a filtered gradient, to achieve faster optimization convergence. Unfortunately, this can result in less noisy but overly blurred scene parameters compared to those obtained using unbiased gradients. This paper proposes a gradient combiner that blends unbiased and biased gradients in parameter space instead of relying solely on one gradient type (i.e., unbiased or biased). We demonstrate that optimization with our combined gradient enables more accurate inference of scene parameters than using unbiased or biased gradients alone.	https://dl.acm.org/doi/abs/10.1145/3721238.3730714	Jeongmin Gu, Bochang Moon
Joint Denoising and Upscaling via Multi-branch and Multi-scale Feature Network	Deep learning-based denoising and upscaling techniques have emerged to enhance framerates for real-time rendering. A single neural network for joint denoising and upscaling offers the advantage of sharing parameters in the feature space, enabling efficient prediction of filter weights for both. However, it is still ongoing research to devise an efficient feature extraction neural network that uses different characteristics in inputs for the two combined problems. We propose a multi-branch, multi-scale feature extraction network for joint neural denoising and upscaling. The proposed multi-branch U-Net architecture is lightweight and effectively accounts for different characteristics in noisy color and noise-free aliased auxiliary buffers. Our technique produces superior quality denoising in a target resolution (4K), given noisy 1spp Monte Carlo renderings and auxiliary buffers in a low resolution (1080p), compared to the state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3728297	Pawel Kazmierczyk, Sungye Kim, Wojciech Uss, Wojciech Kalinski, Tomasz Galaj, Mateusz Maciejewski, Rama Harihara
Jour de vent	Wind appears in a park. People fly away.	https://dl.acm.org/doi/abs/10.1145/3698896.3719331	Martin Chailloux, Ai Kim Crespin, Elise Golfouse, Chloé Lab, Hugo Taillez, Camille Truding
Kernel Predicting Neural Shadow Maps	Existing neural shadow mapping methods [Datta et al. ] have shown to be promising in generating high quality soft shadows. However, it demonstrates limited generalizability to new scenes. In this paper, we present a novel neural method, named to address this issue. Specifically, we explicitly model soft shadow values as pixelwise local filtering from nearby base shadow values (i.e., the classic hard shadow values) in the screen space, where the local filter weights are predicted through a trained neural network. We use dilated filters as the representation of our local filters to maintain a balance between computational efficiency and receptive field of a local filter. We further enhance shadow quality by replacing the classic shadow map algorithm [Williams ] with moment shadow maps [Peters and Klein ] to generate the base shadows values. With carefully designed filters, input features, and loss functions with temporal regularization, our method runs in real-time framerates (i.e., >100 fps for 2048 × 1024 resolution), produces temporally-stable soft shadows with good generalizability, and consistently beats state-of-the-art methods in both visual qualities and numeric measures. Code and model weights are available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730645	Xuejun Hu, Jinfan Lu, Kun Xu
LAM: Large Avatar Model for One-shot Animatable Gaussian Head	We present LAM, an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image. Unlike previous methods that require extensive training on captured video sequences or rely on auxiliary neural networks for animation and rendering during inference, our approach generates Gaussian heads that are immediately animatable and renderable. Specifically, LAM creates an animatable Gaussian head in a single forward pass in seconds, enabling reenactment and rendering without additional networks or post-processing steps. This capability allows for seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across a wide range of platforms, including mobile phones. The centerpiece of our framework is the canonical Gaussian attributes generator, which utilizes FLAME canonical points as queries. These points interact with multi-scale image features through a Transformer to accurately predict Gaussian attributes in the canonical space. The reconstructed canonical Gaussian avatar can then be animated utilizing standard linear blend skinning (LBS) with corrective blendshapes as the FLAME model did and rendered in real-time on various platforms. The experiments demonstrate that LAM outperforms state-of-the-art methods on existing benchmarks.	https://dl.acm.org/doi/abs/10.1145/3721238.3730706	Yisheng He, Xiaodong Gu, Xiaodan Ye, Chao Xu, Zhengyi Zhao, Yuan Dong, Weihao Yuan, Zilong Dong, Liefeng Bo
LSNIF: Locally-Subdivided Neural Intersection Function	Neural representations have shown the potential to accelerate ray casting in a conventional ray-tracing-based rendering pipeline. We introduce a novel approach called Locally-Subdivided Neural Intersection Function (LSNIF) that replaces bottom-level BVHs used as traditional geometric representations with a neural network. Our method introduces a sparse hash grid encoding scheme incorporating geometry voxelization, a scene-agnostic training data collection, and a tailored loss function. It enables the network to output not only visibility but also hit-point information and material indices. LSNIF can be trained offline for a single object, allowing us to use LSNIF as a replacement for its corresponding BVH. With these designs, the network can handle hit-point queries from any arbitrary viewpoint, supporting all types of rays in the rendering pipeline. We demonstrate that LSNIF can render a variety of scenes, including real-world scenes designed for other path tracers, while achieving a memory footprint reduction of up to 106.2 × compared to a compressed BVH.	https://dl.acm.org/doi/abs/10.1145/3728295	Shin Fujieda, Chih-Chen Kao, Takahiro Harada
Large-Scale Multi-Character Interaction Synthesis	Generating large-scale multi-character interactions is a challenging and important task in character animation. Multi-character interactions involve not only natural interactive motions but also characters coordinated with each other for transition. For example, a dance scenario involves characters dancing with partners and also characters coordinated to new partners based on spatial and temporal observations. We term such transitions as coordinated interactions and decompose them into interaction synthesis and transition planning. Previous methods of single-character animation do not consider interactions that are critical for multiple characters. Deep-learning-based interaction synthesis usually focuses on two characters and does not consider transition planning. Optimization-based interaction synthesis relies on manually designing objective functions that may not generalize well. While crowd simulation involves more characters, their interactions are sparse and passive. We identify two challenges to multi-character interaction synthesis, including the lack of data and the planning of transitions among close and dense interactions. Existing datasets either do not have multiple characters or do not have close and dense interactions. The planning of transitions for multi-character close and dense interactions needs both spatial and temporal considerations. We propose a conditional generative pipeline comprising a coordinatable multi-character interaction space for interaction synthesis and a transition planning network for coordinations. Our experiments demonstrate the effectiveness of our proposed pipeline for multi-character interaction synthesis and the applications facilitated by our method show the scalability and transferability.	https://dl.acm.org/doi/abs/10.1145/3721238.3730750	Ziyi Chang, He Wang, George Koulieris, Hubert P. H. Shum
Launcher: balancing stability and flexibility in CG artist software	We present the Launcher, a software environment configuration tool that has contributed to the success of numerous productions over the course of two decades at Animal Logic, now Netflix Animation Studios. We explore the core features that enable us to manage a large number of configurations across numerous departments and projects while balancing stability and flexibility. We touch on the ability to roll back configurations to any moment in time and how the Launcher helps us with the propagation of VFX platform updates and other new workflows.	https://dl.acm.org/doi/abs/10.1145/3721239.3734114	Ghislain Roy-Veilleux, Steve Agland, Federico Naum
LayerFlow: A Unified Model for Layer-aware Video Generation	We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos of different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.	https://dl.acm.org/doi/abs/10.1145/3721238.3730662	Sihui Ji, Hao Luo, Xi Chen, Yuanpeng Tu, Yiyang Wang, Hengshuang Zhao
LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation	3D immersive scene generation is a challenging yet critical task in computer vision and graphics. A desired virtual 3D scene should 1) exhibit omnidirectional view consistency, and 2) allow for large-range exploration in complex scene hierarchies. Existing methods either rely on successive scene expansion via inpainting or employ panorama representation to represent large FOV scene environments. However, the generated scene suffers from semantic drift during expansion and is unable to handle occlusion among scene hierarchies. To tackle these challenges, we introduce LayerPano3D, a novel framework for full-view, explorable panoramic 3D scene generation from a single text prompt. Our key insight is to decompose a reference 2D panorama into multiple layers at different depth levels, where each layer reveals the unseen space from the reference views via diffusion prior. LayerPano3D comprises multiple dedicated designs: 1) We introduce a new panorama dataset Upright360 , comprising 9k high-quality and upright panorama images, and finetune the advanced Flux model on Upright360 for high-quality, upright and consistent panorama generation related tasks. 2) We pioneer the Layered 3D Panorama as underlying representation to manage complex scene hierarchies and lift it into 3D Gaussians to splat detailed 360-degree omnidirectional scenes with unconstrained viewing paths. Extensive experiments demonstrate that our framework generates state-of-the-art 3D panoramic scene in both full view consistency and immersive exploratory experience. We believe that LayerPano3D holds promise for advancing 3D panoramic scene creation with numerous applications. More examples please visit our webpage:	https://dl.acm.org/doi/abs/10.1145/3721238.3730643	Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Gordon Wetzstein, Ziwei Liu, Dahua Lin
Learn OpenUSD: Robotics Best Practices	This course explores OpenUSD's transformative role in robotics simulation, providing a comprehensive framework for converting Universal Robot Description Format (URDF) assets into modular Universal Scene Description (USD) workflows. Participants learn to bridge industry-standard robotics specifications with OpenUSD's interoperability features, enabling seamless integration of robotic components like UR10e arms and 2F-140 grippers into physics-accurate training environments. Techniques include joint...	https://dl.acm.org/doi/abs/10.1145/3721251.3736528	Ji Yuan Feng, Andrew Kaufman
Learning to Draw Is Learning to See: Analyzing Eye Tracking Patterns for Assisted Observational Drawing	Drawing is an artistic process involving extensive observation. Understanding how professional artists observe as they draw has significant value because it offers insight into their perception patterns and acquired skills. While previous studies used eye tracking to analyze the drawing process, they fell short in aligning gaze data with drawing actions due to the spatial and temporal gaps between observation and drawing in a model-to-paper setup. This paper presents a study in an image-to-image setup, in which artists observe a reference image and draw on a blank canvas on a tablet, capturing a clearer mapping between eye movements and drawn strokes. Our analysis demonstrates a strong spatial correlation between observed regions and corresponding strokes. We further find that artists initially follow a more structured region-by-region approach and then switch to a less constrained sequence for details. Based on these findings, we develop an assistive interface that integrates real-time visual guidance from professional artists' eye tracking data, enabling novices to emulate their observation and drawing strategies. A user study shows that novices can draw significantly more accurate shapes using our assistive interface, highlighting the importance of modeling observation and the potential of leveraging eye tracking data in future educational and creativity support tools. Our datasets, analysis code, and assistive interface are available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730734	Fengqi Liu, Longji Huang, Zhengyu Huang, Zeyu Wang
Level-of-Detail for Geometry Processing and Simulation	Level-of-detail (LOD) is a concept we intuitively experience in everyday life—whether it is how our brains filter visual information or how digital maps adjust as we zoom. At its core, LOD is about allocating detail where it matters most, striking a balance between efficiency and precision. While originally developed to accelerate rendering in computer graphics, modern LOD techniques have evolved into a powerful framework for constructing hierarchical shape representations and designing multilevel solvers in geometry processing and simulation. This course begins by highlighting the role of LOD not just in visualization, but also in numerical computation. We introduce key methods for constructing hierarchical structures, then dive into the design of multilevel solvers—focusing on how to transfer quantities and signals across levels, with emphasis on and . We conclude with applications that showcase how these hierarchical frameworks enable efficient, accurate and scalable solutions to problems in geometry processing and physical simulation.	https://dl.acm.org/doi/abs/10.1145/3721241.3733988	Jiayi Eris Zhang, Hsueh-Ti Derek Liu
Lifting the Winding Number: Precise Discontinuities in Neural Fields for Physics Simulation	"Cutting thin-walled deformable structures is common in daily life, but poses significant challenges for simulation due to the introduced spatial discontinuities. Traditional methods rely on mesh-based domain representations, which require frequent remeshing and refinement to accurately capture evolving discontinuities. These challenges are further compounded in reduced-space simulations, where the basis functions are inherently geometry- and mesh-dependent, making it difficult or even impossible for the basis to represent the diverse family of discontinuities introduced by cuts. Recent advances in representing basis functions with neural fields offer a promising alternative, leveraging their discretization-agnostic nature to represent deformations across varying geometries. However, the inherent continuity of neural fields is an obstruction to generalization, particularly if discontinuities are encoded in neural network weights. We present , a novel neural representation designed to accurately model complex cuts in thin-walled deformable structures. Our approach constructs neural fields that reproduce discontinuities precisely at specified locations, without ""baking in"" the position of the cut line. To achieve this, we augment the input coordinates of the neural field with the generalized winding number of any given cut line, effectively lifting the input from two to three dimensions. Lifting allows the network to focus on the easier problem of learning a 3D everywhere- volumetric field, while a corresponding restriction operator enables the final output field to precisely resolve discontinuities. Crucially, our approach does not embed the discontinuity in the neural network's weights, opening avenues to generalization of cut placement. Our method achieves real-time simulation speeds and supports dynamic updates to cut line geometry during the simulation. Moreover, the explicit representation of discontinuities makes our neural field intuitive to control and edit, offering a significant advantage over traditional neural fields, where discontinuities are embedded within the network's weights, and enabling new applications that rely on general cut placement."	https://dl.acm.org/doi/abs/10.1145/3721238.3730597	Yue Chang, Mengfei Liu, Zhecheng Wang, Peter Yichen Chen, Eitan Grinspun
LightLab: Controlling Light Sources in Images with Diffusion Models	We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for the relighting task. We leverage the to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results,	https://dl.acm.org/doi/abs/10.1145/3721238.3730696	Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, Yedid Hoshen
Like Friend, Like Deer	This animation is about some people from the Middle East. It is about the connection and similarity of the people with their living environment, animals, and generally with Nature... all of which are almost in a critical condition.	https://dl.acm.org/doi/abs/10.1145/3698896.3725447	Malek Eghbali
MAGNET: Muscle Activation Generation Networks for Diverse Human Movement	We introduce MAGNET (Muscle Activation Generation Networks), a scalable framework for reconstructing full-body muscle activations across diverse human movements. Our approach employs musculoskeletal simulation with a novel two-level controller architecture trained using three-stage learning methods. Additionally, we develop distilled models tailored for solving downstream tasks or generating real-time muscle activations, even on edge devices. The efficacy of our framework is demonstrated through examples of daily life and challenging behaviors, as well as comprehensive evaluations.	https://dl.acm.org/doi/abs/10.1145/3721238.3730617	Jungnam Park, Euikyun Jung, Jehee Lee, Jungdam Won
MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation	We introduce ( ), a novel and representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of , each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined with a with a parameterized base that the spatial extent of the spherical function to attain locality. We develop a differentiable optimization algorithm capable of converting any point cloud into a MASH representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. Extensive experiments demonstrate that MASH is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features. More information and resources can be found at: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730610	Changhao Li, Yu Xin, Xiaowei Zhou, Ariel Shamir, Hao Zhang, Ligang Liu, Ruizhen Hu
MGPBD: A Multigrid Accelerated Global XPBD Solver	We introduce a novel Unsmoothed Aggregation (UA) Algebraic Multigrid (AMG) method combined with Preconditioned Conjugate Gradient (PCG) to overcome the limitations of Extended Position-Based Dynamics (XPBD) in high-resolution and high-stiffness simulations. While XPBD excels in simulating deformable objects due to its speed and simplicity, its nonlinear Gauss-Seidel (GS) solver often struggles with low-frequency errors, leading to instability and stalling issues, especially in high-resolution, high-stiffness simulations. Our multigrid approach addresses these issues efficiently by leveraging AMG. To reduce the computational overhead of traditional AMG, where prolongator construction can consume up to two-thirds of the runtime, we propose a lazy setup strategy that reuses prolongators across iterations based on matrix structure and physical significance. Furthermore, we introduce a simplified method for constructing near-kernel components by applying a few sweeps of iterative methods to the homogeneous equation, achieving convergence rates comparable to adaptive smoothed aggregation (adaptive-SA) at a lower computational cost. Experimental results demonstrate that our method significantly improves convergence rates and numerical stability, enabling efficient and stable high-resolution simulations of deformable objects.	https://dl.acm.org/doi/abs/10.1145/3721238.3730720	Chunlei Li, Peng Yu, Tiantian Liu, Siyuan Yu, Yuting Xiao, Shuai Li, Aimin Hao, Yang Gao, Qinping Zhao
MIND: Microstructure INverse Design with Generative Hybrid Neural Representation	The inverse design of microstructures plays a pivotal role in optimizing metamaterials with specific, targeted physical properties. While traditional forward design methods are constrained by their inability to explore the vast combinatorial design space, inverse design offers a compelling alternative by directly generating structures that fulfill predefined performance criteria. However, achieving precise control over both geometry and material properties remains a significant challenge due to their intricate interdependence. Existing approaches, which typically rely on voxel or parametric representations, often limit design flexibility and structural diversity. In this work, we present a novel generative model that integrates latent diffusion with Holoplane, an advanced hybrid neural representation that simultaneously encodes both geometric and physical properties. This combination ensures superior alignment between geometry and properties. Our approach generalizes across multiple microstructure classes, enabling the generation of diverse, tileable microstructures with significantly improved property accuracy and enhanced control over geometric validity, surpassing the performance of existing methods. We introduce a multi-class dataset encompassing a variety of geometric morphologies, including truss, shell, tube, and plate structures, to train and validate our model. Experimental results demonstrate the model's ability to generate microstructures that meet target properties, maintain geometric validity, and integrate seamlessly into complex assemblies. Additionally, we explore the potential of our framework through the generation of new microstructures, cross-class interpolation, and the infilling of heterogeneous microstructures. Code and data for this paper are at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730682	Tianyang Xue, Longdu Liu, Lin Lu, Paul Henderson, Pengbin Tang, Haochen Li, Jikai Liu, Haisen Zhao, Hao Peng, Bernd Bickel
Machine Learning Meets Lighting: Using Depth Estimation To Build The Light Rigs	This paper describes the techniques we use to build the complex light rigs in the Sony Pictures Imageworks lighting pipeline. Typically we receive the panoramic HDRI from the set and need to make a light rig from it. Building a light rig has several steps: extracting area lights from HDRI, placing them in a 3D scene, aligning lights with Lidar from set. We describe how this process can be sped up. For example, the positioning of extracted lights is automated using PatchFusion [Li et al. ], an off-the-shelf machine learning model for high resolution monocular depth estimation. PatchFusion computes accurate metric depth directly from the HDRI. The depth map is used as a distance from light to camera to place the area lights in 3D scene. This removes the need for manual distance measurements or guesswork. Our approach significantly reduces manual labor. The time required to build the light rig goes from hours to about a minute.	https://dl.acm.org/doi/abs/10.1145/3721239.3734127	Sergey Shlyaev
Making Worlds Feel Alive with Procedural Systems in Blender	This hands-on class focuses on progressively building a simple space traffic system, using proceduralism. The two main building blocks of this system are the spaceship generator and a guided entity simulation. The goal of the system is to achieve believable behavior, making the background of a scene feel alive. The implementation of the setup makes use of the procedural generation framework Geometry Nodes built into Blender. While the spaceship system will be used as a practical example, the concepts are more generally applicable to similar scenarios. There is no requirement of previous experience with the Geometry Nodes system, while a base level familiarity with Blender is encouraged to follow along.	https://dl.acm.org/doi/abs/10.1145/3721251.3734067	Simon Thommes
Making the Stream of Consciousness in Pixar's Inside Out 2	In Pixar's feature animation (2024), the core Emotions led by Joy find themselves navigating through the Stream of Consciousness in an attempt to reach Riley's Back of the Mind and then retrieve her Sense of Self. Our team was cast to develop the look for the Stream of Consciousness and support its deployment across nearly twenty shots. In particular, it was clear during production that we needed ways to work concurrently with the Animation team, at times even independently, while still able to address notes and changes rapidly. This work discusses our technical solutions for the implementation of the Stream of Consciousness, including in-house procedural tools that facilitated the authoring and stylization of velocity fields interacting with 3D obstacles.	https://dl.acm.org/doi/abs/10.1145/3721239.3734078	Tolga Goktekin, Krzysztof Rost, Ravindra Dwivedi, Fernando de Goes
Manu-Grid: UI for parameter estimation of tilt angle extended generalized projection function in illustrations	Compositing a background image and a foreground image produced from a 3D object requires a projection function that ensures consistency in the scene. We modified the generalized projection of [Yoshimura and Saito ] to allow tilted-angle images and introduced a user interface, Manu-Grid, to estimate the parameters of the projection function corresponding to the drawing method used in a background image. The interface has a useful characteristic that when a user manipulates a vanishing direction, a vanishing line, and a reference point on the ground, the others are pinned.	https://dl.acm.org/doi/abs/10.1145/3721250.3743003	Mamoru Akiyoshi, Ayumu Sato, Suguru Saito
MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models	Assigning realistic materials to 3D models remains a significant challenge in computer graphics. We propose MatCLIP, a novel method that extracts shape- and lighting-insensitive descriptors of Physically Based Rendering (PBR) materials to assign plausible textures to 3D objects based on images, such as the output of Latent Diffusion Models (LDMs) or photographs. Matching PBR materials to static images is challenging because the PBR representation captures the dynamic appearance of materials under varying viewing angles, shapes, and lighting conditions. By extending an Alpha-CLIP-based model on material renderings across diverse shapes and lighting, and encoding multiple viewing conditions for PBR materials, our approach generates descriptors that bridge the domains of PBR representations with photographs or renderings, including LDM outputs. This enables consistent material assignments without requiring explicit knowledge of material relationships between different parts of an object. MatCLIP achieves a top-1 classification accuracy of 76.6%, outperforming state-of-the-art methods such as PhotoShape and MatAtlas by over 15 percentage points on publicly available datasets. Our method can be used to construct material assignments for 3D shape datasets such as ShapeNet, 3DCoMPaT++, and Objaverse. All code and data will be released at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730740	Michael Birsak, John Femiani, Biao Zhang, Peter Wonka
MeshTorrent: A Community-Driven P2P System for AI-Generated 3D Model Creation and Distribution	MeshTorrent is a peer-to-peer platform for automated 3D content creation and exchange, inspired by BitTorrent-style file sharing. By merging AI-based text-to-3D generation with swarm-based distribution, MeshTorrent harnesses the combined bandwidth and storage resources of its users, enabling scalable and decentralized sharing of 3D assets. This paper describes the core design of MeshTorrent, including an AI workflow for generating fresh .glb files, metadata management via a distributed hash table, partial previews for quick inspection, and specialized extensions for 2D sprites ( ) and rigged character models ( ). Preliminary tests show faster content download times than single-host alternatives, reduced server costs, and robust resilience to network churn, advancing an open ecosystem for collaborative 3D model exchange.	https://dl.acm.org/doi/abs/10.1145/3721251.3736272	Ryan Hardesty Lewis
Metaball Madness - The Rigging Of An Implicit Surface Character	We present a novel character rigging solution developed for OOOOO, a liquid supercomputer in Pixar's Elio. OOOOO's design and desired movement necessitated reimagining our conventional way of articulating characters and she became Pixar's first mesh-free character rig. We developed a system that allowed our animators full fidelity control over what is essentially a rigged shader while ensuring downstream renderability. [Luo et al. ] The system's architecture supports a hierarchical arrangement of implicit surface primitives and operators, allowing for complex transformations while preserving normal animation paradigms and offers unprecedented flexibility in character animation.	https://dl.acm.org/doi/abs/10.1145/3721239.3734079	Anna-Christine Lykkegaard, Andrew Butts, Julian Teo
Metaball Madness: Look Development For A Shapeshifting, Implicit Surface Character On Pixar's Elio	"How do you shade a liquid supercomputer created as an implicit surface? 's OOOOO is Pixar's first character made and rigged entirely as a controllable series of metaball-like signed distance functions, rendered for interaction with a GLSL shader [Lykkegaard et al. ] in our animation software, . Although this novel approach facilitates incredible animation, it does not provide a stable mesh for shading, rendering, and deformation purposes. OOOOO is made of a body and blobs, all capable of separating and merging at any time, filled with ""nests"" of moving circuits that are concentrated at the core. This talk addresses the accompanying set of challenges these factors created for look development past the model/rig stage, resulting in a per-frame process. Figure 1: 's OOOOO has an implicit surface rig that can take many shapes ©Pixar"	https://dl.acm.org/doi/abs/10.1145/3721239.3734130	Catherine Luo, Trent Crow, Fernando de Goes, Ferdi Scheepers
Miegakure: a game where you explore and interact with a 4D world	We present Miegakure, a four-dimensional puzzle-platformer game and comprehensive exploration of a 4D interactive world. In the game, players navigate through four-dimensional space to solve spatial puzzles impossible in lower dimensions. We explain our chosen representation method for displaying 4D space on traditional 2D screens using dimensional analogy. We discuss the game's technical implementation of (1) 4D meshes built from tetrahedra rather than triangles, which are sliced to produce 3D triangles for display (2) Procedurally modeling 4D objects and embedding 3D objects in 4D by extruding them and modeling their inside. (3) Texturing meshes procedurally: 4D objects have 3D surfaces and hence 3D textures (or 4D textures if use solid texturing is used).	https://dl.acm.org/doi/abs/10.1145/3721243.3735986	Marc ten Bosch
Minecraft to 3D: A Pipeline for High-Fidelity Reconstruction of Minecraft Worlds	We introduce , a novel pipeline that automatically converts any Minecraft world into a high-quality polygonal scene. A 3D convolutional network recognises Minecraft's default objects, the block surface is resampled into a smooth height‑map, and each recognised object is substituted with a high‑quality 3D model chosen from an external library. Object locations, orientations, and tags are preserved, a separate water plane is exported for engine‑level ocean rendering, and the final scene opens natively in modern 3D engines. The pipeline processes a one‑square‑kilometre world in under three minutes on a single consumer GPU, enabling educators, indie developers, and artists to move rapidly from voxel sketches to fully lit environments.	https://dl.acm.org/doi/abs/10.1145/3721250.3743044	Sean Hardesty Lewis
Mobius: Text to Seamless Looping Video Generation via Latent Shift	We present Mobius, a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation. Our method repurposes the pre-trained video latent diffusion model for generating looping videos from text prompts without any training. During inference, we first construct a latent cycle by connecting the starting and ending noise of the videos. Given that the temporal consistency can be maintained by the context of the video diffusion model, we perform multi-frame latent denoising by gradually shifting the first-frame latent to the end in each step. As a result, the denoising context varies in each step while maintaining consistency throughout the inference process. Moreover, the latent cycle in our method can be of any length. This extends our latent-shifting approach to generate seamless looping videos beyond the scope of the video diffusion model's context. Unlike previous cinemagraphs, the proposed method does not require an image as appearance, which will restrict the motions of the generated results. Instead, our method can produce more dynamic motion and better visual quality. We conduct multiple experiments and comparisons to verify the effectiveness of the proposed method, demonstrating its efficacy in different scenarios. All the code will be made available.	https://dl.acm.org/doi/abs/10.1145/3721238.3730744	Xiuli Bi, Jianfei Yuan, Bo Liu, Yong Zhang, Xiaodong Cun, Chi-Man Pun, Bin Xiao
Model See Model Do: Speech-Driven Facial Animation with Style Control	Speech-driven 3D facial animation plays a key role in applications such as virtual avatars, gaming, and digital content creation. While existing methods have made significant progress in achieving accurate lip synchronization and generating basic emotional expressions, they often struggle to capture and effectively transfer nuanced performance styles. We propose a novel example-based generation framework that conditions a latent diffusion model on a reference style clip to produce highly expressive and temporally coherent facial animations. To address the challenge of accurately adhering to the style reference, we introduce a novel conditioning mechanism called style basis, which extracts key poses from the reference and additively guides the diffusion generation process to fit the style without compromising lip synchronization quality. This approach enables the model to capture subtle stylistic cues while ensuring that the generated animations align closely with the input speech. Extensive qualitative, quantitative, and perceptual evaluations demonstrate the effectiveness of our method in faithfully reproducing the desired style while achieving superior lip synchronization across various speech scenarios.	https://dl.acm.org/doi/abs/10.1145/3721238.3730672	Yifang Pan, Karan Singh, Luiz Gustavo Hafemann
Modeling Soot Oxidation of Flames as Level Set Erosion	From observations of luminous flames, it is evident that soot oxidation behaves as an erosion of the flame. This is due to the underlying physics where the flame interior is oxygen depleted, and oxidation is governed by diffusion and turbulent mixing with the air at the edge of the flame. Motivated by this, we model soot oxidation with a level set equation that combines a physics-based term for naturalistic results with a procedural term providing additional user-control over flame height and thickness. We demonstrate, verify and discuss the capabilities of our method by several examples ranging from small-scale flames to large-scale turbulent fire.	https://dl.acm.org/doi/abs/10.1145/3721239.3734077	Michael B. Nielsen, Rook Bridson
Modeling and Rendering Glow Discharge	Previous research in material models for surface and volume scattering has enabled highly realistic scenes in modern rendering systems. However, there has been comparatively little study of light sources in computer graphics despite their critical importance in illuminating and bringing life into these scenes. In the real world, photons are emitted through numerous physical processes including combustion, incandescence, and fluorescence. The qualities of light produced in each of these processes are unique to their physics, making them interesting to study individually. In this work, we propose a model for glow discharge, a form of light-emitting electrostatic discharge commonly found in Neon lights and gas discharge lamps. We take inspiration from works in computational physics and develop an efficient point-wise solver for the emission due to glow discharge suitable for traditional volume rendering systems. Our model distills the complex mechanics of this process into a set of flexible and interpretable parameters. We demonstrate that our model can replicate the visual qualities of glow discharge under varying gases.	https://dl.acm.org/doi/abs/10.1145/3721238.3730674	Venkataram Edavamadathil Sivaram, Ravi Ramamoorthi, Tzu-Mao Li
Monocular Online Reconstruction with Enhanced Detail Preservation	We propose an online 3D Gaussian-based dense mapping framework for photorealistic details reconstruction from a monocular image stream. Our approach addresses two key challenges in monocular online reconstruction: distributing Gaussians without relying on depth maps and ensuring both local and global consistency in the reconstructed maps. To achieve this, we introduce two key modules: the for effective Gaussian distribution and the for maintaining alignment and coherence at all scales. In addition, we present the (MOHV), a structure that regularizes Gaussians for capturing details across multiple levels of granularity. MOHV ensures accurate reconstruction of both fine and coarse geometries and textures, preserving intricate details while maintaining overall structural integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our framework achieves superior reconstruction quality with high computational efficiency. Moreover, it integrates seamlessly with various tracking systems, ensuring generality and scalability. Project page: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730659	Songyin Wu, Zhaoyang Lv, Yufeng Zhu, Duncan Frost, Zhengqin Li, Ling-Qi Yan, Carl Ren, Richard Newcombe, Zhao Dong
Motion Control via Metric-Aligning Motion Matching	We introduce a novel method for controlling a motion sequence using an arbitrary temporal control sequence using temporal alignment. Temporal alignment of motion has gained significant attention owing to its applications in motion control and retargeting. Traditional methods rely on either learned or hand-craft cross-domain mappings between frames in the original and control domains, which often require large, paired, or annotated datasets and time-consuming training. Our approach, named , achieves alignment by solely considering within-domain distances. It computes distances among patches in each domain and seeks a matching that optimally aligns the two within-domain distances. This framework allows for the alignment of a motion sequence to various types of control sequences, including sketches, labels, audio, and another motion sequence, all without the need for manually defined mappings or training with annotated data. We demonstrate the effectiveness of our approach through applications in efficient motion control, showcasing its potential in practical scenarios.	https://dl.acm.org/doi/abs/10.1145/3721238.3730665	Naoki Agata, Takeo Igarashi
Motion Inversion for Video Customization	In this work, we present a novel approach for motion customization in video generation, addressing the widespread gap in the exploration of motion representation within video generative models. Recognizing the unique challenges posed by the spatiotemporal nature of video, our method introduces Motion Embeddings, a set of explicit, temporally coherent embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity. Our approach provides a compact and efficient solution to motion representation, utilizing two types of embeddings: a Motion Query-Key Embedding to modulate the temporal attention map and a Motion Value Embedding to modulate the attention values. Additionally, we introduce an inference strategy that excludes spatial dimensions from the Motion Query-Key Embedding and applies a debias operation to the Motion Value Embedding, both designed to debias appearance and ensure the embeddings focus solely on motion. Our contributions include the introduction of a tailored motion embedding for customization tasks and a demonstration of the practical advantages and effectiveness of our method through extensive experiments. Project page: https://wileewang.github.io/MotionInversion/	https://dl.acm.org/doi/abs/10.1145/3721238.3730735	Luozhou Wang, Ziyang Mai, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, Ying-Cong Chen
Motion-example-controlled Co-speech Gesture Generation Leveraging Large Language Models	The automatic generation of controllable co-speech gestures has recently gained growing attention. While existing systems typically achieve gesture control through predefined categorical labels or implicit pseudo-labels derived from motion examples, these approaches often compromise the rich details present in the original motion examples. We present MECo, a framework for motion-example-controlled co-speech gesture generation by leveraging large language models (LLMs). Our method capitalizes on LLMs' comprehension capabilities through fine-tuning to simultaneously interpret speech audio and motion examples, enabling the synthesis of gestures that preserve example-specific characteristics while maintaining speech congruence. Departing from conventional pseudo-labeling paradigms, we position motion examples as explicit query contexts within the prompt structure to guide gesture generation. Experimental results demonstrate state-of-the-art performance across three metrics: Fréchet Gesture Distance (FGD), motion diversity, and example-gesture similarity. Furthermore, our framework enables granular control of individual body parts and accommodates diverse input modalities including motion clips, static poses, human video sequences, and textual descriptions.	https://dl.acm.org/doi/abs/10.1145/3721238.3730611	Bohong Chen, Yumeng Li, Youyi Zheng, Yao-Xiang Ding, Kun Zhou
MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation	This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications. Code and model weights are at https://motion-canvas25.github.io	https://dl.acm.org/doi/abs/10.1145/3721238.3730604	Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, Feng Liu
Multi-Person Interaction Generation from Two-Person Motion Priors	Generating realistic human motion with high-level controls is a crucial task for social understanding, robotics, and animation. With high-quality MOCAP data becoming more available recently, a wide range of data-driven approaches have been presented. However, modelling multi-person interactions still remains a less explored area. In this paper, we present Graph-driven Interaction Sampling, a method that can generate realistic and diverse multi-person interactions by leveraging existing two-person motion diffusion models as motion priors. Instead of training a new model specific to multi-person interaction synthesis, our key insight is to spatially and temporally separate complex multi-person interactions into a graph structure of two-person interactions, which we name the Pairwise Interaction Graph. We thus decompose the generation task into simultaneous single-person motion generation conditioned on one other's motion. In addition, to reduce artifacts such as interpenetrations of body parts in generated multi-person interactions, we introduce two graph-dependent guidance terms into the diffusion sampling scheme. Unlike previous work, our method can produce various high-quality multi-person interactions without having repetitive individual motions. Extensive experiments demonstrate that our approach consistently outperforms existing methods in reducing artifacts when generating a wide range of two-person and multi-person interactions.	https://dl.acm.org/doi/abs/10.1145/3721238.3730688	Wenning Xu, Shiyu Fan, Paul Henderson, Edmond S. L. Ho
Nested Attention: Semantic-aware Attention Values for Concept Personalization	Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.	https://dl.acm.org/doi/abs/10.1145/3721238.3730634	Or Patashnik, Rinon Gal, Daniil Ostashev, Sergey Tulyakov, Kfir Aberman, Daniel Cohen-Or
Neural BRDF Importance Sampling by Reparameterization	Neural bidirectional reflectance distribution functions (BRDFs) have emerged as popular material representations for enhancing realism in physically-based rendering. Yet their importance sampling remains a significant challenge. In this paper, we introduce a reparameterization-based formulation of neural BRDF importance sampling that seamlessly integrates into the standard rendering pipeline with precise generation of BRDF samples. The reparameterization-based formulation transfers the distribution learning task to a problem of identifying BRDF integral substitutions. In contrast to previous methods that rely on invertible networks and multi-step inference to reconstruct BRDF distributions, our model removes these constraints, which offers greater flexibility and efficiency. Our variance and performance analysis demonstrates that our reparameterization method achieves the best variance reduction in neural BRDF renderings while maintaining high inference speeds compared to existing baselines.	https://dl.acm.org/doi/abs/10.1145/3721238.3730679	Liwen Wu, Sai Bi, Zexiang Xu, Hao Tan, Kai Zhang, Fujun Luan, Haolin Lu, Ravi Ramamoorthi
Neural Importance Sampling of Many Lights	We propose a neural approach for estimating spatially varying light selection distributions to improve importance sampling in Monte Carlo rendering, particularly for complex scenes with many light sources. Our method uses a neural network to predict the light selection distribution at each shading point based on local information, trained by minimizing the KL-divergence between the learned and target distributions in an online manner. To efficiently manage hundreds or thousands of lights, we integrate our neural approach with light hierarchy techniques, where the network predicts cluster-level distributions and existing methods sample lights within clusters. Additionally, we introduce a residual learning strategy that leverages initial distributions from existing techniques, accelerating convergence during training. Our method achieves superior performance across diverse and challenging scenes.	https://dl.acm.org/doi/abs/10.1145/3721238.3730754	Pedro Figueiredo, Qihao He, Steve Bako, Nima Khademi Kalantari
Neural Performance Toolset: AI-Powered Human Performance Synthesis	The Metaphysic Neural Performance Toolset introduces a groundbreaking framework for photorealistic, AI-driven human performance synthesis. Leveraging advanced neural architectures, identity-specific training, and latent space manipulation, it delivers unparalleled realism and control for both cinematic post-production and real-time applications. Successfully deployed in major productions such as , , and , as well as live performances for Drake and Eminem, this toolset redefines AI-generated content in film and entertainment.	https://dl.acm.org/doi/abs/10.1145/3721239.3734119	Jo Plaete, Matteo Olivieri-Dancey, Oriel Frigo, Mihai Anton, Sebastian Correa, Simon Deckers, Thomas Salama, Terrence Bannon, Tomas Koutsky
Non-uniform Point Cloud Upsampling via Local Manifold Distribution	Existing learning-based point cloud upsampling methods often overlook the intrinsic data distribution characteristics of point clouds, leading to suboptimal results when handling sparse and non-uniform point clouds. We propose a novel approach to point cloud upsampling by imposing constraints from the perspective of manifold distributions. Leveraging the strong fitting capability of Gaussian functions, our method employs a network to iteratively optimize Gaussian components and their weights, accurately representing local manifolds. By utilizing the probabilistic distribution properties of Gaussian functions, we construct a unified statistical manifold to impose distribution constraints on the point cloud. Experimental results on multiple datasets demonstrate that our method generates higher-quality and more uniformly distributed dense point clouds when processing sparse and non-uniform inputs, outperforming state-of-the-art point cloud upsampling techniques.	https://dl.acm.org/doi/abs/10.1145/3728309	Yao Hui Fang, Xing Ce Wang
Normal-guided Detail-Preserving Neural Implicit Function for High-Fidelity 3D Surface Reconstruction	Neural implicit representations have emerged as a powerful paradigm for 3D reconstruction. However, despite their success, existing methods fail to capture fine geometric details and thin structures, especially in scenarios where only sparse multi-view RGB images of the objects of interest are available. This paper shows that training neural representations with first-order differential properties (surface normals) leads to highly accurate 3D surface reconstruction, even with as few as two RGB images. Using input RGB images, we compute approximate ground-truth surface normals from depth maps produced by an off-the-shelf monocular depth estimator. During training, we directly locate the surface point of the SDF network and supervise its normal with the one estimated from the depth map. Extensive experiments demonstrate that our method achieves state-of-the-art reconstruction accuracy with a minimal number of views, capturing intricate geometric details and thin structures that were previously challenging to capture. The source code and additional results are available at .	https://dl.acm.org/doi/abs/10.1145/3728293	Aarya Patel, Hamid Laga, Ojaswa Sharma
OOOOO Energy: Adding Dynamic Shading and Lighting to Environments on Pixar's Elio	"The production design for Pixar's (2025) called for complex and dynamic elements throughout the ""Communiverse"" environments of the film. In this talk we showcase several of these effects and discuss the differences in implementation to acheive specific visual goals, workflow efficencies and render optimizations. Many of these approaches blurred the lines between traditional disciplines and departments such as modeling, shading, lighting, dressing and animation. We leveraged features in Houdini, and USD, as well as custom shader solutions in Pixar's proprietary look development environment Flow to achieve the dynamic features in the film."	https://dl.acm.org/doi/abs/10.1145/3721239.3734113	Francisco De La Torre, Colin Thompson
OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape Generation	Autoregressive models have achieved remarkable success across various domains, yet their performance in 3D shape generation lags significantly behind that of diffusion models. In this paper, we introduce OctGPT, a novel multiscale autoregressive model for 3D shape generation that dramatically improves the efficiency and performance of prior 3D autoregressive approaches, while rivaling or surpassing state-of-the-art diffusion models. Our method employs a serialized octree representation to efficiently capture the hierarchical and spatial structures of 3D shapes. Coarse geometry is encoded via octree structures, while fine-grained details are represented by binary tokens generated using a vector quantized variational autoencoder (VQVAE), transforming 3D shapes into compact suitable for autoregressive prediction. To address the computational challenges of handling long sequences, we incorporate octree-based transformers enhanced with 3D rotary positional encodings, scale-specific embeddings, and token-parallel generation schemes. These innovations reduce training time by 13 folds and generation time by 69 folds, enabling the efficient training of high-resolution 3D shapes, e.g.,1024 , on just four NVIDIA 4090 GPUs only within days. OctGPT showcases exceptional versatility across various tasks, including text-, sketch-, and image-conditioned generation, as well as scene-level synthesis involving multiple objects. Extensive experiments demonstrate that OctGPT accelerates convergence and improves generation quality over prior autoregressive methods, offering a new paradigm for high-quality, scalable 3D content creation.	https://dl.acm.org/doi/abs/10.1145/3721238.3730601	Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, Peng-Shuai Wang
On Planar Shape Interpolation With Logarithmic Metric Blending	We present an interpolation method for planar shapes using logarithmic metric blending. Our approach generalizes prior work on pullback metrics to a framework, allowing us to employ different techniques, such as logarithmic blending of symmetric positive definite matrices, to have precise control over conformal and area distortions. Key contributions include generalizing the continuous blending scheme and its adaptation to discrete mesh interpolation through different conformal and isometric parameterizations. Experimental results demonstrate that our method outperforms existing techniques in achieving bounded distortions, making it a compelling choice for applications in animation and morphing.	https://dl.acm.org/doi/abs/10.1145/3721238.3730697	Alon Feldman, Mirela Ben-Chen
Ooze Control: Procedural Shapeshifting FX in Pixar's Elio	Cloning Clay, a space amoeba-like organic matter generating a variety of laughs, thrills, and clones throughout Pixar's (2025), required a suite of technical and creative FX techniques to land each story beat with a satisfying performance. In regular collaboration with several departments, this method delivered a range of effects, including dynamic hero clay FX, secondary rippling, and full-character transformations. A lightweight Houdini-based workflow was developed to ensure these techniques scaled efficiently across multiple sequences while minimizing per-shot overhead. Most shots were delivered via this base setup, but full-body transformations required significant rig customizations to handle increased complexity.	https://dl.acm.org/doi/abs/10.1145/3721239.3734121	Nate Skeen
Open Shading Language Lab	This lab will cover the cloning of the Academy Software Foundation (ASWF) Open Shading Language (OSL) repository, the installation or building of its software dependencies, and building the repository contents, including the testshade, testrender, and osltoy tools. The writing of custom OSL shaders will follow using osltoy to test and render them. A summary of a few OSL source code repositories will conclude the lab.	https://dl.acm.org/doi/abs/10.1145/3721251.3734058	Mitch J Prater
OpenVDB	This course will cover the compact volume data structure and various tools available in the open-source library OpenVDB. Since its release in 2012 it has become an industry standard and has been used in numerous VFX movies release in the past decade. It is also adopted by most commercial software packages used by the movie industry, including Houdini, RenderMan, Arnold, RealFlow, Clarisse, Guerrilla Render, Maxwell Render, Modo, V-Ray, Octane Render, 3Delight, Embergee, Blender, Chaos Phoenix, Corona, FumeFX, KeyShot, LightWave, ThinkingParticles, Eddy, Bifrost, Siemens NX, Unreal Engine, as well as bindings for Mathmatica. Recently, OpenVDB has also found use in new fields, including SLAM, autonomous driving, industrial design, 3D printing, medical imaging, rocket design, arial surveillance, robotics, and many machine learning applications. Finally, OpenVDB was the first open-source project to be adopted by the Academy Software Foundation (ASWF) and the Linux Foundation (in 2018).	https://dl.acm.org/doi/abs/10.1145/3721241.3733987	Ken Museth, Jeff Budsberg, Alexandre Sirois-Vigneux, Gregory Hurst, Francis Williams, Andre Pradhana, Dan Bailey, Nick Avramoussis
Optimizing Real-Time Gaussian Splat Rendering for Mobile and Standalone VR	This hands-on workshop will focus on rendering gaussian splats in real-time on mobile or standalone VR devices. It is intended to be an intermediate course. It will be done using the UnityGaussianSplatting renderer, first showing how it works, then adding some optimizations on top of it to make it performant on mobile GPUs. Attendees should have programming experience, preferably in C# and HLSL, and a basic understanding of the Unity game engine. Attendees should have Unity 6 and Visual Studio Code pre-loaded on their devices if they intend to follow along. Also it is encouraged to have an Android phone or standalone VR device to run the final optimized renderer, but it is not essential.	https://dl.acm.org/doi/abs/10.1145/3721251.3734056	Olivier Therrien
Out of This World Shading: Look Development for Aliens in Pixar's Elio	"The aliens in Pixar's commanded a unique look in space. We crafted each species individually with respect to its own characteristic features, carefully choosing colors and material properties to create over 18 species. Their designs took inspiration from various flora and fauna including micro-organisms, worm-like, bird-like, and sea creatures. They needed to be appealing, organic, and tactile but without feeling ""earth bound"". To implement these designs and help them feel unique and alien, our team explored combining illumination models and animated shading techniques in ways not commonly found in earth species and materials. This process created a collaborative and interactive approach to our look development to celebrate diversity and inclusivity in 's fantastic space aliens."	https://dl.acm.org/doi/abs/10.1145/3721239.3734107	Trent Crow, Maria Lee, Patrick Yu Wang, Gus Dizon
PAAP: Performer-Aware Automatic Panning System	We propose , the first system to automatically track performer(s) and generate spatial audio panning data integrated with a Digital Audio Workstation (DAW). The system pipeline consists of three main stages: (1) visual cue analysis via performer tracking and monocular depth estimation, (2) spatial information prediction using a custom algorithm that produces DAW-compatible panning parameters, and (3) integration of industry-standard DAW using embedded script processing. We tested and validated the technical feasibility and real-world applicability including Open Sound Control (OSC) based real-time processing. To our knowledge, this is the first complete study of an automatic panning with DAW and we anticipate PAAP to streamline live and studio music production.	https://dl.acm.org/doi/abs/10.1145/3721250.3742989	Kangeun Lee, Sungyoung Kim
PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers	Humans excel in navigating diverse, complex environments with agile motor skills, exemplified by parkour practitioners performing dynamic maneuvers, such as climbing up walls and jumping across gaps. Reproducing these agile movements with simulated characters remains challenging, in part due to the scarcity of motion capture data for agile terrain traversal behaviors and the high cost of acquiring such data. In this work, we introduce PARC (Physics-based Augmentation with Reinforcement Learning for Character Controllers), a framework that leverages machine learning and physics-based simulation to iteratively augment motion datasets and expand the capabilities of terrain traversal controllers. PARC begins by training a motion generator on a small dataset consisting of core terrain traversal skills. The motion generator is then used to produce synthetic data for traversing new terrains. However, these generated motions often exhibit artifacts, such as incorrect contacts or discontinuities. To correct these artifacts, we train a physics-based tracking controller to imitate the motions in simulation. The corrected motions are then added to the dataset, which is used to continue training the motion generator in the next iteration. PARC's iterative process jointly expands the capabilities of the motion generator and tracker, creating agile and versatile models for interacting with complex environments. PARC provides an effective approach to develop controllers for agile terrain traversal, which bridges the gap between the scarcity of motion data and the need for versatile character controllers.	https://dl.acm.org/doi/abs/10.1145/3721238.3730616	Michael Xu, Yi Shi, KangKang Yin, Xue Bin Peng
PDT: Point Distribution Transformation with Diffusion Models	Point-based representations have consistently played a vital role in geometric data structures. Most point cloud learning and processing methods typically leverage the unordered and unconstrained nature to represent the underlying geometry of 3D shapes. However, how to extract meaningful structural information from unstructured point cloud distributions and transform them into semantically meaningful point distributions remains an under-explored problem. We present PDT, a novel framework for point distribution transformation with diffusion models. Given a set of input points, PDT learns to transform the point set from its original geometric distribution into a target distribution that is semantically meaningful. Our method utilizes diffusion models with novel architecture and learning strategy, which effectively correlates the source and the target distribution through a denoising process. Through extensive experiments, we show that our method successfully transforms input point clouds into various forms of structured outputs - ranging from surface-aligned keypoints, and inner sparse joints to continuous feature lines. The results showcase our framework's ability to capture both geometric and semantic features, offering a powerful tool for various 3D geometry processing tasks where structured point distributions are desired. Code will be available at this link: link.	https://dl.acm.org/doi/abs/10.1145/3721238.3730717	Jionghao Wang, Cheng Lin, Yuan Liu, Rui Xu, Zhiyang Dou, Xiaoxiao Long, Haoxiang Guo, Taku Komura, Wenping Wang, Xin Li
PLT: Part-Wise Latent Tokens as Adaptable Motion Priors for Physically Simulated Characters	Physically simulated characters can learn highly natural full-body motion guided by motion capture datasets. However, the range of motion is limited to the existing high-quality datasets, and cannot effectively adapt to challenging scenarios. We propose a novel policy architecture that learns part-wise motion skills, where individual parts can be separately extended and combined for unobserved settings. Our method employs a set of part-specific codebooks, which robustly capture motion dynamics without catastrophic collapse or forgetting. This structured decomposition allows intuitive control over the character's behavior and dynamic exploration for a novel combination of part-wise motion. We further incorporate a refinement network compensating for subtle discrepancies in the disjoint discrete tokens, thus improving motion quality and stability. Our extensive evaluations show that our part-wise latent token achieves superior performance in imitating motions, even those from unseen distribution. We also validate our method in challenging tasks, including body tracking, navigation on complex terrains, and point-goal navigation with damaged body parts. Finally, we introduce a part-wise expansion of motion priors, where the physically simulated character incrementally adapts partial motion and produces unique combinations of whole-body motion, significantly diversifying motions.	https://dl.acm.org/doi/abs/10.1145/3721238.3730637	Jinseok Bae, Younghwan Lee, Donggeun Lim, Young Min Kim
PUPPIX - Real Time Live Performed Digital Characters Using Physical Puppet Twins	Live physical whole-puppet performances can be used to drive digital animation characters and creatures via puppix, a new capture system. The benefits of having a live puppet character interacting in the room with audience, actors, directors and other characters are demonstrated as well as some practical processes of capturing non-human physicalities. Physical puppets allow directors and actors to work with non-human characters with the same flexibility, freedom and immediacy as human actors. Capturing these performances means non-human digital characters can work alongside and be directed like physical actors. Unlike keyframe animation, capture of in-the-moment live performance allows real-world weight, physicality and movement transfer to digital twins. This also disrupts the limitations of human-based motion capture systems and the bulk of learning model training data sets, whose movements are originally from human physicality. The origins of motion capture as a whole come from the technology of puppetry and animatronics. Performance armatures and rigs like Dinosaur Input Device, Sil and Hensons' Waldo operate as control systems for digital performances, with director focus on digital output screens. puppix, a whole-puppet capture system, keeps the performance focus on the character in the room, not on the screen. At present, reference puppets are being used on-set as placeholders for CG characters, providing lighting, position, interaction reference, whilst actors interact with the reference puppeteers' performances. puppix allows the full reference puppet performance to be motion captured. Secondary movements and whole body physicality match the digital characters and transfer to the digital character for free. Director and performers focus in the room, whilst creating digital animated performances. This is a tool like human-based motion capture, but for non-human characters and creatures. Our Real Time Live! segment demonstrates the practicalities of operating a capture puppet on-set during a shoot, allowing interaction with presenter / director and audience, using a puppix capture puppet live-performed by puppeteers during the presentation.	https://dl.acm.org/doi/abs/10.1145/3721243.3735990	Ben Mars, Dik Downey
PaRas: A Rasterizer for Large-Scale Parametric Surfaces	The advantages of higher-order surfaces, such as their ability to represent complex geometry compactly and smoothly, have led to their increasing use in computer graphics. This trend underscores the importance of developing efficient rendering algorithms tailored for these representations. We introduce PaRas, a highly performant rasterizer for real-time rendering of large-scale parametric surfaces with high precision. Unlike conventional graphics pipelines that rely on hardware tessellation to convert smooth surfaces into numerous flat triangles, our method provides a highly efficient and parallel approach to directly rasterize parametric surfaces. PaRas seamlessly integrates into existing workflows, enabling smooth surfaces to be handled with the same ease as triangle meshes. To accomplish this, we formulate the rasterization of parametric surfaces as a point inversion problem, employing a Newton-type iteration on the GPU to compute precise solutions. The framework's effectiveness is demonstrated on quartic triangular Bézier patches and rational Bézier patches, both commonly used in high-precision modeling and industrial applications. Experimental results indicate that our rendering pipeline achieves higher efficiency and greater accuracy compared to traditional hardware tessellation techniques.	https://dl.acm.org/doi/abs/10.1145/3721238.3730658	Kechun Wang, Renjie Chen
Painterly Fur and Feathers of The Wild Robot	"For the Look Development of The Wild Robot, characters needed to exhibit the richness, physicality, and motion of real-world fur and feathers. But unlike a realistic simulation, the look mimics how an artist applies detail to a painting: in layers of brushstrokes that form simplified groupings of light and color, with surgical placement of highlights. ‬While fur and feathers are traditionally shaded as curves, these features needed to respond to light like continuous surfaces to better fit into the stylized painterly world. To achieve the desired look within our traditional modeling, grooming and simulation workflows, new Look Development tools were developed including new shaders, custom groom attributes, an all-new feather system and new LookDev lighting rigs. Custom normals, UVs and derivatives were baked into the groom curves that provided a continuous canvas for a new shader that used artist-controlled ramps to stylize how these curves respond to light at different angles. The shader provided separate layering for each of the specular and diffuse lobes, written out to additional ""accent"" AOVs. While the final look came together in compositing, our renderer added a mechanism to preview this look with its built-in Display Filter compositor. ‬The resulting characters meshed beautifully with the stylized organic island sets, while maintaining the desired physicality.‬‬"	https://dl.acm.org/doi/abs/10.1145/3721239.3734088	Lisa Connors, Baptiste Van Opstal, Dan McCann
Painting with 3D Gaussian Splat Brushes	We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. In particular, we propose a set of algorithms for 1) selecting subsets of Gaussians as a brush pattern interactively, 2) applying the brush interactively to the same or other 3DGS scenes or other 3D surfaces using stamp-based painting, 3) using an inpainting Diffusion Model to adjust stamp seams for seamless and realistic appearance. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. We evaluate our system by showing compelling results on a diverse set of 3D scenes; and a user study with VFX/animation professionals, to validate system features, workflow, and potential for creative impact. Code and data for this paper can be accessed from splatpainting.github.io.	https://dl.acm.org/doi/abs/10.1145/3721238.3730724	Karran Pandey, Anita Hu, Clement Fuji Tsang, Or Perel, Karan Singh, Maria Shugrina
Painting with 3D Gaussian Splat Brushes	We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. In particular, we propose a set of algorithms for 1) selecting subsets of Gaussians as a brush pattern interactively, 2) applying the brush interactively to the same or other 3DGS scenes or other 3D surfaces using stamp-based painting, 3) using an inpainting Diffusion Model to adjust stamp seams for seamless and realistic appearance. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. We evaluate our system by showing compelling results on a diverse set of 3D scenes; and a user study with VFX/animation professionals, to validate system features, workflow, and potential for creative impact. Code and data for this paper can be accessed from splatpainting.github.io.	https://dl.acm.org/doi/abs/10.1145/3721238.3730724	Karran Pandey, Anita Hu, Humbert Hardy, Frank Nadeau, Eloi Champagne, Karan Singh, Maria Shugrina
Painting with 3D Gaussian Splat Brushes	We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. In particular, we propose a set of algorithms for 1) selecting subsets of Gaussians as a brush pattern interactively, 2) applying the brush interactively to the same or other 3DGS scenes or other 3D surfaces using stamp-based painting, 3) using an inpainting Diffusion Model to adjust stamp seams for seamless and realistic appearance. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. We evaluate our system by showing compelling results on a diverse set of 3D scenes; and a user study with VFX/animation professionals, to validate system features, workflow, and potential for creative impact. Code and data for this paper can be accessed from splatpainting.github.io.	https://dl.acm.org/doi/abs/10.1145/3721243.3735989	Karran Pandey, Anita Hu, Clement Fuji Tsang, Or Perel, Karan Singh, Maria Shugrina
Painting with 3D Gaussian Splat Brushes	We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. In particular, we propose a set of algorithms for 1) selecting subsets of Gaussians as a brush pattern interactively, 2) applying the brush interactively to the same or other 3DGS scenes or other 3D surfaces using stamp-based painting, 3) using an inpainting Diffusion Model to adjust stamp seams for seamless and realistic appearance. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. We evaluate our system by showing compelling results on a diverse set of 3D scenes; and a user study with VFX/animation professionals, to validate system features, workflow, and potential for creative impact. Code and data for this paper can be accessed from splatpainting.github.io.	https://dl.acm.org/doi/abs/10.1145/3721243.3735989	Karran Pandey, Anita Hu, Humbert Hardy, Frank Nadeau, Eloi Champagne, Karan Singh, Maria Shugrina
Painting with 3D Gaussian Splat Brushes	We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. Code and data can be accessed from splatpainting.github.io.	https://dl.acm.org/doi/abs/10.1145/3721238.3730724	Karran Pandey, Anita Hu, Clement Fuji Tsang, Or Perel, Karan Singh, Maria Shugrina
Painting with 3D Gaussian Splat Brushes	We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. Code and data can be accessed from splatpainting.github.io.	https://dl.acm.org/doi/abs/10.1145/3721238.3730724	Karran Pandey, Anita Hu, Humbert Hardy, Frank Nadeau, Eloi Champagne, Karan Singh, Maria Shugrina
Painting with 3D Gaussian Splat Brushes	We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. Code and data can be accessed from splatpainting.github.io.	https://dl.acm.org/doi/abs/10.1145/3721243.3735989	Karran Pandey, Anita Hu, Clement Fuji Tsang, Or Perel, Karan Singh, Maria Shugrina
Painting with 3D Gaussian Splat Brushes	We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. Code and data can be accessed from splatpainting.github.io.	https://dl.acm.org/doi/abs/10.1145/3721243.3735989	Karran Pandey, Anita Hu, Humbert Hardy, Frank Nadeau, Eloi Champagne, Karan Singh, Maria Shugrina
Paper Animatronics Workshop 2025	Kids (and many adults) often fail to see engineers and scientists as creative in the same sense as artists, writers, and musicians. This is understandable because STEM classes typically teach basic skills and then ask students to apply these to solve known problems to get to the one, correct, known solution. Creativity is explicitly discouraged. With paper animatronics, we try to break this paradigm, showing kids that technology provides powerful tools for creativity. Students tell the important stories of history, culture, science, or just about any subject via their papercraft which they bring to life with motion and synchronized sound. Their characters literally talk, with mouths moving appropriately as they speak. Kids get that storytelling and papercraft are creative tasks, and they quickly come to see the animatronics parts as simply additional things with which to be creative. Here, the story is the point, with the tech playing an important, but supporting role. In this paper animatronics workshop, you will make your own storytelling paper robots using our latest, easy-to-use, easy-to-afford, animatronics kits. Building physical things is surprisingly fun and engages learner creativity in a very different way than screens. Hopefully, this will spark your interest in bringing these sorts of technology-based, physical storytelling projects to schools in your community.	https://dl.acm.org/doi/abs/10.1145/3721251.3734062	Paul H. Dietz, Sarah Kushner, Catherine Dietz, Jennifer Ginger Alford, Elliot Mueller
PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models	We present the first image editing approach for based on pre-trained diffusion models. Diffusion-based image editing approaches capitalized on the deep understanding of diffusion models of image semantics to perform a variety of edits. However, existing diffusion models lack sufficient understanding of many object parts, hindering fine-grained edits requested by users. To address this, we propose to expand the knowledge of pre-trained diffusion models to allow them to understand various object parts, enabling them to perform fine-grained edits. We achieve this by learning special textual tokens that correspond to different object parts through an efficient token optimization process. These tokens are optimized to produce reliable localization masks to localize the editing region. Leveraging these masks, we design feature-blending and adaptive thresholding strategies to execute the edits seamlessly. To evaluate our approach, we establish a benchmark and an evaluation protocol for part editing. Experiments show that our approach outperforms existing editing methods on all metrics and is preferred by users of the time in conducted user studies.	https://dl.acm.org/doi/abs/10.1145/3721238.3730747	Aleksandar Cvejic, Abdelrahman Eldesokey, Peter Wonka
Path Guiding in Production and Recent Advancements	Over the last decade, advanced data-driven sampling algorithms, such as path guiding, have made their way from the scientific realm into production renderers [Vorba et al. ]. These algorithms enable the rendering of challenging lighting effects (e.g., complex indirect illumination, caustics, volumetric multi-scattering, and occluded direct illumination from multiple lights), which are crucial for generating high-fidelity images. The fact that these algorithms primarily focus on optimizing local importance sampling decisions makes it possible to integrate them into a path tracer, the de facto standard rendering algorithm used in production today ([Fascione et al. ],[Jakob et al. ]). The theory behind these algorithms has been presented and discussed on various occasions (e.g., in presentations or research papers), and their practical applications in production have been explored in the previous course on . Nevertheless, the implementation details or challenges associated with integrating them into a production render are usually unknown or not publicly discussed. This course aims to provide a deeper understanding of how specific guiding algorithms are integrated into and utilized in various production renderers, including Blender's Cycles, Chaos's VRay and Corona, SideFX's Karma, and Disney Animation's Hyperion [Burley et al. ]. The presented algorithms and integrations can be categorized into two main groups: the first aims to guide the entire sampling process by utilizing information about the total light transport of the scene, and the second focuses on guiding specific effects, such as caustics.	https://dl.acm.org/doi/abs/10.1145/3721241.3733994	Sebastian Herholz, Martin Sik, Lea Reichardt, Marco Manzi
Perpetual Ocean 2: Western Boundary Currents	This is a scientific data visualization of ocean currents around the world based on the ocean model, Estimating the Circulation and Climate of the Ocean (ECCO). The visualization is a tour of major currents of the world including western boundary currents and includes both surface and deep currents.	https://dl.acm.org/doi/abs/10.1145/3698896.3722127	Gregory Shirah
Photoreal Scene Reconstruction from an Egocentric Device	In this paper, we investigate the challenges associated with using egocentric devices to photorealistic reconstruct the scene in high dynamic range. Existing methodologies typically assume using frame-rate 6DoF pose estimated from the device's visual-inertial odometry system, which may neglect crucial details necessary for pixel-accurate reconstruction. This study presents two significant findings. Firstly, in contrast to mainstream work treating RGB camera as global shutter frame-rate camera, we emphasize the importance of employing visual-inertial bundle adjustment (VIBA) to calibrate the precise timestamps and movement of the rolling shutter RGB sensing camera in a high frequency trajectory format, which ensures an accurate calibration of the physical properties of the rolling-shutter camera. Secondly, we incorporate a physical image formation model based into Gaussian Splatting, which effectively addresses the sensor characteristics, including the rolling-shutter effect of RGB cameras and the dynamic ranges measured by sensors. Our proposed formulation is applicable to the widely-used variants of Gaussian Splats representation. We conduct a comprehensive evaluation of our pipeline using the open-source Project Aria device under diverse indoor and outdoor lighting conditions, and further validate it on a Meta Quest3 device. Across all experiments, we observe a consistent visual enhancement of +1 dB in PSNR by incorporating VIBA, with an additional +1 dB achieved through our proposed image formation model. Our complete implementation, evaluation datasets, and recording profile are available at https://www.projectaria.com/photoreal-reconstruction/	https://dl.acm.org/doi/abs/10.1145/3721238.3730753	Zhaoyang Lv, Maurizio Monge, Ka Chen, Yufeng Zhu, Michael Goesele, Jakob Engel, Zhao Dong, Richard Newcombe
Physically Based Shading in Theory and Practice	Physically based shading has transformed the way we approach production rendering and simplified the lives of artists in the process. By employing shading models that adhere to physical principles, one can readily create high quality, realistic materials that maintain their appearance under a variety of lighting environments, in contrast to the ad hoc models of yore. However, physically based shading is not a solved problem, and thus the aim of this course is to share the latest theory as well as lessons from production.	https://dl.acm.org/doi/abs/10.1145/3721241.3733991	Stephen Hill, Stephen McAuley, Laurent Belcour, Naty Hoffman, Alain Hostettler, Peter Kutz, Kentaro Suzuki, Hajime Uchimura, Andrea Weidlich, Kenichiro Yasutomi, Pascal Barla, Alban Fichet, Jamie Portsmouth
Physically Controllable Relighting of Photographs	We present a self-supervised approach to in-the-wild image relighting that enables fully controllable, physically based illumination editing. We achieve this by combining the physical accuracy of traditional rendering with the photorealistic appearance made possible by neural rendering. Our pipeline works by inferring a colored mesh representation of a given scene using monocular estimates of geometry and intrinsic components. This representation allows users to define their desired illumination configuration in 3D. The scene under the new lighting can then be rendered using a path-tracing engine. We send this approximate rendering of the scene through a feed-forward neural renderer to predict the final photorealistic relighting result. We develop a differentiable rendering process to reconstruct in-the-wild scene illumination, enabling self-supervised training of our neural renderer on raw image collections. Our method represents a significant step in bringing the explicit physical control over lights available in typical 3D computer graphics tools, such as Blender, to in-the-wild relighting.	https://dl.acm.org/doi/abs/10.1145/3721238.3730666	Chris Careaga, Yağız Aksoy
Physics-inspired Estimation of Optimal Cloth Mesh Resolution	In this paper, we tackle an important yet often overlooked question: What is the optimal mesh resolution for cloth simulation, without relying on preliminary simulations? The optimal resolution should be sufficient to capture fine details of all potential wrinkles, while avoiding an unnecessarily high resolution that wastes computational time and memory on excessive vertices. This challenge stems from the complex nature of wrinkle distribution, which varies spatially, temporally, and anisotropically across different orientations. To address this, we propose a method to estimate the optimal cloth mesh resolution, based on two key factors: material stiffness and boundary conditions. To determine the influence of material stiffness on wrinkle wavelength and amplitude, we apply the experimental theory presented by Cerda and Mahadevan [ ] to calculate the optimal mesh resolution for cloth fabrics. Similarly, for boundary conditions influencing local wrinkle formation, we use the same scaling law to determine the source resolution for stationary boundary conditions introduced by garment-making techniques such as shirring, folding, stitching, and down-filling, as well as predicted areas accounting for dynamic wrinkles introduced by collision compression caused by human motions. To ensure a smooth transition between different source resolutions, we apply another experimental theory from [Vandeparre et al. ] to compute the transition distance. A mesh sizing map is introduced to facilitate smooth transitions, ensuring precision in critical areas while maintaining efficiency in less important regions. Based on these sizing maps, triangular meshes with optimal resolution distribution are generated using Poisson sampling and Delaunay triangulation. The resulting method can not only enhance the realism and precision of cloth simulations but also support diverse application scenarios, making it a versatile solution for complex garment design.	https://dl.acm.org/doi/abs/10.1145/3721238.3730619	Diyang Zhang, Zhendong Wang, Zegao Liu, Xinming Pei, Weiwei Xu, Huamin Wang
Please Don't Write a New Render Farm, Customize Flamenco	is the Open Source render farm software developed by Blender Studio. It can be used for distributed rendering across multiple machines, but also as a single-machine queue runner. This hands-on class will briefly teach how to install and use it, and then focus on customizing it to your needs. This will cover custom job types, as well as modifications to Flamenco itself.	https://dl.acm.org/doi/abs/10.1145/3721251.3734064	Sybren Stüvel
Pocket Time-Lapse	This paper explores how to record, explore, and visualize long-term changes in an environment—at the scale of days, months, and even years—based on data that a single user can conveniently capture using the mobile phone they already carry. Our strategy involves making the data capture process as quick and convenient as possible so that it is easy to integrate into daily routines. This strategy yields large unstructured panoramic image datasets, which we process using novel registration and scene reconstruction approaches. Our central contribution lies in demonstrating pocket time-lapse as a novel application, made possible through several key technical contributions. These include a novel method for quickly and robustly registering thousands of unstructured panoramic images, a novel reconstruction technique for rendering time-lapse and performing state-of-the-art intrinsic image decomposition, and several large hand-captured datasets that span multiple years of data collection, totaling over 6k separate capture sessions and 50k images.	https://dl.acm.org/doi/abs/10.1145/3721238.3730594	Eric Chen, Žiga Kovačič, Madhav Aggarwal, Abe Davis
Position-Normal Manifold for Efficient Glint Rendering on High-Resolution Normal Maps	Detailed microstructures on specular objects often exhibit intriguing glinty patterns under high-frequency lighting, which is challenging to render using a conventional normal-mapped BRDF. In this paper, we present a manifold-based formulation of the glint normal distribution functions (NDF) that precisely captures the surface normal distributions over queried footprints. The manifold-based formulation transfers the integration for the glint NDF construction to a problem of mesh intersections. Compared to previous works that rely on complex numerical approximations, our integral solution is exact and much simpler to compute, which also allows an easy adaptation of a mesh clustering hierarchy to accelerate the NDF evaluation of large footprints. Our performance and quality analysis shows that our NDF formulation achieves similar glinty appearance compared to the baselines but is an order of magnitude faster. Within this framework, we further present a novel derivation of analytical shadow-masking for normal-mapped diffuse surfaces—a component that is often ignored in previous works.	https://dl.acm.org/doi/abs/10.1145/3721238.3730633	Liwen Wu, Fujun Luan, Miloš Hašan, Ravi Ramamoorthi
Power-Linear Polar Directional Fields	We introduce a novel method for directional-field design on meshes, enabling users to specify singularities at any location on a mesh. Our method uses a piecewise power-linear representation for phase and scale, offering precise control over field topology. The resulting fields are smooth and accommodate any singularity index and field symmetry. With this representation, we mitigate the artifacts caused by coarse or uneven meshes. We showcase our approach on meshes with diverse topologies and triangle qualities.	https://dl.acm.org/doi/abs/10.1145/3721238.3730591	Jiabao Brad Wang, Amir Vaxman
Practical Stylized Nonlinear Monte Carlo Rendering	The recent formulation of stylized rendering equation (SRE) models stylization by applying nonlinear functions to reflected radiance recursively at each bounce, allowing seamless blend between stylized and physically based light transport. A naive estimator has to branch at each stylized surface, resulting in exponential computation and storage cost. We propose a practical approach for rendering scenes with SRE at a tractable cost. We first propose nonlinear path filtering (NL-PF) that caches the radiance evaluations at intermediate bounces, reducing the exponential sampling cost of the branching estimator of SRE to polynomial. Despite the effectiveness of NL-PF, its high memory cost makes it less scalable. To further improve efficiency, we propose nonlinear radiance caching (NL-NRC) where we apply a compact neural network to store radiance fields. Our NL-NRC has the same linear time sampling cost as a non-branching path tracer and can solve SRE with a high number of bounces and recursive stylization. Our key insight is that, by allowing the network to learn outgoing radiance prior to applying any nonlinear function, the network converges to the correct solution, even when we only have access to biased gradients due to nonlinearity. Our NL-NRC enables rendering scenes with arbitrary, highly nonlinear stylization while achieving significant speedup over branching estimators.	https://dl.acm.org/doi/abs/10.1145/3721238.3730686	Xiaochun Tong, Toshiya Hachisuka
Predicting Accidents in Conditional Autonomous Driving: A Multimodal Approach Integrating Human Misuse, Biometric Indicators, and Spatial Complexity	Decreased attention, distraction, and complex environments are major contributors to accidents in Level 2 autonomous driving. This study examines how spatial complexity and human factors affect accident risk using scenario-based simulations. We analyzed subjective factors (workload, situation awareness) and biometric data (eye tracking, HRV). Logistic regression identified age, workload, and situation awareness as significant predictors, with 74.2% accuracy (5-fold cross-validation). High spatial complexity increased cognitive load and visual scanning, elevating accident risk. These results support the need for integrated prediction strategies and adaptive driver support systems to enhance safety.	https://dl.acm.org/doi/abs/10.1145/3721250.3743029	Eun Hye Jang, Mi Chang, Woojin Kim, Daesub Yoon
Preserving Intangible Cultural Heritage of Megalithic Sites using Immersive Mobile XR	This poster introduces the INT-ACT project which aims to investigate the use of immersive XR environments for presenting the , and dimensions of Intangible Cultural Heritage (ICH) associated with tangible cultural heritage sites. It also presents a mobile XR demonstrator, developed as part of INT-ACT, that focuses on the ICH related to a megalithic site.	https://dl.acm.org/doi/abs/10.1145/3721250.3743013	Masood Masoodian, Inkeri Aula, Renata Vieira, Áurea Rodrigues, Ivo Santos, António Lacerda Diniz, Camila Campos, Rafael Prezado, Leonor Rocha
Pretraining Support for Cheerleading Stunts using Virtual Reality	Cheerleading stunts are group gymnastics performed by multiple people. As the skills involved become more challenging, it is necessary to devise better practice methods. Thus, in this paper, we propose a pretraining support system for cheerleading stunts using Virtual Reality (VR) technology. This system enables users to experience successfully performing a stunt in the virtual space by adopting the viewpoints of the cheerleaders performing various types of stunts. We evaluated our system through interviews with two experts. Our system has the potential to meaningfully augment the established training method of previsualization of stunts.	https://dl.acm.org/doi/abs/10.1145/3721250.3743007	Mizuki Akiyama, Christian Sandor, Yuki Igarashi
PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive transformer	Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games.	https://dl.acm.org/doi/abs/10.1145/3721238.3730732	Jingwen Ye, Yuze He, Yanning Zhou, Yiqin Zhu, Kaiwen Xiao, Yong-Jin Liu, Wei Yang, Xiao Han
Puppets to Pixels - Using live physical puppet performances for digital animation characters and creatures	Live physical whole-puppet performances can be used to drive digital animation characters and creatures via puppix, a new capture system. The benefits of having a live puppet character in the room with actors, directors and other characters are demonstrated and discussed, as well as practical processes of capturing non-human physicalities. We practically demonstrate the experience of having a live puppet performance in the room via audience interaction with a professionally performed theatrical puppet (Mr D Wolf), which continues at points during the presentation. Physical puppets allow directors and actors to work with non-human characters with the same flexibility, freedom and immediacy as human actors. Capturing these performances means non-human digital characters can work alongside and be directed like physical actors. Unlike keyframe animation, capture of in-the-moment live performance allows real-world weight, physicality and movement transfer to digital twins. This also disrupts the limitations of human-based motion capture systems and the bulk of learning model training data sets, whose movements are originally from human physicality. The origins of motion capture as a whole come from the technology of puppetry and animatronics. Performance armatures and rigs like Dinosaur Input Device, Sil and Hensons' Waldo operate as control systems for digital performances, with director focus on digital output screens. puppix, a whole-puppet capture system, keeps the performance focus on the character in the room, not on the screen. We show examples of pre-recorded puppix outputs alongside the original performance footage. We detail practicalities: of how to build a puppet pair for motion capture, successful live performance methods and processes, data processing considerations, practical capture considerations and extra notes for the VFX supervisor. At present, reference puppets are being used on-set as placeholders for CG characters, providing lighting, position, interaction reference, whilst actors interact with the reference puppeteers' performances. puppix allows the full reference puppet performance to be motion captured. Secondary movements and whole body physicality match the digital characters and transfer to the digital character for free. Director and performers focus in the room, whilst creating digital animated performances. This is a tool like human-based motion capture, but for non-human characters and creatures.	https://dl.acm.org/doi/abs/10.1145/3721239.3734104	Ben Mars, Dik Downey, Kristian Moen, D Wolf
QRBTF - AI QR Code Generator	QRBTF generates artistic QR codes that maintain machine readability while enhancing visual appeal. Our method integrates diffusion models with ControlNet conditioning and adaptive brightness control. Experimental results demonstrate effective brightness contrast control in specific image regions and robust model migration capabilities. Key innovations include: (1) Ternary luminance quantization mapping QR modules to control signals; (2) Style-adaptive generation using LoRA embeddings; (3) Post-processing optimization. The system has generated over 600,000 codes via qrbtf.com , validating its utility in branding and digital marketing applications.	https://dl.acm.org/doi/abs/10.1145/3721250.3743038	Hao Ni, Baiyu Chen, Zhaohan Wang, Zhiyong Chen, Wanyi Miao, Xin Lyu, Nan Cao
Quadtree Tall Cells for Eulerian Liquid Simulation	This paper introduces a novel grid structure that extends tall cell methods for efficient deep water simulation. Unlike previous tall cell methods, which are designed to capture all the fine details around liquid surfaces, our approach subdivides tall cells horizontally, allowing for more aggressive adaptivity and a significant reduction in the number of cells. The foundation of our method lies in a new variational formulation of Poisson's equations for pressure solve tailored for tall-cell grids, which naturally handles the transition of variable-sized cells. This variational view not only permits the use of the efficacy-proven conjugate gradient method but also facilitates monolithic two-way coupled rigid bodies. The key distinction between our method and previous general adaptive approaches, such as tetrahedral or octree grids, is the simplification of adaptive grid construction. Our method performs grid subdivision in a quadtree fashion, rather than an octree. These 2D cells are then simply extended vertically to complete the tall cell population. We demonstrate that this novel form of adaptivity, which we refer to as quadtree tall cells, delivers superior performance compared to traditional uniform tall cells.	https://dl.acm.org/doi/abs/10.1145/3721238.3730652	Fumiya Narita, Nimiko Ochiai, Takashi Kanai, Ryoichi Ando
Quantum Teleportation: Let's Write Code and Run It!	Quantum teleportation is a protocol that allows two people, conventionally named Alice and Bob, to transfer (not copy) quantum information. The transfer is performed seemingly instantaneously, no matter how far apart they are. It requires that Alice an Bob each have one of two entangled qubits, and that Alice sends two classical bits to Bob by conventional means. Note that this is a transfer of , rather than anything physical. Quantum teleportation is not teleportation as usually seen in science fiction, because we can only send quantum information. There's no way for Bob to turn that received information into matter, whether it's a grumpy (but humane) doctor or an exploding warp core.	https://dl.acm.org/doi/abs/10.1145/3721251.3742866	Andrew Glassner
RELATE3D: REfocusing Latent Adapter for Targeted local Enhancement and Editing in 3D Generation	Recent advancements in 3D generation techniques have simplified the tedious manual process of 3D asset production. Among these methods, 3D native latent diffusion models are particularly effective in generating high-quality geometric details. However, achieving local enhancement and editing of the generated 3D models remains a challenge due to the limited understanding of the relationship between text,images,and 3D in terms of local semantics and feature space.We explore and reveal the characteristics of the native 3D latent space, make it decomposable and low-rank, thereby enabling efficient and effective learning for multimodal local alignment. Based on this, we introduce , a novel approach that combines a Refocusing Adapter with part-to-latent correspondence guided training for precise local enhancement and part-level editing of 3D geometry. The Refocusing Adapter incorporates partial image and caption signals, and, combined with part-to-latent mapping, directs modifications to the relevant latent dimensions during latent diffusion process. We validate the effectiveness of our approach through extensive experiments and ablation studies, showcasing the capabilities of our generative local enhancement and editing process, as well as global refinement.	https://dl.acm.org/doi/abs/10.1145/3721238.3730648	Xiao-Lei Li, Hao-Xiang Chen, Yanni Zhang, Kai Ma, Alan Zhao, Tai-Jiang Mu, Hao-Xiang Guo, Ran Zhang
Racing With Legends: Bridging the Physical and Virtual World for a First of a Kind Film	"In January 2023, FCB New York approached The Mill with a brief for a groundbreaking branded film that would be presented as a TV special. ""We want to create an experience for a rookie Formula 1 driver that allows him to virtually race past legends, but in his real car, on a real track. And we want to make it into a TV special."" We knew that this would be one of the most ambitious projects that The Mill has ever undertaken, drawing on every department across several studios around the world. At the heart of this undertaking was a bespoke Augmented Reality display system for the driver. The final system incorporated race-grade GPS tracking, high-speed data transfer, and a custom display system that was safe enough to mount in the driver's helmet. To bring the race to viewers at home, we leveraged AR camera tracking to match the physical camera positions to the virtual world, creating a seamless viewing experience for fans around the world."	https://dl.acm.org/doi/abs/10.1145/3721239.3734081	Adam Smith, Rui An
Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance Field Loss	We present a fast and simple technique to convert images into a radiance surface-based scene representation. Building on existing radiance volume reconstruction algorithms, we introduce a subtle yet impactful modification of the loss function requiring changes to only a few lines of code: instead of integrating the radiance field along rays and supervising the resulting images, we project the training images into the scene to directly supervise the spatio-directional radiance field. The primary outcome of this change is the complete removal of alpha blending and ray marching from the image formation model, instead moving these steps into the loss computation. In addition to promoting convergence to surfaces, this formulation assigns explicit semantic meaning to 2D subsets of the radiance field, turning them into well-defined radiance surfaces. We finally extract a level set from this representation, which results in a high-quality radiance surface model. Our method retains much of the speed and quality of the baseline algorithm. For instance, a suitably modified variant of Instant NGP maintains comparable computational efficiency, while achieving an average PSNR that is only 0.1 dB lower. Most importantly, our method generates explicit surfaces in place of an exponential volume, doing so with a level of simplicity not seen in prior work.	https://dl.acm.org/doi/abs/10.1145/3721238.3730713	Ziyi Zhang, Nicolas Roussel, Thomas Muller, Tizian Zeltner, Merlin Nimier-David, Fabrice Rousselle, Wenzel Jakob
Rags2Riches: Computational Garment Reuse	We present the first algorithm to automatically compute sewing patterns for upcycling existing garments into new designs. Our algorithm takes as input two garment designs along with their corresponding sewing patterns and determines how to cut one of them to match the other by following garment reuse principles. Specifically, our algorithm favors the reuse of seams and hems present in the existing garment, thereby preserving the embedded value of these structural components and simplifying the fabrication of the new garment. Finding optimal reused pattern is computationally challenging because it involves both discrete and continuous quantities. Discrete decisions include the choice of existing panels to cut from and the choice of seams and hems to reuse. Continuous variables include the precise placement of the new panels along seams and hems, and potential deformations of these panels to maximize reuse. Our key idea for making this optimization tractable is quantizing the shape of garment panels. This allows us to frame the search for an optimal reused pattern as a discrete assignment problem, which we solve efficiently with an ILP solver. We showcase our proposed pipeline on several reuse examples, including comparisons with reused patterns crafted by a professional garment designer. Additionally, we manufacture a physical reused garment to demonstrate the practical effectiveness of our approach.	https://dl.acm.org/doi/abs/10.1145/3721238.3730703	Anran Qi, Nico Pietroni, Maria Korosteleva, Olga Sorkine-Hornung, Adrien Bousseau
Real Time Path-Tracing with NVIDIA RTX MegaGeometry	NVIDIA's RTX Mega Geometry is a technology that accelerates Bounding Volume Hierarchy (BVH) building, enabling path tracing of scenes with up to 100x more triangles. With our cluster-based renderer, we introduce several improvements to existing methods and APIs that enable:	https://dl.acm.org/doi/abs/10.1145/3721243.3735983	Manuel Kraemer, Dylan Lacewell, Ardavan Kanani
Real-Time Interactive Graphics on Embedded Systems with ossia score, Puara, and Raspberry Pi	This course explores the creation of real-time interactive graphics on embedded systems, focusing on the integration of , , and Raspberry Pi. Participants will learn to design and implement dynamic visual experiences using GPU shaders, including the use of Interactive Shader Format (ISF) and techniques for porting shaders from platforms such as Shadertoy. The course will cover mapping physical behaviours through the interactions possible with a Raspberry Pi's inputs directly to GPU shaders, enabling responsive and interactive visuals. Through hands-on exercises, attendees will create custom mappings using , a tool for integrating gestural control into real-time graphics. The course will discuss the limitations of embedded systems, such as resolution constraints and the trade-offs between each stage of the rendering pipeline. Practical insights into direct rendering from an application to the GPU, bypassing compositors and display servers, will be provided, offering a comprehensive understanding of optimizing graphics performance on resource-constrained devices. Designed for artists, developers, and researchers, this course bridges the gap between creative expression and technical implementation, empowering participants to push the boundaries of real-time graphics on embedded platforms.	https://dl.acm.org/doi/abs/10.1145/3721251.3734069	Manuel Bolduc, Jean-Michaël Celerier, Eduardo Meneses
Real-Time Markov Chain Path Guiding for Global Illumination and Single Scattering	We present a lightweight and unbiased path guiding algorithm tailored for real-time applications with highly dynamic content. The algorithm demonstrates effectiveness in guiding both direct and indirect illumination. Moreover, it can be extended to guide single scattering events in participating media. Building upon the screen-space approach by Dittebrandt et al. [ ], the incident light distribution is represented as a von Mises-Fisher mixture model, which is controlled by a Markov chain process. To extend the procedure to world space, our algorithm uses a unique Markov chain architecture, which resamples Markov chain states from an ensemble of hash grids. We combine multi-resolution adaptive grids with a static grid, ensuring rapid state exchange without compromising guiding quality. The algorithm imposes minimal prerequisites on scene representation and seamlessly integrates into existing path tracing frameworks. Through continuous multiple importance sampling, it remains independent of the equilibrium distribution of Markov chain and hash grid resampling. We perform an evaluation of the proposed methods across diverse scenarios. Additionally, we explore the algorithm's viability in offline scenarios, showcasing its effectiveness in rendering volumetric caustics. We demonstrate the application of the proposed methods in a path tracing engine for the original Quake game. The demo project features path traced global illumination and single scattering effects at frame rates over 30 FPS on NVIDIA's GeForce 20 series or AMD's Radeon RX 6000 series without upscaling.	https://dl.acm.org/doi/abs/10.1145/3728296	Lucas Domingo Alber, Johannes Hanika, Carsten Dachsbacher
Real-Time Multispectral Lighting Reproduction	We present a real-time algorithm for driving multispectral LED lights in a spherical lighting reproduction stage to achieve optimal color rendition for a dynamic lighting environment. Previous work has driven multispectral LED lights (ours include red, green, blue, white, and amber LEDs) by solving a nonnegative least squares (NNLS) problem for each light source; the solution ensures that each light appears to be the correct RGB color seen by the camera and also optimizes how closely the lights illuminate a color chart to appear as it should in the target lighting environment. We create a real-time version of this technique by pre-computing a lookup table of these NNLS solutions across the full range of input RGB values. Since the proper relative mix of LEDs depends on chrominance and not on luminance, our lookup table can be reduced to 2D saving both storage and computation. With this technique, we can drive several thousand multispectral LED lights at video frame rates with proper color matching and color rendition for a dynamic lighting environment.	https://dl.acm.org/doi/abs/10.1145/3721250.3743035	Xueming Yu, David George, John Millward, Paul Debevec
Real-time graphics in Desmos, with just math and a browser	Desmos 2D and 3D calculators are browser-based tools that enable real-time experimentation with graphics defined by math. The software is all browser-based with purely client-side computation, which lowers the barrier for both creating and sharing. We employ a series of features to enable complicated computation to work natively and performantly in the browser, as well as a real-time rendering pipeline specifically for mathematics. We combine these technological building blocks into a system that allows users to build in real-time with graphics defined by math itself, which lowers the barrier to entry relative to more abstract languages and systems. This browser-first approach has enabled artists and engineers, from students to professionals, to make extraordinary creations using just math and a browser.	https://dl.acm.org/doi/abs/10.1145/3721243.3735987	Pooja Shaw, Eli Luberoff
Reassemble by Packing: Path-Valid Spectral Placement for 3D Fragment Assembly	3D reassembly needs both tight alignment and a collision-free insertion path, but most methods enforce only the former. We recast the task as constrained packing, apply FFT correlation for coarse placement, and prune unreachable poses with a flood-fill path test on the correlation map. A final ICP stage then maximizes surface alignment (instead of minimizing contact as in classic packing). Assuming a known target envelope, this path-aware spectral pipeline yields high-fidelity, physically valid reconstructions, although accuracy still depends on the initial pose sampling.	https://dl.acm.org/doi/abs/10.1145/3721250.3743043	Vinicius Gonçalves Hirono, Paula Dornhofer Paro Costa
Reconstructing Graphic Design Posters via Visual Decomposition and Semantic Layer Translation	This work presents a pipeline to convert rasterized graphic design posters into multi-layered, editable digital assets. It decomposes the input poster into core elements, categorizes them, and converts them into semantically meaningful formats. A novel strategy using Z-index addresses layer ordering and overlap. The pipeline's accuracy was evaluated by comparing over 24,000 original and reconstructed posters of multiple widely used sizes and aspect ratios in print & digital media. Layer semantic accuracy was assessed using the LLaVA-7B model, which showed high confidence scores across image, text, and shape layers. A user-centered evaluation with 20 participants resulted in high satisfaction ratings, confirming the pipeline's ability to accurately reproduce poster designs with excellent fidelity, layout, and overall quality. This pipeline contributes a refined approach to reconstructing rasterized graphic design posters, advancing beyond existing methods.	https://dl.acm.org/doi/abs/10.1145/3721250.3743040	Veeramanohar Avudaiappan, Ritwik Murali
Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual Inversion	Recent years have seen a tremendous improvement in the quality of video generation and editing approaches. While several techniques focus on editing appearance, few address motion. Current approaches using text, trajectories, or bounding boxes are limited to simple motions, so we specify motions with a single motion reference video instead. We further propose to use a pre-trained image-to-video model rather than a text-to-video model. This approach allows us to preserve the exact appearance and position of a target object or scene and helps disentangle appearance from motion. Our method, called , leverages our observation that image-to-video models extract appearance mainly from the (latent) image input, while the text/image embedding injected via cross-attention predominantly controls motion. We thus represent motion using text/image embedding tokens. By operating on an inflated motion-text embedding containing multiple text/image embedding tokens per frame, we achieve a high temporal motion granularity. Once optimized on the motion reference video, this embedding can be applied to various target images to generate videos with semantically similar motions. Our approach does not require spatial alignment between the motion reference video and target image, generalizes across various domains, and can be applied to various tasks such as full-body and face reenactment, as well as controlling the motion of inanimate objects and the camera. We empirically demonstrate the effectiveness of our method in the semantic video motion transfer task, significantly outperforming existing methods in this context. Project website: https://mkansy.github.io/reenact-anything/	https://dl.acm.org/doi/abs/10.1145/3721238.3730668	Manuel Kansy, Jacek Naruniec, Christopher Schroers, Markus Gross, Romann M. Weber
Relightable Full-Body Gaussian Codec Avatars	We propose Relightable Full-Body Gaussian Codec Avatars, a new approach for modeling relightable full-body avatars with fine-grained details including face and hands. The unique challenge for relighting full-body avatars lies in the large deformations caused by body articulation and the resulting impact on appearance caused by light transport. Changes in body pose can dramatically change the orientation of body surfaces with respect to lights, resulting in both local appearance changes due to changes in local light transport functions, as well as non-local changes due to occlusion between body parts. To address this, we decompose the light transport into local and non-local effects. Local appearance changes are modeled using learnable zonal harmonics for diffuse radiance transfer. Unlike spherical harmonics, zonal harmonics are highly efficient to rotate under articulation. This allows us to learn diffuse radiance transfer in a local coordinate frame, which disentangles the local radiance transfer from the articulation of the body. To account for non-local appearance changes, we introduce a shadow network that predicts shadows given precomputed incoming irradiance on a base mesh. This facilitates the learning of non-local shadowing between the body parts. Finally, we use a deferred shading approach to model specular radiance transfer and better capture reflections and highlights such as eye glints. We demonstrate that our approach successfully models both the local and non-local light transport required for relightable full-body avatars, with a superior generalization ability under novel illumination conditions and unseen poses.	https://dl.acm.org/doi/abs/10.1145/3721238.3730739	Shaofei Wang, Tomas Simon, Igor Santesteban, Timur Bagautdinov, Junxuan Li, Vasu Agrawal, Fabian Prada, Shoou-I Yu, Pace Nalbone, Matt Gramlich, Roman Lubachersky, Chenglei Wu, Javier Romero, Jason Saragih, Michael Zollhoefer, Andreas Geiger, Siyu Tang, Shunsuke Saito
RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination	We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport.	https://dl.acm.org/doi/abs/10.1145/3721238.3730595	Chong Zeng, Yue Dong, Pieter Peers, Hongzhi Wu, Xin Tong
Reservoir Splatting for Temporal Path Resampling and Motion Blur	Recent extensions to spatiotemporal path reuse, or ReSTIR, improve rendering efficiency in the presence of high-frequency content by augmenting path reservoirs to represent contributions over full pixel footprints. Still, if historical paths fail to contribute to future frames, these benefits disappear. Prior ReSTIR work to the prior frame to identify paths for reuse. Backprojection can fail to find relevant paths for many reasons, including moving cameras or subpixel geometry with differing motion. We introduce to reduce these failures. Splatting forward-projects the primary hits of prior-frame paths. Unlike backprojection, forward-projected path samples fall into the current-frame pixel relevant to their exact primary hits, making successful reuse more likely. This also enables motion blur for ReSTIR, by splatting at multiple time steps, and supports depth of field without the specialized shift maps needed previously. Beyond enabling motion blur, splatting improves resampling quality over Zhang et al.'s [ ] Area ReSTIR at up to 10% lower cost. To improve robustness, we show how to MIS splatted and backprojected samples to help every current-frame pixel get at least one historical path proposed for reuse.	https://dl.acm.org/doi/abs/10.1145/3721238.3730646	Jeffrey Liu, Daqi Lin, Markus Kettunen, Chris Wyman, Ravi Ramamoorthi
SAWNA: Space-Aware Text to Image Generation	Layout-aware text-to-image generation allows users to synthesize images by specifying object positions through text prompts and layouts. This has proven useful in a variety of creative fields such as advertising, UI design, and animation, where structured scene control is essential. In real-world workflows, however, certain regions are often intentionally left empty—for instance, for headlines in advertisements, buttons in interface prototypes, or subtitles and speech bubbles in animation frames. Existing models lack the ability to explicitly preserve such negative spaces, often resulting in unwanted content and complicating downstream editing. We introduce Space-Controllable Text-to-Image Generation, a task that treats reserved areas as first-class constraints. To address this, we propose SAWNA (Space-Aware Text-to-Image Generation), a training-free diffusion framework that injects nonreactive noise into user-defined masked regions, ensuring they remain empty throughout generation. Our method maintains semantic integrity and visual fidelity without retraining and integrates seamlessly into layout-sensitive workflows in design, advertising, and animation. Experiments demonstrate that SAWNA reliably enforces spatial constraints and improves the practical usability of generated content.	https://dl.acm.org/doi/abs/10.1145/3721250.3743023	Ryugo Morita, Sho Kuno, Ryunosuke Tanaka, Rongzhi Li, Hoang Dai Dinh, Issey Sukeda
SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound	Generating combined visual and auditory sensory experiences is critical for immersive content. We introduce SEE-2-SOUND, a pipeline that turns an image, GIF, or video into 5.1 spatial audio. SEE-2-SOUND sequentially: (i) segments visual sound sources; (ii) estimates their 3-D positions from monocular depth; (iii) synthesises mono audio for every source; and (iv) renders the mix with room acoustics. Built entirely from off-the-shelf models, the method needs no fine-tuning and runs in zero-shot mode on real or generated media. We demonstrate compelling results for generating spatial audio from videos, images, dynamic images, and media generated by learned approaches. Project page: .	https://dl.acm.org/doi/abs/10.1145/3721250.3742965	Rishit Dagli, Shivesh Prakash, Robert Wu, Houman Khosravani
SOAP: Style-Omniscient Animatable Portraits	Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles. While 3D diffusion models advance single-view reconstruction for general objects, outputs often lack animation controls or suffer from artifacts because of the domain gap. We propose SOAP, a style-omniscient framework to generate rigged, topology-consistent avatars from any portrait. Our method leverages a multiview diffusion model trained on 24K 3D heads with multiple styles and an adaptive optimization pipeline to deform the FLAME mesh while maintaining topology and rigging via differentiable rendering. The resulting textured avatars support FACS-based animation, integrate with eyeballs and teeth, and preserve details like braided hair or accessories. Extensive experiments demonstrate the superiority of our method over state-of-the-art techniques for both single-view head modeling and diffusion-based generation of Image-to-3D. Our code and data are publicly available for research purposes at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730691	Tingting Liao, Yujian Zheng, Yuliang Xiu, Adilbek Karmanov, Liwen Hu, Leyang Jin, Hao Li
ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions	Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.	https://dl.acm.org/doi/abs/10.1145/3721238.3730729	Shivangi Aneja, Sebastian Weiss, Irene Baeza, Prashanth Chandran, Gaspard Zoss, Matthias Niessner, Derek Bradley
Scalable Volume Rendering of Billion-Cell CFD Simulations Using VFX Pipelines	We present a modular, scalable workflow for high-fidelity volume rendering of large-scale CFD simulations. Designed with visual effects (VFX) techniques in mind, our workflow transforms unstructured CFD data into cinematic-quality visuals using parallel voxelization and sparse volume export. By leveraging CyclesPhi renderer and OpenVDB, we deliver performance, scalability, and expressive visualization on HPC infrastructure. Results on two large CFD cases demonstrate significant speedups over traditional tools with support for interactive rendering of volumes.	https://dl.acm.org/doi/abs/10.1145/3721250.3743008	Petr Strakos, Milan Jaros, Tomas Brzobohaty, Marketa Faltynkova, Ondrej Meca, Lubomir Riha
Scene-Level Appearance Transfer with Semantic Correspondences	We introduce ReStyle3D, a novel framework for scene-level appearance transfer from a single style image to a real-world scene represented by multiple views. The method combines explicit semantic correspondences with multi-view consistency to achieve precise and coherent stylization. Unlike conventional stylization methods that apply a reference style globally, ReStyle3D uses open-vocabulary segmentation to establish dense, instance-level correspondences between the style and real-world images. This ensures that each object is stylized with semantically matched textures. ReStyle3D first transfers the style to a single view using a training-free semantic-attention mechanism in a diffusion model. It then lifts the stylization to additional views via a learned warp-and-refine network guided by monocular depth and pixel-wise correspondences. Experiments show that ReStyle3D consistently outperforms prior methods in structure preservation, perceptual style similarity, and multi-view coherence. User studies further validate its ability to produce photo-realistic, semantically faithful results. Our code, pretrained models, and dataset will be publicly released, to support new applications in interior design, virtual staging, and 3D-consistent stylization. Project page and code at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730655	Liyuan Zhu, Shengqu Cai, Shengyu Huang, Gordon Wetzstein, Naji Khosravan, Iro Armeni
Scrapyard Challenge: Classic Arcade Game Controller Redesign Workshop	"This paper describes the ""Scrapyard Challenge: Classic Arcade Game Controller Redesign Workshop"", an interactive workshop that allows anyone to create a novel controller for classic arcade games using a custom-built interface board and Raspberry Pi [ ] computers running Retropie Software [ ]. The Scrapyard Challenge workshops are intense workshops ranging in duration from all day to several hours that focus on interface design through the use of found objects and junk materials. Since they began in 2003, the Scrapyard Challenge workshops have been held in 17 countries across the five continents of North America, South America, Europe, Australia, and Asia. The workshops began as a way of introducing interface design to novices without the need to learn electronics by supplying a custom built interface board that people could simply ""plug"" homemade switches and analog controllers into to create custom MIDI controllers, wearable devices, or public art installations. As the cost of game emulation decreased from standard computers such as Macs and PCs to microcomputers such as the Raspberry Pi, the workshop has evolved with these changes over its 20 year history and now allows for participants to build game controllers for classic games and consoles such as Super Mario Brothers (Nintendo Entertainment System), Street Fighter (Multiple Machine Arcade Emulator or MAME), Crash Bandicoot (PlayStation 1), and many other home arcade systems. This Labs workshop integrates games on up to 20 different systems that spans 20 years of home gaming and thousands of potential games to choose from by participants designing controllers for those games."	https://dl.acm.org/doi/abs/10.1145/3721251.3734057	Sebastian Bidegain, Jonah Brucker-Cohen
Secret Level: Crossfire - Good Conflict	Secret Level presents 15 original short stories set in classic video game worlds. Platige Image created a Good Conflict episode for the Crossfire. As a storm approaches, two rival mercenary groups collide, each fighting for their vision of the greater good. Their fates hang in the balance.	https://dl.acm.org/doi/abs/10.1145/3698896.3725488	Damian Nenow, Artur Zicz, Hubert Zegardlo, Kamil Murzyn, Marina Borokhova, Martyna Siwinska
Semantically Consistent Text-to-Motion with Unsupervised Styles	Text-to-stylized human motion generation leverages text descriptions for motion generation with fine-grained style control with respect to a reference motion. However, existing approaches typically rely on supervised style learning with labeled datasets, constraining their adaptability and generalization for effective diverse style control. Additionally, they have not fully explored the temporal correlations between motion, textual descriptions, and style, making it challenging to generate semantically consistent motion with precise style alignment. To address these limitations, we introduce a novel method that integrates unsupervised style from arbitrary references into a text-driven diffusion model to generate semantically consistent stylized human motion. The core innovation lies in leveraging text as a mediator to capture the temporal correspondences between motion and style, enabling the seamless integration of temporally dynamic style into motion features. Specifically, we first train a diffusion model on a text-motion dataset to capture the correlation between motion and text semantics. A style adapter then extracts temporally dynamic style features from reference motions and integrates a novel Semantic-Aware Style Injection (SASI) module to infuse these features into the diffusion model. The SASI module computes the semantic correlation between motion and style features based on text, selectively incorporating style features that align with motion content, ensuring semantic consistency and precise style alignment. Our style adapter does not require a labeled style dataset for training, enhancing adaptability and generalization of style control. Extensive evaluations show that our method outperforms previous approaches in terms of semantic consistency and style expressivity. Our webpage, , includes links to the supplementary video and code.	https://dl.acm.org/doi/abs/10.1145/3721238.3730641	Linjun Wu, Xiangjun Tang, Jingyuan Cong, He Wang, Bo Hu, Xu Gong, Songnan Li, Yuchen Liao, Yiqian Wu, Chen Liu, Xiaogang Jin
Simon Small: English	Bertil is a little boy living with his parents. He is lonely and bored at home, when his parents go to work. One day, all of a sudden, a little thumb size boy appears under his bed! His name is Nils Karlsson Pussling. The 2 boys bond right away and Nils shows Bertil a fascinating magical world right inside his bedroom walls. Neither of them will ever be lonely again.	https://dl.acm.org/doi/abs/10.1145/3698896.3725070	Yaprak Morali, Are Austnes
Simulating the Mechanics of Ant Swarm Aggregations	Ants exhibit unique abilities to self-assemble into animate, living structures. Such structures display properties of both fluid and solid-like, deformable materials. Despite much progress in our understanding of ant aggregation dynamics, simulating such phenomena has been largely overlooked in real-time graphics and animation applications. We present a constraints-based approach for simulating the collective dynamics of ants. We demonstrate ant collective behaviors interactively with compelling physical realism.	https://dl.acm.org/doi/abs/10.1145/3721250.3742999	Matthew Loges, Tomer Weiss
Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates	Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry—especially for loose-fitting clothing—remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation.	https://dl.acm.org/doi/abs/10.1145/3721238.3730651	Ren Li, Cong Cao, Corentin Dumery, Yingxuan You, Hao Li, Pascal Fua
Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing	Recent video editing methods achieve attractive results in style transfer or appearance modification. However, editing the structural content of 3D scenes in videos remains challenging, particularly when dealing with significant viewpoint changes, such as large camera rotations or zooms. Key challenges include generating novel view content that remains consistent with the original video, preserving unedited regions, and translating sparse 2D inputs into realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a sketch-based 3D-aware video editing method to enable detailed local manipulation of videos with significant viewpoint changes. To solve the challenge posed by sparse inputs, we employ image editing methods to generate edited results for the first frame, which are then propagated to the remaining frames of the video. We utilize sketching as an interaction tool for precise geometry control, while other mask-based image editing methods are also supported. To handle viewpoint changes, we perform a detailed analysis and manipulation of the 3D information in the video. Specifically, we utilize a dense stereo method to estimate a point cloud and the camera parameters of the input video. We then propose a point cloud editing approach that uses depth maps to represent the 3D geometry of newly edited components, aligning them effectively with the original 3D scene. To seamlessly merge the newly edited content with the original video while preserving the features of unedited regions, we introduce a 3D-aware mask propagation strategy and employ a video diffusion model to produce realistic edited videos. Extensive experiments demonstrate the superiority of Sketch3DVE in video editing.	https://dl.acm.org/doi/abs/10.1145/3721238.3730623	Feng-Lin Liu, Shi-Yang Li, Yan-Pei Cao, Hongbo Fu, Lin Gao
SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations	We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.	https://dl.acm.org/doi/abs/10.1145/3721238.3730640	Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, Qifeng Chen
Sleeping with the Fishes	A man traumatized by his youth spent in a reformatory is devoted to saving kids destined to a life of misery through death. For him, dying young means living forever in the best version of one's self.	https://dl.acm.org/doi/abs/10.1145/3698896.3718731	Camille Colonna D'Istra, Noa Lavino, Justine Gault, Marion Robe, Nicolas Knoll, Manon Seve, Lucas Wallez
Smartphone-based Simple HMD with Multiple Mirrors and Lenticular Lens for Ultra-Wide Field of View	In this study, we propose a smartphone-based wide field-of-view HMD by expanding the display area using inexpensive mirrors and lenticular lenses. Lenticular lenses placed on the both edge of the display convert these areas into the multi-view displays. The expansion of the display area is achieved by observing multi-view images via the properly placed mirrors.	https://dl.acm.org/doi/abs/10.1145/3721250.3742980	Tomohiro Kamide, Naoki Hashimoto
Snow Bear	When a lonely polar bear can't find a friend... he makes one.	https://dl.acm.org/doi/abs/10.1145/3698896.3725500	Aaron Blaise
Spatial Design and CoreML for the Apple Vision Pro	Spatial user interfaces (UI) and graphics are important components in three-dimensional environments found across many applications including augmented reality (AR), mixed reality (MR), virtual reality (VR), and video games. As spatial computing enters the mainstream through products like the Apple Vision Pro, the design of intuitive, accessible 3D user interfaces (UI) is critical. Creating UI in spatial applications that interact naturally with the physical and digital worlds requires engineering and design considerations that go beyond traditional 2D interfaces. This course challenges participants to rethink 3D UI design to enhance spatial experiences through intuitive and predictive interfaces with machine learning to explore an ambitiously comprehensive look at the challenges of developing UI/UX for the Apple Vision Pro. We begin with core design principles of 2D and 3D UI before diving deeper into 3D UI, focusing on how depth perception, spatial awareness, and natural gestures work together to create cohesive user experiences and interactions. Then, we apply these concepts to introduce individuals to programming and designing with CoreML and SwiftUI. Finally, we examine how leveraging AI/ML enables personalization for individual needs, a key in revolutionizing the accessibility and future of spatial UI.	https://dl.acm.org/doi/abs/10.1145/3721251.3734063	Deborah Yuen, Mark Ramos
Spatial Sensing: Augmenting Human Understanding in Data-Driven Exploration	Machine-Guided Spatial Sensing is a novel measurement technique that combines augmented reality (AR), active learning, and human-in-the-loop interaction to measure environmental fields with high accuracy and efficiency. This system employs a head-mounted display (HMD) and a handheld sensor to capture various physical quantities, such as flow fields and gas concentrations, in real-time. A central data model processes the collected measurements continuously and updates predictions of the environmental field. Using active learning, the system identifies regions of high uncertainty and guides the operator to optimal sampling locations through intuitive AR visualizations. This closed-loop framework effectively transfers the sampling expertise from the operator to the machine learning algorithm, enabling efficient and accurate field estimation. Experimental evaluations demonstrate that the proposed method achieves high accuracy and reduces measurement times significantly compared to traditional sampling techniques. The system's flexibility allows for integration with various environmental sensors, making it suitable for applications in engineering, scientific research, and environmental protection. By leveraging real-time data analysis and human-machine collaboration, Machine-Guided Spatial Sensing provides a robust, user-friendly solution for complex spatial measurement challenges. Future research will focus on enhancing sensor fusion and adapting the system to dynamic environmental conditions. These promising results indicate that the approach reduces setup complexity, lowers costs, and enhances data reliability across diverse environments.	https://dl.acm.org/doi/abs/10.1145/3721239.3736525	Julian Humml
Spatial Sensing: Real-Time Immersive Environmental Sensing	This Real-Time Live! demonstration presents Machine-Guided Spatial Sensing, a novel framework for real-time, operator-in-the-loop environmental measurement using augmented reality (AR). The system integrates active learning with spatialized visual guidance to enable non-expert users to perform high-accuracy sampling of complex environmental fields—such as airflows. During the demonstration, an operator equipped with a head-mounted display and a handheld sensor is guided through the measurement process via dynamic AR overlays. A live data model continuously assimilates measurements, estimates the underlying physical field, and quantifies uncertainty. Based on this evolving model, the system generates spatial cues that direct the user to the most informative sampling locations, effectively transferring domain expertise to the algorithm and streamlining the measurement process. This demonstration highlights the system's capacity to render previously invisible environmental phenomena directly into the user's field of view, enabling intuitive exploration and high-fidelity reconstruction. The approach generalizes across sensor modalities, making it adaptable for applications in scientific research, environmental monitoring, and engineering diagnostics. By showcasing interactive spatial sensing in a live performance setting, this work illustrates the transformative potential of human-machine teaming in real-world data acquisition.	https://dl.acm.org/doi/abs/10.1145/3721243.3735981	Julian Humml
Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors	Indoor lighting estimation from a single image or video remains a challenge due to its highly ill-posed nature, especially when the lighting condition of the scene varies spatially and temporally. We propose a method that estimates from an input video a continuous light field describing the spatiotemporally varying lighting of the scene. We leverage 2D diffusion priors for optimizing such light field represented as a MLP. To enable zero-shot generalization to in-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict lighting at multiple locations by jointly inpainting multiple chrome balls as light probes. We evaluate our method on indoor lighting estimation from a single image or video and show superior performance over compared baselines. Most importantly, we highlight results on spatiotemporally consistent lighting estimation from in-the-wild videos, which is rarely demonstrated in previous works.	https://dl.acm.org/doi/abs/10.1145/3721238.3730749	Mutian Tong, Rundi Wu, Changxi Zheng
Spherical Lighting with Spherical Harmonics Hessian	In this paper, we introduce a second-order derivative of spherical harmonics, , and , a variant of spherical harmonics, to the computer graphics community. These mathematical tools are used to develop an analytical representation of the Hessian matrix of spherical harmonics coefficients for spherical lights. We apply our analytic representation of the Hessian matrix to grid-based SH lighting rendering applications with many spherical lights that store the incident light field as spherical harmonics coefficients and their spatial gradient at sparse grid. We develop a Hessian-based error metric, with which our method automatically and adaptively subdivides the grid whether the interpolation using the spatial gradient is appropriate. Our method can be easily incorporated into the grid-based precomputed radiance transfer (PRT) framework with small additional storage. We demonstrate that our adaptive grid subdivided by using the Hessian-based error metric can substantially improve the rendering quality in equal-time grid construction.	https://dl.acm.org/doi/abs/10.1145/3721238.3730689	Kei Iwasaki, Yoshinori Dobashi
Splat and Replace: 3D Reconstruction with Repetitive Elements	We leverage repetitive elements in 3D scenes to improve novel view synthesis. Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly improved novel view synthesis but renderings of unseen and occluded parts remain low-quality if the training views are not exhaustive enough. Our key observation is that our environment is often full of repetitive elements. We propose to leverage those repetitions to improve the reconstruction of low-quality parts of the scene due to poor coverage and occlusions. We propose a method that segments each repeated instance in a 3DGS reconstruction, registers them together, and allows information to be shared among instances. Our method improves the geometry while also accounting for appearance variations across instances. We demonstrate our method on a variety of synthetic and real scenes with typical repetitive elements, leading to a substantial improvement in the quality of novel view synthesis.	https://dl.acm.org/doi/abs/10.1145/3721238.3730727	Nicolas Violante, Andréas Meuleman, Alban Gauthier, Fredo Durand, Thibault Groueix, George Drettakis
Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation	Generating high-quality 4D content from monocular videos—for applications such as digital humans and AR/VR—poses challenges in ensuring temporal and spatial consistency, preserving intricate details, and incorporating user guidance effectively. To overcome these challenges, we introduce Splat4D, a novel framework enabling high-fidelity 4D content generation from a monocular video. Splat4D achieves superior performance while maintaining faithful spatial-temporal coherence, by leveraging multi-view rendering, inconsistency identification, a video diffusion model, and an asymmetric U-Net for refinement. Through extensive evaluations on public benchmarks, Splat4D consistently demonstrates state-of-the-art performance across various metrics, underscoring the efficacy of our approach. Additionally, the versatility of Splat4D is validated in various applications such as text/image conditioned 4D generation, 4D human generation, and text-guided content editing, producing coherent outcomes following user instructions. Project page: https://visual-ai.github.io/splat4d	https://dl.acm.org/doi/abs/10.1145/3721238.3730752	Minghao Yin, Yukang Cao, Songyou Peng, Kai Han
SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting	Achieving high-fidelity 3D reconstruction from monocular video remains challenging due to the inherent limitations of traditional methods like Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene details. While differentiable rendering techniques such as Neural Radiance Fields (NeRF) address some of these challenges, their high computational costs make them unsuitable for real-time applications. Additionally, existing 3D Gaussian Splatting (3DGS) methods often focus on photometric consistency, neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and pose updates for scene refinement. We propose a framework integrating dense SLAM with 3DGS for near real-time, high-fidelity dense reconstruction. Our approach introduces , which dynamically updates and densifies the Gaussian model by leveraging dense point clouds from SLAM. Additionally, we incorporate , which combines edge-aware geometric constraints and photometric consistency to jointly optimize appearance and geometry of the 3DGS scene representation, enabling detailed and accurate SLAM mapping reconstruction. Experiments on the Replica and TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving state-of-the-art results among monocular systems. Specifically, our method achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica, representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by 10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the potential of our framework in bridging the gap between photometric and geometric dense 3D scene representations, paving the way for practical and efficient monocular dense reconstruction. A demonstration of the results can be found in the accompanying video: .	https://dl.acm.org/doi/abs/10.1145/3728310	Yue Hu, Rong Liu, Meida Chen, Peter Beerel, Andrew Feng
Spline Deformation Field	Trajectory modeling of dense points usually employs implicit deformation fields, represented as neural networks that map coordinates to relate canonical spatial positions to temporal offsets. However, the inductive biases inherent in neural networks can hinder spatial coherence in ill-posed scenarios. Current methods focus either on enhancing encoding strategies for deformation fields, often resulting in opaque and less intuitive models, or adopt explicit techniques like linear blend skinning, which rely on heuristic-based node initialization. Additionally, the potential of implicit representations for interpolating sparse temporal signals remains under-explored. To address these challenges, we propose a spline-based trajectory representation, where the number of knots explicitly determines the degrees of freedom. This approach enables efficient analytical derivation of velocities, preserving spatial coherence and accelerations, while mitigating temporal fluctuations. To model knot characteristics in both spatial and temporal domains, we introduce a novel low-rank time-variant spatial encoding, replacing conventional coupled spatiotemporal techniques. Our method demonstrates superior performance in temporal interpolation for fitting continuous fields with sparse inputs. Furthermore, it achieves competitive dynamic scene reconstruction quality compared to state-of-the-art methods while enhancing motion coherence without relying on linear blend skinning or as-rigid-as-possible constraints.	https://dl.acm.org/doi/abs/10.1145/3721238.3730676	Mingyang Song, Yang Zhang, Marko Mihajlovic, Siyu Tang, Markus Gross, Tunç Ozan Aydın
SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars	Gaussian-based human avatars have achieved an unprecedented level of visual fidelity. However, existing approaches based on high-capacity neural networks typically require a desktop GPU to achieve real-time performance for a single avatar, and it remains non-trivial to animate and render such avatars on mobile devices including a standalone VR headset due to substantially limited memory and computational bandwidth. In this paper, we present SqueezeMe, a simple and highly effective framework to convert high-fidelity 3D Gaussian full-body avatars into a lightweight representation that supports both animation and rendering with mobile-grade compute. Our key observation is that the decoding of pose-dependent Gaussian attributes from a neural network creates non-negligible memory and computational overhead. Inspired by blendshapes and linear pose correctives widely used in Computer Graphics, we address this by distilling the pose correctives learned with neural networks into linear layers. Moreover, we further reduce the parameters by sharing the correctives among nearby Gaussians. Combining them with a custom splatting pipeline based on Vulkan, we achieve, for the first time, simultaneous animation and rendering of 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset.	https://dl.acm.org/doi/abs/10.1145/3721238.3730599	Forrest Iandola, Stanislav Pidhorskyi, Igor Santesteban, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon, Shunsuke Saito
Stabilizing Impossible Collisions with Loki	We present a suite of techniques from our in-house simulation framework, Loki, addressing the pervasive challenge of collision instabilities in character effects, particularly in cases where nonphysical pinching prevents collision resolution. We introduce a proximity-tolerant mode for contact projection that trades collision residual for stability, a compliant kinematic mechanism for on-demand gap expansion, and contact-aware strain limiting to prevent penetrations while enforcing target edge lengths. Additionally, we showcase our tools for collider management, including hierarchical collision exclusion, one-sided collision handling, and paintable collision thickness maps. These techniques collectively demonstrate a robust and intuitive workflow for combining physically based collisions with challenging production animations.	https://dl.acm.org/doi/abs/10.1145/3721239.3734080	Xiao Zhai, Eston Schweickart, Jefri Haryono, Nikolay Ilinov, Andrea Merlo
Stable Cosserat Rods	Cosserat rods have become an increasingly popular framework for simulating complex bending and twisting in thin elastic rods, used for hair, tree, and yarn-level cloth models. However, traditional approaches often encounter significant challenges in robustly and efficiently solving for valid quaternion orientations, even when employing small time steps or computationally expensive global solvers. We introduce , a new solver that can achieve high accuracy with high stiffness levels and maintain stability under large time steps. It is also inherently suitable for parallelization. Our key contribution is a split position and rotation optimization scheme with a closed-form Gauss-Seidel quasi-static orientation update. This solver significantly improves the numerical stability with Cosserat rods, allowing faster computation and larger time steps. We validate our method across a wide range of applications, including simulations of hair, trees, yarn-level cloth, slingshots, and bridges, demonstrating its ability to handle diverse material behaviors and complex geometries. Furthermore, we show that our method is orders of magnitude faster and more stable than alternative rod solvers, such as extended position-based dynamics and discrete elastic rods.	https://dl.acm.org/doi/abs/10.1145/3721238.3730618	Jerry Hsu, Tongtong Wang, Kui Wu, Cem Yuksel
StableMakeup: When Real-World Makeup Transfer Meets Diffusion Model	Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain the content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to various tasks such as cross-domain makeup transfer, makeup-guided text-to-image generation, and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields.	https://dl.acm.org/doi/abs/10.1145/3721238.3730702	Yuxuan Zhang, Yirui Yuan, Yiren Song, Jiaming Liu
State of the Art in Grid-Free Monte Carlo Methods for Partial Differential Equations	"This 3 hour course provides a detailed overview of grid-free Monte Carlo methods for solving partial differential equations (PDEs) based on the walk on spheres (WoS) algorithm, with a special emphasis on problems with high geometric complexity. PDEs are a basic building block of models and algorithms used throughout science, engineering and visual computing. Yet despite decades of research, conventional PDE solvers struggle to capture the immense geometric complexity of the natural world. A perennial challenge is spatial discretization: traditionally, one must partition the domain into a high-quality volumetric mesh—a process that can be brittle, memory intensive, and difficult to parallelize. WoS makes a radical departure from this approach, by reformulating the problem in terms of recursive integral equations that can be estimated using the Monte Carlo method, eliminating the need for spatial discretization. Since these equations strongly resemble those found in light transport theory, one can leverage deep knowledge from Monte Carlo rendering to develop new PDE solvers that share many of its advantages: no meshing, trivial parallelism, and the ability to evaluate the solution at any point without solving a global system of equations. The course is divided into two parts. Part I will cover the basics of using WoS to solve fundamental PDEs like the Poisson equation. Topics include formulating the solution as an integral equation, generating samples via recursive random walks, and employing accelerated distance and ray intersection queries to efficiently handle complex geometries. Participants will also gain experience setting up demo applications involving data interpolation, heat transfer, and geometric optimization using the open-source ""Zombie"" library, which implements various grid-free Monte Carlo PDE solvers. Part II will feature a mini-panel of academic and industry contributors covering advanced topics including variance reduction, differentiable and multi-physics simulation, and applications in industrial design and robust geometry processing."	https://dl.acm.org/doi/abs/10.1145/3721241.3734001	Rohan Sawhney, Bailey Miller, Ioannis Gkioulekas, Keenan Crane
Steerable Perlin Noise	Perlin noise is an integral tool for generating procedural textures for computer graphics applications. It's widely used across multiple industries and applications due to its simplicity, speed and controllability. We present 2 simple changes to the Perlin noise algorithm that allows for the inclusion of anisotropy in the output noise. We believe these small adjustments open the door to new texturing workflows, as well as simple vector field visualizations in multiple dimensions.	https://dl.acm.org/doi/abs/10.1145/3721239.3734101	Jacob Benjamin Rice
Sticking Information in Plain Sight: Encoding and Detecting Hidden Stickers in the Real World	While there are many techniques (e.g., QR codes) that convey information via visual patterns, many applications would benefit from having those codes be imperceptible to the human eye. We present a method for designing subtle code-conveying patterns that can be printed on transparent sticker paper, then applied to real-world surfaces. An image of a scene with an encoded sticker can be sent through our localization and decoding modules, where the sticker subsection is robustly localized and decoded. We jointly optimize the encoding, localization, and decoding modules end to end, taking into account both imperceptibility and accuracy. Notably, we also account for human error when placing stickers, as pixel-perfect alignment is not something that can be reliably expected. Our model encodes and decodes 100-bit secrets, which, with BCH error correction, means that a sticker could encode 56 data bits with 40 parity bits. Experimental results show that this method is robust to sticker placement errors while being easy to deploy in the real world.	https://dl.acm.org/doi/abs/10.1145/3721250.3743018	Christina Shatford, Szymon Rusinkiewicz
Stitch-A-Shape: Bottom-up Learning for B-Rep Generation	"Boundary representation (B-Rep) models serve as the primary representation format in modern CAD systems for describing 3D shapes. While deep learning has achieved success with various geometric representations, B-Reps remain challenging due to their hybrid nature of combining continuous geometry with discrete topological relationships. In this paper, we present Stitch-A-Shape, a B-Rep generation framework that directly models both topology and geometry. This strategy departs from prior work that focuses on either topology or geometry while recovering the other through post-processing. Our method consists of a geometry module that determines the spatial configuration of geometric elements (vertices, curves, and surface control points) and a topology module that establishes connectivity relationships and identifies boundary structures, including outer and inner loops. Our approach leverages a sequential ""stitching"" representation that mirrors the native data structure and inherent bottom-up organization of B-Rep, assembling geometric entities from vertices through curves to faces. We validate that our framework can handle topological and geometric ambiguities, as well as open surfaces and compound solids. Experiments show that Stitch-A-Shape achieves superior generation quality and computational efficiency compared to existing approaches in unconditional generation tasks, while exhibiting effective capabilities in class-conditional generation and B-Rep autocompletion applications."	https://dl.acm.org/doi/abs/10.1145/3721238.3730661	Pu Li, Wenhao Zhang, Jinglu Chen, Dongming Yan
Stochastic Barnes-Hut Approximation for Fast Summation on the GPU	We present a novel stochastic version of the Barnes-Hut approximation. Regarding the level-of-detail (LOD) family of approximations as control variates, we construct an unbiased estimator of the kernel sum being approximated. Through several examples in graphics applications such as winding number computation and smooth distance evaluation, we demonstrate that our method is well-suited for GPU computation, capable of outperforming a GPU-optimized implementation of the deterministic Barnes-Hut approximation by achieving equal median error in up to 9.4x less time.	https://dl.acm.org/doi/abs/10.1145/3721238.3730725	Abhishek Madan, Nicholas Sharp, Francis Williams, Ken Museth, David I.W. Levin
StreamME: Simplify 3D Gaussian Avatar within Live Stream	We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: .	https://dl.acm.org/doi/abs/10.1145/3721238.3730635	Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu
Stressful Tree Modeling: Breaking Branches with Strands	We propose a novel approach for the computational modeling of lignified tissues, such as those found in tree branches and timber. We leverage a state-of-the-art strand-based representation for tree form, which we extend to describe biophysical processes at short and long time scales. Simulations at short time scales enable us to model different breaking patterns due to branch bending, twisting, and breaking. On long timescales, our method enables the simulation of realistic branch shapes under the influence of plausible biophysical processes, such as the development of compression and tension wood. We specifically focus on computationally fast simulations of woody material, enabling the interactive exploration of branches and wood breaking. By leveraging Cosserat rod physics, our method enables the generation of a wide variety of breaking patterns. We showcase the capabilities of our method by performing and visualizing numerous experiments.	https://dl.acm.org/doi/abs/10.1145/3721238.3730745	Bosheng Li, Nikolas Schwarz, Wojtek Palubicki, Sören Pirk, Dominik L. Michels, Bedrich Benes
Stroke Imprint: Knitting Reassurance into Anxious Moments	Imagine if, during moments of heightened anxiety, you could once again feel the gentle, familiar touch of a loved one's hand. Stroke Imprint is a knitted wearable that simulates stroking sensations to comfort young women experiencing anxiety through pressure sensing and SMA-based actuation. Paired with a digital interface, the glove allows users to record personalized tactile sensation. Through user interviews, design iterations, and user testing, the study demonstrates its potential as an anxiety tracking, therapeutic wearable within a closed biofeedback loop.	https://dl.acm.org/doi/abs/10.1145/3721250.3743045	Yuqing Liu, Rinchong Kim, Kyunghee Kim
Stroke Transfer for Participating Media	We present a method for generating stroke-based painterly drawings of participating media, such as smoke, fire, and clouds, by transferring stroke attributes—color, width, length, and orientation—from exemplar to animation frames. Building on the stroke transfer framework, we introduce features and basis fields capturing lighting-, view-, and geometry-dependent information, extending surface-based ones (e.g., intensity, apparent normals and curvatures, and distance from silhouettes) to volumetric scenes while supporting traditional surface objects. Novel features, including apparent relative velocity and mean free-path, address non-rigid flow and dynamic scenes. Our system combines automated exemplar selection, user-guided style learning, and temporally coherent stroke generation, enabling artistic and expressive visualizations of dynamic media.	https://dl.acm.org/doi/abs/10.1145/3721238.3730603	Naoto Shirashima, Hideki Todo, Yuki Yamaoka, Shizuo Kaji, Kunihiko Kobayashi, Haruna Shimotahira, Yonghao Yue
Style Customization of Text-to-Vector Generation with Image Diffusion Priors	Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data. To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is .	https://dl.acm.org/doi/abs/10.1145/3721238.3730707	Peiying Zhang, Nanxuan Zhao, Jing Liao
Stylized Organic Environments in The Wild Robot	The main challenge in creating the stylized organic environments of was to transform our 3D Look Dev Artists into more of a traditional 2D Artist. We wanted to capture the impromptu and organic techniques of 2D art and apply that to a 3D world. A 2D artist is not limited by the technical constraints of a 3D Artist and can express their creativity more freely. In order to recreate the spontaneity of a 2D Artist we needed first to overcome many of the limitations of the 3D medium.	https://dl.acm.org/doi/abs/10.1145/3721239.3734095	John Wake, Michael Losure, Baptiste Van Opstal
SugART: Mixed Reality Sugar Painting for Intangible Cultural Heritage Learning at Home	This paper introduces SugART, a Mixed Reality (MR) project that enables users to learn and recreate traditional sugar painting at home. By combining hand tracking, virtual guidance, and real-time feedback, our project supports creative expression and cultural education, thereby lowering barriers to participation in intangible cultural heritage through accessible and interactive digital experiences.	https://dl.acm.org/doi/abs/10.1145/3721250.3742997	Haowei Xiong, Kexin Nie, Jiachen Zeng, Shujing Shen, Mengyao Guo
Super Resolution for Humans	Super-resolution (SR) is crucial for delivering high-quality content at lower bandwidths and supporting modern display demands in VR and AR. Unfortunately, state-of-the-art neural network SR methods remain computationally expensive. Our key insight is to leverage the limitations of the human visual system (HVS) to selectively allocate computational resources, such that perceptually important image regions, identified by our low-level perceptual model, are processed by more demanding SR methods, while less critical areas use simpler methods. This approach, inspired by content-aware foveated rendering [Tursun et al. ], optimizes efficiency without sacrificing perceived visual quality. User studies and quantitative results demonstrate that our method achieves a reduction in computational requirements with no perceptible quality loss. The technique is architecture-agnostic and well-suited for VR/AR, where focusing effort on foveal vision offers significant computational savings.	https://dl.acm.org/doi/abs/10.1145/3721250.3742985	Volodymyr Karpenko, Taimoor Tariq, Jorge Condor, Piotr Didyk
Superman: Harnessing Numeric Power for Cinematic Magic	Superman makes the third feature film and fifth project in total on which Wētā FX has had the pleasure of working with James Gunn. In this talk you can hear how the team got stuck in to bringing Superman's latest fight against an arch nemesis to life.	https://dl.acm.org/doi/abs/10.1145/3698897.3718748	Guy Williams, Joanna Davison, Claudio Gonzalez
Supper	A table is set with an array of symbolic objects - fish, apple, hourglass, diamond, and a heart -prepared as an intricate meal. A colossal entity sits to partake. Is this entity a deity, the universe, or the essence of nature itself? The eternity humanity so fervently desires and the meaning it tirelessly seeks are reduced to nothing more than a single meal for this vast, inscrutable being.	https://dl.acm.org/doi/abs/10.1145/3698896.3721997	Erick Oh
Surface Constraint Wing Fold System on The Wild Robot	Animating a bird's wing fold in computer animation is a very complicated task. Even after achieving a beautifully folded pose, keeping the wings firmly on the torso during actions and performances often requires labor-intensive counter animations. This talk introduces the surface constraint wing fold system developed for , enabling animators to maintain folded wings seamlessly during dynamic movements. By reducing manual adjustments, this system streamlined the process, saving time and allowing animators to focus on compelling performances.	https://dl.acm.org/doi/abs/10.1145/3721239.3734102	Yukinori Inagaki, Evan Boucher, Jakob Jensen, Mike Amos
Surrounded	"The latest in the ""Get an Airbnb"" series, ""Surrounded"" takes the miniature, tactile world we've developed for Airbnb in an exciting new direction. Instead of positioning an Airbnb on a mostly empty stage environment where the focus is on the home itself, the Airbnb is nestled into a full-frame, immersive forest."	https://dl.acm.org/doi/abs/10.1145/3698896.3724151	Joe Mullen
SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation	Recent advancements in large vision-language models have enabled highly expressive and diverse vector sketch generation. However, state-of-the-art methods rely on a time-consuming optimization process involving repeated feedback from a pretrained model to determine stroke placement. Consequently, despite producing impressive sketches, these methods are limited in practical applications. In this work, we introduce , a diffusion model for image-conditioned vector sketch generation that can produce high-quality sketches in less than a second. SwiftSketch operates by progressively denoising stroke control points sampled from a Gaussian distribution. Its transformer-decoder architecture is designed to effectively handle the discrete nature of vector representation and capture the inherent global dependencies between strokes. To train SwiftSketch, we construct a dataset of image-sketch pairs, addressing the limitations of existing sketch datasets, which are often created by non-artists and lack professional quality. For generating these synthetic sketches, we introduce ControlSketch, a method that enhances SDS-based techniques by incorporating precise spatial control through a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across diverse concepts, efficiently producing sketches that combine high fidelity with a natural and visually appealing style.	https://dl.acm.org/doi/abs/10.1145/3721238.3730612	Ellie Arar, Yarden Frenkel, Daniel Cohen-Or, Ariel Shamir, Yael Vinker
Synth2Track Editor for Efficient Match-Animation	A critical step in VFX production is capturing the movement of actors to integrate 3D digital assets into live-action footage. In recent years, advances in regression-based computer vision models such as human detection and motion models have enabled new workflows to emerge where parts of the Match-Animation process are automated. However, challenging shots that contain ambiguous visual cues, strong occlusions, or challenging appearances can cause automated systems to fail and users must revert to manual specification or to the previous generation of semi-automatic tools based on local feature-based tracking [Bregler et al. ; Sullivan et al. ]. Our key insight is that regression models can be used not only at the beginning of the process, but throughout by using manually specified cues. For example, given a partially detected actor, the user can specify a few landmarks manually, which once re-injected into a model, will yield new detections for the rest of the body. Based on this insight, we developed new tools that significantly reduces the time required for complex shots, combining automation with human expertise to overcome the limitations of current markerless motion capture systems.	https://dl.acm.org/doi/abs/10.1145/3721239.3734082	Jakob Buhmann, Douglas L. Moore, Dominik Borer, Martin Guay
THE DEEP ABOVE - First Hunt	A vast ocean at the firmament. Humans suffering in the darkness underneath, longing for the warm light of the Lumathans - giant sea creatures worshiped as gods and the only light source in the world. Will the explorer SINH manage to reach the ocean and steal the Lumathan's light? THE DEEP ABOVE - First Hunt is one part of the epic story world of THE DEEP ABOVE and takes place approximately 100 years before the adventure game THE DEEP ABOVE - Last Lights. bit.ly/m/deepabove www.deep-above.com	https://dl.acm.org/doi/abs/10.1145/3698896.3720295	Niklas Wolff
TRUST	In a near future dominated by the metaverse, AI agents investigate cybercrime. Violet, an AI detective, is assigned to question Mia, a defiant youth. But upon hearing Violet's name, Mia reveals a shocking truth. In this sealed interrogation room, what revelation awaits? A neo-noir suspense short with a bold visual style.	https://dl.acm.org/doi/abs/10.1145/3698896.3725073	Tomofumi Inoue, Kazuma Shimizu, Chizuru Kakimoto
Tackling Gaussian Splats, Physics Simulation and Visualization with NVIDIA Kaolin and Warp Libraries	Recent advances in 3D reconstruction and 3D deep learning have introduced new representations such as 3D Gaussian Splatting ([Kerbl et al. ]) and its variants ([Du et al. ; Gao et al. ; Liang et al. ; Moenne-Loccoz et al. ]). While promising, working with these representations poses practical challenges in rendering/visualization, and interactive simulation. This course introduces researchers and practitioners to the open-source tools built within NVIDIA Kaolin using NVIDIA's Warp library for effectively working with and simulating 3D Gaussian Splats. We cover foundational features of these libraries and provide a hands-on deep dive into enabling elasto-dynamic physics simulation with contact, directly on these 3D Gaussian Splat objects as well as other implicit and explicit representations all in the same scene. Designed for accessibility, the course requires no prior background in Gaussian Splats or physics simulation. Attendees will follow interactive examples in Jupyter notebooks, with dedicated GPU resources provided to each participant. By the end of the course, participants will be equipped to integrate Kaolin and Warp into their workflows, enhancing their research and development capabilities in 3D learning and simulation.	https://dl.acm.org/doi/abs/10.1145/3721251.3736529	Vismay Modi, Clement Fuji Tsang, Masha Shugrina, Gilles Daviet
Taking Control: Procedural Diffusion Guidance for Architectural Facade Editing	Our training-free method enables photorealistic facade editing by combining hierarchical procedural structure control with diffusion models. Starting from a facade image, we reconstruct, edit, and guide generation to produce high-fidelity, photorealistic variations. The method ensures structural consistency and appearance preservation, demonstrating the power of symbolic modeling for controllable image synthesis.	https://dl.acm.org/doi/abs/10.1145/3721250.3742991	Aleksander Plocharski, Jan Swidzinski, Przemyslaw Musialski
TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling	Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.	https://dl.acm.org/doi/abs/10.1145/3721238.3730710	Gengyan Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler
Text-based Animatable 3D Avatars with Morphable Model Alignment	The generation of high-quality, animatable 3D head avatars from text has enormous potential in content creation applications such as games, movies, and embodied virtual assistants. Current text-to-3D generation methods typically combine parametric head models with 2D diffusion models using score distillation sampling to produce 3D-consistent results. However, they struggle to synthesize realistic details and suffer from misalignments between the appearance and the driving parametric model, resulting in unnatural animation results. We discovered that these limitations stem from ambiguities in the 2D diffusion predictions during 3D avatar distillation, specifically: i) the avatar's appearance and geometry is underconstrained by the text input, and ii) the semantic alignment between the predictions and the parametric head model is insufficient because the diffusion model alone cannot incorporate information from the parametric model. In this work, we propose a novel framework, , for text-based realistic animatable 3DGS avatar generation with morphable model alignment, and introduce two key strategies to address these challenges. First, we tackle appearance and geometry ambiguities by utilizing prior information from a pretrained text-to-3D model to initialize a 3D avatar with robust appearance, geometry, and rigging relationships to the morphable model. Second, we refine the initial 3D avatar for dynamic expressions using a ControlNet that is conditioned on semantic and normal maps of the morphable model to ensure accurate alignment. As a result, our method outperforms existing approaches in terms of synthesis quality, alignment, and animation fidelity. Our experiments show that the proposed method advances the state of the art in text-based, animatable 3D head avatar generation. Code and model for this paper are at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730680	Yiqian Wu, Malte Prinzler, Xiaogang Jin, Siyu Tang
The Art of Crowds Animation	Crowds scenes, in which multiple characters come to life to achieve a cohesive performance, present a unique set of animation challenges. While the techniques, workflows, organization and execution have all evolved for more than 100 years at Walt Disney Animation Studios, the goals, the underlying principles, and the fundamentals have remained the same. We present a distillation of the essence of the with examples from a wide range of Disney's animated films.	https://dl.acm.org/doi/abs/10.1145/3721239.3734085	Alberto J Luceño Ros, Jeff Sullivan
"The Cinematography of Songs in Disney's ""Moana 2"""	"In Walt Disney Animation Studios' ""Moana 2"", our main character, Moana, embarks on a journey which takes her farther from home than she has ever been before. She undergoes a mental odyssey of dealing with what it means to be so distant from those she loves. Moana's voyage takes her through magical and mystical worlds that greatly differ in visual style. This visual design and the camera language used across the songs of ""Moana 2"" support the progression of Moana's character and the cinematography celebrates the distinct look and feel of these unique worlds."	https://dl.acm.org/doi/abs/10.1145/3721239.3734100	Sucheta Bhatawadekar, Behzad Mansoori-Dara, Adolph Lusinsky, Rob Dressel
The Environments of Dune: Prophecy through the Gaussian Splat	We describe a production‑proven workflow that converts deep renders from into compact, photoreal Gaussian‑splat assets for real‑time scouting. A custom Houdini Digital Asset (HDA) assembles COLMAP‑compatible training data from 3 K deep images, then Jawset Postshot trains ∼16 M‑point splats in 3 h on a single RTX 4090. Creatives explored both day and night versions of the kilometer‑scale Imperial Palace set at 60–90 fps, this opened the possibility of digital scouting a full CG environment. The approach delivers the visual fidelity of offline shading while slashing preview latency to seconds and paves the way for using trained splats as final‑pixel backgrounds in compositing.	https://dl.acm.org/doi/abs/10.1145/3721239.3734124	Julien Hery
The Gesture Lives On: A VR-Driven Puppet Performance in Immersive Space	The Gesture Lives On is a real-time VR performance system that reimagines traditional Taiwanese glove puppetry through immersive, interactive means. Rooted in precise gestural movement, this art form faces decline amid shifting audience engagement. Using VR-based gesture recognition, a performer co-creates a digital duet with a virtual puppet. This hybrid space transforms tradition into contemporary expression, offering audiences a new way to experience puppetry within a shared virtual–physical environment.	https://dl.acm.org/doi/abs/10.1145/3721250.3742981	Yi Jen Lin, Wei-Chen Yen, Chun-Cheng Hsu
The Modern Vulkan Course	This course offers an exploration of some features of Vulkan 1.3 and 1.4, equipping developers with the tools and understanding needed to build modern Vulkan renderers. The course is divided into three focused parts that cover the essentials of modern Vulkan, synchronization techniques, and bindless rendering. Participants will gain exposure to real-world applications of modern Vulkan concepts and strategies to implement bindless on mobile architectures.	https://dl.acm.org/doi/abs/10.1145/3721241.3733990	Sergey Kosarevsky, Mauricio Maurer, Preetish Kakkar
The Mooning	The Mooning is an animated mockumentary that reveals the truth behind the moon landing. This hard hitting doc uncovers America's lost astronaut, Doug Reynolds.	https://dl.acm.org/doi/abs/10.1145/3698896.3725458	Mason Klesch, Vivian Osness
The Pillars of Creation and the Interplay of Stars and Dust	Explore the famous Pillars of Creation using data from NASA's Hubble and Webb Space Telescopes.	https://dl.acm.org/doi/abs/10.1145/3698896.3725417	Frank Summers, Gregory Bacon
The Red Heel	A midcentury misogynist gets what he deserves when he's forced to spend a day in heels.	https://dl.acm.org/doi/abs/10.1145/3698896.3725457	Elena Serenko
The Temporality of Duration: An Exploratory Study on Slit-Scan Photography	This study develops an automated Slit-Scan Photography (SSP) system addressing four technical limitations in capturing temporal continuity: exposure complexity (16-parameter matrix), reciprocity failure, mechanical instability (±0.01mm precision), and slit contamination. Integrating quaternary exposure controls, servo-driven slit mechanisms, and adaptive ND filters (>14-stop range), the prototype achieves spatiotemporal synthesis of 2D space with 1D time vectors. Grounded in Kolb's experiential learning theory, it materializes Bergson's durée concept while demonstrating commercial potential through patented shutter technology. The system establishes new paradigms for temporal visualization in artistic and scientific imaging, reconciling technical precision with Deleuzian time-image aesthetics.	https://dl.acm.org/doi/abs/10.1145/3721239.3734110	Wen Bo Wang, Yuan Zhang, Yin Bing, Yan Song Chen, Heng Yi, Ji Ma, Gang Zhi Wang
The VFX of The Penguin	"For the HBO Original Limited Series THE PENGUIN, 3107 VFX shots were created across 8 episodes. In this direct sequel to the feature film THE BATMAN, the city is on its knees, making way for Oswald ""Oz"" Cobb to rise up as the new kingpin of Gotham City."	https://dl.acm.org/doi/abs/10.1145/3698897.3718982	Johnny Han, Sigurjon Gardarsson, Goran Pavles, Adrien Saint Girons, Nathaniel Larouche, Eugene Bondar
The design opportunities of moving to Houdini for lighting within the world of Animation	"Lighting is a critical part of our animated films. We developed a new Lighting workflow in Houdini Solaris for the feature production, Walt Disney Animation Studios' ""Moana 2"". Our goal was to empower artists with greater control while reducing barriers to creative expression. How do we enable new workflows with greater control while lowering the barrier for our artists to bring their visions to life? What experiences do we mirror in the new tooling landscape and what do you leave behind? This talk explores the lessons learned and some best practices for building and adopting new tools within a legacy system while creating a seamless experience for our artists. We will also look at a few example toolsets we developed to help our Lighting artists transition to the new workflow."	https://dl.acm.org/doi/abs/10.1145/3721239.3734106	Norman Joseph, Rehan Butt
Through the Storm: The VFX of 'The Last of Us' from Season One to Season Two	Join DNEG VFX Supervisor Stephen James, DFX Supervisor Melaina Mace, and FX Supervisor Roberto Rodricks for a behind-the-scenes look at the visual effects that brought the incredible episodic series `The Last of Us' to life!	https://dl.acm.org/doi/abs/10.1145/3698897.3716552	Stephen James, Melaina Mace, Roberto Rodricks, Jennie Kitchen
Thunderscapes: Simulating the Dynamics of Mesoscale Convective System	A Mesoscale Convective System (MCS) is a collection of thunderstorms operating as a unified system, showcasing nature's untamed power. They represent a phenomenon widely referenced in both the natural sciences and the visual effects (VFX) industries. However, in computer graphics, visually accurate simulation of MCS dynamics remains a significant challenge due to the inherent complexity of atmospheric microphysical processes. To achieve a high level of visual quality while ensuring practical performance, we introduce , the first physically based simulation framework for visually realistic MCS tailored to graphical applications. Our model integrates mesoscale cloud microphysics with hydrometeor electrification processes to simulate thunderstorm development and lightning flashes. By capturing various thunderstorm types and their associated lightning activities, demonstrates the versatility and physical accuracy of the proposed approach.	https://dl.acm.org/doi/abs/10.1145/3728312	Tianchen Hao, Jinxian Pan, Yangcheng Xiang, Xiangda Shen, Xinsheng Li, Yanci Zhang
Time-Traveling VFX: AI-Driven De-Aging in Here	Join Production Supervisor Kevin Baillie and Metaphysic VFX Supervisor Jo Plaete for a deep dive into how artist-empowering artificial intelligence enabled unprecedented workflows on Robert Zemeckis's new film, Here. Told from a single perspective that transcends time, Here follows characters played by Tom Hanks, Robin Wright, Paul Bettany, and Kelly Reilly across multiple decades of their lives. While these monumental age spans were crucial to the narrative, the production faced an immense technical challenge: how to maintain the actors' performances and emotional realism while radically altering their appearances.	https://dl.acm.org/doi/abs/10.1145/3698897.3718750	Kevin Baillie, Jo Plaete
To Infinity and Beyond: a GPU-driven memory sharing pipeline to generate and process infinite synthetic data	In this work, we present a novel pipeline based on Unreal Engine 5, which allows us to generate, render, and process graphics data entirely on the GPU. By keeping the data stored in GPU memory throughout all the steps, we bypass the traditional bottlenecks related to CPU-GPU transfers, significantly accelerating data manipulation and enabling fast training of deep learning algorithms. Traditional storage systems impose latency and capacity limitations, which become increasingly problematic as data volume increases. Our method demonstrates substantial performance improvements on multiple benchmarks, offering a new paradigm for integrating game engines with data-driven applications. More information on our project page: https://mmlab-cv.github.io/Infinity/	https://dl.acm.org/doi/abs/10.1145/3721250.3742983	Daniele Della Pietra, Gino Lanzo Hahn, Nicola Garau
Towards AI-Driven 3D Creation at the Speed of Thought	This talk explores how modern foundation models, particularly vision-language models (VLMs), can intelligently interface with and augment 3D graphical tools like Blender. Through three major contributions — BlenderAlchemy, BlenderGym, and FirePlace — we investigate the potential of AI-driven automation in 3D content creation while ensuring human control and editability. We show that even though VLMs of today have many weaknesses, inference-compute scaling and complementary tool-use are powerful methods that can improve their performance on graphical editing tasks in a training-free manner. Through the works I'll share 4 different ways of scaling inference compute to improve VLMs' capabilities on operating human graphical tools to accomplish graphical edits.	https://dl.acm.org/doi/abs/10.1145/3721239.3734117	Ian Huang
Towards Comprehensive Neural Materials: Dynamic Structure-Preserving Synthesis with Accurate Silhouette at Instant Inference Speed	Photorealistic rendering aims to accurately replicate real-world appearances. Traditional methods, like microfacet-based models, often struggle with complex visuals. Consequently, neural material techniques have emerged, typically offering improved performance over traditional approaches. However, these neural material approaches only attempt to address one or a few essential aspects of the complete appearance while neglecting others ( , , , ). Although these aspects may seem separate, they are inherently intertwined as part of the complete appearance which cannot be isolated. In this paper, we challenge the comprehensive neural material representation by thoroughly considering the essential aspects of the complete appearance. We introduce an int8-quantized neural network that keeps high fidelity ( ) while achieving an order of magnitude speedup ( ) compared to previous methods. We also present a controllable structure-preserving synthesis strategy ( ), along with accurate displacement effects ( ) through a dynamic two-step displacement tracing technique.	https://dl.acm.org/doi/abs/10.1145/3721238.3730626	Zilin Xu, Xiang Chen, Chen Liu, Beibei Wang, Lu Wang, Zahra Montazeri, Ling-Qi Yan
Towards Understanding Depth Perception in Foveated Rendering	The true vision for real-time virtual and augmented reality is reproducing our visual reality in its entirety on immersive displays. To this end, foveated rendering leverages the limitations of spatial acuity in human peripheral vision to allocate computational resources to the fovea while reducing quality in the periphery. Such methods are often derived from studies on the spatial resolution of the human visual system and its ability to perceive blur in the periphery, enabling the potential for high spatial quality in real-time. However, the effects of blur on other visual cues that depend on luminance contrast, such as depth, remain largely unexplored. It is critical to understand this interplay, as accurate depth representation is a fundamental aspect of visual realism. In this paper, we present the first evaluation exploring the effects of foveated rendering on stereoscopic depth perception. We design a psychovisual experiment to quantitatively study the effects of peripheral blur on depth perception. Our analysis demonstrates that stereoscopic acuity remains unaffected (or even improves) by high levels of peripheral blur. Based on our studies, we derive a simple perceptual model that determines the amount of foveation that does not affect stereoacuity. Furthermore, we analyze the model in the context of common foveation practices reported in literature. The findings indicate that foveated rendering does not impact stereoscopic depth perception, and stereoacuity remains unaffected with up to 2 × stronger foveation than commonly used. Finally, we conduct a validation experiment and show that our findings hold for complex natural stimuli.	https://dl.acm.org/doi/abs/10.1145/3721238.3730609	Sophie Kergaßner, Taimoor Tariq, Piotr Didyk
Towards accelerating polarization path tracing of multi-bounce Smith microfacet BSDFs	The polarization state of light is described in a local coordinate frame, where the oscillation of the electronic and magnetic fields occurs. In physics, this frame is rotated according to the surface normal of the object. In this study, we investigate the effect of this frame rotation while evaluating multi-bounce Smith microfacet BSDFs. We show evidence that we can speed up the evaluation if the frame rotation does not significantly matter. Then, we experimentally show the acceleration can be practically feasible.	https://dl.acm.org/doi/abs/10.1145/3721250.3742996	Hidehito Ohba, Tatsuya Yatagawa, Shigeo Morishima
Towards automated corrections in video-driven animation transfer	We describe a novel method that improves Digital Domain's hybrid video-driven animation transfer technique for facial motion capture [Serra et al. ]. In this work, we automate the animation correction pass, otherwise done by artists, and accelerate the production cycle while minimizing subjectivity associated with matching the actor's facial expressions to the CG character. We leverage our video-driven animation transfer model that produces images of the CG character matching the actor's performance, by using those images as targets in a differentiable renderer optimization loop. Thus improving the model's initially predicted geometry. Furthermore, lighting parameters are removed from the optimization by training light-invariant models with a simple augmentation strategy. The corrected animations can be used directly in shots or to fine-tune the base model, as done in the earlier approach. Validation tests confirmed our method's efficacy, and it is now being integrated into Digital Domain's facial motion capture workflow.	https://dl.acm.org/doi/abs/10.1145/3721239.3734122	Jose Serra, Lucio Moser
Transferable: n/a	In a world ravaged by an incurable illness, a devoted husband is forced to make an impossible choice as his wife's life hangs in the balance.	https://dl.acm.org/doi/abs/10.1145/3698896.3722472	Jacob Gardner, David Hubert
Transform-Aware Sparse Voxel Directed Acyclic Graphs	Sparse Voxel Directed Acyclic Graphs (SVDAGs) have proven to be an efficient data structure for storing sparse binary voxel scenes. The SVDAG exploits repeating geometric patterns; which can be improved when considering mirror symmetries. We extend the previous work by providing a generalized framework to efficiently involve additional types of transformations and propose a novel translation matching for even more geometry reuse. Our new data structure is stored using a novel pointer encoding scheme to achieve a practical reduction in memory usage.	https://dl.acm.org/doi/abs/10.1145/3728301	Mathijs Molenaar, Elmar Eisemann
Trash	In a dark alley, a scrawny rat has no choice but to fight with a pigeon for a small slice of pizza. Without a second thought they throw themselves in a vertiginous chase from the top to the bottom of the street.	https://dl.acm.org/doi/abs/10.1145/3698896.3718728	Maxime Crançon, Fanny Vecchie, Margaux Lutz, Romain Fleischer, Alexis Le Ral, Grégory Bouzid, Robin Delaporte, Mattéo Durant
Turbulence	First flight. No parents. Total panic. A terrified boy just wants to survive takeoff, but the plane—and its deranged passengers—have other plans. One guy treats turbulence like a free massage, another is way too at peace with crashing, and sanity is nowhere to be found. As the plane bucks and twists, so does reality—faces stretch, limbs flail, and gravity calls it quits. Trapped at 30,000 feet in an airborne freak show, the boy must hold on for dear life. . . or completely lose it.	https://dl.acm.org/doi/abs/10.1145/3698896.3721039	Christopher Rutledge, Magnus Igland Møller
USD in Production	"Building on the content from the previous year's course, the path of adoption is again the focus of the class. This iteration aims to guide the student through more specific and advanced pillars of USD by highlighting some increasingly complex integration scenarios. These include the trends and challenge of adopting USD, innovative solutions for USD structure as well as how DCCs like Maya are continuing to push USD integration within their architectures. Using real world examples, the instructors will present challenges faced in many integrations, some common pitfalls that arise and ultimately concepts on how instructors proposed to solve them. Throughout this course, each presenter will provide insights and detailed learning materials from different aspects of USD, based on recent production experience across different facilities. This course is not intended to present a definitive, ""one-true"" approach on how to use USD, but rather it seeks to provide insightful lessons for how productions are working with its current abilities and constraints, aiming at these lessons to be informative and inspiring to others putting USD into productions. By the end of the course, attendees will have acquired knowledge that will help them make informed decisions on why they would choose one approach over another when confronted with a USD challenge."	https://dl.acm.org/doi/abs/10.1145/3721241.3733995	Nick Porcino, Reuben Bloom, Rong Zhuang, Paolo Selva, Tymon Pitts, Pallav Sharma
Uncertainty for SVBRDF Acquisition using Frequency Analysis	This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view captures. Under uncontrolled illumination and unstructured viewpoints, there is no guarantee that the observations contain enough information to reconstruct the appearance properties of a captured object. We study this ambiguity, or uncertainty, using entropy and accelerate the analysis by using the frequency domain, rather than the domain of incoming and outgoing viewing angles. The result is a method that computes a map of uncertainty over an entire object within a millisecond. We find that the frequency model allows us to recover SVBRDF parameters with competitive performance, that the accelerated entropy computation matches results with a physically-based path tracer, and that there is a positive correlation between error and uncertainty. We then show that the uncertainty map can be applied to improve SVBRDF acquisition using capture guidance, sharing information on the surface, and using a diffusion model to inpaint uncertain regions. Our code is available at .	https://dl.acm.org/doi/abs/10.1145/3721238.3730592	Ruben Wiersma, Julien Philip, Miloš Hašan, Krishna Mullia, Fujun Luan, Elmar Eisemann, Valentin Deschaintre
Unsupervised Decomposition of 3D Shapes into Expressive and Editable Extruded Profile Primitives	Transforming 3D shapes into representations that support part-level editing, flexible redesign, and efficient compression is vital for asset customization, content creation, and optimization in digital design. Despite its importance, achieving a representation that balances expressivity, editability, compactness, and interpretability remains a challenge. We introduce 3D2EP, a novel method for 3D shape decomposition that represents objects as a collection of differentiable, parametric primitives. Given a 3D shape represented by a voxel grid, 3D2EP decomposes this into a set of primitive parts, each generated by extruding a scaled 2D profile along a 3D curve, with the requisite components being predicted in a feedforward manner. That is, each primitive is constrained to have a single cross-section profile, up to scale. This enables the primitives to adapt to the data, capturing the geometry with precision but without excess degrees-of-freedom that would stymie editability. Extensive evaluations highlight 3D2EP's ability to reconstruct complex shapes with a compact and interpretable representation, emphasizing its suitability for a wide range of 3D modeling applications.	https://dl.acm.org/doi/abs/10.1145/3721238.3730704	Chunyi Sun, Junlin Han, Runjia Li, Weijian Deng, Dylan Campbell, Stephen Gould
Using Local Virtual Machines to Create OS-Agnostic Workstations	At the BYU Center for Animation, we have more students than workstations. Since lab seating is arbitrary, each student needs to be able to work on any machine at any time and each machine must support a wide range of activities, including video game development, animated film production, live class instruction, and homework assignments. For the past three years, we have used VMs in our computer lab disk image to facilitate a hybrid Linux and Windows environment that is flexible, maintainable, and artist-friendly. We present strategies for maximizing performance within the virtual machine, ensuring consistent availability of resources such as file systems and networks between operating systems, securely authenticating, and making the dual-OS workflow artist-friendly. We outline the architecture, challenges faced, and performance benchmarks, demonstrating the potential of VM-enabled, OS-agnostic lab environments for enhancing small studio or educational workflows and maximizing resource utilization.	https://dl.acm.org/doi/abs/10.1145/3721239.3734118	Scott Milner, Matthew Minson, Dallin Clark, Craig Van Dyke
VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image	Understanding bimanual hand interactions is essential for realistic 3D pose and shape reconstruction. However, existing methods struggle with occlusions, ambiguous appearances, and computational inefficiencies. To address these challenges, we propose Vision Mamba Bimanual Hand Interaction Network (VM-BHINet), introducing state space models (SSMs) into hand reconstruction to enhance interaction modeling while improving computational efficiency. The core component, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock), combines SSMs with local and global feature operations, enabling deep understanding of hand interactions. Experiments on the InterHand2.6M dataset show that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean per-vertex position error (MPVPE) by 2-3 , significantly surpassing state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3728308	Han Bi, Ge Yu, Yu He, Wenzhuo Liu, Zijie Zheng
VR Graphics – Challenges and Optimizations	This course starts with challenges of VR graphics, and then introduces details of many optimization techniques to tackle those challenges from different aspects. All the introduced techniques have been used in the on-market VR devices. For each detail, we explained how it works and what benefit it brings to us to improve user experience for VR graphics. The course concludes with a wrap up of the contents together with more challenges that are not well addressed at this time. The goal of this course is to improve mutual understanding between academia and industry, introduce the challenges of VR graphics and the state-of-the-art optimization techniques, and finally, inspire and attract more research results in this area in the future.	https://dl.acm.org/doi/abs/10.1145/3721241.3735314	Guodong Rong
VR That Doesn't Make You Sick: Understanding & Mitigating Cybersickness	"Cybersickness remains a persistent challenge in virtual and extended reality (VR/XR), affecting user comfort, adoption, and long-term engagement. This course provides developers, designers, and researchers with science-backed strategies for mitigating cybersickness in immersive environments. Attendees will explore theoretical foundations, practical mitigation techniques, and real-world case studies, including locomotion strategies such as HyperJump, leaning-based navigation, and UI/scene optimizations. Through interactive discussions, live polling, and collaborative exercises, attendees will gain a structured understanding of cybersickness causes, measurement, and prevention. The session will conclude with a structured ""Do's & Don'ts"" guide for immediate application in VR/XR projects. This course ensures attendees leave with actionable strategies to enhance XR usability. Ideal for VR/XR developers, UX designers, and researchers, this course translates scientific insights into practical solutions, equipping attendees with the knowledge and tools to identify, assess, and mitigate cybersickness, thereby enhancing user comfort, engagement, and adoption in their XR projects."	https://dl.acm.org/doi/abs/10.1145/3721241.3733992	Bernhard E. Riecke, Behrang Keshavarz
VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian Splatting and Neural Points	Recent advances in novel view synthesis have demonstrated impressive results in fast photorealistic scene rendering through differentiable point rendering, either via Gaussian Splatting (3DGS) [Kerbl et al. ] or neural point rendering [Aliev et al. ]. Unfortunately, these directions require either a large number of small Gaussians or expensive per-pixel post-processing for reconstructing fine details, which negatively impacts rendering performance. To meet the high performance demands of virtual reality (VR) systems, primitive or pixel counts therefore must be kept low, affecting visual quality. In this paper, we propose a novel hybrid approach based on foveated rendering as a promising solution that combines the strengths of both point rendering directions regarding performance sweet spots. Analyzing the compatibility with the human visual system, we find that using a low-detailed, few primitive smooth Gaussian representation for the periphery is cheap to compute and meets the perceptual demands of peripheral vision. For the fovea only, we use neural points with a convolutional neural network for the small pixel footprint, which provides sharp, detailed output within the rendering budget. This combination also allows for synergistic method accelerations with point occlusion culling and reducing the demands on the neural network. Our evaluation confirms that our approach increases sharpness and details compared to a standard VR-ready 3DGS configuration, and participants of a user study overwhelmingly preferred our method. Our system meets the necessary performance requirements for real-time VR interactions, ultimately enhancing the user's immersive experience. The project page can be found at:	https://dl.acm.org/doi/abs/10.1145/3728302	Linus Franke, Laura Fink, Marc Stamminger
VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality	3D Gaussian Splatting (3DGS) has rapidly become a leading technique for novel-view synthesis, providing exceptional performance through efficient software-based GPU rasterization. Its versatility enables real-time applications, including on mobile and lower-powered devices. However, 3DGS faces key challenges in virtual reality (VR): (1) temporal artifacts, such as popping during head movements, (2) projection-based distortions that result in disturbing and view-inconsistent floaters, and (3) reduced framerates when rendering large numbers of Gaussians, falling below the critical threshold for VR. Compared to desktop environments, these issues are drastically amplified by large field-of-view, constant head movements, and high resolution of head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine and extend several recent advancements in 3DGS to address challenges of VR holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal Projection can complement each other, by modifying the individual techniques and core 3DGS rasterizer. Additionally, we propose an efficient foveated rasterizer that handles focus and peripheral areas in a single GPU launch, avoiding redundant computations and improving GPU utilization. Our method also incorporates a fine-tuning step that optimizes Gaussian parameters based on StopThePop depth evaluations and Optimal Projection. We validate our method through a controlled user study with 25 participants, showing a strong preference for VRSplat over other configurations of Mini-Splatting. VRSplat is the first, systematically evaluated 3DGS approach capable of supporting modern VR applications, achieving 72+ FPS while eliminating popping and stereo-disrupting floaters.	https://dl.acm.org/doi/abs/10.1145/3728311	Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, Fernando de la Torre
Variational Elastodynamic Simulation	"Numerical schemes for time integration are the cornerstone of dynamical simulations for deformable solids. The most popular time integrators for isotropic distortion energies rely on nonlinear root-finding solvers, most prominently, Newton's method. These solvers are computationally expensive and sensitive to ill-conditioned Hessians and poor initial guesses; these difficulties can particularly hamper the effectiveness of integrators, whose momentum conservation properties require reliable root-finding. To tackle these difficulties, this paper shows how to express variational time integration for a large class of elastic energies as an optimization problem with a ""hidden"" convex substructure. This hidden convexity suggests uses of optimization techniques with rigorous convergence analysis, guaranteed inversion-free elements, and conservation of physical invariants up to tolerance/numerical precision. In particular, we propose an Alternating Direction Method of Multipliers (ADMM) algorithm combined with a proximal operator step to solve our formulation. Empirically, our integrator improves the performance of elastic simulation tasks, as we demonstrate in a number of examples."	https://dl.acm.org/doi/abs/10.1145/3721238.3730726	Leticia Mattos Da Silva, Silvia Sellán, Natalia Pacheco-Tallaj, Justin Solomon
Verdun	During World War I, a family is torn apart by the horrors of war.	https://dl.acm.org/doi/abs/10.1145/3698896.3717732	Martin Morant, Maxime Lu Cong Sang, Macéo Chavey, Lilian Lefait, Mélie Morice, Mathieu Bron
VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control	Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motion at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications ( talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.	https://dl.acm.org/doi/abs/10.1145/3721238.3730647	Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, Hengshuang Zhao
VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control	Video inpainting, crucial for the media industry, aims to restore corrupted content. However, current methods relying on limited pixel propagation or single-branch image inpainting architectures face challenges with generating fully masked objects, balancing background preservation with foreground generation, and maintaining ID consistency over long video. To address these issues, we propose , an efficient dual-branch framework featuring a lightweight context encoder. This plug-and-play encoder processes masked videos and injects background guidance into any pre-trained video diffusion transformer, generalizing across arbitrary mask types, enhancing background integration and foreground generation, and enabling user-customized control. We further introduce a strategy to resample inpainting regions for maintaining ID consistency in any-length video inpainting. Additionally, we develop a scalable dataset pipeline using advanced vision models and construct and —the largest video inpainting dataset with segmentation masks and dense caption (>390K clips) —to support large-scale training and evaluation. We also show 's promising potential in downstream applications such as video editing. Extensive experiments demonstrate 's state-of-the-art performance in any-length video inpainting and editing across 8 key metrics, including video quality, mask region preservation, and textual coherence.	https://dl.acm.org/doi/abs/10.1145/3721238.3730673	Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, Qiang Xu
VirCHEW Reality: On-Face Kinesthetic Feedback for Enhancing Food-Intake Experience in Virtual Reality	While haptic interfaces for virtual reality (VR) has received extensive research attention, on-face haptics in VR remained less explored, especially for virtual food intake. In this paper, we introduce , a face-worn haptic device designed to provide on-face kinesthetic force feedback, to enhance the virtual food-chewing experience in VR. Leveraging a pneumatic actuation system, controlled the process of air inflation and deflation, to simulate the mechanical properties of food textures, such as hardness, cohesiveness, and stickiness. We evaluated the system through three user studies. First, a just-noticeable difference (JND) study examined users' sensitivity to and the system's capability of rendering different levels of on-face pneumatic-based kinesthetic feedback while users performing chewing action. Building upon the user-distinguishable signal ranges found in the first study, we further conducted a matching study to explore the correspondence between the kinesthetic stimuli provided by our device and user-perceived food textures, revealing the capability of simulating food texture properties during chewing (e.g., hardness, cohesiveness, stickiness). Finally, a user study in a VR eating scenario showed that could significantly improve the users' ratings on the sense of presence, compared to the condition without haptic feedback. Our findings further highlighted possible applications in virtual/remote dining, healthcare, and immersive entertainment.	https://dl.acm.org/doi/abs/10.1145/3721238.3730694	Qingqin Liu, Ziqi Fang, Jiayi Wu, Shaoyu Cai, Jianhui Yan, Tiande Mo, Shuk Ching Chan, Kening Zhu
Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes	3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital 3D assets from multi-view images by leveraging a set of 3D Gaussian primitives for rendering. Its explicit and discrete representation facilitates the seamless composition of complex digital worlds, offering significant advantages over previous neural implicit methods. However, when applied to large-scale compositions, such as crowd-level scenes, it can encompass numerous 3D Gaussians, posing substantial challenges for real-time rendering. To address this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D Gaussian clusters and dynamically selects only the necessary ones to accelerate rendering speed. Our approach consists of two stages: (1) Offline Build, where hierarchical clusters are generated using a local splatting method to minimize visual differences across granularities, and (2) Online Selection, where footprint evaluation determines perceptible clusters for efficient rasterization during rendering. We curate a dataset of synthetic and real-world scenes, including objects, trees, people, and buildings, each requiring 0.1 billion 3D Gaussians to capture fine details. Experiments show that our solution balances rendering efficiency and visual quality across user-defined tolerances, facilitating downstream interactive applications that compose extensive 3DGS assets for consistent rendering performance.	https://dl.acm.org/doi/abs/10.1145/3721238.3730602	Xijie Yang, Linning Xu, Lihan Jiang, Dahua Lin, Bo Dai
Visual Bird Sounds	Visualising Bird Sounds.	https://dl.acm.org/doi/abs/10.1145/3698896.3725207	Andy Thomas
Visualization Analysis and Design	This introductory course will present a systematic, comprehensive framework for thinking about visualization in terms of principles and design choices. It features a unified approach encompassing information visualization techniques for abstract data, scientific visualization techniques for spatial data, and visual analytics techniques for interweaving data transformation and analysis with interactive visual exploration. It emphasizes the careful validation of effectiveness and the consideration of function before form. It breaks down visualization design according to three questions: what data users need to see, why users need to carry out their tasks, and how the visual representations proposed can be constructed and manipulated. It will walk through the use of space and color to visually encode data in a view. Three major data types will be covered: tables, networks, and sampled spatial data. The course will also cover the four major families of strategies for handling complexity: deriving new data to show within a view, interactivity to change a view over time, faceting across multiple views that can be shown side by side, and reducing what is shown within a single view using aggregation and filtering. The emphasis of this course is the space of design choices; algorithms will not be covered. It is suitable for a broad audience, from beginners to more experienced designers. It does not assume any previous experience in programming, mathematics, human–computer interaction, or graphic design. The course format will be lectures interspersed with question and answer sessions.	https://dl.acm.org/doi/abs/10.1145/3721241.3733989	Tamara Munzner
Wednesdays with Gramps	"When a teenage boy visits his Gramps at a seemingly boring assisted living facility, he comes to find that they have much more in common than he thought. ""Wednesdays with Gramps"" is a story about connection, communication, and commonality, without saying a word."	https://dl.acm.org/doi/abs/10.1145/3698896.3725445	Chris Copeland, Justin Copeland, Shabrayia Cleaver
Weight Illusion Induced by AR Visual Effects on the Arm	"This study investigates the effective range of the weight illusion induced by AR visual effects displayed on the arm. The results show that AR visual effects on the arm can create a ""strong"" impression and that using such visual effects can induce a weight illusion in which weights ranging from 100 g to 500 g are perceived as lighter when lifted with the arm augmented by these visual effects."	https://dl.acm.org/doi/abs/10.1145/3721250.3743028	Mie Sato, Kazuki Takeyama, Naoki Hashimoto
Welcome to the City of Love	Nexus Studios' Fx Goby draws a parallel between romantic love and the passion athletes feel for their sports in this launch film for the Olympic Games coverage. Fx and the Nexus Studios team led an artful choreography of 58 shots and 36 athletes in 60 seconds of breathtaking film.	https://dl.acm.org/doi/abs/10.1145/3698896.3743046	Fx Goby
What is HDR? Perceptual Impact of Luminance and Contrast in Immersive Displays	The contrast and luminance capabilities of a display are central to the quality of the image. High dynamic range (HDR) displays have high luminance and contrast, but it can be difficult to ascertain whether a given set of characteristics qualifies for this label. This is especially unclear for new display modes, such as virtual reality (VR). This paper studies the perceptual impact of peak luminance and contrast of a display, including characteristics and use cases representative of VR. To achieve this goal, we first developed a haploscope testbed prototype display capable of achieving 1,000 nits peak luminance and 1,000,000:1 contrast with high precision. We then collected a novel HDR video dataset targetting VR-relevant content types. We also implemented custom tone mapping operators to map between display parameter sets. Finally, we collected subjective preference data spanning 3 orders of magnitude in each dimension. Our data was used to fit a model, which was validated using a subjective study on an HDR VR prototype headmounted display (HMD). Our model helps provide guidance for future display design, and helps standardize the understanding of HDR .	https://dl.acm.org/doi/abs/10.1145/3721238.3730629	Kenneth Chen, Nathan Matsuda, Jon McElvain, Yang Zhao, Thomas Wan, Qi Sun, Alexandre Chapiro
"When Mud Toys Meet Digital M(B)uddies: How ""Play with Earth"" Bridges Traditional Craftsmanship and AI-Assisted Creation"	"Play with Earth"" introduces a novel project that addresses the preservation and innovation of intangible cultural heritage (ICH), with a focus on traditional mud toys from China's Yellow River. Based on a comprehensive documentation of 15,686 photographs of mud toys and interviews with inheritors, our project achieved an interactive platform combining traditional craftsmanship with AI-assisted creativity."	https://dl.acm.org/doi/abs/10.1145/3721250.3743000	Mengyao Guo, Junfeng Meng
Woke Dataviz: Equitable Data Design For Visualizing People And Social Outcomes.	"Wokeness is a problem in the United States. Specifically, it is argued, Americans are not nearly woke enough. And this lack of wokeness could be our undoing. Widespread social misbeliefs are not just dangerous for the people being misunderstood, they put everyone at risk, undermining public health, education, economic progress, and democracy itself. These distortions can be small and odd (e.g. questionable hygiene choices) or big and dangerous (e.g. vaccine skepticism, gun violence, systemic inequality, rising fascism). In this context, woke dataviz is not simply a question of ethics or social justice, it is a matter of self-preservation. The way we visualize others can reinforce the harmful misbeliefs that put us all at risk. For example, bar charts of social disparities can subtly misdirect blame and promote harmful stereotypes. On the other hand, more transparent, expressive visualizations can interrupt these biases. Similar effects may be possible in other visual media, such as news photography, casting elves of color in prestige fantasy shows, or Shadowheart's ""jiggle physics."" This course explores the surprising interplay between visual representation and social cognition. It looks at how visual rhetoric influences perception, how those perceptions support broader social narratives, and how those narratives, in turn, can shape our reality. It also branches out into related visual media, communication research, and political psychology to show how widespread — and unsettling — these effects can be. This course will also be practical, covering frameworks for unpacking the social implications of data design, and techniques for clear, constructive representations of the people and systems around us. Participants will leave with a sharper eye, a few added design tricks, and a justifiably smug attitude toward bar charts."	https://dl.acm.org/doi/abs/10.1145/3721241.3736523	Eli Holder
Wormwood	The day the radiation disappears, Simon rushes to the heart of the zone, taking his colleague Agathe with him, in the hope of rediscovering a lost past.	https://dl.acm.org/doi/abs/10.1145/3698896.3719339	Matthieu Dupille, Chenhe Liu, Philémon Martin, Ninon Quéméner, Alexander Vanderplank, Binlin Xie
Wētā FX Presents: The Last Of Us Season 2: The Horde	Starting with an exciting and challenging client brief — an infected horde thawing from frozen stasis to overrun the town of Jackson, and a tight timeline for this ambitious work, meant our Wētā FX team really had something to sink our teeth into.	https://dl.acm.org/doi/abs/10.1145/3698897.3718747	Nick Epstein, Dennis Yoo
"You Can Fly! The Making of ""Peter Pan's Never Land Adventure"""	Have you ever wondered what it would be like to fly? To soar among the clouds on an adventure with Peter Pan? That was the question Walt Disney Imagineering and Walt Disney Animation Studios set out to answer with Peter Pan's Never Land Adventure, the new attraction which opened in June of 2024 in Tokyo DisneySea's Fantasy Springs. The development of this major new ride-through adventure took over seven years, with hundreds of artists, technicians, and software developers partnering to get it off the ground.	https://dl.acm.org/doi/abs/10.1145/3698897.3718744	Kendall Litaker, Darin Hollings, Maya Vyas
You Can Grow Here: A Therapeutic VR Journey for Anxiety Management	You Can Grow Here is an immersive VR experience developed for the CAVE2™ environment, aligning with the UN Sustainable Development Goal of Good Health and Well-Being. In response to the mental health challenges intensified by the COVID-19 pandemic, the project explores how interactive storytelling, ambient sound, and 3D typography can support emotional reflection and teach anxiety coping strategies. Built in Unity with custom assets from Blender and Maya, the experience differs from most clinical VR programs, allowing users to independently explore emotions, manage anxiety, and practice evidence-based calming techniques within a safe, narrative-driven space that builds emotional resilience.	https://dl.acm.org/doi/abs/10.1145/3721250.3743037	Gaeun Lee, Hope Jo, Cindy Nakhammouane, Khin Yuupar Myat
pOps: Photo-Inspired Diffusion Operators	Text-guided image generation enables the creation of visual content from textual descriptions. However, certain visual concepts cannot be effectively conveyed through language alone. This has sparked a renewed interest in utilizing the CLIP image embedding space for more visually-oriented tasks through methods such as IP-Adapter. Interestingly, the CLIP image embedding space has been shown to be semantically meaningful, where linear operations within this space yield semantically meaningful results. Yet, the specific meaning of these operations can vary unpredictably across different images. To harness this potential, we introduce , a framework that trains specific semantic operators directly on CLIP image embeddings. Each operator is built upon a pretrained Diffusion Prior model. While the Diffusion Prior model was originally trained to map between text embeddings and image embeddings, we demonstrate that it can be tuned to accommodate new input conditions, resulting in a diffusion operator. Working directly over image embeddings not only improves our ability to learn semantic operations but also allows us to directly use a textual CLIP loss as an additional supervision when needed. We show that can be used to learn a variety of photo-inspired operators with distinct semantic meanings. These operators can then serve as creative tools within a design process, enabling artists to semantically manipulate visual concepts as part of their generative workflow. Finally, we show that can be easily plugged into pretrained image diffusion models alongside existing spatial adapters, offering control over both semantics and structure.	https://dl.acm.org/doi/abs/10.1145/3721238.3730615	Elad Richardson, Yuval Alaluf, Ali Mahdavi-Amiri, Daniel Cohen-Or
xADA: Controllable and Expressive Audio-Driven Animation	We introduce ADA, a generative model for creating expressive and realistic animation of the face, tongue, and head directly from speech audio. Our approach leverages the pretrained Whisper audio encoder to extract rich speech features which are decoded into face and head animation using a series of gated recurrent unit (GRU) networks. The generated animation maps directly onto MetaHuman compatible rig controls enabling seamless integration into industry-standard content creation pipelines. ADA operates fully automatically, with an option for users to override the detected emotion and/or blink timings. ADA generalizes across languages, and voice styles, and can animate non-verbal sounds. Quantitative evaluation and a user study demonstrate that ADA produces state-of-the-art animation with high realism, frequently indistinguishable from ground truth performance. Additionally, we outline a comprehensive data capture protocol designed to collect an extensive range of speech and non-verbal sounds for training animation models.	https://dl.acm.org/doi/abs/10.1145/3721238.3730711	Sarah Taylor, Salvador Medina, Jonathan Windle, Erica Alcusa Sáez, Iain Matthews
