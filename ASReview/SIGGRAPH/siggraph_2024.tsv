title	abstract	url	authors
#DoudouChallenge	On the road to her family vacation, Olivia, a 10-year old girl hooked on her phone and social media, is abandoned by her parents in a highway service area. Alone with her plushie, he tries to get her off her phone to play, but things take an unexpected turn!	https://dl.acm.org/doi/abs/10.1145/3641230.3648634	Julie Majcher, Alexandra Delaunay-Fernandez, Sixtine Emerat, Marine Benabdallah-Crolais, Scott Pardailhé-Galabrun, Noémie Segalowicz
2D Gaussian Splatting for Geometrically Accurate Radiance Fields	3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering. Project page at https://surfsplatting.github.io.	https://dl.acm.org/doi/abs/10.1145/3641519.3657428	Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao
360° Animation Projects by Students with a Background in Traditional Stop-Motion Techniques	Innovations in media technology, exemplified by extended reality, have begun. Hence, the seamless presentation of media, such as animation, film, and games, is progressing at an impressive rate. Integrating these advanced media and experiences into educational environments has excellent potential. Similarly, students who have studied traditional stop-motion animation can discover new forms of expression and enhance their abilities by challenging themselves to incorporate advanced media. This study introduces a unique challenge of stop-motion animation in 360° as a first step toward VR for students who have learned traditional handcrafted object modeling and stop-motion animation. No game engine or VR painting application will be used to ensure that this assignment is free from technical hurdles. Instead, students will complete the project using only an omni-directional camera and standard video editing software.	https://dl.acm.org/doi/abs/10.1145/3641235.3664439	Nahomi Maki
3D Gaussian Blendshapes for Head Avatar Animation	We introduce 3D Gaussian blendshapes for modeling photorealistic head avatars. Taking a monocular video as input, we learn a base head model of neutral expression, along with a group of expression blendshapes, each of which corresponds to a basis expression in classical parametric face models. Both the neutral model and expression blendshapes are represented as 3D Gaussians, which contain a few properties to depict the avatar appearance. The avatar model of an arbitrary expression can be effectively generated by combining the neutral model and expression blendshapes through linear blending of Gaussians with the expression coefficients. High-fidelity head avatar animations can be synthesized in real time using Gaussian splatting. Compared to state-of-the-art methods, our Gaussian blendshape representation better captures high-frequency details exhibited in input video, and achieves superior rendering performance.	https://dl.acm.org/doi/abs/10.1145/3641519.3657462	Shengjie Ma, Yanlin Weng, Tianjia Shao, Kun Zhou
3D Gaussian Splatting with Deferred Reflection	The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting.	https://dl.acm.org/doi/abs/10.1145/3641519.3657456	Keyang Ye, Qiming Hou, Kun Zhou
3D Printing Shape-Changing Devices with Inductive Sensing	We present a novel technique that converts arbitrary 3D models into 3D printable shape-changing devices with sensing capability through the integrated helix-and-lattice structure. The structure comprises a central hollow helical channel, surrounded by lattice padding and a surface mesh, which allows the device to become elastic and deformable. By inserting a conductive steel wire into the hollow helical channel, inductive signals are generated when a current runs through the wire. When the device undergoes deformations, these inductive signals vary distinctively, reflecting the specific changes in the device's shape. These signals, specific to each type of deformation, are interpreted as interactive user input using a trained machine-learning classifier. We also showcase three example applications, including a deformable Christmas tree, a Snake game joystick controller, and a snowman-like light control.	https://dl.acm.org/doi/abs/10.1145/3641234.3671039	Hsuanling Lee, Liang He
3D-Layers: Bringing Layer-Based Color Editing to VR Painting	The ability to represent artworks as stacks of layers is fundamental to modern graphics design, as it allows artists to easily separate visual elements, edit them in isolation, and blend them to achieve rich visual effects. Despite their ubiquity in 2D painting software, layers have not yet made their way to VR painting, where users paint strokes directly in 3D space by gesturing a 6-degrees-of-freedom controller. But while the concept of a stack of 2D layers was inspired by real-world layers in cell animation, what should 3D layers be? We propose to define as groups of 3D strokes, and we distinguish the ones that represent 3D geometry from the ones that represent color modifications of the geometry. We call the former and the latter Strokes in appearance layers modify the color of the substrate strokes they intersect. Thanks to this distinction, artists can define sequences of color modifications as stacks of appearance layers, and edit each layer independently to finely control the final color of the substrate. We have integrated into a VR painting application and we evaluate its flexibility and expressiveness by conducting a usability study with experienced VR artists.	https://dl.acm.org/doi/abs/10.1145/3658183	Emilie Yu, Fanny Chevalier, Karan Singh, Adrien Bousseau
3DCrewCap: Applying 3D Volumetric video capture for XR helicopter rescue crew training and simulation.	Helicopter rescue crew training scenarios are complex and hard to simulate in game engines with animated 3D digital characters. This challenge is compounded when simulating Realtime photorealistic animated character sequences on XR-based Head-Mounted Displays (HMD). In this research, we present a practical early-stage development of 3D volumetric video capture and playback workflow for use in helicopter rescue crew training on XR HMDs. We break down the workflow of using Gaussian Splat approaches to construct keyframed 3D animated models of the rescue crew training actions. This novel application approach provides a practical example of the photorealistic capture and XR display of helicopter rescue crew performing training scenarios.	https://dl.acm.org/doi/abs/10.1145/3641234.3671037	John McGhee, Conan Bourke, Robert Lawther, Hao Zhou, Rolf Petersen
3Doodle: Compact Abstraction of Objects with 3D Strokes	While free-hand sketching has long served as an efficient representation to convey characteristics of an object, they are often subjective, deviating significantly from realistic representations. Moreover, sketches are not consistent for arbitrary viewpoints, making it hard to catch 3D shapes. We propose 3Dooole, generating descriptive and view-consistent sketch images given multi-view images of the target object. Our method is based on the idea that a set of 3D strokes can efficiently represent 3D structural information and render view-consistent 2D sketches. We express 2D sketches as a union of view-independent and view-dependent components. 3D cubic Bézier curves indicate view-independent 3D feature lines, while contours of superquadrics express a smooth outline of the volume of varying viewpoints. Our pipeline directly optimizes the parameters of 3D stroke primitives to minimize perceptual losses in a fully differentiable manner. The resulting sparse set of 3D strokes can be rendered as abstract sketches containing essential 3D characteristic shapes of various objects. We demonstrate that 3Doodle can faithfully express concepts of the original images compared with recent sketch generation approaches.	https://dl.acm.org/doi/abs/10.1145/3658156	Changwoon Choi, Jaeah Lee, Jaesik Park, Young Min Kim
4D-Rotor Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes	We consider the problem of novel-view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or generating high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DRotorGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes [Kerbl et al. 2023]. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DRotorGS demonstrates powerful capabilities for modeling complicated dynamics and fine details—especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DRotorGS, which consistently outperforms existing methods both quantitatively and qualitatively.	https://dl.acm.org/doi/abs/10.1145/3641519.3657463	Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen
A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose	Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate. Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation. In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses. Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world. During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images. We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection. We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization. These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods. We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information. Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset.	https://dl.acm.org/doi/abs/10.1145/3641519.3657427	Kaiwen Jiang, Yang Fu, Mukund Varma T, Yash Belhe, Xiaolong Wang, Hao Su, Ravi Ramamoorthi
A Differential Monte Carlo Solver For the Poisson Equation	The Poisson equation is an important partial differential equation (PDE) with numerous applications in physics, engineering, and computer graphics. Conventional solutions to the Poisson equation require discretizing the domain or its boundary, which can be very expensive for domains with detailed geometries. To overcome this challenge, a family of grid-free Monte Carlo solutions has recently been developed. By utilizing walk-on-sphere (WoS) processes, these techniques are capable of efficiently solving the Poisson equation over complex domains. In this paper, we introduce a general technique that differentiates solutions to the Poisson equation with Dirichlet boundary conditions. Specifically, we devise a new boundary-integral formulation for the derivatives with respect to arbitrary parameters including shapes of the domain. Further, we develop an efficient walk-on-spheres technique based on our new formulation—including a new approach to estimate normal derivatives of the solution field. We demonstrate the effectiveness of our technique over baseline methods using several synthetic examples.	https://dl.acm.org/doi/abs/10.1145/3641519.3657460	Zihan Yu, Lifan Wu, Zhiqian Zhou, Shuang Zhao
A Diffusion-Based Texturing Pipeline for Production-Grade Assets	We introduce an artist-centric Stable Diffusion pipeline, which takes production-grade mesh assets as input, and given text and image prompts generates texture maps instantaneously. While generative AI methods have recently enjoyed rapid growth, most existing works target a broad audience and are not directly usable in professional artist workflows, which require fast iteration time, precise editability, and compatibility with standard toolchains. We build a system that takes these requirements into consideration from the bottom up. Our pipeline allows manual overrides for maximal artist control and ultimately enables artists to rapidly iterate on their work without disruption to existing workflows.	https://dl.acm.org/doi/abs/10.1145/3641233.3664322	Winnie Lin, Dmitriy Smirnov, Richard Smith
A Dynamic Duo of Finite Elements and Material Points	This paper presents a novel method to couple Finite Element Methods (FEM), typically employed for modeling Lagrangian solids such as flesh, cloth, hair, and rigid bodies, with Material Point Methods (MPM), which are well-suited for simulating materials undergoing substantial deformation and topology change, including Newtonian/non-Newtonian fluid, granular materials, and fracturing materials. The challenge of coupling these diverse methods arises from their contrasting computational needs: implicit FEM integration is often favored to enjoy stability and large timesteps, while explicit MPM integration benefits from its allowance for efficient GPU optimization and flexibility of applying different plasticity models, which only allows for moderate timesteps. To bridge this gap, a mixed implicit-explicit time integration (IMEX) approach is proposed, utilizing principles from time splitting for partial differential equations and optimization-based time integrators. This method adopts incremental potential contact (IPC) to define a variational frictional contact model between the two materials, serving as the primary coupling mechanism. Our method enables implicit FEM and explicit MPM to coexist with significantly different timestep sizes while preserving two-way coupling. Experimental results demonstrate the potential of our method as a strong foundation for future exploration and enhancement in the field of multi-material simulation.	https://dl.acm.org/doi/abs/10.1145/3641519.3657449	Xuan Li, Minchen Li, Xuchen Han, Huamin Wang, Yin Yang, Chenfanfu Jiang
A Free-Space Diffraction BSDF	"Free-space diffractions are an optical phenomenon where light appears to ""bend"" around the geometric edges and corners of scene objects. In this paper we present an efficient method to simulate such effects. We derive an edge-based formulation of Fraunhofer diffraction, which is well suited to the common (triangular) geometric meshes used in computer graphics. Our method dynamically constructs a free-space diffraction BSDF by considering the geometry around the intersection point of a ray of light with an object, and we present an importance sampling strategy for these BSDFs. Our method is unique in requiring only ray tracing to produce free-space diffractions, works with general meshes, requires no geometry preprocessing, and is designed to work with path tracers with a linear rendering equation. We show that we are able to reproduce accurate diffraction lobes, and, in contrast to any existing method, are able to handle complex, real-world geometry. This work serves to connect free-space diffractions to the efficient path tracing tools from computer graphics."	https://dl.acm.org/doi/abs/10.1145/3658166	Shlomi Steinberg, Ravi Ramamoorthi, Benedikt Bitterli, Arshiya Mollazainali, Eugene D'Eon, Matt Pharr
A Fully-correlated Anisotropic Micrograin BSDF Model	We introduce an improved version of the micrograin BSDF model [Lucas et al. 2023] for the rendering of anisotropic porous layers. Our approach leverages the properties of micrograins to take into account the correlation between their height and normal, as well as the correlation between the light and view directions. This allows us to derive an exact analytical expression for the Geometrical Attenuation Factor (GAF), summarizing shadowing and masking inside the porous layer. This fully-correlated GAF is then used to define appropriate mixing weights to blend the BSDFs of the porous and base layers. Furthermore, by generalizing the micrograins shape to anisotropy, combined with their fully-correlated GAF, our improved BSDF model produces effects specific to porous layers such as retro-reflection visible on dust layers at grazing angles or height and color correlation that can be found on rusty materials. Finally, we demonstrate very close matches between our BSDF model and light transport simulations realized with explicit instances of micrograins, thus validating our model.	https://dl.acm.org/doi/abs/10.1145/3658224	Simon Lucas, Mickaël Ribardière, Romain Pacanowski, Pascal Barla
A Heat Method for Generalized Signed Distance	We introduce a method for approximating the signed distance function (SDF) of geometry corrupted by holes, noise, or self-intersections. The method implicitly defines a completed version of the shape, rather than explicitly repairing the given input. Our starting point is a modified version of the for geodesic distance, which diffuses normal vectors rather than a scalar distribution. This formulation provides robustness akin to , but provides distance function rather than just an inside/outside classification. Our formulation also offers several features not common to classic distance algorithms, such as the ability to simultaneously fit multiple level sets, a notion of distance for geometry that does not topologically bound any region, and the ability to mix and match signed and unsigned distance. The method can be applied in any dimension and to any spatial discretization, including triangle meshes, tet meshes, point clouds, polygonal meshes, voxelized surfaces, and regular grids. We evaluate the method on several challenging examples, implementing normal offsets and other morphological operations directly on imperfect curve and surface data. In many cases we also obtain an inside/outside classification dramatically more robust than the one obtained provided by GWN.	https://dl.acm.org/doi/abs/10.1145/3658220	Nicole Feng, Keenan Crane
A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets	Novel view synthesis has seen major advances in recent years, with 3D Gaussian splatting offering an excellent level of visual quality, fast training and real-time rendering. However, the resources needed for training and rendering inevitably limit the size of the captured scenes that can be represented with good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual quality for very large scenes, while offering an efficient Level-of-Detail (LOD) solution for efficient rendering of distant content with effective level selection and smooth transitions between levels. We introduce a divide-and-conquer approach that allows us to train very large scenes in independent chunks. We consolidate the chunks into a hierarchy that can be optimized to further improve visual quality of Gaussians merged into intermediate nodes. Very large captures typically have sparse coverage of the scene, presenting many challenges to the original 3D Gaussian splatting training method; we adapt and regularize training to account for these issues. We present a complete solution, that enables real-time rendering of very large scenes and can adapt to available resources thanks to our LOD method. We show results for captured scenes with up to tens of thousands of images with a simple and affordable rig, covering trajectories of up to several kilometers and lasting up to one hour.	https://dl.acm.org/doi/abs/10.1145/3658160	Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, George Drettakis
A Linear Method to Consistently Orient Normals of a 3D Point Cloud	Correctly and consistently orienting a set of normal vectors associated with a point cloud sampled from a surface in 3D is a difficult procedure necessary for further downstream processing of sampled 3D geometry, such as surface reconstruction and registration. It is difficult because correct orientation cannot be achieved without global considerations of the entire point cloud. We present an algorithm to orient a given set of normals of a 3D point cloud of size N, whose main computational component is the least-squares solution of an O(N) linear system, mostly sparse, derived from the classical Stokes' theorem. We show experimentally that our method can successfully orient sets of normals computed locally from point clouds containing a moderate amount of noise, representing also 3D surfaces with non-smooth features (such as corners and edges), in a fraction of the time required by state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3641519.3657429	Craig Gotsman, Kai Hormann
A Live Demo of Single-Photon Imaging and Applications	Single-photon sensors are a novel class of imaging sensors that are sensitive to the individual arrival of photons. In recent years, single-photon sensors have witnessed rapid growth in key sensor characteristics such as array resolution, which, for the first time, has made computer vision and imaging with these emerging sensors possible. This rapid growth has also spurred the development of algorithms that can harness the rich spatio-temporal scene information captured by single-photon sensors, which has led to several new imaging capabilities. These capabilities include imaging in challenging SNR conditions, high-dynamic range imaging, compensating extreme camera motion extents, and even providing the functionality of numerous imaging systems via post-capture processing. Our demo will showcase these exciting capabilities to a wide computer vision and graphics audience, and in doing so, make a case for the mainstream adoption of single-photon technology.	https://dl.acm.org/doi/abs/10.1145/3641517.3664397	Sacha Jungerman, Varun Sundar, Mohit Gupta
A Modernization of the DreamWorks Feather System	The DreamWorks Feather System is a toolset used to design feathered characters. It enables artists to manage all aspects of feathers from card layout through shot work by using intuitive tools and automated pipeline processes. While the previous system proved inaccessible for some due to its highly technical nature, these new developments have enabled dozens of artists to successfully groom characters and run their feathers in shots. An initial feather layout can now be created in a matter of minutes, with a suite of comprehensive tools allowing for further refinement and precise control over feather card layout, feather curves, and animated motion.	https://dl.acm.org/doi/abs/10.1145/3641233.3664332	William Sokoloski, Cory Sivek, Arunachalam Somasundaram, Lakshika Udakandage, Kurt Phillips, Alexa Mahajan, Michael Juarez, Charles Edward Billingsley
A Monocular Dialogue	"A Monocular Dialogue incarnates myths in the age of Information and Technology. It is an encounter with a single-eyed robot that stares at you and endlessly whispers its inner ruminations. Underneath this confluence of a Cyclops from Greek mythology and Artificial Intelligence (AI) from modern technology, lie intriguing, albeit metaphorical, similarities. This 3D printed Polyphemus, figuratively and literally, symbolizes their narrow and limited perspectives; suggests their creation from Gods and higher powers; alludes to the monumental tasks they perform and refers to their ambivalent brute force and potential destructiveness. Between irony and nostalgia, this eye-to-eye dialogue is staged as ""the AI is present"" and it embodies AI prowess into a mythical monstruous, yet seemingly inoffensive, figure."	https://dl.acm.org/doi/abs/10.1145/3641523.3665176	Louis-Philippe Demers
A Multi-modal Framework for 3D Facial Animation Control	3D facial reconstruction and animation has advanced significantly in the past decades. However, existing methods based on single-modal input struggle with specific facial part control and require post-processing for natural rendering. Recent approaches integrate audio and 2D estimation results to enhance facial animations, which improves the naturalness of head pose, eye and mouth animations. In this paper, We present a multi-modal 3D facial animation framework that uses video and audio input simultaneously. We experiment on different settings of motion control to produce natural and precise facial animation through user studies.	https://dl.acm.org/doi/abs/10.1145/3641234.3671035	Chong Cao, Qiuyang Xiao, Chengwei Shi
A Neural Network Model for Efficient Musculoskeletal-Driven Skin Deformation	We present a comprehensive neural network to model the deformation of human soft tissues including muscle, tendon, fat and skin. Our approach provides kinematic and active correctives to linear blend skinning [Magnenat-Thalmann et al. 1989] that enhance the realism of soft tissue deformation at modest computational cost. Our network accounts for deformations induced by changes in the underlying skeletal joint state as well as the active contractile state of relevant muscles. Training is done to approximate quasistatic equilibria produced from physics-based simulation of hyperelastic soft tissues in close contact. We use a layered approach to equilibrium data generation where deformation of muscle is computed first, followed by an inner skin/fascia layer, and lastly a fat layer between the fascia and outer skin. We show that a simple network model which decouples the dependence on skeletal kinematics and muscle activation state can produce compelling behaviors with modest training data burden. Active contraction of muscles is estimated using inverse dynamics where muscle moment arms are accurately predicted using the neural network to model kinematic musculotendon geometry. Results demonstrate the ability to accurately replicate compelling musculoskeletal and skin deformation behaviors over a representative range of motions, including the effects of added weights in body building motions.	https://dl.acm.org/doi/abs/10.1145/3658135	Yushan Han, Yizhou Chen, Carmichael Ong, Jingyu Chen, Jennifer Hicks, Joseph Teran
A New Kingdom: Weta FX Returns to The Planet of The Apes	Featuring dynamic interactions between apes and humans, a world reclaimed by nature, and large-scale combustion and water simulations, Kingdom of the Planet of The Apes is the result of several years of technical advancements. This paper will explore how artists brought the CG apes to life, and the myriads of landscapes that were built, dressed and eventually destroyed.	https://dl.acm.org/doi/abs/10.1145/3641233.3665344	Erik Winquist, Phillip Leonhardt, Paul Story, Alex Nowotny
A Pipeline for Effective and Extensible Stylization	"Inspired by turn-of-the-century watercolor illustrations, the art direction for Walt Disney Animation Studio's ""Wish"" called for a unique watercolor storybook style. Traditional CG renders were insufficient to judge the final look of the film without additional processing in departments upstream from lighting. In order to prevent this issue, we created a stylization pipeline to (1) automatically provide all upstream departments stylized renders representative of the final stylized look of the film, (2) provide the ability to iterate closely with lighting to when stylization changes were needed, and (3) provide a centralized area for lighting to manage sweeping stylization changes across the show."	https://dl.acm.org/doi/abs/10.1145/3641233.3664336	Harmony Li, Angela Mcbride, Sari Rodrig, Gregory Culp
A Position Based Material Point Method	The explicit Material Point Method (MPM) is an easily implemented scheme for the simulation of a wide variety of different physical materials. However, explicit integration has well known stability issues. We have implemented a novel semi-implicit compliant constraint formulation of MPM that is stable at any time-step while remaining as easy to implement as an explicit integrator. We call this method Position Based MPM (PB-MPM). This work significantly improves the utility of MPM for real-time applications.	https://dl.acm.org/doi/abs/10.1145/3641233.3664323	Christopher Lewin
A Procedural Production System for Autonomous Vehicle Simulation	Simulations provide the only safe and reliable way to test autonomous vehicle systems in situations that may be unusual or undesirable to witness on the road. In order to make simulation at scale possible and its accuracy sufficient, production systems must be built which adapt state-of-the-art graphics production practices to this new field. In this work we discuss the procedurally based system developed at Aurora, which forms a key part of the company's approach to testing and validation.	https://dl.acm.org/doi/abs/10.1145/3641233.3664317	Vincent Serritella, Viktor Lundqvist, Matt Webb, Taylor Shaw, Riley Niu, Zach Repasky, Robert Graf, Magnus Wrenninge
A Real-time Visualization System of Pseudo-muscle Activity Using a VR Device	The goal of this research is to provide a real-time display of muscle activity in response to human motion input. We developed a system for real-time visualization of pseudo-muscle activity using a VR device. The user controls a CG avatar in VR space by inputting a body motion with the VR device. We constructed simplified models to output muscle activity corresponding to the posture of a CG avatar based on muscle-analysis data estimated from motion data. This system allows users to visualize the position of active muscles using a CG avatar in a VR space. This system can present different muscle activities depending on the input of elbow flexion/extension movement and the state of the upper limb at that time. The position of active muscles and the magnitude of their activity are represented by the color and expansion of the muscles on a CG avatar. By assigning controller input to other parts of the body rather than just the hands, users can move those parts in virtual space and visualize muscle activity.	https://dl.acm.org/doi/abs/10.1145/3641234.3671040	Asako Soga, Taichi Yano, Kunihiko Oda
A Realistic Multi-scale Surface-based Cloth Appearance Model	Surface-based cloth appearance models have been rapidly advancing, shifting from detail-less BRDFs to modern per-point shading models with accurate spatially-varying reflection, transmission, and so on. However, the increased complexity has brought about realism-performance trade-offs: from closeup, rendered cloth can be highly inaccurate due to the missing, unaffordable parallax effects; from far away, significant amount of noise will show up since every point can be shaded differently inside a pixel's footprint. In this paper, we aim at eliminating the trade-off with a realistic multi-scale surface-based cloth appearance model. We propose a comprehensive micro-scale model focusing on correct parallax effects, and a practical meso-scale integration scheme, emphasizing efficiency while losslessly preserving accurate highlights and self-shadowing. We further improve its performance using our novel Clustered Control Variates (CCV) and Summed-Area Table (SAT) integration scheme, and its practicality using an efficient Clustered Principal Component Analysis (C-PCA) compression method. As a result, our multi-scale model achieves a 30 × acceleration compared to the state-of-the-art, is able to represent a variety of realistic cloth appearance, and can be potentially applied in real-time applications.	https://dl.acm.org/doi/abs/10.1145/3641519.3657426	Junqiu Zhu, Christophe Hery, Lukas Bode, Carlos Aliaga, Adrian Jarabo, Ling-Qi Yan, Matt Jen-Yuan Chiang
A Resampled Tree for Many Lights Rendering	We propose a new hybrid method for efficiently sampling many lights on a scene that combines a simplified spatial tree with a resampling stage. Building on previous methods that work with a split or cut of the light tree, we introduce the idea of probabilistic splitting to eliminate noise boundaries. This yields a subset of lights that is then reduced to a smaller and bounded set for full light/BSDF evaluation and resampling. Our main contribution is the stochastic splitting formulation combined with a reservoir set technique which limits samples to an arbitrary number to avoid variable size collections.	https://dl.acm.org/doi/abs/10.1145/3641233.3664352	Alejandro Conty, Pascal Lecocq, Chris Hellmuth
A Study on Tactile Illusions of Temperature and Touch Through Virtual Hand Illusion	"Recently, a wide variety of VR content has been provided, and the development of devices that approach the sense of touch is attracting attention. We investigated the sensory experience of ""temperature,"" ""smoothness,"" and ""hardness"" in virtual space. Unlike previous studies that perform haptic feedback using a glove-type device, we conducted a tactile experiment without using a device by inducing a tactile illusion from visual information using a virtual hand illusion. The results suggests that visual information can evoke a tactile illusion of ""temperature"" and ""smoothness."""	https://dl.acm.org/doi/abs/10.1145/3641234.3671089	Shosuke Iida, Tomokazu Ishikawa
A Surface-based Appearance Model for Pennaceous Feathers	The appearance of a real-world feather is the result of a complex light interaction with its multi-scale biological structure including the central shaft, branching barbs and interlocking barbules on those barbs. In this work, we propose a practical appearance model for feathers encoded as 2D textures where the overall appearance is a weighted BSDF of the implicit representations of the key biological structures. This BSDF can be applied to any surface and does not require the explicit geometrical modeling of the internal microstructures (barbs and barbules) as in previous works. Our model accounts for the particular characteristics of feather fibers such as the non-cylindrical cross-sections of barbules and the hierarchical cylindrical cross-sections of barbs. To model the relative visibility between barbs and barbules, we derive a masking term for the differential projected areas of the different components of the feather's microgeometry, which allows us to analytically compute the masking between barbs and barbules without costly Monte Carlo integration.	https://dl.acm.org/doi/abs/10.1145/3641234.3671065	Juan Raul Padron Griffe, Dario Lanza, Adrian Jarabo, Adolfo Mu&#241;oz
A Unified Differentiable Boolean Operator with Fuzzy Logic	This paper presents a unified differentiable boolean operator for implicit solid shape modeling using Constructive Solid Geometry (CSG). Traditional CSG relies on min, max operators to perform boolean operations on implicit shapes. But because these boolean operators are discontinuous and discrete in the choice of operations, this makes optimization over the CSG representation challenging. Drawing inspiration from fuzzy logic, we present a unified boolean operator that outputs a continuous function and is differentiable with respect to operator types. This enables optimization of both the primitives and the boolean operations employed in CSG with continuous optimization techniques, such as gradient descent. We further demonstrate that such a continuous boolean operator allows the modeling of both sharp mechanical objects and smooth organic shapes with the same framework. Our proposed boolean operator opens up new possibilities for future research toward fully continuous CSG optimization.	https://dl.acm.org/doi/abs/10.1145/3641519.3657484	Hsueh-Ti Derek Liu, Maneesh Agrawala, Cem Yuksel, Tim Omernick, Vinith Misra, Stefano Corazza, Morgan Mcguire, Victor Zordan
A Vocal Landscape	A Vocal Landscape is a hyperrealistic VR film that explores the strange anatomy of a conversation between two people. Through a dreamlike journey, you travel to an ever-changing room where hyperrealistic visuals highlight the unspoken associations each make: opening a fresh way of looking at the uneven paths of communication.	https://dl.acm.org/doi/abs/10.1145/3641231.3648630	Anne Jeppesen, Omid Zarei
A Vortex Particle-on-Mesh Method for Soap Film Simulation	This paper introduces a novel physically-based vortex fluid model for films, aimed at accurately simulating cascading vortical structures on deforming thin films. Central to our approach is a novel mechanism decomposing the film's tangential velocity into circulation and dilatation components. These components are then evolved using a hybrid particle-mesh method, enabling the effective reconstruction of three-dimensional tangential velocities and seamlessly integrating surfactant and thickness dynamics into a unified framework. By coupling with its normal component and surface-tension model, our method is particularly adept at depicting complex interactions between in-plane vortices and out-of-plane physical phenomena, such as gravity, surfactant dynamics, and solid boundary, leading to highly realistic simulations of complex thin-film dynamics, achieving an unprecedented level of vortical details and physical realism.	https://dl.acm.org/doi/abs/10.1145/3658165	Ningxiao Tao, Liangwang Ruan, Yitong Deng, Bo Zhu, Bin Wang, Baoquan Chen
AI in mixed reality - Copilot on HoloLens: Spatial computing with large language models	Mixed reality together with AI presents a human-first interface that promises to transform operations. Copilot can assist industrial workers in real-time with speech and holograms; generative AI is used to search technical documentation, service records, training content, and other sources. Copilot then summarizes to provide interactive guidance.	https://dl.acm.org/doi/abs/10.1145/3641520.3665305	Andy Klein, Ethan Arnowitz
AI3D Desktop: Creating 3D using AI in Skeuomorphic XR and The Future of Work	AI3D Desktop is an XR app platform that lets anyone create 3D models using AI in an intuitive and delightful way by using skeuomorphic design motifs. Innovations include: Creating virtual humans and other AI3D objects using a hybrid desktop draft-to-reticle approach, AI3D user-created-content e-commerce as a virtual catalog, and aiQuery and sQuery language layers in RealityScript to easily chain AI models and spatial computing HCI elements together (also, as represented as gears and cogs!). We showcase such approaches for all AI3D methods: text to 3D, text to image to 3D, image to 3D, 3D to 3D - and also multimodal AI3D methods for text/image/3D to scene, and text/image/3D to interactive 3D. The current implementation is for Apple Vision Pro.	https://dl.acm.org/doi/abs/10.1145/3664294.3664363	Yosun Chang
AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion	Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering–based surface reconstruction methods.	https://dl.acm.org/doi/abs/10.1145/3641519.3657446	Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya Pediredla, Christopher A Metzler
Accelerating Saccadic Response through Spatial and Temporal Cross-Modal Misalignments	Human senses and perception are our mechanisms to explore the external world. In this context, visual saccades –rapid and coordinated eye movements– serve as a primary tool for awareness of our surroundings. Typically, our perception is not limited to visual stimuli alone but is enriched by cross-modal interactions, such as the combination of sight and hearing. In this work, we investigate the temporal and spatial relationship of these interactions, focusing on how auditory cues that precede visual stimuli influence saccadic latency –the time that it takes for the eyes to react and start moving towards a visual target. Our research, conducted within a virtual reality environment, reveals that auditory cues preceding visual information can significantly accelerate saccadic responses, but this effect plateaus beyond certain temporal thresholds. Additionally, while the spatial positioning of visual stimuli influences the speed of these eye movements, as reported in previous research, we find that the location of auditory cues with respect to their corresponding visual stimulus does not have a comparable effect. To validate our findings, we implement two practical applications: first, a basketball training task set in a more realistic environment with complex audiovisual signals, and second, an interactive farm game that explores previously untested values of our key factors. Lastly, we discuss various potential applications where our model could be beneficial.	https://dl.acm.org/doi/abs/10.1145/3641519.3657432	Daniel Jiménez Navarro, Xi Peng, Yunxiang Zhang, Karol Myszkowski, Hans-Peter Seidel, Qi Sun, Ana Serrano
Acoustic Garden: Exploring Accessibility and Interactive Music with Distance-related Audio Effect Modulation in XR	In AR/XR design, spatial audio and audio-driven narratives are generally considered as secondary to visual interface and story-telling. Non-visual content production and research for visually impaired users are underrepresented. Similarly, in the field of music, visual interfaces dominate discussions on gesture-based sound synthesis, leaving intuitive, audience-driven experiences unexplored. Historically, studies in architecture and music focused on spatial and musical sequences, but they largely remained confined to print media representations. In this project, inspired by the progressive structure of electronic music, we introduce a spatialized sound synthesis method based on distance-related audio effect modulation combined with binaural spatialization. This approach is intended to navigate users through space without depending on visual indicators, utilizing auditory cues from a multitude of virtual audio objects instead. These objects are responsive to user movements, providing dynamic and immersive musical experiences on a walking scale. Interaction with specific audio objects enables users to dictate different musical progressions, which follows a self-similar spatial structure. Unlike traditional sonification methods that translate data into plain audio, our approach emphasizes the emotional impact of auditory messages. It unveils the potential for audience-involved, spatially-driven musical narratives. Testing across various platforms, we faced challenges in sound design, hardware limitations, and cognitive processing.	https://dl.acm.org/doi/abs/10.1145/3641521.3664402	Yufan Xie, Lisa Little, Melvin R Lewis, Wei Wu, Daniel Kish
Adaptive Sampling for Monte-Carlo Event Imagery Rendering	This paper presents a novel event-based camera simulation system based on physically accurate Monte Carlo path tracing with adaptive path sampling. The adaptive sampling performed in the proposed method is based on the probability of event occurrence. First, our rendering system collects logarithmic luminances rather than raw luminance based on the circuit characteristics of event-based cameras. We calculate the probability of how much different the logarithmic luminance gap is from the preset event threshold. This means how likely an event will occur at the pixel. Then, we sample paths adaptively based on the sample rate combined with a previous adaptive sampling method. We demonstrate that our method achieves higher rendering quality than the baseline approach which allocates path samples uniformly for each pixel.	https://dl.acm.org/doi/abs/10.1145/3641234.3671028	Yuichiro Manabe, Tatsuya Yatagawa, Shigeo Morishima, Hiroyuki Kubo
Adaptive grid generation for discretizing implicit complexes	We present a method for generating a simplicial (e.g., triangular or tetrahedral) grid to enable adaptive discretization of implicit shapes defined by a vector function. Such shapes, which we call implicit complexes, are generalizations of implicit surfaces and useful for representing non-smooth and non-manifold structures. While adaptive grid generation has been extensively studied for polygonizing implicit surfaces, few methods are designed for implicit complexes. Our method can generate adaptive grids for several implicit complexes, including arrangements of implicit surfaces, CSG shapes, material interfaces, and curve networks. Importantly, our method adapts the grid to the geometry of not only the implicit surfaces but also their lower-dimensional intersections. We demonstrate how our method enables efficient and detail-preserving discretization of non-trivial implicit shapes.	https://dl.acm.org/doi/abs/10.1145/3658215	Yiwen Ju, Xingyi Du, Qingnan Zhou, Nathan Carr, Tao Ju
Adjacencies	"""Adjacencies"" (2022 — ongoing) is an exercise in distance collaboration and creative exchange between artists and animation classes at Carnegie Mellon University and Yale School of Art. The exchange is modeled upon the principles of cadavre exquis, wherein participants contribute to a process of exchange that results in a collective assembly of animation. This project leverages the affordances of distance learning platforms, file sharing, and thematic prompts as organizing structures to guide student participation, collaboration, and learning across groups of artists located in physically remote locations."	https://dl.acm.org/doi/abs/10.1145/3641235.3664441	Johannes Deyoung, Mike Rader
Advanced Practices in Optimizing Motion Capture Pipelines for Games and VFX	Covering the critical stages of subject labeling, data capture, post-processing and final retargeting onto custom character models, this course provides best practices and advanced techniques for streamlining motion capture pipelines. Vicon presents the latest capabilities culminating from 30+ years of development, including evolving use cases such as markerless and multi-participant. Motion capture is an expansive, complex discipline with many different ways of doing things. This course is unique in that it is a first-hand explanation of the technology and best practices to follow when using motion capture for games and VFX, from the creators themselves. The course will cover everything from key considerations when planning the layout and topology of a motion capture volume, to perfecting captured performer data ready for packaging into a gaming experience, to integrating the latest features and capabilities available.	https://dl.acm.org/doi/abs/10.1145/3664475.3674998	David 'Ed' Edwards, Garry Gray
Advances in Real Time Audio Rendering - Part 1	Hearing is the most time-sensitive of the human senses. The technology underlying real-time audio rendering must provide control over our physical, perceptual, cultural, and aesthetic worlds within the tightest of deadlines and with perfect temporal coherence. This course offers an introduction to state-of-the-art real-time audio rendering technology. We dive into the core concepts and challenges that define the problem space and touch on similarities shared by real-time graphic rendering and non-real-time audio rendering.	https://dl.acm.org/doi/abs/10.1145/3664475.3664731	Aaron McLeran
Advances in Real Time Audio Rendering - Part 2	"We originally referred to ""real-time"" audio systems to draw a distinction with ""non-real-time"" systems where a series of audio samples is entirely determined and computed in advance, originally because computers were not fast enough to perform the needed mathematical calculations. If you can't listen to the sound as it is produced, you won't be able to change it live and know what you're changing. Thus, the desire to turn a computer into something more like a ""musical instrument"" was a primary motivation in the development of real-time audio systems."	https://dl.acm.org/doi/abs/10.1145/3664475.3664734	David Zicarelli
After Grandpa	Haunted by the terrifying ghost of his Grandpa, Loup, a little boy obsessed with insects, will have to outcome his fear to discover why it came back.	https://dl.acm.org/doi/abs/10.1145/3641230.3652581	Juliette Michel, Swann Valenza, Florian Gomes Freitas, Axel Sense, Victoria Leviaux
Alba: A Multimodal Rendering System for Autonomous Vehicle Simulation	Physically based rendering and path tracing have become the norm in visual effects and animation thanks to the level of realism that they offer. On top of these foundations, modern rendering systems implement layers of specializations that target the use case of images for human consumption. In this paper, we discuss Alba, a multimodal rendering framework for camera, lidar and radar, which targets the use case of autonomous vehicle simulation. We discuss its architecture and the different design choices made to optimize for accuracy, scale, and machine consumption.	https://dl.acm.org/doi/abs/10.1145/3641233.3664330	Ryusuke Villemin, Magnus Wrenninge, Steve Capell, Anton Gribovskiy, I-Chen Jwo, Steve Bako
Algorithmic Amplification	Algorithmic Amplifications leverages the power of Large Language Models (LLMs) and a custom instant voice cloning pipeline to create a tangible representation of digital echo chambers, underscoring how language shapes our perceptions and reinforces our beliefs. It prompts a critical examination of the impact of AI on our linguistic and cultural landscapes, spotlighting the paradox of technological advancement: while LLMs mark a significant leap in AI, they also mirror and potentially amplify societal biases.	https://dl.acm.org/doi/abs/10.1145/3641523.3665172	Theodoros Papatheodorou, Jessica Wolpert
Alignment conditions for NURBS-based design of mixed tension-compression grid shells	"In architecture, shapes of surfaces that can withstand gravity with no bending action are considered ideal for Those shells have special geometries through which they can stream gravitational force toward the ground via stresses strictly tangent to the surface, making them highly efficient. The process of finding these special forms is called Recently, [Miki and Mitchell 2022] presented a method to reliably produce mixed tension-compression continuum shells, a type of shells known to be especially difficult to form-find. The key to this method was to use the concept of the Airy stress function to derive a valid bending-free shell shape by iterating on both the shell shape and the Airy stress function; this turns a problem that is over-constrained in general into a problem with many solutions. In [Miki and Mitchell 2022], it was proposed that the method could also be used to design grid shells by tracing curves on a continuum shell such that the resulting grid has bars that are both bending-free and form flat panels, a property useful for construction of real grid shells made of glass and steel. However, this special type of grid is guaranteed to exist in general on a mixed-tension compression shell, even when the shell is in bending-free equilibrium [Miki and Mitchell 2023]. Additional conditions must be imposed on the shell shape to guarantee the existence of simultaneously bending-free and conjugate grid directions. The current study resolves the existence issue by adding We consider several practical curve alignment conditions: alignment with the lines of curvature of the shell, approximate alignment with a bidirectional set of user-prescribed guide curves, and exact alignment with a single direction of user-prescribed guide curves. We report that the variable projection method originally used to solve the form-finding problem in the work of [Miki and Mitchell 2022] can be successfully extended to solve the newly introduced alignment conditions, and conclude with results for several practical design examples. To our knowledge, this is the first method that can take a user-input grid and find a ""nearby"" grid that is both flat-panelled and in bending-free equilibrium for the general case of mixed tension-compression grid shells."	https://dl.acm.org/doi/abs/10.1145/3658142	Masaaki Miki, Toby Mitchell
Amplify AR HUD User-Experience with Real-World Sunlight Simulation in Virtual Scene	In this paper, we introduce the concept of real-world sunlight simulation in virtual scenes of Head-Up Display (HUD) application. We have adopted solar position calculation logic [Zhang, 2020] and demonstrated its capability with experiments ran on real vehicle (Figure 1). In addition, we suggest UX approaches featuring the interaction between virtual 3D objects and the simulated virtual sunlight for expanded AR experiences in HUD application.	https://dl.acm.org/doi/abs/10.1145/3641234.3671043	Eun Hye Kim, Hyocheol Ro, Hyunjin Park
An Artist-Friendly Method for Procedural Skin Generation and Visualisation in Houdini	Realistic-looking digital humans in visual effects are a crucial aspect of creating a believable experience for the viewer. In order to create a convincing result, a lot of effort is put into the details. In this talk, we look specifically into the details of the human skin and how we implemented a fully procedural approach to generate and visualise skin textures. The tool is integrated into our VFX-pipeline as a SOP (surface operator) in Houdini and provides an artist-friendly interface with many ways to modify and tweak the output appearance. Additionally, we provide a way to utilise the underlying animation to produce pores and wrinkles dynamically driven by stretch and compression in the current frame.	https://dl.acm.org/doi/abs/10.1145/3641233.3665166	Josefine Klintberg, Rasmus Haapaoja
An Induce-on-Boundary Magnetostatic Solver for Grid-Based Ferrofluids	This paper introduces a novel Induce-on-Boundary (IoB) solver designed to address the magnetostatic governing equations of ferrofluids. The IoB solver is based on a single-layer potential and utilizes only the surface point cloud of the object, offering a lightweight, fast, and accurate solution for calculating magnetic fields. Compared to existing methods, it eliminates the need for complex linear system solvers and maintains minimal computational complexities. Moreover, it can be seamlessly integrated into conventional fluid simulators without compromising boundary conditions. Through extensive theoretical analysis and experiments, we validate both the convergence and scalability of the IoB solver, achieving state-of-the-art performance. Additionally, a straightforward coupling approach is proposed and executed to showcase the solver's effectiveness when integrated into a grid-based fluid simulation pipeline, allowing for realistic simulations of representative ferrofluid instabilities.	https://dl.acm.org/doi/abs/10.1145/3658124	Xingyu Ni, Ruicheng Wang, Bin Wang, Baoquan Chen
An Introduction to Creating Real-Time Interactive Computer Graphics Applications	The emphasis of this course focuses on the capabilities of the WebGL application programming interface (i.e., programming library, often called an API).	https://dl.acm.org/doi/abs/10.1145/3664475.3664545	Dave Shreiner, Edward Angel
An Introduction to Quantum Computing	"Computer graphics relies on computing hardware for everything from animation to rendering. An emerging new technology exploits the properties of quantum objects to offer radically new ways to think about, create, and run algorithms. For example, quantum computers can evaluate many different input values simultaneously, where ""many"" can be larger than the number of atoms in the visible universe. The catch is that only one output can be obtained from each run. Quantum computing may change how we think about computer graphics. For example, future quantum computers may be able to intersect astronomical numbers of rays with a similarly massive database of objects in a single execution, or evaluate enormous numbers of simulation parameters in parallel to return the one set that produces a desired result. This course describes, without advanced math, the core ideas of quantum computing. We start with the quantum version of the classical bit (called a qubit) and the basic operators that modify qubits. We introduce the four essential properties that distinguish quantum computers from familiar classical computers: superposition, interference, entanglement, and measurement. We show how these properties are orchestrated to build quantum algorithms. We conclude with a brief overview of some of the most well-known quantum algorithms, and some of their possible applications in computer graphics. Quantum computers are already here, and are increasing in size and reliability at a rapid pace. Open-source simulators abound, and small quantum computers are even available for free use on the web. Classical hardware has served computer graphics well. Quantum computing offers a fundamentally new way to design and execute algorithms, which could change our field in fundamental ways. Now is the perfect time to starting thinking about quantum computing for graphics."	https://dl.acm.org/doi/abs/10.1145/3664475.3664536	Andrew Glassner
An Introduction to the Fourier Transform	The Fourier Transform is a fundamental tool in computer graphics. It explains where aliasing comes from, how to filter textures, why noise can be a good thing, how to simplify shapes, how to select sampling patterns, how the JPEG image compression scheme works, and why wagon wheels start to rotate backwards when they spin too fast. The Fourier Transform not only describes the source of many problems in graphics, it also often tells us how to avoid or suppress them. Understanding the Fourier transform gives us the ability to identify, diagnose, and fix objectionable artifacts that might otherwise remain mysterious in rendering, modeling, and animation code. Unfortunately, the Fourier Transform is unfamiliar to many people, and opaque to many others, often because they are put off by its technical language and complicated looking mathematics. Though the notation can be daunting at first contact, it's really just a terse way of expressing specific sequences of multiplications and additions. In this course we only assume you know a little vector algebra, like the material in any graphics book (if you remember how to multiply two matrices, you're all set). We'll carefully build up to the full Discrete-Time Fourier Transform (and its inverse) that we use every day, taking small steps and illustrating the process with pictures.	https://dl.acm.org/doi/abs/10.1145/3664475.3664537	Andrew Glassner
An OpenUSD Production Pipeline with Very Little Coding: Empowering 3D artists with a parallel workflow using off-the-shelf software	We present a streamlined animation production pipeline leveraging OpenUSD and Houdini's procedural strengths to enable collaborative, parallel workflows. Our approach minimizes coding requirements, empowering artists to iterate simultaneously on shared assets and shots for real-time project visualization. Developed and proven in an academic setting, this pipeline demonstrates adaptability and scalability for small studio environments, successfully fostering iterative workflows among users.	https://dl.acm.org/doi/abs/10.1145/3641233.3664307	Francois Lord
Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model	Visual In-Context Learning (ICL) has emerged as a promising research area due to its capability to accomplish various tasks with limited example pairs through analogical reasoning. However, training-based visual ICL has limitations in its ability to generalize to unseen tasks and requires the collection of a diverse task dataset. On the other hand, existing methods in the inference-based visual ICL category solely rely on textual prompts, which fail to capture fine-grained contextual information from given examples and can be time-consuming when converting from images to text prompts. To address these challenges, we propose Analogist, a novel inference-based visual ICL approach that exploits both visual and textual prompting techniques using a text-to-image diffusion model pretrained for image inpainting. For visual prompting, we propose a self-attention cloning (SAC) method to guide the fine-grained structural-level analogy between image examples. For textual prompting, we leverage GPT-4V's visual reasoning capability to efficiently generate text prompts and introduce a cross-attention masking (CAM) operation to enhance the accuracy of semantic-level analogy guided by text prompts. Our method is out-of-the-box and does not require fine-tuning or optimization. It is also generic and flexible, enabling a wide range of visual tasks to be performed in an in-context manner. Extensive experiments demonstrate the superiority of our method over existing approaches, both qualitatively and quantitatively. Our project webpage is available at https://analogist2d.github.io.	https://dl.acm.org/doi/abs/10.1145/3658136	Zheng Gu, Shiyuan Yang, Jing Liao, Jing Huo, Yang Gao
Animated Ink Bleeding with Computational Fluid Dynamics	This work presents an advanced computational approach for simulating the traditional ink-on-wet paper art technique, focusing on the dynamic interplay between ink, water, and paper using the Lattice Boltzmann Method (LBM). We model ink motion using the advection-diffusion equation coupled with Navier-Stokes equations in a porous medium. We explore the impact of the background material's permeability on the realism of digital ink simulations. Our results highlight this method's potential in digital art, offering artists and animators a novel tool for creating realistic ink effects.	https://dl.acm.org/doi/abs/10.1145/3641234.3671060	Grzegorz Gruszczynski, Mateusz Tokarz, Przemyslaw Musialski
Aperture-Aware Lens Design	Optics designers use simulation tools to assist them in designing lenses for various applications. Commercial tools rely on finite differencing and sampling methods to perform gradient-based optimization of lens design objectives. Recently, differentiable rendering techniques have enabled more efficient gradient calculation of these objectives. However, these techniques are unable to optimize for light throughput, often an important metric for many applications. We develop a method for calculating the gradients of optical systems with respect to both focus and light throughput. We formulate lens performance as an integral loss over a dynamic domain, which allows for the use of differentiable rendering techniques to calculate the required gradients. We also develop a ray tracer specifically designed for refractive lenses and derive formulas for calculating gradients that simultaneously optimize for focus and light throughput. Explicitly optimizing for light throughput produces lenses that outperform traditional optimized lenses that tend to prioritize for only focus. To evaluate our lens designs, we simulate various applications where our lenses:	https://dl.acm.org/doi/abs/10.1145/3641519.3657398	Arjun Teh, Ioannis Gkioulekas, Matthew O'Toole
Area ReSTIR: Resampling for Real-Time Defocus and Antialiasing	Recent advancements in spatiotemporal reservoir resampling (ReSTIR) leverage sample reuse from neighbors to efficiently evaluate the path integral. Like rasterization, ReSTIR methods implicitly assume a pinhole camera and evaluate the light arriving at a pixel through a single predetermined subpixel location at a time (e.g., the pixel center). This prevents efficient path reuse in and near pixels with high-frequency details. We introduce , extending ReSTIR reservoirs to also integrate each pixel's 4D ray space, including 2D areas on the film and lens. We design novel subpixel-tracking temporal reuse and shift mappings that maximize resampling quality in such regions. This robustifies ReSTIR against high-frequency content, letting us importance sample subpixel and lens coordinates and efficiently render antialiasing and depth of field.	https://dl.acm.org/doi/abs/10.1145/3658210	Song Zhang, Daqi Lin, Markus Kettunen, Cem Yuksel, Chris Wyman
Art Directable Underwater Explosion Simulation	We present a technique for simulating underwater explosions using an animated volume control method that allows us to visually approximate the expansion and contraction of underwater explosions measured in existing literature. The foundation of this technique is a FLIP/APIC bubble simulation coupled with a surrounding sparsely allocated volumetric water field in a multi-phase solve. We achieve the desired compression and expansion effects by animating the target bubbles volume via adjusting the equilibrium FLIP particle counts per voxel. Adjusting bubble density with volume and adding surface tension improves the match to real world references. Because our method can be animated to any timing desired by the artist, it is more practical for achieving art-direction.	https://dl.acm.org/doi/abs/10.1145/3641233.3664314	Sean Flynn, Alexey Stomakhin, Joel Wretborn, Daniel White
Art Directing for ICVFX: A Look at Star Wars, Antman and the Wasp: Quantumania, and More	Narwhal Studios' presentation provides a comprehensive exploration of the art of directing for virtual production and ICVFX, featuring insights into both AAA productions and smaller projects. Led by Narwhal's CEO and Creative Director, Felix Jorge, along with Studio Art Director Safari Sosebee and Sr. Real-time Artist Dallas Drapeau, this discussion promises valuable perspectives.	https://dl.acm.org/doi/abs/10.1145/3641232.3649341	Felix Jorge, Safari Sosebee
Art-Directing Asha's Braids in Disney's Wish	"For Walt Disney Animation Studios' 100th anniversary feature, the filmmakers wanted to honor the studio's legacy with a stylized look that draws from the rich artistic heritage of Disney's earliest films. In addition to the challenges of a highly art-directed stylized look, Asha's hairstyle comprises a full head of long, thin, tightly-braided locks which was far more complex than our previous braid grooms. In order to art direct Asha's stylized hair performance in ""Wish"", several advancements were made in tools and workflows across the character departments for grooming, simulation, and stylization techniques."	https://dl.acm.org/doi/abs/10.1145/3641233.3664338	Avneet Kaur, Jennifer Stratton, David Hutchins, Nikki Mull
Astra	A Mixed Reality Experience that transports you from Earth to the deepest corners of the cosmos as you embark on a quest to uncover the key ingredients of life in the Universe. Step foot on planets and their dark moons in a search for future worlds and far away beings.	https://dl.acm.org/doi/abs/10.1145/3641231.3649151	Aurélie Leduc
Atmospheric Carbon Dioxide Tagged by Source	Carbon dioxide (CO2) is the most prevalent greenhouse gas driving global climate change. However, its increase in the atmosphere would be even more rapid without land and ocean carbon sinks, which collectively absorb about half of human emissions every year. Advanced computer modeling techniques in NASA's Global Modeling and Assimilation Office allow us to disentangle the influences of sources and sinks and to better understand where carbon is coming from and going to.	https://dl.acm.org/doi/abs/10.1145/3641230.3653486	A. J. Christensen, Greg Shirah, Helen-Nicole Kostis, Anansa B. Keaton-Ashanti, Mark Subbarao, Brenda Lopez-Silva, Lesley Ott
Atomic Chicken	A chicken coop set up at the foot of a nuclear power plant sees its daily life turned upside down by a series of comic, cartoonstyle mutations.	https://dl.acm.org/doi/abs/10.1145/3641230.3649097	Thibault Ermeneux, Lucie Lyfoung, Solène Polet, Capucine Prat, Morgane Siriex, Anna Uglova
Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture	In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection. It holds significant implications and guidance for string instrument pedagogy, animation, and virtual concerts, as well as for both musical performance analysis and generation. Our code and SPD dataset are available at https://github.com/Yitongishere/string_performance.	https://dl.acm.org/doi/abs/10.1145/3658235	Yitong Jin, Zhiping Qiu, Yi Shi, Shuangpeng Sun, Chongwu Wang, Donghao Pan, Jiachen Zhao, Zhenghao Liang, Yuan Wang, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai
Audio2Rig: Artist-oriented deep learning tool for facial and lip sync animation	Creating realistic or stylized facial and lip sync animation is a tedious task. It requires lot of time and skills to sync the lips with audio and convey the right emotion to the character's face. To allow animators to spend more time on the artistic and creative part of the animation, we present Audio2Rig: a new deep learning based tool leveraging previously animated sequences of a show, to generate facial and lip sync rig animation from an audio file. Based in Maya, it learns from any production rig without any adjustment and generates high quality and stylized animations which mimic the style of the show. Audio2Rig fits in the animator workflow: since it generates keys on the rig controllers, the animation can be easily retaken. The method is based on 3 neural network modules which can learn an arbitrary number of controllers. Hence, different configurations can be created for specific parts of the face (such as the tongue, lips or eyes). With Audio2Rig, animators can also pick different emotions and adjust their intensities to experiment or customize the output, and have high level controls on the keyframes setting. Our method shows excellent results, generating fine animation details while respecting the show style. Finally, as the training relies on the studio data and is done internally, it ensures data privacy and prevents from copyright infringement.	https://dl.acm.org/doi/abs/10.1145/3641233.3664312	Bastien Arcelin, Nicolas Chaverou
Automatic Digital Garment Initialization from Sewing Patterns	The rapid advancement of digital fashion and generative AI technology calls for an automated approach to transform digital sewing patterns into well-fitted garments on human avatars. When given a sewing pattern with its associated sewing relationships, the primary challenge is to establish an initial arrangement of sewing pieces that is free from folding and intersections. This setup enables a physics-based simulator to seamlessly stitch them into a digital garment, avoiding undesirable local minima. To achieve this, we harness AI classification, heuristics, and numerical optimization. This has led to the development of an innovative hybrid system that minimizes the need for user intervention in the initialization of garment pieces. The seeding process of our system involves the training of a classification network for selecting seed pieces, followed by solving an optimization problem to determine their positions and shapes. Subsequently, an iterative selection-arrangement procedure automates the selection of pattern pieces and employs a phased initialization approach to mitigate local minima associated with numerical optimization. Our experiments confirm the reliability, efficiency, and scalability of our system when handling intricate garments with multiple layers and numerous pieces. According to our findings, 68 percent of garments can be initialized with zero user intervention, while the remaining garments can be easily corrected through user operations.	https://dl.acm.org/doi/abs/10.1145/3658128	Chen Liu, Weiwei Xu, Yin Yang, Huamin Wang
BNNAction-Net: Binary Neural Network on Hands Gesture Recognitions	With the rise of wearable technology and real-time gesture recognition, lightweight and efficient models are essential. Traditional approaches struggle with computational demands and power consumption. We present BNNAction-Net, a hand gesture recognition system using Binary Neural Networks (BNNs) to reduce computational complexity. Evaluated on the EgoGesture dataset, our system simulates a real use case with a headset and frontal RGB-D cameras. Optimized with binary layers, pooling, and normalization, it achieves accuracy comparable to floating-point networks with lower resource consumption. Our findings highlight the efficiency of BNNs for wearable devices without significant accuracy loss.	https://dl.acm.org/doi/abs/10.1145/3641234.3671047	Federico Fontana, Alessandro Di Matteo, Luigi Cinque, Giuseppe Placidi, Marco Raoul Marini
Back in the Eye of the Storm, The Visual Effects of 'Twisters'	We begin with an exploration of the evolution from the original 1996 Twister to 2024's Twisters, highlighting technological advancements and their impact on visual storytelling. The seamless integration of special effects (SFX) and visual effects (VFX) is crucial for achieving realism, and we will discuss the collaboration between these teams to create cohesive, immersive experiences. A key focus is on the elemental analysis of storms, breaking down the visual components that form tornadoes. This includes understanding the recipe and rules of storms, their lifecycle, and their environmental impact. The tools and setups developed to recreate these phenomena will be showcased, illustrating the blend of science and creativity. Photography plays a vital role in capturing the essence of real storms. We will examine the extensive reference photography process, from hi-res storm chaser footage driving into storms to everyday iPhone and GoPro clips, and how these references shape our imagination and design of tornadoes. Finally, the talk will cover the process of transforming tornadoes into characters within the narrative. Each tornado in Twisters is unique in size, shape, and form, following the simple physics of these natural phenomena while pushing creative boundaries. This section will reveal how Industrial Light & Magic (ILM) rigged and animated different tornadoes, making them central figures in the story. This talk offers an in-depth look at the magic that brings nature's fury to life, inspiring and educating the audience on the potential of visual effects in film to replicate nature.	https://dl.acm.org/doi/abs/10.1145/3641233.3665346	Greg Grusby, Ben Snow, Charles Lai, Florian Witzel
Balboa Park Alive!: Exploring Biodiversity through Mobile Augmented Reality	Balboa Park Alive! is a series of immersive, interactive phone-based installations that leverage Augmented Reality (AR) to explore biodiversity of San Diego/Tijuana. This application combines Niantic Lightship ARDK, Mapbox and mobile hand tracking in service of situated and embodied encounters with 3D digital renderings of some of the plants, insects, and animals that make this region one of the top biodiversity hotspots in the world. Unlike related conservation-centered mobile AR apps, Balboa Park Alive! prioritizes first-hand perspective taking and evidence-based methods of inquiry, using guided movement and visualizations to invite families to engage with their surroundings and each other.	https://dl.acm.org/doi/abs/10.1145/3664294.3664358	Ying Choon Wu, Biayna Bogosian, Jacob Yenney, Patrick Coleman, Daniel Jalali, Joshua Pallag, Akshit Nassa, Thomas Sharkey
Belzebubs — The 360° Hexperience	Virtual black metal band Belzebubs invite you front row and center on their unique, immersive --- and regrettably illstarred --- mini gig, full of energy, surprises and humorous details. Welcome to the show!	https://dl.acm.org/doi/abs/10.1145/3641231.3649294	Jussi-Pekka Ahonen
Bent Out of Shape	When a bright, circular woman moves into a dull, square neighborhood, things begin to change... much to her square neighbor's dismay.	https://dl.acm.org/doi/abs/10.1145/3641230.3653410	Chloe Merwin
Beyond Life and Death: Exploring Digital Legacy with Spatial Media, Emerging Technologies, and Evolving Ethics	This course covers how we use technology to capture and preserve ourselves and others, and the philosophical, legal and ethical considerations involved.	https://dl.acm.org/doi/abs/10.1145/3664475.3664559	Keram Malicki-Sánchez, Jacquelyn Ford Morie, Gregory Panos
Beyond the Farm	When a sprightly, young goat uncovers a majestic view of ancient Greece, he sings about leaving the farm to explore the world, but is held back by his protective dad.	https://dl.acm.org/doi/abs/10.1145/3641230.3653412	Allison Fraidenburg
Biharmonic Coordinates and their Derivatives for Triangular 3D Cages	As a natural extension to the harmonic coordinates, the biharmonic coordinates have been found superior for planar shape and image manipulation with an enriched deformation space. However, the 3D biharmonic coordinates and their derivatives have remained unexplored. In this work, we derive closed-form expressions for biharmonic coordinates and their derivatives for 3D triangular cages. The core of our derivation lies in computing the closed-form expressions for the integral of the Euclidean distance over a triangle its derivatives. The derived 3D biharmonic coordinates not only fill a missing component in methods of generalized barycentric coordinates but also pave the way for various interesting applications in practice, including producing a family of biharmonic deformations, solving variational shape deformations, and even unlocking the closed-form expressions for recently-introduced Somigliana coordinates for both fast and accurate evaluations.	https://dl.acm.org/doi/abs/10.1145/3658208	Jean-Marc Thiery, Élie Michel, Jiong Chen
Bilateral Guided Radiance Field Processing	"Neural Radiance Fields (NeRF) achieves unprecedented performance in synthesizing novel view synthesis, utilizing multi-view consistency. When capturing multiple inputs, image signal processing (ISP) in modern cameras will independently enhance them, including exposure adjustment, color correction, local tone mapping, etc. While these processings greatly improve image quality, they often break the multi-view consistency assumption, leading to ""floaters"" in the reconstructed radiance fields. To address this concern without compromising visual aesthetics, we aim to first disentangle the enhancement by ISP at the NeRF training stage and re-apply user-desired enhancements to the reconstructed radiance fields at the finishing stage. Furthermore, to make the re-applied enhancements consistent between novel views, we need to perform imaging signal processing in 3D space (i.e. ""3D ISP""). For this goal, we adopt the bilateral grid, a locally-affine model, as a generalized representation of ISP processing. Specifically, we optimize per-view 3D bilateral grids with radiance fields to approximate the effects of camera pipelines for each input view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank 4D bilateral grid from a given single view edit, lifting photo enhancements to the whole 3D scene. We demonstrate our approach can boost the visual quality of novel view synthesis by effectively removing floaters and performing enhancements from user retouching. The source code and our data are available at: https://bilarfpro.github.io."	https://dl.acm.org/doi/abs/10.1145/3658148	Yuehao Wang, Chaoyi Wang, Bingchen Gong, Tianfan Xue
Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis	"While surface-based view synthesis algorithms are appealing due to their low computational requirements, they often struggle to reproduce thin structures. In contrast, more expensive methods that model the scene's geometry as a volumetric density field (e.g. NeRF) excel at reconstructing fine geometric detail. However, density fields often represent geometry in a ""fuzzy"" manner, which hinders exact localization of the surface. In this work, we modify density fields to encourage them to converge towards surfaces, without compromising their ability to reconstruct thin structures. First, we employ a discrete opacity grid representation instead of a continuous density field, which allows opacity values to discontinuously transition from zero to one at the surface. Second, we anti-alias by casting multiple rays per pixel, which allows occlusion boundaries and subpixel structures to be modelled without using semi-transparent voxels. Third, we minimize the binary entropy of the opacity values, which facilitates the extraction of surface geometry by encouraging opacity values to binarize towards the end of training. Lastly, we develop a fusion-based meshing strategy followed by mesh simplification and appearance model fitting. The compact meshes produced by our model can be rendered in real-time on mobile devices and achieve significantly higher view synthesis quality compared to existing mesh-based approaches. Our interactive webdemo is available at https://binary-opacity-grid.github.io."	https://dl.acm.org/doi/abs/10.1145/3658130	Christian Reiser, Stephan Garbin, Pratul Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan Barron, Peter Hedman, Andreas Geiger
Bird listening: An application to experience the call and beauty of wild birds	Bird listening is an application to find and appreciate ten types of wild birds by using spatial acoustic bird sounds. When the user faces the direction of a bird's call and holds a digital device in front of him or her, the bird appears through the leaves of the tree.	https://dl.acm.org/doi/abs/10.1145/3664294.3664362	Masatoshi Hamanaka
BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation	We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the denoising iterations. Latent tri-plane extrapolation produces semantically and geometrically meaningful transitions that harmoniously blend with the existing scene. A 2D layout conditioning mechanism is used to control the placement and arrangement of scene elements. Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios.	https://dl.acm.org/doi/abs/10.1145/3658188	Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, Pan Ji
Blue noise for diffusion models	Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network. Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored. In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account. More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask. Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only. Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow. We perform both qualitative and quantitative evaluations on a variety of datasets using our method, achieving improvements on different tasks over existing deterministic diffusion models in terms of FID metric. Code will be available at https://github.com/xchhuang/bndm.	https://dl.acm.org/doi/abs/10.1145/3641519.3657435	Xingchang Huang, Corentin Salaun, Cristina Vasconcelos, Christian Theobalt, Cengiz Oztireli, Gurprit Singh
BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes	While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality, their protracted training duration remains a limitation. Generalizable and MVS-based NeRFs, although capable of mitigating training time, often incur tradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFs to enhance the rendering quality of MVS-based NeRFs in large-scale scenes. We first identify limitations in MVS-based NeRF methods, such as restricted viewport coverage and artifacts due to limited input views. Then, we address these limitations by proposing a new method that selects and combines multiple cost volumes during volume rendering. Our method does not require training and can adapt to any MVS-based NeRF methods in a feed-forward fashion to improve rendering quality. Furthermore, our approach is also end-to-end trainable, allowing fine-tuning on specific scenes. We demonstrate the effectiveness of our method through experiments on large-scale datasets, showing significant rendering quality improvements in large-scale scenes and unbounded outdoor scenarios. We release the source code of BoostMVSNeRFs at https://su-terry.github.io/BoostMVSNeRFs.	https://dl.acm.org/doi/abs/10.1145/3641519.3657416	Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, Yu-Lun Liu
Breaking the low sample rate equals low power paradigm using an event based vision approach for hand tracking in AR	"As spatial computers move towards glasses, where constraints on size, weight and power are critical, a shift in how we use sensors is needed. Ultraleap has developed the first hand tracking pipeline using event cameras on an AR headset. Event cameras allow to break the frame-imaging paradigm of ""low power equates to lo sample rate"" and enhance user experience while maintaining low power budget."	https://dl.acm.org/doi/abs/10.1145/3641517.3664389	Ryan Page, Paolo Baesso, Rory Clark, Greg Baker
BrepGen: A B-rep Generative Diffusion Model with Structured Latent Geometry	This paper presents , a diffusion-based generative approach that directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD) model. represents a B-rep model as a novel structured latent geometry in a hierarchical tree. With the root node representing a whole CAD solid, each element of a B-rep model (i.e., a face, an edge, or a vertex) progressively turns into a child-node from top to bottom. B-rep geometry information goes into the nodes as the global bounding box of each primitive along with a latent code describing the local geometric shape. The B-rep topology information is implicitly represented by node duplication. When two faces share an edge, the edge curve will appear twice in the tree, and a T-junction vertex with three incident edges appears six times in the tree with identical node features. Starting from the root and progressing to the leaf, employs Transformer-based diffusion models to sequentially denoise node features while duplicated nodes are detected and merged, recovering the B-Rep topology information. Extensive experiments show that advances the task of CAD B-rep generation, surpassing existing methods on various benchmarks. Results on our newly collected furniture dataset further showcase its exceptional capability in generating complicated geometry. While previous methods were limited to generating simple prismatic shapes, incorporates free-form and doubly-curved surfaces for the first time. Additional applications of include CAD autocomplete and design interpolation. The code, pretrained models, and dataset are available at https://github.com/samxuxiang/BrepGen.	https://dl.acm.org/doi/abs/10.1145/3658129	Xiang Xu, Joseph Lambourne, Pradeep Jayaraman, Zhengqing Wang, Karl Willis, Yasutaka Furukawa
Bridging the Gap between Education and Practice in Japan's CG Industry: A Focus on Diversity and Evolving Perceptions	This study focuses on the competency gap between computer graphics (CG) educational institutions and CG companies in Japan, with an emphasis on the competencies required in the entertainment industry. The goal is to seek and examine viable solutions for establishing a sustainable CG education framework.	https://dl.acm.org/doi/abs/10.1145/3641235.3664435	Harutaka Matsunaga, Kazunori Miyata
Bringing Adventure Gaming to Life Using Real-Time Generative AI on Your PC	Imagine a new kind of tabletop gaming experience, where a narrator is describing a complex fantasy world, and players gathered around the table can see the events of their world unfolding in real-time, on their PC devices. In this talk, we will show attendees how to execute multi-modal Generative AI (Gen AI) models, running on a PC, in real-time to create immersive scenery, followed by an interactive live demonstration. This talk walks through the optimization of Gen AI modalities, chaining audio transcription and diffusion models, together in real-time. We compress these models with the OpenVINO™ Toolkit and leverage the Intel® Core™ Ultra processor to split these workloads across CPU, integrated GPU, and Neural Processing Unit (NPU), getting the best performance for each model. We also cover exciting new developments with temporal-consistent and depth estimation approaches towards high-resolution, 3D, pop-up, scenery generation. This talk equips participants with hands-on tools to resolve challenges with real-time, high-quality gaming scenery generation on their PC for gaming.	https://dl.acm.org/doi/abs/10.1145/3641233.3664354	Garth Long, Arisha Kumar, Ria Cheruvu, Paula Ramos, Dmitriy Pastushenkov, Zhuo Wu, Raymond Lo
Broken Spectre	25 years ago your father climbed the mountain and never came back. Now he's calling for you.	https://dl.acm.org/doi/abs/10.1145/3641231.3648623	Evan Jones
Build Your Own IoT Love Messengers: Get hands-on experience with building your own electronic devices that can communicate across the world!	Our pair of DIY Love Messengers are 3D-printed, small and light devices that allow users to communicate with each other, no matter how far apart they are around the world. The devices are powered by ESP32 microcontrollers, which allow direct communication between one another using Internet of Things (IoT) technology. The microcontrollers are able to send and read data from each other via a real-time database that stores the information sent by each device. They are also extremely straightforward in functionality - when either user presses the button on their Love Messenger, the lights on both Love Messengers light up simultaneously. This is a simple gesture that enriches relationships and connections, and is a meaningful gift for friends, loved ones and especially any form of long-distance relationship. The DIY Love Messengers are completely open-sourced and beginner-friendly to build, and are a simple application to demonstrate the interactive possibilities of IoT in enriching human relationships.	https://dl.acm.org/doi/abs/10.1145/3641236.3664418	Yi Qing Ng, Julia Daser
CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets	In the realm of digital creativity, our potential to craft intricate 3D worlds from imagination is often hampered by the limitations of existing digital tools, which demand extensive expertise and efforts. To narrow this disparity, we introduce CLAY, a 3D geometry and material generator designed to effortlessly transform human imagination into intricate 3D digital structures. CLAY supports classic text or image inputs as well as 3D-aware controls from diverse primitives (multi-view images, voxels, bounding boxes, point clouds, implicit representations, etc). At its core is a large-scale generative model composed of a multi-resolution Variational Autoencoder (VAE) and a minimalistic latent Diffusion Transformer (DiT), to extract rich 3D priors directly from a diverse range of 3D geometries. Specifically, it adopts neural fields to represent continuous and complete surfaces and uses a geometry generative module with pure transformer blocks in latent space. We present a progressive training scheme to train CLAY on an ultra large 3D model dataset obtained through a carefully designed processing pipeline, resulting in a 3D native geometry generator with 1.5 billion parameters. For appearance generation, CLAY sets out to produce physically-based rendering (PBR) textures by employing a multi-view material diffusion model that can generate 2K resolution textures with diffuse, roughness, and metallic modalities. We demonstrate using CLAY for a range of controllable 3D asset creations, from sketchy conceptual designs to production ready assets with intricate details. Even first time users can easily use CLAY to bring their vivid 3D imaginations to life, unleashing unlimited creativity.	https://dl.acm.org/doi/abs/10.1145/3658146	Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, Jingyi Yu
CNS-Edit: 3D Shape Editing via Coupled Neural Shape Optimization	This paper introduces a new approach based on a coupled representation and a neural volume optimization to implicitly perform 3D shape editing in latent space. This work has three innovations. First, we design the coupled neural shape (CNS) representation for supporting 3D shape editing. This representation includes a latent code, which captures high-level global semantics of the shape, and a 3D neural feature volume, which provides a spatial context to associate with the local shape changes given by the editing. Second, we formulate the coupled neural shape optimization procedure to co-optimize the two coupled components in the representation subject to the editing operation. Last, we offer various 3D shape editing operators, i.e., copy, resize, delete, and drag, and derive each into an objective for guiding the CNS optimization, such that we can iteratively co-optimize the latent code and neural feature volume to match the editing target. With our approach, we can achieve a rich variety of editing results that are not only aware of the shape semantics but are also not easy to achieve by existing approaches. Both quantitative and qualitative evaluations demonstrate the strong capabilities of our approach over the state-of-the-art solutions.	https://dl.acm.org/doi/abs/10.1145/3641519.3657412	Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Hao Zhang, Chi-Wing Fu
CWF: Consolidating Weak Features in High-quality Mesh Simplification	In mesh simplification, common requirements like accuracy, triangle quality, and feature alignment are often considered as a trade-off. Existing algorithms concentrate on just one or a few specific aspects of these requirements. For example, the well-known Quadric Error Metrics (QEM) approach [Garland and Heckbert 1997] prioritizes accuracy and can preserve strong feature lines/points as well, but falls short in ensuring high triangle quality and may degrade weak features that are not as distinctive as strong ones. In this paper, we propose a smooth functional that simultaneously considers all of these requirements. The functional comprises a normal anisotropy term and a Centroidal Voronoi Tessellation (CVT) [Du et al. 1999] energy term, with the variables being a set of movable points lying on the surface. The former inherits the spirit of QEM but operates in a continuous setting, while the latter encourages even point distribution, allowing various surface metrics. We further introduce a decaying weight to automatically balance the two terms. We selected 100 CAD models from the ABC dataset [Koch et al. 2019], along with 21 organic models, to compare the existing mesh simplification algorithms with ours. Experimental results reveal an important observation: the introduction of a decaying weight effectively reduces the conflict between the two terms and enables the alignment of weak features. This distinctive feature sets our approach apart from most existing mesh simplification methods and demonstrates significant potential in shape understanding. Please refer to the teaser figure for illustration.	https://dl.acm.org/doi/abs/10.1145/3658159	Rui Xu, Longdu Liu, Ningna Wang, Shuangmin Chen, Shiqing Xin, Xiaohu Guo, Zichun Zhong, Taku Komura, Wenping Wang, Changhe Tu
Capacitive Touch Sensing on General 3D Surfaces	Mutual-capacitive sensing is the most common technology for detecting multi-touch, especially on flat and simple curvature surfaces. Its extension to a more complex shape is still challenging, as a uniform distribution of sensing electrodes is required for consistent touch sensitivity across the surface. To overcome this problem, we propose a method to adapt the sensor layout of common capacitive multi-touch sensors to more complex 3D surfaces, ensuring high-resolution, robust multi-touch detection. The method automatically computes a grid of transmitter and receiver electrodes with as regular distribution as possible over a general 3D shape. It starts with the computation of a proxy geometry by quad meshing used to place the electrodes through the dual-edge graph. It then arranges electrodes on the surface to minimize the number of touch controllers required for capacitive sensing and the number of input/output pins to connect the electrodes with the controllers. We reach these objectives using a new simplification and clustering algorithm for a regular quad-patch layout. The reduced patch layout is used to optimize the routing of all the structures (surface grooves and internal pipes) needed to host all electrodes on the surface and inside the object's volume, considering the geometric constraints of the 3D shape. Finally, we print the 3D object prototype ready to be equipped with the electrodes. We analyze the performance of the proposed quad layout simplification and clustering algorithm using different quad meshing and characterize the signal quality and accuracy of the capacitive touch sensor for different non-planar geometries. The tested prototypes show precise and robust multi-touch detection with good Signal-to-Noise Ratio and spatial accuracy of about 1mm.	https://dl.acm.org/doi/abs/10.1145/3658185	Gianpaolo Palma, Narges Pourjafarian, Jürgen Steimle, Paolo Cignoni
Categorical Codebook Matching for Embodied Character Controllers	Translating motions from a real user onto a virtual embodied avatar is a key challenge for character animation in the metaverse. In this work, we present a novel generative framework that enables mapping from a set of sparse sensor signals to a full body avatar motion in real-time while faithfully preserving the motion context of the user. In contrast to existing techniques that require training a motion prior and its mapping from control to motion separately, our framework is able to learn the motion manifold as well as how to sample from it at the same time in an end-to-end manner. To achieve that, we introduce a technique called codebook matching which matches the probability distribution between two categorical codebooks for the inputs and outputs for synthesizing the character motions. We demonstrate this technique can successfully handle ambiguity in motion generation and produce high quality character controllers from unstructured motion capture data. Our method is especially useful for interactive applications like virtual reality or video games where high accuracy and responsiveness are needed.	https://dl.acm.org/doi/abs/10.1145/3658209	Sebastian Starke, Paul Starke, Nicky He, Taku Komura, Yuting Ye
Character Stylization in Disney's Wish	"""Wish"" pays homage to a century of Disney Animation's legacy and the look of its characters is inspired by the watercolor storybook style of some of our earlier Disney classics, including ""Snow White and the Seven Dwarfs"", ""Pinocchio"", and ""Sleeping Beauty"". Careful attention to detail in appearance, graphic shading, and streamlined geometric shapes of models, clothing and grooms, provide the foundation of the unique stylized look and art-directed character performance. A final procedural compositing treatment is applied in lighting to produce the more painterly watercolor style combined with hand-drawn influenced linework. Modifications were required in all of the asset departments' workflows to support this overall stylization."	https://dl.acm.org/doi/abs/10.1145/3641233.3664343	Avneet Kaur, Jennifer Stratton
CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization	BNRist, Department of Computer Science and Technology, Tsinghua University, China In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model. This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.	https://dl.acm.org/doi/abs/10.1145/3658217	Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu
Chin Interfaces for Peripheral Interaction	The chin interface is a wearable device that allows for peripheral interaction during everyday activities. Simple sensors and software use steady and unsteady movement data generated from everyday activities like eating, exercising, practicing guitar, or using the computer to interact with peripheral screens directed through the chin. One, two, and three button iterations of the chin interface require different levels of attention for distinct peripheral interaction experiences. Different experiences with peripheral screens can include playing back text one word at a time, slowly coloring in the letters of large words, or patiently typing out statements by selecting keys on an onscreen keyboard.	https://dl.acm.org/doi/abs/10.1145/3641236.3664426	Brandon Ables
Choreofil: Dancing Filament Light Bulb	The ChoreoFil is a light bulb system with dancing filaments. The shape-memory alloy actuator as filaments bends like tentacles of a sea anemone, and the top of the actuator glows softly. By downsizing the previous system, the driving circuit could be fitted inside a light bulb. This project is still only an actuator-driven system, when connected to networks, information, and music, it will become the ambient media with an organism-like presence.	https://dl.acm.org/doi/abs/10.1145/3641234.3671018	Akira Nakayasu
Ciallo: GPU-Accelerated Rendering of Vector Brush Strokes	This paper introduces novel GPU-based rendering techniques for digital painting and animation to bridge the gap between raster and vector stroke representations. We propose efficient rendering methods for vanilla, stamp, and airbrush strokes that integrate the expressiveness of raster-based textures with the ease of real-time editing. Based on our stroke representation, we implement an open-source prototype drawing system with a vector fill feature, demonstrating that our techniques can enhance the expressiveness, efficiency, and edibility of digital drawing. Our work can serve as a foundation for future research on vector-based and GPU-accelerated rendering techniques in industrial-level brush engines.	https://dl.acm.org/doi/abs/10.1145/3641519.3657418	Shen Ciao, Zhongyue Guan, Qianxi Liu, Li-Yi Wei, Zeyu Wang
Co-Presence, Connection and Co-Creation: Building Real-time Cross-person Neurofeedback Interactions	How can we push the boundaries of human connection by tapping into the power of immersive interpersonal brainwave interactions? While biosignal visualizations and interactions have advanced in both research and art scenes, most experiences and applications that utilize brainwaves and other biosignals involve single-person or one-way interactions from one individual to another, underexploiting the prosocial potential of these biosignals. This talk explores cross- and multi-person environments in virtual reality (VR) driven by shared brainwaves. Through case studies of two installations in a series, each showcasing unique relationships cultivated by the immersive experience via distinct EEG manifestations and visualization rules - feeling each other's presence, reinforcing mutual emotional connections, and co-creating art through synchronized brainwaves - the talk describes the design and development process of cross-person neurofeedback experiences in virtual reality. It explains various innovative approaches to real-time brainwave visualizations and interactions that integrate technology, science, storytelling and gamification, and examines their significance in expanding non-verbal channels of communication, elevating interpersonal connections, and fostering collective creativity.	https://dl.acm.org/doi/abs/10.1145/3641233.3664334	Tiange Wang, Xin Feng
CoCapture: Group Photo App without Assistants	CoCapture is a mobile app tailored for group photography, allowing users to easily take great group photos and stunning scenic shots during travels, outings, or social events without the need for selfie sticks, tripods, or external help. This application assists in capturing cherished moments, ensuring our unforgettable experiences endure. CoCapture's innovative method involves users first capturing a background shot, then taking turns photographing the rest of the group with the individual who needs the group photo. Once three photos are taken, CoCapture allows users to seamlessly merge these images into a complete group photo that includes all members.	https://dl.acm.org/doi/abs/10.1145/3664294.3664366	Binlin Feng, Keyi Zeng, Mingyang Su, Wen Ku, Feifei Wu, Xiu Li
Coffee Brake	In the drab confines of a grey office, Gary and Carol await their daily respite: The tea break. The highlight: A lone doughnut. The race for the last doughnut ignites as the clock strikes 3. Gary and Carol transform their office chairs into turbocharged racing machines, engaging in a high-speed chase through the office. But who will get there first?	https://dl.acm.org/doi/abs/10.1145/3641230.3652247	Alex Weight, Raphael Gadot, Ross Anderson
Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning	As humans, we aspire to create media content that is both freely willed and readily controlled. Thanks to the prominent development of generative techniques, we now can easily utilize 2D diffusion methods to synthesize images controlled by raw sketch or designated human poses, and even progressively edit/regenerate local regions with masked inpainting. However, similar workflows in 3D modeling tasks are still unavailable due to the lack of controllability and efficiency in 3D generation. In this paper, we present a novel controllable and interactive 3D assets modeling framework, named Coin3D. Coin3D allows users to control the 3D generation using a coarse geometry proxy assembled from basic shapes, and introduces an interactive generation workflow to support seamless local part editing while delivering responsive 3D object previewing within a few seconds. To this end, we develop several techniques, including the 3D adapter that applies volumetric coarse shape control to the diffusion model, proxy-bounded editing strategy for precise part editing, progressive volume cache to support responsive preview, and volume-SDS to ensure consistent mesh reconstruction. Extensive experiments of interactive generation and editing on diverse shape proxies demonstrate that our method achieves superior controllability and flexibility in the 3D assets generation task. Code and data are available on the project webpage: https://zju3dv.github.io/coin3d/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657425	Wenqi Dong, Bangbang Yang, Lin Ma, Xiao Liu, Liyuan Cui, Hujun Bao, Yuewen Ma, Zhaopeng Cui
College Football is HUGE: Delivering a AAA Sports Game at Scale	Following an 11-year hiatus, EA SPORTS College Football is back. A video game that recreates the greatest sights in America's most beloved sport, this enormous production created over 11,000 unique playable characters, representing 134 schools, their stadiums, athletes, uniforms, mascots, traditions and so much more. This project's scale, quality and time demands necessitated a complete reimagining of traditional content creation and rendering methods. This talk explores the strategies employed by our art, technical art and rendering teams to meet these challenges. We discuss our approach in creating and rendering every element of the game, from dogs to cows, cannons to swords, helmets to cleats.	https://dl.acm.org/doi/abs/10.1145/3641233.3665345	Ishaan Singh, Jay Goodman, Richard Burgess-Dawson
ColorVideoVDP: A visual difference predictor for image, video and display distortions	ColorVideoVDP is a video and image quality metric that models spatial and temporal aspects of vision for both luminance and color. The metric is built on novel psychophysical models of chromatic spatiotemporal contrast sensitivity and cross-channel contrast masking. It accounts for the viewing conditions, geometric, and photometric characteristics of the display. It was trained to predict common video-streaming distortions (e.g., video compression, rescaling, and transmission errors) and also 8 new distortion types related to AR/VR displays (e.g., light source and waveguide non-uniformities). To address the latter application, we collected our novel XR-Display-Artifact-Video quality dataset (XR-DAVID), comprised of 336 distorted videos. Extensive testing on XR-DAVID, as well as several datasets from the literature, indicate a significant gain in prediction performance compared to existing metrics. ColorVideoVDP opens the doors to many novel applications that require the joint automated spatiotemporal assessment of luminance and color distortions, including video streaming, display specification, and design, visual comparison of results, and perceptually-guided quality optimization. The code for the metric can be found at https://github.com/gfxdisp/ColorVideoVDP.	https://dl.acm.org/doi/abs/10.1145/3658144	Rafal K. Mantiuk, Param Hanji, Maliha Ashraf, Yuta Asano, Alexandre Chapiro
Compressed Skinning for Facial Blendshapes	We present a new method to bake classical facial animation blendshapes into a fast linear blend skinning representation. Previous work explored skinning decomposition methods that approximate general animated meshes using a dense set of bone transformations; these optimizers typically alternate between optimizing for the bone transformations and the skinning weights. We depart from this alternating scheme and propose a new approach based on proximal algorithms, which effectively means adding a projection step to the popular Adam optimizer. This approach is very flexible and allows us to quickly experiment with various additional constraints and/or loss functions. Specifically, we depart from the classical skinning paradigms and restrict the transformation coefficients to contain only about 90% non-zeros, while achieving similar accuracy and visual quality as the state-of-the-art. The sparse storage enables our method to deliver significant savings in terms of both memory and run-time speed. We include a compact implementation of our new skinning decomposition method in PyTorch, which is easy to experiment with and modify to related problems.	https://dl.acm.org/doi/abs/10.1145/3641519.3657477	Ladislav Kavan, John Doublestein, Martin Prazak, Matthew Cioffi, Doug Roble
Computational Homogenization for Inverse Design of Surface-based Inflatables	Surface-based inflatables are composed of two thin layers of nearly inextensible sheet material joined together along carefully selected fusing curves. During inflation, pressure forces separate the two sheets to maximize the enclosed volume. The fusing curves restrict this expansion, leading to a spatially varying in-plane contraction and hence metric frustration. The inflated structure settles into a 3D equilibrium that balances pressure forces with the internal elastic forces of the sheets. We present a computational framework for analyzing and designing surface-based inflatable structures with arbitrary fusing patterns. Our approach employs numerical homogenization to characterize the behavior of parametric families of periodic inflatable patch geometries, which can then be combined to tessellate the sheet with smoothly varying patterns. We propose a novel parametrization of the underlying deformation space that allows accurate, efficient, and systematical analysis of the stretching and bending behavior of inflated patches with potentially open boundaries. We apply our homogenization algorithm to create a database of geometrically diverse fusing patterns spanning a wide range of material properties and deformation characteristics. This database is employed in an inverse design algorithm that solves for fusing curves to best approximate a given input target surface. Local patches are selected and blended to form a global network of curves based on a geometric flattening algorithm. These fusing curves are then further optimized to minimize the distance of the deployed structure to target surface. We show that this approach offers greater flexibility to approximate given target geometries compared to previous work while significantly improving structural performance.	https://dl.acm.org/doi/abs/10.1145/3658125	Yingying Ren, Julian Panetta, Seiichi Suzuki, Uday Kusupati, Florin Isvoranu, Mark Pauly
Computational Illusion Knitting	"Illusion-knit fabrics reveal distinct patterns or images depending on the viewing angle. Artists have manually achieved this effect by exploiting ""microgeometry,"" i.e., small differences in stitch heights. However, past work in computational 3D knitting does not model or exploit designs based on stitch height variation. This paper establishes a foundation for exploring illusion knitting in the context of computational design and fabrication. We observe that the design space is highly constrained, elucidate these constraints, and derive strategies for developing effective, machine-knittable illusion patterns. We partially automate these strategies in a new interactive design tool that reduces difficult patterning tasks to familiar image editing tasks. Illusion patterns also uncover new fabrication challenges regarding mixed colorwork and texture; we describe new algorithms for mitigating fabrication failures and ensuring high-quality knit results."	https://dl.acm.org/doi/abs/10.1145/3658231	Amy Zhu, Yuxuan Mei, Benjamin Jones, Zachary Tatlock, Adriana Schulz
Conditional Mixture Path Guiding for Differentiable Rendering	The efficiency of inverse optimization in physically based differentiable rendering heavily depends on the variance of Monte Carlo estimation. Despite recent advancements emphasizing the necessity of tailored differential sampling strategies, the general approaches remain unexplored. In this paper, we investigate the interplay between local sampling decisions and the estimation of light path derivatives. Considering that modern differentiable rendering algorithms share the same path for estimating differential radiance and ordinary radiance, we demonstrate that conventional guiding approaches, conditioned solely on the last vertex, cannot attain this density. Instead, a mixture of different sampling distributions is required, where the weights are conditioned on all the previously sampled vertices in the path. To embody our theory, we implement a conditional mixture path guiding that explicitly computes optimal weights on the fly. Furthermore, we show how to perform positivization to eliminate sign variance and extend to scenes with millions of parameters. To the best of our knowledge, this is the first generic framework for applying path guiding to differentiable rendering. Extensive experiments demonstrate that our method achieves nearly one order of magnitude improvements over state-of-the-art methods in terms of variance reduction in gradient estimation and errors of inverse optimization. The implementation of our proposed method is available at https://github.com/mollnn/conditional-mixture.	https://dl.acm.org/doi/abs/10.1145/3658133	Zhimin Fan, Pengcheng Shi, Mufan Guo, Ruoyu Fu, Yanwen Guo, Jie Guo
Consistent Point Orientation for Manifold Surfaces via Boundary Integration	This paper introduces a new approach for generating globally consistent normals for point clouds sampled from manifold surfaces. Given that the generalized winding number (GWN) field generated by a point cloud with globally consistent normals is a solution to a PDE with jump boundary conditions and possesses harmonic properties, and the Dirichlet energy of the GWN field can be defined as an integral over the boundary surface, we formulate a boundary energy derived from the Dirichlet energy of the GWN. Taking as input a point cloud with randomly oriented normals, we optimize this energy to restore the global harmonicity of the GWN field, thereby recovering the globally consistent normals. Experiments show that our method outperforms state-of-the-art approaches, exhibiting enhanced robustness to noise, outliers, complex topologies, and thin structures. Our code can be found at https://github.com/liuweizhou319/BIM.	https://dl.acm.org/doi/abs/10.1145/3641519.3657475	Weizhou Liu, Xingce Wang, Haichuan Zhao, Xingfei Xue, Zhongke Wu, Xuequan Lu, Ying He
Contact detection between curved fibres: high order makes a difference	Computer Graphics has a long history in the design of effective algorithms for handling contact and friction between solid objects. For the sake of simplicity and versatility, most methods rely on low-order primitives such as line segments or triangles, both for the detection and the response stages. In this paper we carefully analyse, in the case of fibre systems, the impact of such choices on the retrieved contact forces. We highlight the presence of artifacts in the force response that are tightly related to the low-order geometry used for contact detection. Our analysis draws upon thorough comparisons between the high-order super-helix model and the low-order discrete elastic rod model. These reveal that when coupled to a low-order, segment-based detection scheme, both models yield spurious jumps in the contact force profile. Moreover, these artifacts are shown to be all the more visible as the geometry of fibres at contact is curved. In order to remove such artifacts we develop an accurate high-order detection scheme between two smooth curves, which relies on an efficient adaptive pruning strategy. We use this algorithm to detect contact between super-helices at high precision, allowing us to recover, in the range of wavy to highly curly fibres, much smoother force profiles during sliding motion than with a classical segment-based strategy. Furthermore, we show that our approach offers better scaling properties in terms of efficiency vs. precision compared to segment-based approaches, making it attractive for applications where accurate and reliable forces are desired. Finally, we demonstrate the robustness and accuracy of our fully high-order approach on a challenging hair combing scenario.	https://dl.acm.org/doi/abs/10.1145/3658191	Octave Crespel, Emile Hohnadel, Thibaut Metivet, Florence Bertails-Descoubes
ContourCraft: Learning to Resolve Intersections in Neural Multi-Garment Simulations	Learning-based approaches to cloth simulation have started to show their potential in recent years. However, handling collisions and intersections in neural simulations remains a largely unsolved problem. In this work, we present ContourCraft, a learning-based solution for handling intersections in neural cloth simulations. Unlike conventional approaches that critically rely on intersection-free inputs, ContourCraft robustly recovers from intersections introduced through missed collisions, self-penetrating bodies, or errors in manually designed multi-layer outfits. The technical core of ContourCraft is a novel intersection contour loss that penalizes interpenetrations and encourages rapid resolution thereof. We integrate our intersection loss with a collision-avoiding repulsion objective into a neural cloth simulation method based on graph neural networks (GNNs). We demonstrate our method's ability across a challenging set of diverse multi-layer outfits under dynamic human motions. Our extensive analysis indicates that ContourCraft significantly improves collision handling for learned simulation and produces visually compelling results.	https://dl.acm.org/doi/abs/10.1145/3641519.3657408	Artur Grigorev, Giorgio Becherini, Michael Black, Otmar Hilliges, Bernhard Thomaszewski
Controllable Neural Reconstruction for Autonomous Driving	Neural scene reconstruction is gaining importance in autonomous driving, especially for closed-loop simulation of real-world recordings. This paper introduces an automated pipeline for training neural reconstruction models, utilizing sensor streams captured by a data collection vehicle. Subsequently, these models are deployed to replicate a virtual counterpart of the actual world. Additionally, the scene can be replayed or manipulated in a controlled manner. To achieve this, our in-house simulator is employed to augment the recreated static environment with dynamic agents, managing occlusion and lighting. The simulator's versatility allows for various parameter adjustments, including dynamic agent behavior and weather conditions.	https://dl.acm.org/doi/abs/10.1145/3641234.3671082	Máté Tóth, Péter Kovács, Zoltán Bendefy, Zoltán Hortsin, Tamás Matuszka
Controllable Neural Style Transfer for Dynamic Meshes	In recent years, animation movies are shifting from realistic representations to more stylized depictions that support unique design languages. To favor that, recent works implemented a Neural Style Transfer (NST) pipeline that supports the stylization of 3D assets by 2D images. In this paper we propose a novel mesh stylization technique that improves previous NST works in several ways. First, we replace the standard Gram-Matrix style loss by a Neural Neighbor formulation that enables sharper and artifact-free results. To support large mesh deformations, we reparametrize the optimized mesh positions through an implicit formulation based on the Laplace-Beltrami operator that better captures silhouette gradients that are common in inverse differentiable rendering setups. This reparametrization is coupled with a coarse-to-fine stylization setup, which enables deformations that can change large structures of the mesh. We provide artistic control through a novel method that enables directional and temporal control over synthesized styles by a guiding vector field. Lastly, we improve the previous time-coherency schemes and develop an efficient regularization that controls volume changes during the stylization process. These improvements enable high quality mesh stylizations that can create unique looks for both simulations and 3D assets.	https://dl.acm.org/doi/abs/10.1145/3641519.3657474	Guilherme Gomes Haetinger, Jingwei Tang, Raphael Ortiz, Paul Kanyuk, Vinicius Azevedo
Controlling the color appearance of objects by optimizing the illumination spectrum	We have developed an innovative lighting system that changes specific target colors while keeping the lights appearing naturally white. By precisely controlling the spectral power distribution (SPD) of illumination and harnessing the unique phenomenon of metamerism, our system achieves unique color variations in ways you've never seen before. Our system calculates the optimal SPDs of illumination for given materials to intensively induce metamerism, and then synthesizes the illumination using various colors of LEDs. We successfully demonstrated the system's implementation at Paris Fashion Week 2024. As models step onto the stage, their dresses initiate a captivating transformation. Our system altering the colors of the dresses, showcasing an impressive transition from one stunning color to another.	https://dl.acm.org/doi/abs/10.1145/3641517.3664388	Mariko Yamaguchi, Masaru Tsuchida, Takahiro Matsumoto, Tetsuro Tokunaga, Takayoshi Mochiduki
Controlling the color appearance of objects by optimizing the illumination spectrum	We have developed an innovative lighting system that changes specific target colors while keeping the lights appearing naturally white. By precisely controlling the spectral power distribution (SPD) of illumination and harnessing the unique phenomenon of metamerism, our system achieves unique color variations in ways you've never seen before. Our system calculates the optimal SPDs of illumination for given materials to intensively induce metamerism, and then synthesizes the illumination using various colors of LEDs. We successfully demonstrated the system's implementation at Paris Fashion Week 2024. As models step onto the stage, their dresses initiate a captivating transformation. Our system altering the colors of the dresses, showcasing an impressive transition from one stunning color to another.	https://dl.acm.org/doi/abs/10.1145/3641517.3664388	Mariko Yamaguchi, Masaru Tsuchida, Takahiro Matsumoto, Tetsuro Tokunaga, Takayoshi Mochiduki
Controlling the color appearance of objects by optimizing the illumination spectrum	We have developed an innovative lighting system that changes specific target colors while keeping the lights appearing naturally white. By precisely controlling the spectral power distribution (SPD) of illumination and harnessing the unique phenomenon of metamerism, our system achieves unique color variations in ways you've never seen before. Our system calculates the optimal SPDs of illumination for given materials to intensively induce metamerism, and then synthesizes the illumination using various colors of LEDs. We successfully demonstrated the system's implementation at Paris Fashion Week 2024. As models step onto the stage, their dresses initiate a captivating transformation. Our system altering the colors of the dresses, showcasing an impressive transition from one stunning color to another.	https://dl.acm.org/doi/abs/10.1145/3641233.3664351	Mariko Yamaguchi, Masaru Tsuchida, Takahiro Matsumoto, Tetsuro Tokunaga, Takayoshi Mochiduki
Controlling the color appearance of objects by optimizing the illumination spectrum	We have developed an innovative lighting system that changes specific target colors while keeping the lights appearing naturally white. By precisely controlling the spectral power distribution (SPD) of illumination and harnessing the unique phenomenon of metamerism, our system achieves unique color variations in ways you've never seen before. Our system calculates the optimal SPDs of illumination for given materials to intensively induce metamerism, and then synthesizes the illumination using various colors of LEDs. We successfully demonstrated the system's implementation at Paris Fashion Week 2024. As models step onto the stage, their dresses initiate a captivating transformation. Our system altering the colors of the dresses, showcasing an impressive transition from one stunning color to another.	https://dl.acm.org/doi/abs/10.1145/3641233.3664351	Mariko Yamaguchi, Masaru Tsuchida, Takahiro Matsumoto, Tetsuro Tokunaga, Takayoshi Mochiduki
Controlling the color appearance of objects by optimizing the illumination spectrum	We have developed an innovative lighting system that changes specific target colors while keeping the lights appearing naturally white. By precisely controlling the spectral power distribution (SPD) of illumination and harnessing the unique phenomenon of metamerism, our system achieves unique color variations in ways you've never seen before. Our system calculates the optimal SPDs of illumination for given materials to intensively induce metamerism, and then synthesizes the illumination using various colors of LEDs. We successfully demonstrated the system's implementation at Paris Fashion Week 2024. As models step onto the stage, their dresses initiate a captivating transformation. Our system altering the colors of the dresses, showcasing an impressive transition from one stunning color to another.	https://dl.acm.org/doi/abs/10.1145/3641517.3664388	Mariko Yamaguchi, Masaru Tsuchida, Takahiro Matsumoto, Tetsuro Tokunaga, Takayoshi Mochiduki
Controlling the color appearance of objects by optimizing the illumination spectrum	We have developed an innovative lighting system that changes specific target colors while keeping the lights appearing naturally white. By precisely controlling the spectral power distribution (SPD) of illumination and harnessing the unique phenomenon of metamerism, our system achieves unique color variations in ways you've never seen before. Our system calculates the optimal SPDs of illumination for given materials to intensively induce metamerism, and then synthesizes the illumination using various colors of LEDs. We successfully demonstrated the system's implementation at Paris Fashion Week 2024. As models step onto the stage, their dresses initiate a captivating transformation. Our system altering the colors of the dresses, showcasing an impressive transition from one stunning color to another.	https://dl.acm.org/doi/abs/10.1145/3641517.3664388	Mariko Yamaguchi, Masaru Tsuchida, Takahiro Matsumoto, Tetsuro Tokunaga, Takayoshi Mochiduki
Controlling the color appearance of objects by optimizing the illumination spectrum	We have developed an innovative lighting system that changes specific target colors while keeping the lights appearing naturally white. By precisely controlling the spectral power distribution (SPD) of illumination and harnessing the unique phenomenon of metamerism, our system achieves unique color variations in ways you've never seen before. Our system calculates the optimal SPDs of illumination for given materials to intensively induce metamerism, and then synthesizes the illumination using various colors of LEDs. We successfully demonstrated the system's implementation at Paris Fashion Week 2024. As models step onto the stage, their dresses initiate a captivating transformation. Our system altering the colors of the dresses, showcasing an impressive transition from one stunning color to another.	https://dl.acm.org/doi/abs/10.1145/3641233.3664351	Mariko Yamaguchi, Masaru Tsuchida, Takahiro Matsumoto, Tetsuro Tokunaga, Takayoshi Mochiduki
Controlling the color appearance of objects by optimizing the illumination spectrum	We have developed an innovative lighting system that changes specific target colors while keeping the lights appearing naturally white. By precisely controlling the spectral power distribution (SPD) of illumination and harnessing the unique phenomenon of metamerism, our system achieves unique color variations in ways you've never seen before. Our system calculates the optimal SPDs of illumination for given materials to intensively induce metamerism, and then synthesizes the illumination using various colors of LEDs. We successfully demonstrated the system's implementation at Paris Fashion Week 2024. As models step onto the stage, their dresses initiate a captivating transformation. Our system altering the colors of the dresses, showcasing an impressive transition from one stunning color to another.	https://dl.acm.org/doi/abs/10.1145/3641233.3664351	Mariko Yamaguchi, Masaru Tsuchida, Takahiro Matsumoto, Tetsuro Tokunaga, Takayoshi Mochiduki
Crafting an Immersive SciViz Experience for the 2023 SIGGRAPH Electronic Theater Pre-Show	The 2023 SIGGRAPH Electronic Theater pre-show was a fun and spectacular live event that used computer vision to analyze how the 2500 audience members moved their bodies in the room, visualizing that data with a variety of graphics across three giant screens. The walk-in experience was an abstract and artistic representation of the data, followed by the main pre-show: a tour through the Universe, where the audience decided what celestial wonder to visit next. This session goes into the behind-the-scenes of how a dispersed, remote team of artists, programmers, producers, and astrophysicists pulled off this event, without ever stepping foot into the venue or having access to test audiences until on-site at SIGGRAPH.	https://dl.acm.org/doi/abs/10.1145/3641232.3675355	Kalina Borkiewicz, Dawn Fidrick, Bradley Thompson
Creating Infinite Characters From a Single Template: How Automation May Give Super Powers to 3D Artists	Game character creation is a time-consuming and highly manual process that often requires several days to complete one non-player character. In addition, each game has unique character art style and technical specifications (mesh, rig, textures). This explains why game worlds with thousands of unique characters are rare. To address this problem, we propose an automated character generation pipeline that can produce a full-body, rigged, dressed, and accessorized 3D character in less than a minute from an initial template character, based on 2D pictures and/or facial text descriptors. This pipeline helps scale up character creation and populate games with unlimited variations at reduced cost, while ensuring consistency with the game art style.	https://dl.acm.org/doi/abs/10.1145/3641233.3664339	Mariana Dias, Pedro Coelho, Rui Figueiredo, Rita Carvalho, Veronica Orvalho, Alexis Roche
Creating LEGO Figurines from Single Images	This paper presents a computational pipeline for creating personalized, physical LEGO figurines from user-input portrait photos. The generated figurine is an assembly of coherently-connected LEGO bricks detailed with uv-printed decals, capturing prominent features such as hairstyle, clothing style, and garment color, and also intricate details such as logos, text, and patterns. This task is non-trivial, due to the substantial domain gap between unconstrained user photos and the stylistically-consistent LEGO figurine models. To ensure assemble-ability by LEGO bricks while capturing prominent features and intricate details, we design a three-stage pipeline: (i) we formulate a CLIP-guided retrieval approach to connect the domains of user photos and LEGO figurines, then output physically-assemble-able LEGO figurines with decals excluded; (ii) we then synthesize decals on the figurines via a symmetric U-Nets architecture conditioned on appearance features extracted from user photos; and (iii) we next reproject and uv-print the decals on associated LEGO bricks for physical model production. We evaluate the effectiveness of our method against eight hundred expert-designed figurines, using a comprehensive set of metrics, which include a novel GPT-4V-based evaluation metric, demonstrating superior performance of our method in visual quality and resemblance to input photos. Also, we show our method's robustness by generating LEGO figurines from diverse inputs and physically fabricating and assembling several of them.	https://dl.acm.org/doi/abs/10.1145/3658167	Jiahao Ge, Mingjun Zhou, Wenrui Bao, Hao Xu, Chi-Wing Fu
Creating the Wishes of Rosas	"The wishes of Rosas are a central story device in Disney Animation's 100th anniversary film ""Wish."" Appearing as worlds revealed within magical galaxies living inside palm-sized orbs, the wishes needed to be highly dynamic and deeply dimensional. The film's art direction required that the internal animation and lighting of the wishes react to external story events and interact with characters, while the narrative required that wishes be choreographed en masse, selectively revealed and concealed while in motion for musical numbers and plot points, and remain identifiable to characters and audiences. The breadth of these requirements, along with a need for scalability in authoring, drove the development of our unified wish asset and shot pipeline."	https://dl.acm.org/doi/abs/10.1145/3641233.3664337	Neelima Karanam, Joel Einhorn, Emily Vo, Harmony Li
Cricket: A Self-Powered Chirping Pixel	We present a sensor that can measure light and wirelessly communicate the measurement, without the need for an external power source or a battery. Our sensor, called cricket, harvests energy from incident light. It is asleep for most of the time and transmits a short and strong radio frequency chirp when its harvested energy reaches a specific level. The carrier frequency of each cricket is fixed and reveals its identity, and the duration between consecutive chirps is a measure of the incident light level. We have characterized the radiometric response function, signal-to-noise ratio and dynamic range of cricket. We have experimentally verified that cricket can be miniaturized at the expense of increasing the duration between chirps. We show that a cube with a cricket on each of its sides can be used to estimate the centroid of any complex illumination, which has value in applications such as solar tracking. We also demonstrate the use of crickets for creating untethered sensor arrays that can produce video and control lighting for energy conservation. Finally, we modified cricket's circuit to develop battery-free electronic sunglasses that can instantly adapt to environmental illumination.	https://dl.acm.org/doi/abs/10.1145/3658196	Shree K. Nayar, Jeremy Klotz, Nikhil Nanda, Mikhail Fridberg
Cross-Image Attention for Zero-Shot Appearance Transfer	Recent advancements in text-to-image generative models have demonstrated a remarkable ability to capture a deep semantic understanding of images. In this work, we leverage this semantic knowledge to transfer the visual appearance between objects that share similar semantics but may differ significantly in shape. To achieve this, we build upon the self-attention layers of these generative models and introduce a cross-image attention mechanism that implicitly establishes semantic correspondences across images. Specifically, given a pair of images — one depicting the target structure and the other specifying the desired appearance — our cross-image attention combines the queries corresponding to the structure image with the keys and values of the appearance image. This operation, when applied during the denoising process, leverages the established semantic correspondences to generate an image combining the desired structure and appearance. In addition, to improve the output image quality, we harness three mechanisms that either manipulate the noisy latent codes or the model's internal representations throughout the denoising process. Importantly, our approach is zero-shot, requiring no optimization or training. Experiments show that our method is effective across a wide range of object categories and is robust to variations in shape, size, and viewpoint between the two input images.	https://dl.acm.org/doi/abs/10.1145/3641519.3657423	Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, Daniel Cohen-Or
Crowdabunga! The Crowd Challenges of TMNT: Mutant Mayhem	"From the original comic book to the TV shows, the universe of the Ninja Turtles is very well-known for its unique characters, whether lead or secondary. To deliver believable and directable crowds for ""Teenage Mutant Ninja Turtles: Mutant Mayhem"" (TMNT: MM), the teams at Mikros Animation had to think of new approaches for the Crowd department to populate dozens of shots while matching the animation and visual style of the show. We faced multiple challenges: the crowd population were close to the camera and had to be diverse while matching the asymmetrical / clay-like style and capabilities of the main characters; the main action takes place in NYC and portrays well known crowded situations; the results had to remain faithful to the 2D teenage style of the show. In this paper, we detail the tools we developed to tackle these challenges. First, a Character Building Tool was developed to generate hundreds of stylized morphologies from an input of 4 Character Templates, hero garments and pencil strokes. Then, crowd artists took advantage of casting tools to populate their shots, assign them with predefined animations and enhance the final results with procedural behaviors. Finally, post simulation tools were implemented to alter animations and lookdev, based on camera distance."	https://dl.acm.org/doi/abs/10.1145/3641233.3664316	Benjamin Dupin, Gaëtan Luzy, Nicolas Chaverou
Crowdsourced Streetview: Integrating Real-Time Imagery Updates into Google Streetview	This paper presents Crowdsourced Streetview, a system that integrates real-time imagery updates into Google Streetview by leveraging crowdsourced images from social media platforms. Our approach utilizes advanced image alignment and feature detection algorithms to overlay user-contributed images onto existing Streetview data, achieving real-time performance with an image alignment runtime of 19ms per image pair. Crowdsourced Streetview has the potential to provide up-to-date and real-time visual representations of locations, particularly in rapidly changing environments and during events such as natural disasters.	https://dl.acm.org/doi/abs/10.1145/3641234.3671067	Ryan Hardesty Lewis
Cuisine Exchange	Co-Directors Yves Geleyn and Michael Thurmeier, and Art Director Mike Knapp reunite to tell a heartwarming tale about how great food connects us all. This holiday spot evokes the same warmth as its predecessor while taking a fresh approach with a classic Kroji look.	https://dl.acm.org/doi/abs/10.1145/3641230.3653318	Yves Geleyn, Michael Thurmeier
Curvature-Driven Conformal Deformations	In this paper, we introduce a novel approach for computing conformal deformations in ℝ while minimizing curvature-based energies. Curvature-based energies serve as fundamental tools in geometry processing, essential for tasks such as surface fairing, deformation, and approximation using developable or cone metric surfaces. However, accurately computing the geometric embedding, especially for the latter, has been a challenging endeavor. The complexity arises from inherent numerical instabilities in curvature estimation and the intricate nature of differentiating these energies. To address these challenges, we concentrate on conformal deformations, leveraging the curvature tensor as the primary variable in our model. This strategic choice renders curvature-based energies easily applicable, mitigating previous manipulation difficulties. Our key contribution lies in identifying a previously unknown integrability condition that establishes a connection between conformal deformations and changes in curvature. We use this insight to deform surfaces of arbitrary genus, aiming to minimize bending energies or prescribe Gaussian curvature while sticking to positional constraints.	https://dl.acm.org/doi/abs/10.1145/3658145	Etienne Corman
Cybermove: a Critical Practice on Virtualizing Our Daily Life	"Cybermove prompts exploration of ""housing"". The artist used 3D scanning to virtually recreate her childhood room, packed all her belongings into two suitcases, and began her journey abroad. This digital space was then transported with the artist to a foreign country, unveiled and shared with visitors through a VR headset. The moving process was displayed on social media and connected with viewers' experiences of housing issues, amassing over 3 million views."	https://dl.acm.org/doi/abs/10.1145/3641234.3671032	Jiahe Zhao, Liyu Chen
Cybernetic Oracle	Do Large Language Models produce the truth or technological hallucinations? Cybernetic Oracle invites visitors to use cartomancy to gain insight into their past, present and future with a custom deck of 78 tarot cards and a book of tarot interpretations designed by the artist in collaboration with generative AI models.	https://dl.acm.org/doi/abs/10.1145/3641523.3665169	Hannen Wolfe
Cybersickness Reduction via Gaze-Contingent Image Deformation	Virtual reality has ushered in a revolutionary era of immersive content perception. However, a persistent challenge in dynamic environments is the occurrence of cybersickness arising from a conflict between visual and vestibular cues. Prior techniques have demonstrated that limiting illusory self-motion, so-called vection, by blurring the peripheral part of images, introducing tunnel vision, or altering the camera path can effectively reduce the problem. Unfortunately, these methods often alter the user's experience with visible changes to the content. In this paper, we propose a new technique for reducing vection and combating cybersickness by subtly lowering the screen-space speed of objects in the user's peripheral vision. The method is motivated by our hypothesis that small modifications to the objects' velocity in the periphery and geometrical distortions in the peripheral vision can remain unnoticeable yet lead to reduced vection. This paper describes the experiments supporting this hypothesis and derives its limits. Furthermore, we present a method that exploits these findings by introducing subtle, screen-space geometrical distortions to animation frames to counteract the motion contributing to vection. We implement the method as a realtime post-processing step that can be integrated into existing rendering frameworks. The final validation of the technique and comparison to an alternative approach confirms its effectiveness in reducing cybersickness.	https://dl.acm.org/doi/abs/10.1145/3658138	Colin Groth, Marcus Magnor, Steve Grogorick, Martin Eisemann, Piotr Didyk
Cycle	Nest, an eleven year old girl, decorates her cocoon : everything is calm and minimalist, surrounded by vegetation with a pastel hue. Suddenly, a shadow changes the brightness of her surroundings.	https://dl.acm.org/doi/abs/10.1145/3641230.3652584	Amelie Devauchelle, Eva Degli-Innocenti, Loreline Clément, Lucie Amherdt, Clemence Fischbach
Cyclogenesis: Simulating Hurricanes and Tornadoes	Cyclones are large-scale phenomena that result from complex heat and water transfer processes in the atmosphere, as well as from the interaction of multiple , i.e., water and ice particles. When cyclones make landfall, they are considered natural disasters and spawn dread and awe alike. We propose a physically-based approach to describe the 3D development of cyclones in a visually convincing and physically plausible manner. Our approach allows us to capture large-scale heat and water continuity, turbulent microphysical dynamics of hydrometeors, and mesoscale cyclonic processes within the planetary boundary layer. Modeling these processes enables us to simulate multiple hurricane and tornado phenomena. We evaluate our simulations quantitatively by comparing to real data from storm soundings and observations of hurricane landfall from climatology research. Additionally, qualitative comparisons to previous methods are performed to validate the different parts of our scheme. In summary, our model simulates cyclogenesis in a comprehensive way that allows us to interactively render animations of some of the most complex weather events.	https://dl.acm.org/doi/abs/10.1145/3658149	Jorge Alejandro Amador Herrera, Jonathan Klein, Daoming Liu, Wojtek Pałubicki, Sören Pirk, Dominik L. Michels
DAE-Net: Deforming Auto-Encoder for fine-grained shape co-segmentation	We present an unsupervised 3D shape co-segmentation method which learns a set of deformable part templates from a shape collection. To accommodate structural variations in the collection, our network composes each shape by a selected subset of template parts which are affine-transformed. To maximize the expressive power of the part templates, we introduce a per-part deformation network to enable the modeling of diverse parts with substantial geometry variations, while imposing constraints on the deformation capacity to ensure fidelity to the originally represented parts. We also propose a training scheme to effectively overcome local minima. Architecturally, our network is a branched autoencoder, with a CNN encoder taking a voxel shape as input and producing per-part transformation matrices, latent codes, and part existence scores, and the decoder outputting point occupancies to define the reconstruction loss. Our network, coined DAE-Net for Deforming Auto-Encoder, can achieve unsupervised 3D shape co-segmentation that yields fine-grained, compact, and meaningful parts that are consistent across diverse shapes. We conduct extensive experiments on the ShapeNet Part dataset, DFAUST, and an animal subset of Objaverse to show superior performance over prior methods. Code and data are available at https://github.com/czq142857/DAE-Net.	https://dl.acm.org/doi/abs/10.1145/3641519.3657528	Zhiqin Chen, Qimin Chen, Hang Zhou, Hao Zhang
Dandelion	In a dystopian world, a coal-burning robot working on a mine one day finds something that he has never seen before - a dandelion.	https://dl.acm.org/doi/abs/10.1145/3641230.3653416	Ling Zhao, Zhengwu Gu
Data Materialized: An Introduction to a Creative Workflow Translating Data into Form	This hands-on course will guide participants through a creative workflow we call data materialization. Data materialization is a sub-genre of data art/data physicalization that utilizes data connected to a concept to assist in generating a 3D form to be fabricated in a meaningful material. Participants will be introduced to computational design fundamentals and foundational elements and principles of design, guided through a workflow that begins by collecting data and culminates in the design and fabrication plans of a designed data object. Tapping into the universal feeling of worry, participants in this course will design meaningful worry bead designs from associated data.	https://dl.acm.org/doi/abs/10.1145/3641236.3664420	Courtney Starrett, Susan Reiser
Deep Fourier-based Arbitrary-scale Super-resolution for Real-time Rendering	As a prevailing tool for effectively reducing rendering costs in many graphical applications, frame super-resolution has seen important progress in recent years. However, most of prior works designed for rendering contents face a common limitation: once a model is trained, it can only afford a single fixed scale. In this paper, we attempt to eliminate this limitation by supporting arbitrary-scale super-resolution for a trained neural model. The key is a Fourier-based implicit neural representation which maps arbitrary and naturally coordinates in the high-resolution spatial domain to valid pixel values. By observing that high-resolution G-buffers possess similar spectrum to high-resolution rendered frames, we design a High-Frequency Fourier Mapping (HFFM) module to recover fine details from low-resolution inputs, without introducing noticeable artifacts. A Low-Frequency Residual Learning (LFRL) strategy is adopted to preserve low-frequency structures and ensure low biasedness caused by network inference. Moreover, different rendering contents are well separated by our spatial-temporal masks derived from G-buffers and motion vectors. Several light-weight designs to the neural network guarantee the real-time performance on a wide range of scenes.	https://dl.acm.org/doi/abs/10.1145/3641519.3657439	Haonan Zhang, Jie Guo, Jiawei Zhang, Haoyu Qin, Zesen Feng, Ming Yang, Yanwen Guo
Deep Hybrid Camera Deblurring for Smartphone Cameras	Mobile cameras, despite their significant advancements, still have difficulty in low-light imaging due to compact sensors and lenses, leading to longer exposures and motion blur. Traditional blind deconvolution methods and learning-based deblurring methods can be potential solutions to remove blur. However, achieving practical performance still remains a challenge. To address this, we propose a learning-based deblurring framework for smartphones, utilizing wide and ultra-wide cameras as a hybrid camera system. We simultaneously capture a long-exposure wide image and short-exposure burst ultra-wide images, and utilize the burst images to deblur the wide image. To fully exploit burst ultra-wide images, we present HCDeblur, a practical deblurring framework that includes novel deblurring networks, HC-DNet and HC-FNet. HC-DNet utilizes motion information extracted from burst images to deblur a wide image, and HC-FNet leverages burst images as reference images to further enhance a deblurred output. For training and evaluating the proposed method, we introduce the HCBlur dataset, which consists of synthetic and real-world datasets. Our experiments demonstrate that HCDeblur achieves state-of-the-art deblurring quality. Codes and datasets are available at https://cg.postech.ac.kr/research/HCDeblur.	https://dl.acm.org/doi/abs/10.1145/3641519.3657507	Jaesung Rim, Junyong Lee, Heemin Yang, Sunghyun Cho
Deep Sketch Vectorization via Implicit Surface Extraction	We introduce an algorithm for sketch vectorization with state-of-the-art accuracy and capable of handling complex sketches. We approach sketch vectorization as a surface extraction task from an unsigned distance field, which is implemented using a two-stage neural network and a dual contouring domain post processing algorithm. The first stage consists of extracting unsigned distance fields from an input raster image. The second stage consists of an improved neural dual contouring network more robust to noisy input and more sensitive to line geometry. To address the issue of under-sampling inherent in grid-based surface extraction approaches, we explicitly predict undersampling and keypoint maps. These are used in our post-processing algorithm to resolve sharp features and multi-way junctions. The keypoint and undersampling maps are naturally controllable, which we demonstrate in an interactive topology refinement interface. Our proposed approach produces far more accurate vectorizations on complex input than previous approaches with efficient running time.	https://dl.acm.org/doi/abs/10.1145/3658197	Chuan Yan, Yong Li, Deepali Aneja, Matthew Fisher, Edgar Simo-Serra, Yotam Gingold
Demonstrating real-time slow-motion experience through parallel video presentation	"Real-time and slow-motion are basically incompatible properties in the technologies of recording and playback. We propose a framework to realize the coexistence of both using parallel video presentation based on the integrative capacity and the temporal nature of cognition. In this framework, camera input is divided into short time durations, distributed into layered timelines, and then stretched out. And then, all layers are composited and presented in a display simultaneously. In our demonstration, users can experience their surroundings through see-through goggles or digital mirrors in which this framework is embedded. Under certain conditions, the perception of the layers is integrated into the cognitive process, resulting in the coexistence of real-time and slow-motion. This research suggests a new field of ""Temporal Editing to Humans."""	https://dl.acm.org/doi/abs/10.1145/3641517.3664399	Goki Muramoto, Hiroto Saito, Sohei Wakisaka, Masahiko Inami
Denoising Monte Carlo Renders with Diffusion Models	Physically-based renderings contain Monte-Carlo noise, with variance that increases as the number of rays per pixel decreases. This noise, while zero-mean for good modern renderers, can have heavy tails (most notably, for scenes containing specular or refractive objects). Learned methods for restoring low fidelity renders are highly developed, because suppressing render noise means one can save compute and use fast renders with few rays per pixel. We demonstrate that a diffusion model can denoise low fidelity renders successfully. Furthermore, our method can be conditioned on a variety of natural render information, and this conditioning helps performance. Quantitative experiments show that our method is competitive with SOTA across a range of sampling rates; qualitative evidence suggests that the image prior applied by a diffusion method strongly favors reconstructions that are like real images, with straight shadow boundaries, curved specularities, and no fireflies. In contrast, existing methods that do not rely on image foundation models struggle to generalize when pushed outside the training distribution.	https://dl.acm.org/doi/abs/10.1145/3641234.3671026	Vaibhav Vavilala, Rahul Vasanth, David Forsyth
Development of Real-Time QA/QC Tools for AEC in Unity	Here, we are focusing on how our real-time visualizations are being used to improve the QA/QC process. While the AEC industry has utilized 3D CAD design software for years, the review process still typically involves commenting on printed 2D drawings or PDFs. We have developed a QA/QC tool that allows our engineers and project managers to review designs in real-time 3D, placing comment markers in 3D space for others to see. Built in Unity, this tool supports viewing and commenting on everything from individual CAD models and components to sprawling miles-long infrastructure projects, with all markup data being stored securely in the cloud for easy access to authorized contributors. This is opposed to the traditional method, which involved building a 3D environment, rendering a video, sending it out for review, then having the review team take screenshots and compile it in a PDF. By allowing reviewers to mark up the 3D environment directly, we are drastically speeding up the iteration process. In addition, users have better insight into the current status of revisions as markups can be configured to different stages of completion as updates are made and the project evolves. We have also added multi-user capabilities – using game engine networking functionality allows for multiple users to be in the tool at once, interacting and communicating within the same environment together in real-time. And our tool is able to be built for multiple platforms, so users can access and interact with it using web, mobile devices, VR, etc. As a result, not only does using a real-time game engine speed up the QA/QC process, it also opens new opportunities for improved communication and collaboration.	https://dl.acm.org/doi/abs/10.1145/3641233.3664728	Philip Luhn, Adam Liss, David Willard
DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation	This paper presents a novel method for exerting fine-grained lighting control during text-driven diffusion-based image generation. While existing diffusion models already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting. Moreover, text prompts lack the necessary expressional power to describe detailed lighting setups. To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene geometry with a homogeneous canonical material under the target lighting. However, the scene geometry needed to produce the radiance hints is unknown. Our key observation is that we only need to guide the diffusion process, hence exact radiance hints are not necessary; we only need to point the diffusion model in the right direction. Based on this observation, we introduce a three stage method for controlling the lighting during image generation. In the first stage, we leverage a standard pretrained diffusion model to generate a provisional image under uncontrolled lighting. Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined diffusion model, named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image. To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet. Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object. We demonstrate and validate our lighting controlled diffusion model on a variety of text prompts and lighting conditions.	https://dl.acm.org/doi/abs/10.1145/3641519.3657396	Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, Xin Tong
DiffCAD: Weakly-Supervised Probabilistic CAD Model Retrieval and Alignment from an RGB Image	Perceiving 3D structures from RGB images based on CAD model primitives can enable an effective, efficient 3D object-based representation of scenes. However, current approaches rely on supervision from expensive yet imperfect annotations of CAD models associated with real images, and encounter challenges due to the inherent ambiguities in the task - both in depth-scale ambiguity in monocular perception, as well as inexact matches of CAD database models to real observations. We thus propose DiffCAD, the first weakly-supervised probabilistic approach to CAD retrieval and alignment from an RGB image. We learn a probabilistic model through diffusion, modeling likely distributions of shape, pose, and scale of CAD objects in an image. This enables multi-hypothesis generation of different plausible CAD reconstructions, requiring only a few hypotheses to characterize ambiguities in depth/scale and inexact shape matches. Our approach is trained only on synthetic data, leveraging monocular depth and mask estimates to enable robust zero-shot adaptation to various real target domains. Despite being trained solely on synthetic data, our multi-hypothesis approach can even surpass the supervised state-of-the-art on the Scan2CAD dataset by 5.9% with 8 hypotheses.	https://dl.acm.org/doi/abs/10.1145/3658236	Daoyi Gao, David Rozenberszki, Stefan Leutenegger, Angela Dai
DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models	The generation of stylistic 3D facial animations driven by speech presents a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. In particular, our style includes the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset are at https://diffposetalk.github.io.	https://dl.acm.org/doi/abs/10.1145/3658221	Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, Yong-Jin Liu
DiffSound: Differentiable Modal Sound Rendering and Inverse Rendering for Diverse Inference Tasks	Accurately estimating and simulating the physical properties of objects from real-world sound recordings is of great practical importance in the fields of vision, graphics, and robotics. However, the progress in these directions has been limited—prior differentiable rigid or soft body simulation techniques cannot be directly applied to modal sound synthesis due to the high sampling rate of audio, while previous audio synthesizers often do not fully model the accurate physical properties of the sounding objects. We propose DiffSound, a differentiable sound rendering framework for physics-based modal sound synthesis, which is based on an implicit shape representation, a new high-order finite element analysis module, and a differentiable audio synthesizer. Our framework can solve a wide range of inverse problems thanks to the differentiability of the entire pipeline, including physical parameter estimation, geometric shape reasoning, and impact position prediction. Experimental results demonstrate the effectiveness of our approach, highlighting its ability to accurately reproduce the target sound in a physics-based manner. DiffSound serves as a valuable tool for various sound synthesis and analysis applications.	https://dl.acm.org/doi/abs/10.1145/3641519.3657493	Xutong Jin, Chenxi Xu, Ruohan Gao, Jiajun Wu, Guoping Wang, Sheng Li
Differentiable Geodesic Distance for Intrinsic Minimization on Triangle Meshes	Computing intrinsic distances on discrete surfaces is at the heart of many minimization problems in geometry processing and beyond. Solving these problems is extremely challenging as it demands the computation of on-surface distances along with their derivatives. We present a novel approach for intrinsic minimization of distance-based objectives defined on triangle meshes. Using a variational formulation of shortest-path geodesics, we compute first and second-order distance derivatives based on the implicit function theorem, thus opening the door to efficient Newton-type minimization solvers. We demonstrate our differentiable geodesic distance framework on a wide range of examples, including geodesic networks and membranes on surfaces of arbitrary genus, two-way coupling between hosting surface and embedded system, differentiable geodesic Voronoi diagrams, and efficient computation of Karcher means on complex shapes. Our analysis shows that second-order descent methods based on our differentiable geodesics outperform existing first-order and quasi-Newton methods by large margins.	https://dl.acm.org/doi/abs/10.1145/3658122	Yue Li, Logan Numerow, Bernhard Thomaszewski, Stelian Coros
Differentiable Voronoi Diagrams for Simulation of Cell-Based Mechanical Systems	Navigating topological transitions in cellular mechanical systems is a significant challenge for existing simulation methods. While abstract models lack predictive capabilities at the cellular level, explicit network representations struggle with topology changes, and per-cell representations are computationally too demanding for large-scale simulations. To address these challenges, we propose a novel cell-centered approach based on differentiable Voronoi diagrams. Representing each cell with a Voronoi site, our method defines shape and topology of the interface network implicitly. In this way, we substantially reduce the number of problem variables, eliminate the need for explicit contact handling, and ensure continuous geometry changes during topological transitions. Closed-form derivatives of network positions facilitate simulation with Newton-type methods for a wide range of per-cell energies. Finally, we extend our differentiable Voronoi diagrams to enable coupling with arbitrary rigid and deformable boundaries. We apply our approach to a diverse set of examples, highlighting splitting and merging of cells as well as neighborhood changes. We illustrate applications to inverse problems by matching soap foam simulations to real-world images. Comparative analysis with explicit cell models reveals that our method achieves qualitatively comparable results at significantly faster computation times.	https://dl.acm.org/doi/abs/10.1145/3658152	Logan Numerow, Yue Li, Stelian Coros, Bernhard Thomaszewski
Diffusion Illusions: Hiding Images in Plain Sight	We explore the problem of computationally generating special images that produce multi-arrangement optical illusions when physically arranged and viewed in a certain way, which we call 'prime' images. First, we propose a formal definition for this problem. Next, we introduce Diffusion Illusions, the first comprehensive pipeline designed to automatically generate a wide range of these multi-arrangement illusions. Specifically, we both adapt the existing 'score distillation loss' and propose a new 'dream target loss' to optimize a group of differentially parametrized prime images, using a frozen text-to-image diffusion model. We study three types of illusions, each where the prime images are arranged in different ways and optimized using the aforementioned losses such that images derived from them align with user-chosen text prompts or images. We conduct comprehensive experiments on these illusions and verify the effectiveness of our proposed method qualitatively and quantitatively. Additionally, we showcase the successful physical fabrication of our illusions — as they are all designed to work in the real world. Code and examples are publicly available at our interactive project website: https://diffusionillusion.github.io/	https://dl.acm.org/doi/abs/10.1145/3641519.3657500	Ryan Burgert, Xiang Li, Abe Leite, Kanchana Ranasinghe, Michael Ryoo
Diffusion Texture Painting	We present a technique that leverages 2D generative diffusion models (DMs) for interactive texture painting on the surface of 3D meshes. Unlike existing texture painting systems, our method allows artists to paint with any complex image texture, and in contrast with traditional texture synthesis, our brush not only generates seamless strokes in real-time, but can inpaint realistic transitions between different textures. To enable this application, we present a stamp-based method that applies an adapted pre-trained DM to inpaint patches in local render space, which is then projected into the texture image, allowing artists control over brush stroke shape and texture orientation. We further present a way to adapt the inference of a pre-trained DM to ensure stable texture brush identity, while allowing the DM to hallucinate infinite variations of the source texture. Our method is the first to use DMs for interactive texture painting, and we hope it will inspire work on applying generative models to highly interactive artist-driven workflows. Code and data for this paper are at github.com/nv-tlabs/DiffusionTexturePainting.	https://dl.acm.org/doi/abs/10.1145/3641519.3657458	Anita Hu, Nishkrit Desai, Hassan Abu Alhaija, Seung Wook Kim, Maria Shugrina
Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion	Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for multiple objects as well as camera's pan and zoom movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. Project page and code are available at https://direct-a-video.github.io/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657481	Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, Jing Liao
Disruptive Critters	Disruptive Critters is a playable interactive audiovisual installation designed to augment live multimedia sound art performances by Duckworth and Hullick (Duckworth Hullick Duo). At a time when artificial intelligence continues to advance, Disruptive Critters provides audiences with a humorous and playful exploration of future creativity and digital disruption through the vocalized human performative possibilities of autonomous computer-generated critters.	https://dl.acm.org/doi/abs/10.1145/3641523.3665174	Jonathan Duckworth, James Hullick, Ross Eldridge
Distance-adaptive unsupervised CNN model for computer-generated holography	Convolutional neural networks (CNN) are useful for overcoming the trade-off between generation speed and accuracy in the process of synthesizing computer-generated holograms (CGH). However, methods using a CNN cannot specify the propagation distance when reproducing a hologram, thereby limiting their practical usage across various contexts. Therefore, in this study, we developed a CNN model that can generate CGH by specifying the target image and propagation distance. The proposed method demonstrates performance comparable to traditional methods with a fixed distance and achieves the generation accuracy and speed necessary for practical use even when the propagation distance is changed.	https://dl.acm.org/doi/abs/10.1145/3641234.3671051	Yuto Asano, Kenta Yamamoto, Tatsuki Fushimi, Yoichi Ochiai
Doptelet Mechanism of Action Animation	This mechanism-of-action animation showcases the treatment of the autoimmune disorder, Immune thrombocytopenia (ITP). This technical and immersive animation shows how the compound may stimulate platelet production without inhibiting native thrombopoietin. This aids in effective platelet generation for chronic ITP patients who may be unresponsive to other treatments.	https://dl.acm.org/doi/abs/10.1145/3641230.3653313	Jack Nelson, Nick Klein, Andrew Swift, Pete Matthews, Emily Drapal
Dragon's Path: Synthesizing User-Centered Flying Creature Animation Paths for Outdoor Augmented Reality Experiences	Advances in augmented reality promise to deliver highly immersive storytelling experiences by animating virtual characters naturally in the real world. However, creating such realistic animated content for viewing in augmented reality is non-trivial and challenging. In this paper, we present a novel approach to automatically generate user-centered flying creature animation paths for outdoor augmented reality experiences. Given a sequence of storyline actions, our approach finds suitable locations for the character to perform its actions via a location compatibility predictor trained with user preferences, synthesizing a corresponding animation path optimized with respect to the user's perspective. We applied our approach to synthesize user-centered augmented reality experiences based on different storyline actions and environments. We also conducted user study experiments to validate the efficacy of our approach for synthesizing desirable augmented reality experiences.	https://dl.acm.org/doi/abs/10.1145/3641519.3657397	Minyoung Kim, Rawan Alghofaili, Changyang Li, Lap-Fai Yu
Draw For Change: We Exist, We Resist	Enter the vibrant world of Maremoto, join her on an afternoon of making street art and experience what it feels like to move as a woman through a metropolis in Mexico, a country where 10 women are murdered every day and tens of thousands go missing.	https://dl.acm.org/doi/abs/10.1145/3641231.3644779	Sara Fatucci, Mariana Cadenas Sangronis, Evelien De Graef, Hanne Phlypo
Drawn In	The world of a busy artist is suddenly turned upside-down when his designs start coming to life.	https://dl.acm.org/doi/abs/10.1145/3641230.3649599	Arthur Collie
DreamFont3D: Personalized Text-to-3D Artistic Font Generation	Text-to-3D artistic font generation aims to assist users for innovative and customized 3D font design by exploring novel concepts and styles. Despite of the advances in the text-to-3D tasks for general objects or scenes, the additional challenge of 3D font generation is to preserve the geometric structures of strokes in an appropriate extent, which determines the generation quality in terms of the recognizability and the local effect control of the 3D fonts. This paper presents a novel approach for text-to-3D artistic font generation, named DreamFont3D, which utilizes multi-view font masks and layout conditions to constrain the 3D font structure and local font effects. Specifically, to enhance the recognizability of 3D fonts, we propose the multi-view mask constraint (MC) to optimize the differentiable 3D representation while preserving the font structure. We also present a progressive mask weighting (MW) module to ensure a trade-off between the text-guided stylization of font effects and the mask-guided preservation of font structure. For precise control over local font effects, we design the multi-view attention modulation (AM) that guides the visual concepts to appear in specific regions according to the provided layout conditions. Compared with existing text-to-3D methods, DreamFont3D shows its own superiority in the consistency between font effects and text prompts, the recognizability, and the localization of font effects. Code and data at https://moonlight03.github.io/DreamFont3D/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657476	Xiang Li, Lei Meng, Lei Wu, Manyi Li, Xiangxu Meng
DreamMat: High-quality PBR Material Generation with Geometry- and Light-aware Diffusion Models	Recent advancements in 2D diffusion models allow appearance generation on untextured raw meshes. These methods create RGB textures by distilling a 2D diffusion model, which often contains unwanted baked-in shading effects and results in unrealistic rendering effects in the downstream applications. Generating Physically Based Rendering (PBR) materials instead of just RGB textures would be a promising solution. However, directly distilling the PBR material parameters from 2D diffusion models still suffers from incorrect material decomposition, such as baked-in shading effects in albedo. We introduce , an innovative approach to resolve the aforementioned problem, to generate high-quality PBR materials from text descriptions. We find out that the main reason for the incorrect material distillation is that large-scale 2D diffusion models are only trained to generate final shading colors, resulting in insufficient constraints on material decomposition during distillation. To tackle this problem, we first finetune a new light-aware 2D diffusion model to condition on a given lighting environment and generate the shading results on this specific lighting condition. Then, by applying the same environment lights in the material distillation, DreamMat can generate high-quality PBR materials that are not only consistent with the given geometry but also free from any baked-in shading effects in albedo. Extensive experiments demonstrate that the materials produced through our methods exhibit greater visual appeal to users and achieve significantly superior rendering quality compared to baseline methods, which are preferable for downstream tasks such as game and film production.	https://dl.acm.org/doi/abs/10.1145/3658170	Yuqing Zhang, Yuan Liu, Zhiyu Xie, Lei Yang, Zhongyuan Liu, Mengzhou Yang, Runze Zhang, Qilong Kou, Cheng Lin, Wenping Wang, Xiaogang Jin
DressCode: Autoregressively Sewing and Generating Garments from Text Guidance	Apparel's significant role in human appearance underscores the importance of garment digitalization for digital human creation. Recent advances in 3D content creation are pivotal for digital human creation. Nonetheless, garment generation from text guidance is still nascent. We introduce a text-driven 3D garment generation framework, DressCode, which aims to democratize design for novices and offer immense potential in fashion design, virtual try-on, and digital human creation. We first introduce SewingGPT, a GPT-based architecture integrating cross-attention with text-conditioned embedding to generate sewing patterns with text guidance. We then tailor a pre-trained Stable Diffusion to generate tile-based Physically-based Rendering (PBR) textures for the garments. By leveraging a large language model, our framework generates CG-friendly garments through natural language interaction. It also facilitates pattern completion and texture editing, streamlining the design process through user-friendly interaction. This framework fosters innovation by allowing creators to freely experiment with designs and incorporate unique elements into their work. With comprehensive evaluations and comparisons with other state-of-the-art methods, our method showcases superior quality and alignment with input prompts. User studies further validate our high-quality rendering results, highlighting its practical utility and potential in production settings. Our project page is https://IHe-KaiI.github.io/DressCode/.	https://dl.acm.org/doi/abs/10.1145/3658147	Kai He, Kaixin Yao, Qixuan Zhang, Jingyi Yu, Lingjie Liu, Lan Xu
Driving Through the Data: Extended Reality Perceptual Support System (XRPSS)	This work introduces the Extended Reality Perceptual Support System (XRPSS) that provides real-time 3D LiDAR sensor data to off-road vehicle drivers with interactive XR for enhancing in-situ spatial awareness in natural environments. The system extends to ex-situ data exploration via motion driving simulators and VR. We created a proof-of-concept system that can both visualize real-time LiDAR point clouds in the field and generate virtual experiences for the motion simulator post-drive.	https://dl.acm.org/doi/abs/10.1145/3641234.3671084	Jay H. Matsushiba, Mia Fitzpatrick, Nicholas Hedley
Drosera Obscura	Drosera Obscura is an interactive installation that explores the evolutionary journey of a carnivorous plant thriving in the Pacific Northwest after humanity's departure. We imagine the remnants of human existence—plastics, metals, and machinery—become integral elements in the life and ecosystem of this resilient organism. This work incorporates aspects of touch, smell, sound, and animatronics to create an immersive experience.	https://dl.acm.org/doi/abs/10.1145/3641521.3664415	Thomas Tucker, Tohm Judson
Dynamic Acousto-Caustics in Dual-Optimized Holographic Fields	"We aims to computationally replicate the dynamic caustics observed in nature, such as the captivating light patterns found at the pool or within flowing rivers. Traditional computer science approaches have only been able to reproduce these light patterns through static fabrication. We introduce ""Dynamic Acousto-Caustics"" a method that merges acoustofluidics with optics to dynamically manipulate light caustics by controlling the shape of liquid surfaces. Using computer-controlled acoustic fields to deform the surface of a liquid medium, we generate dynamic light behaviors in fluid patterns unachievable with static refractive surfaces. This research not only extends the understanding and application of controlled caustics across various technical and creative domains but also exemplifies the intersections made possible at the convergence of multidisciplinary research. While devices exist to generate waves by vibrating water surfaces, they could not produce continuous caustics at specific locations and timings. Employing ultrasonics to deform the water surface allows us to directly manipulate the surface to create more continuous animated patterns. We shape caustics through acoustic field manipulation and optimize the visual outcome emanating from the geometry of these manipulated objects within a dynamic system. Our approach leverages the Digital Twin methodology as an optimization strategy to fine-tune the interplay between sound and light. This enhances the understanding and application of controlled caustics in numerous technical, demonstrating the crossing points enabled by the convergence of interdisciplinary research streams."	https://dl.acm.org/doi/abs/10.1145/3641517.3664384	Koki Nagakura, Tatsuki Fushimi, Ayaka Tsutsui, Yoichi Ochiai
Dynamic Screen Space Textures for Coherent Stylization	"Achieving a watercolor look was an important goal for the style of Walt Disney Animation Studios' ""Wish"", and screen space textures were critical for achieving this, for example to convey a sense of the watercolor paper texture. However, using traditional screen space textures would have resulted in a distracting shower-door effect where the animation appears to swim through the texture. Our novel dynamic screen space textures overcame this problem by tracking animation and camera movement while maintaining the screen space qualities of the texture."	https://dl.acm.org/doi/abs/10.1145/3641233.3664321	Brent Burley, Brian Green, Daniel Teece
EASI-Tex: Edge-Aware Mesh Texturing from Single Image	We present a novel approach for , which employs a diffusion model with judicious conditioning to seamlessly transfer an object's texture from a single RGB image to a given 3D mesh object. We do not assume that the two objects belong to the same category, and even if they do, there can be significant discrepancies in their geometry and part proportions. Our method aims to rectify the discrepancies by conditioning a pre-trained Stable Diffusion generator with edges describing the mesh through ControlNet, and features extracted from the input image using IP-Adapter to generate textures that respect the underlying geometry of the mesh and the input texture without any optimization or training. We also introduce , a novel technique to quickly personalize the diffusion model for a single concept using a , for cases where the pre-trained IP-Adapter falls short in capturing all the details from the input image faithfully. Experimental results demonstrate the efficiency and effectiveness of our edge-aware single-image mesh texturing approach, coined EASI-Tex, in preserving the details of the input texture on diverse 3D objects, while respecting their geometry. Code: https://github.com/sairajk/easi-tex	https://dl.acm.org/doi/abs/10.1145/3658222	Sai Raj Kishore Perla, Yizhi Wang, Ali Mahdavi-Amiri, Hao Zhang
EDAVS: Emotion-Driven Audiovisual Synthesis Experience	In this paper, we proposed a novel approach to the creation of immersive multimedia content. At its core, EDAVS harnesses the subtle inflexions of human speech, translating emotive cues into dynamic visual narratives and corresponding soundscapes. This amalgamation of auditory sentiment and visual splendour is predicated upon cutting-edge machine-learning algorithms capable of deep semantic and emotional analysis.	https://dl.acm.org/doi/abs/10.1145/3641234.3671080	Sihang Chen, Junliang Chen, Xiaojuan Gu
EMSJUMP: Electrical Muscle Stimulation as a Wearable Training Tool for Take-off Phase of Ski Jumping	We developed a wearable Electrical Muscle Stimulation (EMS) system aimed at enhancing kinesthesia and promoting skill acquisition in ski jumping. Through experiments, it was demonstrated that the application of EMS during jumping significantly increased the angular velocity of knee extension and altered perception.	https://dl.acm.org/doi/abs/10.1145/3641234.3671055	Shon Nakamura, Tatsuki Fushimi, Yoichi Ochiai
ENVIZ: DreamWorks GPU-Accelerated Interactive Environment Deformation and Visualization Toolset	ENVIZ is a toolset at DreamWorks Animation that uses the power of the GPU for both interactive deformation and visualization of environments in the 3-D viewport. It replicates the final rendered hi-fidelity motion, and reacts to material shaders and lights to produce a well shaded and lit scene. Meshes with millions of points can be deformed and visualized real-time in the viewport which can help the artist evaluate motion for a shot without having to run costly renders thus providing significant time and resource savings.	https://dl.acm.org/doi/abs/10.1145/3641233.3664324	Arunachalam Somasundaram, Chris Michael
Effective Visual Feedback for Virtual Grasping	The purpose of this study is to examine effectiveness of visual feedback in virtual grasping with a bare hand. We propose two types of visual feedback that can generate a pseudo-haptics and compare them with five types of existing visual feedback. In an experiment, a subject performs a pick-and-drop task, in which a virtual object, which is a sphere with a radius of 5 cm, is grasped, moved and released toward a target. Quantitative and qualitative evaluation by twenty subjects revealed that the one type of the proposed visual feedback helped the subjects perform the task most accurately and the other type of the proposed visual feedback was preferred most in the other visual feedback.	https://dl.acm.org/doi/abs/10.1145/3641234.3671066	Shota Imaizumi, Mie Sato
Efficient Debris-flow Simulation for Steep Terrain Erosion	Erosion simulation is a common approach used for generating and authoring mountainous terrains. While water is considered the primary erosion factor, its simulation fails to capture steep slopes near the ridges. In these low-drainage areas, erosion is often approximated with slope-reducing erosion, which yields unrealistically uniform slopes. However, geomorphology observed that another process dominates the low-drainage areas: erosion by debris flow, which is a mixture of mud and rocks triggered by strong climatic events. We propose a new method to capture the interactions between debris flow and fluvial erosion thanks to a new mathematical formulation for debris flow erosion derived from geomorphology and a unified GPU algorithm for erosion and deposition. In particular, we observe that sediment and debris deposition tend to intersect river paths, which motivates the design of a new, approximate flow routing algorithm on the GPU to estimate the water path out of these newly formed depressions. We demonstrate that debris flow carves distinct patterns in the form of erosive scars on steep slopes and cones of deposited debris competing with fluvial erosion downstream.	https://dl.acm.org/doi/abs/10.1145/3658213	Aryamaan Jain, Bedrich Benes, Guillaume Cordonnier
Efficient Position-Based Deformable Colon Modeling for Endoscopic Procedures Simulation	Current endoscopy simulators oversimplify navigation and interaction within tubular anatomical structures to maintain interactive frame rates, neglecting the intricate dynamics of permanent contact between the organ and the medical tool. Traditional algorithms fail to represent the complexities of long, slender, deformable tools like endoscopes and hollow organs, such as the human colon, and their interaction. In this paper, we address longstanding challenges hindering the realism of surgery simulators, explicitly focusing on these structures. One of the main components we introduce is a new model for the overall shape of the organ, which is challenging to retain due to the complex surroundings inside the abdomen. Our approach uses eXtended Position-Based Dynamics (XPBD) with a Cosserat rod constraint combined with a mesh of tetrahedrons to retain the colon's shape. We also introduce a novel contact detection algorithm for tubular structures, allowing for real-time performance. This comprehensive representation captures global deformations and local features, significantly enhancing simulation fidelity compared to previous works. Results showcase that navigating the endoscope through our simulated colon seemingly mirrors real-world operations. Additionally, we use real-patient data to generate the colon model, resulting in a highly realistic virtual colonoscopy simulation. Integrating efficient simulation techniques with practical medical applications arguably advances surgery simulation realism.	https://dl.acm.org/doi/abs/10.1145/3641519.3657454	Marcelo Martins, Lucas Morais, Rafael Torchelsen, Luciana Nedel, Anderson Maciel
Elemental - Fireplace Flooding FX	In Disney and Pixar's Elemental, I was tasked with developing and executing flood water effects for the fireshop flooding sequence. The effect starts out as water spray emitting from gaps in the door and then grows in intensity as it breaks through the doors and windows with stronger force. This starts to flood the the fireshop and puts the main character Ember, who is made of fire in great peril. The characters eventually end up trapped inside a small reading room with water spouts spraying in from debris blocking the entrance. During the initial stages, we reviewed the storyboards, concept art and the layout staging and decided to group the shots in to 3 sections. The first part involved all shots with the water spouts spraying from the doorway. We created a sequence level effects simulation for the majority of the shots in this group. When the flooding got stronger and more turbulent, we decided to make a multishot Houdini rig to tackle these group of shots. For the water spouts in the reading room, we created sequence level effects simulation similiar to the door spray water. While crafting the effect we made sure to make the flooding feel progressively more and more turbulent and dangerous during the first 2 sections. During this time, we also identified dependencies with character/set animation and other effects like floating props and destruction effects.	https://dl.acm.org/doi/abs/10.1145/3641233.3664344	Amit G Baadkar
Ellic's Exercise Camp: Engaging Children in Physical Activity Through Virtual Reality Gaming	Incorporating virtual reality (VR) games into children's physical activity routines offers an innovative way to promote exercise in a fun and engaging manner. By immersing children in captivating digital worlds that require physical movement to navigate, VR games transform daily sports activities into exciting adventures, as shown in Figure 1. Set within a vibrant cartoon environment, this game features a virtual exercise camp that captivates and draws children in, keeping them engaged and enthusiastic about participating in physical activities. We have crafted several mini-games inspired by various sports activities, each designed to be both fun and easy to learn. Adorable cartoon characters come to life to guide players through the gameplay, enhancing the interactive experience. This immersive approach entertains and encourages children to embrace exercise as a joyful part of their daily routine.	https://dl.acm.org/doi/abs/10.1145/3641521.3664407	Lizhou Cao, Cielo Serna, Huadong Zhang, Chao Peng
EmBelt: A Haptic Device for Extended Reality Experience to Better Understand Body Image Concerns-Related Eating Disorders	This study introduces EmBelt, an innovative haptic device integrated with VR storytelling that creates an extended reality (XR) experience designed to the enhance understanding of eating disorders (EDs) related to body image concern (BIC) among individuals without such anxieties. The system dynamically adjusts compression levels in response to user interactions, providing a portrayal of the pressures linked to the thin ideal and extending related topics. Future directions include potential platform expansion to reach a broader audience and applications in medical treatment, underscoring EmBelt's potential as a tool for various intervention uses.	https://dl.acm.org/doi/abs/10.1145/3641517.3665785	Hsuan Hui Yi, Yi Chun Ko
Emerging Approaches in CG Education Aimed at Enhancing Visual Communication Skills through Reverse Engineering	"The proposed research applies reverse engineering techniques in the field of computer graphics (CG) production education at Japanese vocational schools. It involves the development of a ""Philosophical Observation Decomposition Table"" and a ""Concept Decomposition Table"" to analyze the relationship between words and visuals, along with artistic elements. This methodology is designed to be both educational and enjoyable. Furthermore, the study suggests the utilization of AI technologies, such as ChatGPT, to expand the scope of CG education beyond technical skills, encompassing soft skills like communication and creativity."	https://dl.acm.org/doi/abs/10.1145/3641235.3664430	Harutaka Matsunaga, Kazunori Miyata
Emperor	An interactive and narrative experience which invites the user to travel inside the brain of a father who is suffering from aphasia. Alongside his daughter, we journey into the father's mental space---imagined as a hand-drawn, monochrome landscape---as she seeks to learn more about his inner self, now obscured by illness.	https://dl.acm.org/doi/abs/10.1145/3641231.3649149	Oriane Hurard
Empowering Creativity with Generative AI in Digital Art Education	Artificial intelligence is dramatically changing the creative process for many practices. We see this as an opportunity to enrich student projects within our classroom. We created educational materials and conducted an initial study in the Fall of 2023. The study focuses on the impact that image-based generative AI tools could have on the creative process for students in the 3D Animation classroom. We found that, within our class, most students found AI useful for their productivity, but further work was needed to educate students and to create a safe space for students to explore how these tools can enhance their creative work.	https://dl.acm.org/doi/abs/10.1145/3641235.3664438	Caleb Kicklighter, Jinsil Hwaryoung Seo, Mayet Andreassen, Emily Bujnoch
Empowering Non-Technical Users: Building Paper Circuit Boards with Zone-Based Circuit Stickers	We introduce a user-friendly method for creating intricate paper circuit boards. Complex paper circuits are typically printed using conductive ink, but the expertise and experience required for this technique can be a barrier to entry for many. Our solution involves the use of zone-based circuit stickers, which empower individuals with little to no technical background to construct intricate paper circuits. We made those stickers from copper tape using a household vinyl cutter. In the presentation and demo, we will provide a detailed overview of the design process for zone-based circuits, along with insights from various design iterations. This work is an important step in democratizing the small-batch production of complex circuit boards, making them more approachable and flexible for a wider audience.	https://dl.acm.org/doi/abs/10.1145/3641236.3664425	Ruhan Yang, Ellen Yi-Luen Do
Enhancing Narratives with SayMotion's text-to-3D animation and LLMs	SayMotion, a generative AI text-to-3D animation platform, utilizes deep generative learning and advanced physics simulation to transform text descriptions into realistic 3D human motions for applications in gaming, extended reality (XR), film production, education and interactive media. SayMotion addresses challenges due to the complexities of animation creation by employing a Large Language Model (LLM) fine-tuned to human motion with further AI-based animation editing components including spatial-temporal Inpainting via a proprietary Large Motion Model (LMM). SayMotion is a pioneer in the animation market by offering a comprehensive set of AI generation and AI editing functions for creating 3D animations efficiently and intuitively. With an LMM at its core, SayMotion aims to democratize 3D animations for everyone through language and generative motion.	https://dl.acm.org/doi/abs/10.1145/3641520.3665309	Kevin He, Annette Lapham, Zenan Li
Enhancing Radiography Education through Co-located Mixed Reality with Tracker-based X-ray Imaging Simulation	This poster proposes a co-located collaborative Mixed Reality (MR) teaching system to enhance Veterinary Radiography training. By integrating real X-ray equipment with virtual representations of horses, the system simulates radiographic procedures effectively. The cost-effective setup combines real equipment with virtual annotations, such as collimation beams, to achieve synchronized virtual objects and generate real-time radiography. This innovative approach enables instructors to demonstrate equine techniques and assess students' proficiency more effectively. An expert review study was conducted to gather feedback and evaluate user experience related to this novel teaching approach.	https://dl.acm.org/doi/abs/10.1145/3641234.3671064	Xuanhui Xu, Antonella Puggioni, David Kilroy, Abraham Campbell
Enhancing VR Customer Service Training: A System for Generating Customer Queries and Evaluating Trainee Responses	Virtual customer service training using avatars eliminates the need for physical facilities, reducing costs and enabling remote participation. However, the challenge lies in the authoring cost of preparing service specific question and the necessity of experienced instructors. To address this, we employ LLM technology to generate service specific question and evaluate trainee's answer mitigating authoring costs. Despite LLMs having general knowledge limitations, we integrate RAG to imbue them with specific service knowledge. This innovation facilitates the generation of service specific questions. Furthermore, we are developing a self-training system where RAG assesses the correctness of trainee answers, enabling independent learning (without an instructor). This paper discusses the issues faced and insights gained during the development of our system.	https://dl.acm.org/doi/abs/10.1145/3641234.3671058	Takenori Hara
Enhancing the Digital Inheritance and Embodied Experience of Zen based on Multimodal Mixed Reality System	Zen is an important part of the world's intangible cultural heritage (ICH) but is facing difficulties nowadays. We present an immersive multimodal system for audiences to meditate and experience Zen by integrating AI, natural interaction, and mixed reality (MR). The evaluation (N = 66) shows that the system provides the audience with a high-quality sensory experience, enhancing their engagement and empathy. This approach brings new insights into the computer graphic (CG) and ICH community.	https://dl.acm.org/doi/abs/10.1145/3641234.3671076	Wenchen Guo, Wenbo Zhao, Guoyu Sun, Yanni Li, Yangyi Ye, Su Wang
Errances	"""Errances"" is a VR trip through a desolate & majestic world aboard a futuristic train. Through electronic music by Ryvage, this immersive meditation reveals our technological choices and consequences."	https://dl.acm.org/doi/abs/10.1145/3641231.3646968	Gwenael François
Eulerian-Lagrangian Fluid Simulation on Particle Flow Maps	We propose a novel Particle Flow Map (PFM) method to enable accurate long-range advection for incompressible fluid simulation. The foundation of our method is the observation that a particle trajectory generated in a forward simulation naturally embodies a perfect flow map. Centered on this concept, we have developed an Eulerian-Lagrangian framework comprising four essential components: Lagrangian particles for a natural and precise representation of bidirectional flow maps; a dual-scale map representation to accommodate the mapping of various flow quantities; a particle-to-grid interpolation scheme for accurate quantity transfer from particles to grid nodes; and a hybrid impulse-based solver to enforce incompressibility on the grid. The efficacy of PFM has been demonstrated through various simulation scenarios, highlighting the evolution of complex vortical structures and the details of turbulent flows. Notably, compared to NFM, PFM reduces computing time by up to 49 times and memory consumption by up to 41%, while enhancing vorticity preservation as evidenced in various tests like leapfrog, vortex tube, and turbulent flow.	https://dl.acm.org/doi/abs/10.1145/3658180	Junwei Zhou, Duowen Chen, Molin Deng, Yitong Deng, Yuchen Sun, Sinan Wang, Shiying Xiong, Bo Zhu
Evolving a Testsuite for Shading and Rendering	This talk will present some of the software testing methodologies used within the back-end divisions (rendering, shading etc) of Sony Pictures Imageworks. A key focus will be the renderingtestsuite, an image-based testing system. We will delve into its evolution, and discuss the challenges of large-scale image-based testing.	https://dl.acm.org/doi/abs/10.1145/3641233.3664350	Daniel Greenstein
Expansive Field-of-View Head-Mounted Display based on Dynamic Projection Mapping	In this study, we propose a method for extending the peripheral field of view of conventional head-mounted displays (HMDs) based on dynamic projection mapping. External projectors are used to dynamically project images onto peripheral screens attached to the periphery of the HMD. A wide field of view is achieved without significantly increasing the HMD weight.	https://dl.acm.org/doi/abs/10.1145/3641234.3671021	Naoki Hashimoto, Kazuto Saito
Exploring Embodied Interactive Techniques: with Touchdesigner and Mediapipe	"Creating embodied interactive experiences has historically required a host of custom hardware sensors. With developments in computer vision and accessible software solutions it is now possible to create a variety of interactive experiences using only a webcam and laptop. Our hands-on lab, ""Exploring Embodied Interactive Techniques: with TouchDesigner and MediaPipe,"" delves into these possibilities. Throughout the lab, participants will be introduced to TouchDesigner and MediaPipe, alongside other computer vision and AI techniques. Techniques covered include, Hand / Gesture Tracking, Facial Landmark Detection, Pose tracking, and Object Tracking. This mix of techniques highlights the diverse set of inputs and outputs available to creators. Through demonstrations and activities, we will showcase the potential applications of these tools in creating interactive experiences. Participants will gain practical skills and knowledge with these freely available tools. Furthermore, examples and case studies from the presenters will add real world context to the presentation."	https://dl.acm.org/doi/abs/10.1145/3641236.3665164	Michael Bruner
Exploring the Application of Multi-Sensory Device Interaction in Traditional Chinese Medicine Popular Science Education	"The origins of traditional Chinese medicine (TCM) span thousands of years, rooted in extensive medical and life experiences. Central to TCM is herbal medicine, integrating natural science principles. However, accessibility to fresh herbs is limited due to growth conditions, regulations, and transportation costs. To address this, we propose the ""TCM Herbal Education"" system, merging 3D scanning, interactive VR, and olfactory devices to educate on Chinese herbal science multidisciplinarily. It offers insights into cultivation, pest control, prescriptions, and herbal aromas. By virtualizing traditional education, users interact with herbs via VR, enhancing understanding through sensory experiences. Exploratory research evaluates and refines the system for improved cross-reality experiences."	https://dl.acm.org/doi/abs/10.1145/3641234.3671027	Chin-Chun Chen, Tung-Hua Yang, Cheng-Ta Yang, Chi-Yu Lin, Ping-Hsuan Han, Si-Han Chen, Yi-Ru Yang, Ching Hui Chung
Exploring the Dichotomy of Nature and Tech Through an Interactive Media Assignment	In this submission I present a mixed media assignment that asks students to challenge the dichotomy of nature and technology. For this assignment, students use animation, 3D modeling, and physical making to create an interactive piece that embodies the aesthetics of nature through visuals and sound or visuals and touch. The primary goal of this assignment is to strengthen a connectedness to nature through creativity, while also challenging the students by encouraging mixing media in creative ways. The current description of this assignment is open and flexible; however, it can also be focused on a specific discipline (e.g. animation or game development) or scaled by adjusting the scope of the final deliverable. This paper will give a brief overview of the assignment including the prompt, process, materials, and examples of student work.	https://dl.acm.org/doi/abs/10.1145/3641235.3664443	Justin Johnson
Extensions of HyperPose: Echo, Savant, Loop	This presentation introduces several solutions which came as byproduct from our work on HyperPose. Echo is a technique for applying stylistic modifications to select poses in character animation, and extending these effects to unaltered data. Savant is a novel approach to generating runtime secondary motion for character cloth, hair, muscles, props, etc. Loop describes an improved method for mocap aimed to serve next generation of realism in character animation	https://dl.acm.org/doi/abs/10.1145/3641233.3664310	Alex Bereznyak, Mehdi Farrokhtala
EyeIR: Single Eye Image Inverse Rendering In the Wild	We propose a method to decompose a single eye region image in the wild into albedo, shading, specular, normal and illumination. This inverse rendering problem is particularly challenging due to inherent ambiguities and complex properties of the natural eye region. To address this problem, first we construct a synthetic eye region dataset with rich diversity. Then we propose a synthetic to real adaptation framework to leverage the supervision signals from synthetic data to guide the direction of self-supervised learning. We design region-aware self-supervised losses based on image formation and eye region intrinsic properties, which can refine each predicted component by mutual learning and reduce the artifacts caused by ambiguities of natural eye images. Particularly, we address the demanding problem of specularity removal in the eye region. We show high-quality inverse rendering results of our method and demonstrate its use for a number of applications.	https://dl.acm.org/doi/abs/10.1145/3641519.3657506	Shijun Liang, Haofei Wang, Feng Lu
FEELTECH Wear : Enhancing Mixed Reality Experience with Wrist to Finger Haptic Attribution	FEEL TECH Wear is a system that facilitates haptic interactions while keeping most of the palm free, by presenting directional force through rotational skin-stretch distribution feedback to the wrist and providing texture sensation through vibration feedback to the fingertips. With advancements in hand tracking and passthrough technologies, hand interactions in Mixed Reality (MR) environments have become more accessible, necessitating palm-free haptic feedback methods that do not hinder interactions with real objects or impair vision-based hand tracking. The hardware of FEEL TECH Wear primarily consists of two components: a hand-mounted device for each hand and a control unit located at the back of the head. The hand-mounted device is equipped with four channels of rotational skin-stretch tactors at the wrist and vibration tactors at the thumb and index finger. Using FEEL TECH Wear, three applications have been realized: haptic feedback for virtual objects, haptic augmentation for real objects, and haptic guidance towards objects.	https://dl.acm.org/doi/abs/10.1145/3641517.3664395	Rodan Umehara, Harunobu Taguchi, Arata Horie, Yusuke Kamiyama, Shin Sakamoto, Hironori Ishikawa, Kouta Minamizawa
FIRE: Mid-Air Thermo-Tactile Display	We demonstrate an ultrasound haptic-based mid-air thermo-tactile display system, designed as a proof-of-concept with an open-top chamber, heat modules, and an ultrasound haptic display. Our method involves directing heated airflow toward the focused pressure point produced by the ultrasound display, delivering thermal and tactile cues in mid-air simultaneously. We showcase our system across four different VR environments—campfire, water fountain, kitchen, and candle—to illustrate the rich user experiences that result from integrating thermal and tactile feedback.	https://dl.acm.org/doi/abs/10.1145/3641517.3664396	Yatharth Singhal, Haokun Wang, Jin Ryong Kim
Fabric Tessellation: Realizing Freeform Surfaces by Smocking	We present a novel method for realizing freeform surfaces with pieces of flat fabric, where curvature is created by stitching together points on the fabric using a technique known as Smocking is renowned for producing intricate geometric textures with voluminous pleats. However, it has been mostly used to realize flat shapes or manually designed, limited classes of curved surfaces. Our method combines the computation of directional fields with continuous optimization of a Tangram graph in the plane, which together allow us to realize surfaces of arbitrary topology and curvature with smocking patterns of diverse symmetries. Given a target surface and the desired smocking pattern, our method outputs a corresponding 2D smocking pattern that can be fabricated by sewing specified points together. The resulting textile fabrication approximates the target shape and exhibits visually pleasing pleats. We validate our method through physical fabrication of various smocked examples.	https://dl.acm.org/doi/abs/10.1145/3658151	Aviv Segall, Jing Ren, Amir Vaxman, Olga Sorkine-Hornung
Fabricable 3D Wire Art	This paper presents a computational method for automatically creating fabricable 3D wire sculptures from various input modalities, including 3D models, images, and even text. There are several challenges to wire art creation. For example, artists must express the desired visual as a sparse wire representation. It is also difficult to manually bend wires in the air without guidance to fabricate the designed 3D curves. Our workflow solves these challenges by using two core techniques. First, we present an algorithm that automatically generates a fabricable 3D curve representation of the target based on a loss function that measures the semantic distance between the rendered curve and the target. The loss function can be defined using different pre-trained vision-language neural networks to generate wire art from different input types. The loss function is then optimized using differentiable rendering specifically targeting 3D parametric curves. Our method can incorporate various fabrication constraints on the wire as additional regularization terms in the optimization process. Second, we present an algorithm to generate a 3D printable jig structure that can be used to fabricate the generated wire path. The major challenge in the jig generation stems from the design of an intersection-free surface mesh for 3D printing, which we address with our inflation algorithm. The experimental results indicate that our method can handle a wider range of input types and can produce physically fabricable wire shapes compared to previous wire generation methods. Various wire arts have been fabricated using our 3D-printed jig to demonstrate its effectiveness in 3D wire bending.	https://dl.acm.org/doi/abs/10.1145/3641519.3657453	Kenji Tojo, Ariel Shamir, Bernd Bickel, Nobuyuki Umetani
Fabricating Haptic Paper Interface	Our research focuses on developing haptic feedback mechanisms for paper interfaces. We present a comprehensive overview of our efforts and introduce approaches to incorporating haptic feedback into paper interfaces. Traditional paper interfaces primarily emphasize interactive features and often disregard haptic feedback, which is critical for enhancing user confidence in operation. Our research investigates how cardboard structures and static magnets can be utilized to incorporate haptic feedback features into paper interfaces. These techniques help to enhance the user experience associated with paper interfaces.	https://dl.acm.org/doi/abs/10.1145/3641234.3671052	Ruhan Yang, Ellen Yi-Luen Do
Factorized Motion Fields for Fast Sparse Input Dynamic View Synthesis	Designing a 3D representation of a dynamic scene for fast optimization and rendering is a challenging task. While recent explicit representations enable fast learning and rendering of dynamic radiance fields, they require a dense set of input viewpoints. In this work, we focus on learning a fast representation for dynamic radiance fields with sparse input viewpoints. However, the optimization with sparse input is under-constrained and necessitates the use of motion priors to constrain the learning. Existing fast dynamic scene models do not explicitly model the motion, making them difficult to be constrained with motion priors. We design an explicit motion model as a factorized 4D representation that is fast and can exploit the spatio-temporal correlation of the motion field. We then introduce reliable flow priors including a combination of sparse flow priors across cameras and dense flow priors within cameras to regularize our motion model. Our model is fast, compact and achieves very good performance on popular multi-view dynamic scene datasets with sparse input viewpoints. The source code for our model can be found on our project page: https://nagabhushansn95.github.io/publications/2024/RF-DeRF.html.	https://dl.acm.org/doi/abs/10.1145/3641519.3657498	Nagabhushan Somraj, Kapil Choudhary, Sai Harsha Mupparaju, Rajiv Soundararajan
FactoryDecoder: Expertise-Free Digital Twin Generation and Modification Tool	Digital twins are essential visualization applications in the manufacturing field. However, their development requires specialized 3D engineers, which often complicates modifications during the operation stage. To address this complexity, we introduce FactoryDecoder, a development tool for non-expert users in 3D engineering to generate and modify digital twins using natural language inputs. FactoryDecoder converts users' descriptions of production line into hierarchical asset codes, facilitating the automated layout and simplified modification of digital twins. Furthermore, if the system finds that the 3D asset library lacks appropriate device representations, it will automatically use a 3D mesh generator to create new ones. We evaluate the performance of large language models (LLMs) to optimize FactoryDecoder's capabilities. Preliminary user studies highlight FactoryDecoder's effectiveness.	https://dl.acm.org/doi/abs/10.1145/3641234.3671031	Jiachun Du, Hanjin Zhong, Liang Zhou, Jianye Li
Familiar Feelings: Emotion Look Development on Pixar's Inside Out 2	The emotion characters on Pixar's Inside Out (2015) were composed of multiple elements that gave them an ethereal look. They were an ingenious composite of a core volume that behaved and illuminated as a glowing surface, moving particles that hovered over it, an edge volume tinted differently based on the lighting direction, and strands of dots that resembled hairs from a distance but sparkles up close. On Pixar's Inside Out 2 (2024) we had the challenging task of bringing these well known, technically complex characters back to life. We recreated the core five emotions, straddling the delicate balance of upgrading them to take advantage of newer technology while still maintaining their familiar look. We also created a whole new cast of emotions for Riley's adolescent mind.	https://dl.acm.org/doi/abs/10.1145/3641233.3665159	Ana Lacaze, Ben Porter, Jacob Kuenzel, Te Hu, Masha Ellsworth, Athena Xenakis, Markus Kranzler, Alexis Angelidis
Fate of the Minotaur: A scalable location based VR experience	"Within our narrative location-based VR experience ""Fate of the Minotaur"", the players embody the role of human sacrifices from Athens who are sent into the labyrinth of the Minotaur by Minos, King of Crete. The players learn about the tragic family story behind the ancient Greek myth and have to pick a side by either killing the Minotaur or sparing the troubled creature's life. In a unique approach, the content can be experienced in different immersive scale levels, depending on the technical and physical limitations of the location presenting the experience. From a technical perspective, a novel engine-agnostic and flexible open-source virtual production framework was used to realise the multiplayer network part of the game. Our non-photorealistic visual approach is inspired by ancient Greek murals and vases, allowing us to provide the experience with a small footprint in energy consumption and required equipment."	https://dl.acm.org/doi/abs/10.1145/3641521.3664406	Andreas Dahn, Leszek Plichta, Simon Spielmann, Eduard Schäfer, Justus Blönnigen
Filter-Guided Diffusion for Controllable Image Generation	Recent advances in diffusion-based generative models have shown incredible promise for zero shot image-to-image translation and editing. Most of these approaches work by combining or replacing network-specific features used in the generation of new images with those taken from the inversion of some guide image. Methods of this type are considered the current state-of-the-art in training-free approaches, but have some notable limitations: they tend to be costly in runtime and memory, and often depend on deterministic sampling that limits variation in generated results. We propose Filter-Guided Diffusion (FGD), an alternative approach that leverages fast filtering operations during the diffusion process to support finer control over the strength and frequencies of guidance and can work with non-deterministic samplers to produce greater variety. With its efficiency, FGD can be sampled over multiple seeds and hyperparameters in less time than a single run of other SOTA methods to produce superior results based on structural and semantic metrics. We conduct extensive quantitative and qualitative experiments to evaluate the performance of FGD in translation tasks and also demonstrate its potential in localized editing when used with masks.	https://dl.acm.org/doi/abs/10.1145/3641519.3657489	Zeqi Gu, Ethan Yang, Abe Davis
FlexScale: Modeling and Characterization of Flexible Scaled Sheets	We present a computational approach for modeling the mechanical behavior of flexible scaled sheet materials---3D-printed hard scales embedded in a soft substrate. Balancing strength and flexibility, these structured materials find applications in protective gear, soft robotics, and 3D-printed fashion. To unlock their full potential, however, we must unravel the complex relation between scale pattern and mechanical properties. To address this problem, we propose a contact-aware homogenization approach that distills native-level simulation data into a novel macromechanical model. This macro-model combines piecewise-quadratic uniaxial fits with polar interpolation using circular harmonics, allowing for efficient simulation of large-scale patterns. We apply our approach to explore the space of isohedral scale patterns, revealing a diverse range of anisotropic and nonlinear material behaviors. Through an extensive set of experiments, we show that our models reproduce various scale-level effects while offering good qualitative agreement with physical prototypes on the macro-level.	https://dl.acm.org/doi/abs/10.1145/3658175	Juan Sebastian Montes Maestre, Yinwei Du, Ronan Hinchet, Stelian Coros, Bernhard Thomaszewski
Flexible Motion In-betweening with Diffusion Models	Motion in-betweening, a fundamental task in character animation, consists of generating motion sequences that plausibly interpolate user-provided keyframe constraints. It has long been recognized as a labor-intensive and challenging process. We investigate the potential of diffusion models in generating diverse human motions guided by keyframes. Unlike previous inbetweening methods, we propose a simple unified model capable of generating precise and diverse motions that conform to a flexible range of user-specified spatial constraints, as well as text conditioning. To this end, we propose Conditional Motion Diffusion In-betweening (CondMDI) which allows for arbitrary dense-or-sparse keyframe placement and partial keyframe constraints while generating high-quality motions that are diverse and coherent with the given keyframes. We evaluate the performance of CondMDI on the text-conditioned HumanML3D dataset and demonstrate the versatility and efficacy of diffusion models for keyframe in-betweening. We further explore the use of guidance and imputation-based approaches for inference-time keyframing and compare CondMDI against these methods.	https://dl.acm.org/doi/abs/10.1145/3641519.3657414	Setareh Cohan, Guy Tevet, Daniele Reda, Xue Bin Peng, Michiel van de Panne
FlowZen: Using Hybrid-Haptic and Particle for Enhancing Immersive Experience via Continuous Illusion of Wind	With the advancement of haptic technology, we can enable a haptic experience to simulate environmental changes seamlessly, even as the user moves around in the immersive virtual reality world. For instance, stationary non-contact tactile devices, such as electric fans or heat lights, can be utilized to replicate wind and heat sensations in the virtual environment. While these stationary devices can effectively provide a full-body experience, the impact diminishes quickly with distance. Conversely, portable devices can provide better experience, but the haptic feedback might affect only partial body areas and the direction of source is limited. Therefore, taking advantage of both portable and stationary devices to provide a complementary effect is a potential approach. We present FlowZen, a hybrid-haptic system with particle effect that enhances wind and thermal sensations through tactile illusion, integrating handheld haptic devices with contact haptic feedback and a stationary device with non-contact feedback. In our approach, visual particle effects are employed to connect the feedback from both haptic devices.	https://dl.acm.org/doi/abs/10.1145/3641521.3664400	Yang-Sheng Chen, Grace Theodore, Jing-Yuan Huang, Chiao-En Hsieh, Ping-Hsuan Han, Yi-Ping Hung
Fluid Control with Laplacian Eigenfunctions	Physics-based fluid control has long been a challenging problem in balancing efficiency and accuracy. We introduce a novel physics-based fluid control pipeline using Laplacian Eigenfluids. Utilizing the adjoint method with our provided analytical gradient expressions, the derivative computation of the control problem is efficient and easy to formulate. We demonstrate that our method is fast enough to support real-time fluid simulation, editing, control, and optimal animation generation. Our pipeline naturally supports multi-resolution and frequency control of fluid simulations. The effectiveness and efficiency of our fluid control pipeline are validated through a variety of 2D examples and comparisons.	https://dl.acm.org/doi/abs/10.1145/3641519.3657468	Yixin Chen, David Levin, Timothy Langlois
Fractal World Building: Infinitely Engaging Experience Design at Meow Wolf	Meow Wolf is a Santa Fe-based arts and entertainment group that creates unforgettable immersive and interactive experiences that transport audiences of all ages into fantastic realms of story and exploration.	https://dl.acm.org/doi/abs/10.1145/3664475.3675000	Derek Pendergrass, Tasia Miles, Eric Heep
Freelance	A young hopeful knight reports for duty at the kings court. Little does he know that slaying dragons might not be his biggest challenge.	https://dl.acm.org/doi/abs/10.1145/3641230.3650721	Magnus Igland Møller, Peter Smith, Luciano A. Munos Sessarego
From microfacets to participating media: A unified theory of light transport with stochastic geometry	Stochastic geometry models have enjoyed immense success in graphics for modeling interactions of light with complex phenomena such as participating media, rough surfaces, fibers, and more. Although each of these models operates on the same principle of replacing intricate geometry by a random process and deriving the average light transport across all instances thereof, they are each tailored to one specific application and are fundamentally distinct. Each type of stochastic geometry present in the scene is firmly encapsulated in its own appearance model, with its own statistics and light transport average, and no cross-talk between different models or deterministic and stochastic geometry is possible. In this paper, we derive a theory of light transport on , a geometry model capable of expressing deterministic geometry, microfacet surfaces, participating media, and an exciting new continuum in between containing aggregate appearance, non-classical media, and more. Our model naturally supports , missing from most existing stochastic models. Our theory paves the way for tractable rendering of scenes in which all geometry is described by the same stochastic model, while leaving ample future work for developing efficient sampling and rendering algorithms.	https://dl.acm.org/doi/abs/10.1145/3658121	Dario Seyb, Eugene d'Eon, Benedikt Bitterli, Wojciech Jarosz
GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis	We introduce GEM3D 1 – a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the state-of-the-art, also involving challenging scenarios of reconstructing and synthesizing structurally complex, high-genus shape surfaces from Thingi10K and ShapeNet.	https://dl.acm.org/doi/abs/10.1145/3641519.3657415	Dmitry Petrov, Pradyumn Goyal, Vikas Thamizharasan, Vladimir Kim, Matheus Gadelha, Melinos Averkiou, Siddhartha Chaudhuri, Evangelos Kalogerakis
GaussianPrediction: Dynamic 3D Gaussian Prediction for Motion Extrapolation and Free View Synthesis	Forecasting future scenarios in dynamic environments is essential for intelligent decision-making and navigation, a challenge yet to be fully realized in computer vision and robotics. Traditional approaches like video prediction and novel-view synthesis either lack the ability to forecast from arbitrary viewpoints or to predict temporal dynamics. In this paper, we introduce GaussianPrediction, a novel framework that empowers 3D Gaussian representations with dynamic scene modeling and future scenario synthesis in dynamic environments. GaussianPrediction can forecast future states from any viewpoint, using video observations of dynamic scenes. To this end, we first propose a 3D Gaussian canonical space with deformation modeling to capture the appearance and geometry of dynamic scenes, and integrate the lifecycle property into Gaussians for irreversible deformations. To make the prediction feasible and efficient, a concentric motion distillation approach is developed by distilling the scene motion with key points. Finally, a Graph Convolutional Network is employed to predict the motions of key points, enabling the rendering of photorealistic images of future scenarios. Our framework shows outstanding performance on both synthetic and real-world datasets, demonstrating its efficacy in predicting and rendering future environments. Code is available on the project webpage: https://zju3dv.github.io/gaussian-prediction.	https://dl.acm.org/doi/abs/10.1145/3641519.3657417	Boming Zhao, Yuan Li, Ziyu Sun, Lin Zeng, Yujun Shen, Rui Ma, Yinda Zhang, Hujun Bao, Zhaopeng Cui
GenUSD: 3D scene generation made easy	We introduce GenUSD, an end-to-end text-to-scene generation framework that transforms natural language queries into realistic 3D scenes, including 3D objects and layouts. The process involves two main steps: 1) A Large Language Model (LLM) generates a scene layout hierarchically. It first proposes a high-level plan to decompose the scene into multiple functionally and spatially distinct subscenes. Then, for each subscene, the LLM proposes objects with detailed positions, poses, sizes, and descriptions. To manage complex object relationships and intricate scenes, we introduce object layout design meta functions as tools for the LLM. 2) A novel text-to-3D model generates each 3D object with surface meshes and high-resolution texture maps based on the LLM's descriptions. The assembled 3D assets form the final 3D scene, represented as a Universal Scene Description (USD) format. GenUSD ensures physical plausibility by incorporating functions to prevent collisions.	https://dl.acm.org/doi/abs/10.1145/3641520.3665306	Tsung-Yi Lin, Chen-Hsuan Lin, Yin Cui, Yunhao Ge, Seungjun Nah, Arun Mallya, Zekun Hao, Yifan Ding, Hanzi Mao, Zhaoshuo Li, Yen-Chen Lin, Xiaohui Zeng, Qinsheng Zhang, Donglai Xiang, Qianli Ma, J.P. Lewis, Jingyi Jin, Pooya Jannaty, Ming-Yu Liu
Gender Diversity of Graphics Conference Leadership	Non-male identifying researchers, including women and non-binary individuals, are underrepresented in leadership roles within computer graphics conferences. Analyzing data on these roles over time across key conferences highlights this disparity, while also identifying positive trends and worrisome constants. The goal of the following data collection is to provide grounded statistics to inform future decision making. Recognizing even small successes from the past may inspire and accelerate future improvement. The academic community's recent failure to sufficiently address sexual harassment and malevolent behavior from members of our community (e.g. [Obr 2023]) is an urgent reminder to improve. This is admittedly arm-chair statistics. Anyone with the internet could collect this data and create the following statistics and plots. Nevertheless, I would like to be thorough about the methodology, largely replicating Graesser et al. [2021], who examined robotics.	https://dl.acm.org/doi/abs/10.1145/3641233.3664309	Alec Jacobson
Generative AI for 2D Character Animation	In this pilot project, we teamed up with artists to develop new workflows for 2D animation while producing a short educational cartoon. We created several workflows streamlining the animation process, bringing the artists' vision to the screen more effectively.	https://dl.acm.org/doi/abs/10.1145/3641234.3671054	Jaime Guajardo, Ozgun Bursalioglu, Dan B Goldman
Generative Escher Meshes	This paper proposes a fully-automatic, text-guided generative method for producing perfectly-repeating, periodic, tile-able 2D imagery, such as the one seen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to square texture images that are seamless when tiled, our method generates non-square tilings which comprise solely of repeating copies of the same object. It achieves this by optimizing both geometry and texture of a 2D mesh, yielding a non-square tile in the shape and appearance of the desired object, with close to no additional background details, that can tile the plane without gaps nor overlaps. We enable optimization of the tile's shape by an unconstrained, differentiable parameterization of the space of all valid tileable meshes for given boundary conditions stemming from a symmetry group. Namely, we construct a differentiable family of linear systems derived from a 2D mesh-mapping technique - Orbifold Tutte Embedding - by considering the mesh's Laplacian matrix as differentiable parameters. We prove that the solution space of these linear systems is exactly all possible valid tiling configurations, thereby providing an end-to-end differentiable representation for the entire space of valid tiles. We render the textured mesh via a differentiable renderer, and leverage a pre-trained image diffusion model to induce a loss on the resulting image, updating the mesh's parameters so as to make its appearance match the text prompt. We show our method is able to produce plausible, appealing results, with non-trivial tiles, for a variety of different periodic tiling patterns.	https://dl.acm.org/doi/abs/10.1145/3641519.3657452	Noam Aigerman, Thibault Groueix
Generative Models for Visual Content Editing and Creation	Welcome to the SIGGRAPH 2024 Course on Generative Models for Visual Content Editing and Creation! In this course, you will embark on an exciting journey into the realm of generative models and their groundbreaking applications in computer graphics. Over the duration of this course, you will gain a comprehensive understanding of generative models and diffusion models, explore fundamental machine learning and deep learning techniques, and discover cutting-edge applications for high-fidelity image synthesis, video generation, 3D content creation, and more. Here's what you can expect to learn:	https://dl.acm.org/doi/abs/10.1145/3664475.3664553	Anyi Rao, Yuanbo Xiangli, Yuwei Guo, Mia Tang, Chenlin Meng, Maneesh Agrawala
Geometry enhanced 3D Gaussian Splatting for high quality deferred rendering	Reconstructing and rendering the real world with high visual fidelity is crucial for VR applications. 3D Gaussian Splatting (GS) method offers high-quality novel views but struggles with realistic shadows under complex lighting, including both directional lighting and point lighting. We proposed a novel geometry enhanced 3D GS method, in which 3D Gaussians are learned efficiently with additional attributes of both normal and depth. The proposed representation enhances high-fidelity novel view rendering and integrates seamlessly into commercial real-time engines. We developed a deferred rendering pipeline for rasterization, enabling real-time complex illumination effects with high-precision depth and normal attributes learned by our 3D GS method. Our pipeline surpasses previous 3D GS renderers in accurate illumination, shadows, and directional lighting. Applied to VR applications like live broadcasting, it generates immersive virtual environments in real-time on consumer devices. Experimental results show our method delivers superior realistic rendering at real-time speeds, benefiting numerous VR applications.	https://dl.acm.org/doi/abs/10.1145/3641234.3671044	Shuo Wang, Cong Xie, Shengdong Wang, Shaohui Jiao
Going with the Flow	Given a sequence of poses of a body we study the motion resulting when the body is immersed in a (possibly) moving, incompressible medium. With the poses given, say, by an animator, the governing second-order ordinary differential equations are those of a rigid body with time-dependent inertia acted upon by various forces. Some of these forces, like lift and drag, depend on the motion of the body in the surrounding medium. Additionally, the inertia must encode the effect of the medium through its We derive the corresponding dynamics equations which generalize the standard rigid body dynamics equations. All forces are based on local computations using only physical parameters such as mass density. Notably, we approximate the effect of the medium on the body through local computations avoiding any global simulation of the medium. Consequently, the system of equations we must integrate in time is only 6 dimensional (rotation and translation). Our proposed algorithm displays linear complexity and captures intricate natural phenomena that depend on body-fluid interactions.	https://dl.acm.org/doi/abs/10.1145/3658164	Yousuf Soliman, Marcel Padilla, Oliver Gross, Felix Knöppel, Ulrich Pinkall, Peter Schröder
Goodbye my World	The world is about to end, and a man stuck in a fish costume attempts to cross the chaos of the city to reach a mysterious tower.	https://dl.acm.org/doi/abs/10.1145/3641230.3652582	Quentin Devred, Estelle Bonnardel, Maxime Foltzer, Astrid Novais, Florian Maurice, Baptiste Duchamps
Hair Universe: Meorikarak Woojoo	A grieving boy finds the hair of his recently deceased mother caught in the drainage hole during the shower, and gets swept into the 'Hair Universe', an unknown world underneath the bathtub.	https://dl.acm.org/doi/abs/10.1145/3641230.3643652	Jinuk Choi
Hand-Object Interaction Controller (HOIC): Deep Reinforcement Learning for Reconstructing Interactions with Physics	Hand manipulating objects is an important interaction motion in our daily activities. We faithfully reconstruct this motion with a single RGBD camera by a novel deep reinforcement learning method to leverage physics. Firstly, we propose object compensation control which establishes direct object control to make the network training more stable. Meanwhile, by leveraging the compensation force and torque, we seamlessly upgrade the simple point contact model to a more physical-plausible surface contact model, further improving the reconstruction accuracy and physical correctness. Experiments indicate that without involving any heuristic physical rules, this work still successfully involves physics in the reconstruction of hand-object interactions which are complex motions hard to imitate with deep reinforcement learning. Our code and data are available at https://github.com/hu-hy17/HOIC.	https://dl.acm.org/doi/abs/10.1145/3641519.3657505	Haoyu Hu, Xinyu Yi, Zhe Cao, Jun-Hai Yong, Feng Xu
HaptoRoom: Using Vibrotactile Floor Interfaces To Enable Reconfigurable Haptic Interaction Onto Any Furniture Surfaces	Vibrotactile interaction is an essential source of HCI and should be on a large scale, such as room scale, whereas in the past many devices have been small. However, there is a practical difficulties for implementing vibrotactile actuators and sensors to every furnitures in the room. To address this problem, our approach, HaptoRoom, archives reconfigurable ubiquitous haptic interaction to existing furniture without additional equipment by installing actuators only in the floor. A user can design detailed spatial distribution of vibrotactile pattern, leveraging with multi-point individual control of actuators array inside floor interfaces.	https://dl.acm.org/doi/abs/10.1145/3641517.3664394	Kiryu Tsujita, Takatoshi Yoshida, Kohei Kobayashi, Arata Horie, Nobuhisa Hanamitsu, Kouta Minamizawa
Head in The Clouds	A quirky student who daydreams with her head high in the clouds must creatively elude her strict math teacher to remain there.	https://dl.acm.org/doi/abs/10.1145/3641230.3653418	Kaylee Tian Lin Tan
HeadArtist: Text-conditioned 3D Head Generation with Self Score Distillation	We present HeadArtist for 3D head generation following human-language descriptions. With a landmark-guided ControlNet serving as a generative prior, we come up with an efficient pipeline that optimizes a parameterized 3D head model under the supervision of the prior distillation itself. We call such a process self score distillation (SSD). In detail, given a sampled camera pose, we first render an image and its corresponding landmarks from the head model, and add some particular level of noise onto the image. The noisy image, landmarks, and text condition are then fed into a frozen ControlNet twice for noise prediction. We conduct two predictions via the same ControlNet structure but with different classifier-free guidance (CFG) weights. The difference between these two predicted results directs how the rendered image can better match the text of interest. Experimental results show that our approach produces high-quality 3D head sculptures with rich geometry and photo-realistic appearance, which significantly outperforms state-of-the-art methods. We also show that our pipeline supports editing operations on the generated heads, including both geometry deformation and appearance change. Project page:https://kumapowerliu.github.io/HeadArtist.	https://dl.acm.org/doi/abs/10.1145/3641519.3657512	Hongyu Liu, Xuan Wang, Ziyu Wan, Yujun Shen, Yibing Song, Jing Liao, Qifeng Chen
High Performance Graphics 2024 Conference	This panel will present a short overview of the High-Performance Graphics (HPG) 2024 conference program. The conference is co-sponsored by ACM SIGGRAPH and Eurographics and, in 2024, is co-located with SIGGRAPH, taking place during the previous days, in Denver. HPG brings together researchers, engineers, and architects to discuss the complex interactions of parallel and custom hardware, novel programming models, and efficient algorithms in the design of systems for current and future graphics and visual computing applications. The panel will comprise short presentations by the keynote speakers, who will highlight their talks, and by the program and paper chairs, who will present a short overview of the papers program. This panel is an opportunity for all SIGGRAPH attendants to have an overview of the HPG content. Since it happens before SIGGRAPH starts, we understand that taking a short overview of the conference content can be of great value for both conferences.	https://dl.acm.org/doi/abs/10.1145/3641516.3664585	Esteban Clua, Christiaan Gribble, John Burgess, Karen Ghavam, Peter Shirley, Won-Jong Lee
High-quality Surface Reconstruction using Gaussian Surfels	We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods.	https://dl.acm.org/doi/abs/10.1145/3641519.3657441	Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu
Holographic Parallax	Cutting-edge computer-generated holography (CGH) algorithms have realized significant advancements in reconstructing the desired light field, enabling the creation of 4D light field holograms. These holograms offer view-dependent effects, such as motion parallax, occlusion, and specular reflections, which are crucial for enhancing the perceptual realism of 3D scenes. Our holographic display prototype leverages sophisticated CGH algorithms in conjunction with a high-speed binary spatial light modulator (SLM) to deliver an immersive experience of 4D light field holograms. This demonstration highlights the latest progress in CGH technology and emphasizes the critical role of parallax cues in providing perceptually realistic 3D contents.	https://dl.acm.org/doi/abs/10.1145/3641517.3664386	Seung-Woo Nam, Dongyeon Kim, Suyeon Choi, Juhyun Lee, Siwoo Lee, Manu Gopakumar, Brian Chao, Gordon Wetzstein, Yoonchan Jeong
Holographic Parallax Improves 3D Perceptual Realism	Holographic near-eye displays are a promising technology to solve long-standing challenges in virtual and augmented reality display systems. Over the last few years, many different computer-generated holography (CGH) algorithms have been proposed that are supervised by different types of target content, such as 2.5D RGB-depth maps, 3D focal stacks, and 4D light fields. It is unclear, however, what the perceptual implications are of the choice of algorithm and target content type. In this work, we build a perceptual testbed of a full-color, high-quality holographic near-eye display. Under natural viewing conditions, we examine the effects of various CGH supervision formats and conduct user studies to assess their perceptual impacts on 3D realism. Our results indicate that CGH algorithms designed for specific viewpoints exhibit noticeable deficiencies in achieving 3D realism. In contrast, holograms incorporating parallax cues consistently outperform other formats across different viewing conditions, including the center of the eyebox. This finding is particularly interesting and suggests that the inclusion of parallax cues in CGH rendering plays a crucial role in enhancing the overall quality of the holographic experience. This work represents an initial stride towards delivering a perceptually realistic 3D experience with holographic near-eye displays.	https://dl.acm.org/doi/abs/10.1145/3658168	Dongyeon Kim, Seung-Woo Nam, Suyeon Choi, Jong-Mo Seo, Gordon Wetzstein, Yoonchan Jeong
How to control Mayhem: The painted look of hair on TMNT	TMNT Mutant Mayhem (2023) represented a great challenge artistically and technically. The director aimed for a look that would give the illusion that each frame would be hand-crafted 2D concept art. To break away from a digital or procedural feeling, we needed to approach the assets from modeling to surfacing in a unique way. This was especially challenging for the look and feel of the groom. To treat the groom in a way that it would feel cohesive with all the other objects and integrate seamlessly in the rendering scenes, we developed a layered system that combined modeled geometry, curves, a stylized surfacing, rendering, and lighting approach connecting the character work of modeling, grooming, surfacing, and lighting artists.	https://dl.acm.org/doi/abs/10.1145/3641233.3664353	Brian Gossart, Olivier Pierre, Anne-Claire Leroux, Gregory Coelho
I Tell the Moon My Secret and the Moon Tells Me Yours	I Tell the Moon My Secret and the Moon Tells Me Yours (ITMMSMTMY) is an interactive piece that pairs a robotic arm with the Moon to create light painting photographs to visualize the gathered secrets. The audience can share their secrets on a portal that generates the waveform. The waveform data, once generated, are computed into coordinates and dispatched to the robotic system. The robot arm continues to depict the waveform, but the movement can only be successfully captured by the long-exposure camera when the Moon appears within a certain field of view, since the Moon is the exclusive light source during the long-exposure process.	https://dl.acm.org/doi/abs/10.1145/3641523.3665171	Zoe Qi-Jing Li
I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models	Text-guided image-to-video (I2V) generation aims to generate a coherent video that preserves the identity of the input image and semantically aligns with the input prompt. Existing methods typically augment pretrained text-to-video (T2V) models by either concatenating the image with noised video frames channel-wise before being fed into the model or injecting the image embedding produced by pretrained image encoders in cross-attention modules. However, the former approach often necessitates altering the fundamental weights of pretrained T2V models, thus restricting the model's compatibility within the open-source communities and disrupting the model's prior knowledge. Meanwhile, the latter typically fails to preserve the identity of the input image. We present I2V-Adapter to overcome such limitations. I2V-Adapter adeptly propagates the unnoised input image to subsequent noised frames through a cross-frame attention mechanism, maintaining the identity of the input image without any changes to the pretrained T2V model. Notably, I2V-Adapter only introduces a few trainable parameters, significantly alleviating the training cost and also ensures compatibility with existing community-driven personalized models and control tools. Moreover, we propose a novel Frame Similarity Prior to balance the motion amplitude and the stability of generated videos through two adjustable control coefficients. Our experimental results demonstrate that I2V-Adapter is capable of producing high-quality videos. This performance, coupled with its agility and adaptability, represents a substantial advancement in the field of I2V, particularly for personalized and controllable applications.	https://dl.acm.org/doi/abs/10.1145/3641519.3657407	Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, Chongyang Ma
IWÁJÚ - Cinesite	Iwájú is a six-part series animated by Cinesite for Disney+, with Pan-African studio Kugali. Set in a futuristic Lagos, Nigeria, creating the vibrant, densely-populated and asset filled mainland market was a significant achievement. Lagos has been created with meticulous attention paid to cultural accuracy, including food, customs, mannerisms and language.	https://dl.acm.org/doi/abs/10.1145/3641230.3653333	Ellen Poon, Adel Abada, Andrew Gartner, Stéphanie Roy
Illusory Eyescape - Exploring the Variations of Consciousness through Generative Art and Eye-Tracking Techniques	"Historically, art has been considered a form of expression uniquely reserved for humans. However, technological advances, particularly in computer graphics, have expanded the boundaries of artistic creation beyond human exclusivity. The emergence of Generative Art in recent years has not only transformed the artistic process but has also initiated extensive dialogues on the interactions between humans and machines. To delve deeper into this theme, we developed an interactive art installation titled ""Illusory Eyescape."" This installation integrates Generative Art and Eye-tracking Technologies to probe the fundamental nature of consciousness in humans and machines, questioning the role of art in this new epoch of human-machine synergy. This exploration broadens our comprehension of the potential amalgamations of art and technology and challenges our conventional perceptions of art."	https://dl.acm.org/doi/abs/10.1145/3641234.3671068	Sin-Fei Lee, Ming-Te Chi
ImmerseSketch: Transforming Creative Prompts into Vivid 3D Environments in VR	We propose ImmerseSketch, a framework designed to transform creative prompts into vivid and detailed 3D content within a Virtual Reality (VR) environment. Our aim is to inspire creative ideation and to provide creators with 3D reference content for painting. However, the computational demands of diffusion models for 3D model generation and the scarcity of high-quality 3D datasets limit the diversity and intricacy of the generated environments. To overcome these obstacles, we focus on generating initial panoramic images through diffusion models and then converting these images into rich 3D environments with the aid of depth estimation. The pilot study shows that the proposed ImmerseSketch can provide an immersive environment and assist the process of creation.	https://dl.acm.org/doi/abs/10.1145/3641234.3671078	Alfred Lan, Tai-Chen Tsai, Chih-Chuan Huang, Pu Ching, Tse-Yu Pan, Min-Chun Hu
Implicit Swept Volume SDF: Enabling Continuous Collision-Free Trajectory Generation for Arbitrary Shapes	"In the field of trajectory generation for objects, ensuring continuous collision-free motion remains a huge challenge, especially for non-convex geometries and complex environments. Previous methods either oversimplify object shapes, which results in a sacrifice of feasible space or rely on discrete sampling, which suffers from the ""tunnel effect"". To address these limitations, we propose a novel hierarchical trajectory generation pipeline, which utilizes the Swept Volume Signed Distance Field (SVSDF) to guide trajectory optimization for Continuous Collision Avoidance (CCA). Our interdisciplinary approach, blending techniques from graphics and robotics, exhibits outstanding effectiveness in solving this problem. We formulate the computation of the SVSDF as a Generalized Semi-Infinite Programming model, and we solve for the numerical solutions at query points implicitly, thereby eliminating the need for explicit reconstruction of the surface. Our algorithm has been validated in a variety of complex scenarios and applies to robots of various dynamics, including both rigid and deformable shapes. It demonstrates exceptional universality and superior CCA performance compared to typical algorithms. The code will be released at https://github.com/ZJU-FAST-Lab/Implicit-SVSDF-Planner for the benefit of the community."	https://dl.acm.org/doi/abs/10.1145/3658181	Jingping Wang, Tingrui Zhang, Qixuan Zhang, Chuxiao Zeng, Jingyi Yu, Chao Xu, Lan Xu, Fei Gao
Inclusive Community: Accessibility and Neurodiversity in Classroom, Industry and Academia	Universal design/accessibility, inclusive classrooms and workplaces are essential components for the success of individuals affected by various challenges. Neurodivergent and disabled individuals have a variety of needs that are often unmet by schools and industries in the SIGGRAPH community. This panel discusses the current state of the issue, the challenges of neurodivergent and disabled individuals, different types of accommodations and their availability and effectiveness, and helpful ideas to make the learning and working environments more inclusive. Although accommodations for some issues are required by law in the United States (including Americans with Disability Act (ADA) compliance), levels of enforcement and awareness vary. Moreover, accommodations to provide inclusive classrooms are not necessarily required in other jurisdictions. Yet, we know that persons who face different types of challenges are still able to make significant contributions to whatever endeavors in which they happen to be involved and there is an awareness of the importance of creating inclusive classrooms and workplaces that accommodate individuals with neurodiversity as well as people with various (and sometimes more obvious) physical disabilities. The panel discusses different types of accommodations, and what may be available for classrooms and workplaces. Students in tertiary education are coming from a system where supervising adults (parents, guardians, counselors) worked on their behalf to ensure they received the accommodations needed in primary and secondary schools (generally K-12 in the United States). Upon arrival at university, they often do not know how to self-advocate and parents or guardians, no longer able (or allowed) to advocate on behalf of their young adults, don't know how to support them effectively. Meanwhile, faculty, administrators, and staff may not have been trained in effective methods of accommodating the variety of students in a classroom. Challenges that all students face are sometimes more difficult for those with challenges including how to manage time, effectively study, and manage interactions with instructors and classmates. They also need to learn about school policies, procedures and resources that may be available to help them. These questions and concerns are being addressed in several different ways, in the classroom, through civic engagement, and within the workplace. Some accommodations are for physical challenges, and others for those who are neurodivergent. These accommodations can sometimes overlap and can benefit both. For example, scaffolding assignments can help all students be less overwhelmed with a project, by breaking it into smaller milestone goals. Scaffolding projects can be mirrored in the workforce as well by allowing employees to have several small goals that lead to the final larger goal and can give room for feedback and possible iteration. Other strategies that can be employed include pre-recording lectures when intellectual property issues are not being violated. Offering Lectures as PDFs in the Learning Management System your class uses allows for students to have access to the relevant notes of the in-class lectures and accommodates any students who require lecture notes to help them succeed. Setting up auto-reminders to the class via email regarding assignments and project deadlines can help all students with their time-management skills and meeting project deadlines. Neurodivergent students [Asbell-Clarke et al. 2022], however, have a range of different challenges that need to be accommodated and may include issues of noise, light, learning style(s), sensitivity to issues of personal space, and ability to process information. It has been announced that providing a place with a relaxed environment is effective for people with mental disorders such as autism and developmental disorders. For example, at SIGGRAPH 23, Inclusive Quiet Room was exhibited as a product for people with mental disabilities[Kimura et al. 2023]. This preaches that even if there are people with mental disorders in the SIGGRAPH community, It as a community should be tolerant of diversity and accept everyone.This product can also be applied to schools. Even a so-called single issue, like dyslexia, may range from mild to severe. Topics such as active learning, breaking apart and/or flexibility in assignments and projects, physical/spatial accommodations, attendance policies, industry-academy partnerships, and civic engagement are all topics of discussion. Establishing a welcoming atmosphere for all forms of diversity in the classroom and the studio is essential for student and employee success. This includes recognizing and addressing additional needs of individuals like those with PTSD, veterans, current military members, and parents of young children. Creating a supportive environment means understanding and accommodating their unique challenges, such as providing hybrid attendance policies, seating near the exit doors, regular breaks, clear safety procedures, and listing availability for mental health and childcare resources. Examples of ways to help some of those with these types of issues are discussed by the panel. Schools and companies may believe that they do not have the wherewithal to effectively accommodate a specialized population, leading to questions such as, what is the reality of working in an industry known for being deadline-driven and collaborative? How can we foster more contributions from neurodiverse individuals to the community? Excerpts from short interviews with industry professionals are included as part of the panel. Supporting neurodivergent and disabled students and employees requires a comprehensive approach [Junko Saimoto 2023], [John Hopkins University 2022] that can benefit all. By implementing these strategies, we create more inclusive and effective educational and professional environments. However, it is essential to continuously assess and improve practices, staying informed about new research and emerging technologies that can further aid inclusivity. Future efforts could include developing more advanced assistive technologies [Boyd 2023], offering specialized training programs for faculty and staff, and fostering partnerships with organizations specializing in neurodiversity and disability services. It is an explicit goal of this panel to create/improve a culture of inclusivity by promoting ongoing dialogue within the SIGGRAPH community, encouraging feedback, and adapting to the evolving needs of our evolving and increasingly diverse population. By prioritizing inclusivity, we not only support the success of individuals with unique challenges but also enrich our community as a whole, fostering innovation and creativity through diverse perspectives and experiences.	https://dl.acm.org/doi/abs/10.1145/3641235.3664445	Mayet Andreassen
Ink Harmony: An AI- and VR-enhanced System for Calligraphy Education	This work introduces an assistive system designed to deepen the engagement with the art of calligraphy and Chinese culture. Users input the text they wish to learn, which is then processed by an AI framework to create a thematic photo. This photo forms part of a virtual environment, establishing an environment that contextualizes the calligraphy experience. Simultaneously, the system communicates with a robotic arm, guiding the user's hand to improve writing technique and engagement with the art form. This combination of AI, virtual reality, and robotic assistance paves the way for interactive learning and appreciation of calligraphy.	https://dl.acm.org/doi/abs/10.1145/3641521.3680540	Yuchao Zhuo, Bingyuan Wang, Zeyu Wang
Integrating Independent Contributions in a Game Programming Assignment	"In game development curriculum, mastering programming skills is important. Traditional methods to teach programming in computer science curriculum often face challenges in relevance for game development students, leaning towards individual mastery of the knowledge. This has shown a sign of contrast with the collaborative nature of game design and development courses where teamwork is emphasized. In our approach, especially for a game programming assignment that is often integrated with computer graphics requirements, we introduce the strategy of embedding ""imagined social elements"" into individual programming assignments, simulating team dynamics without a formal team structure. Through this approach, an assignment promotes the integration of independent contributions while maintaining the individual rigor essential for programming practices. The proposed assignment aims to be adopted in a game programming course. We have set up a git repository for this assignment (https://github.com/LizhouCao/Integrating-Independent-Contributions-in-a-Game-Programming-Assignment.git), with build instructions and required dependencies included."	https://dl.acm.org/doi/abs/10.1145/3641235.3664431	Chao Peng, Lizhou Cao, David Schwartz, Huadong Zhang
Interactive Character Control with Auto-Regressive Motion Diffusion Models	Real-time character control is an essential component for interactive experiences, with a broad range of applications, including physics simulations, video games, and virtual reality. The success of diffusion models for image synthesis has led to the use of these models for motion synthesis. However, the majority of these motion diffusion models are primarily designed for offline applications, where space-time models are used to synthesize an entire sequence of frames simultaneously with a pre-specified length. To enable real-time motion synthesis with diffusion model that allows time-varying controls, we propose A-MDM (Auto-regressive Motion Diffusion Model). Our conditional diffusion model takes an initial pose as input, and auto-regressively generates successive motion frames conditioned on the previous frame. Despite its streamlined network architecture, which uses simple MLPs, our framework is capable of generating diverse, long-horizon, and high-fidelity motion sequences. Furthermore, we introduce a suite of techniques for incorporating interactive controls into A-MDM, such as task-oriented sampling, in-painting, and hierarchical reinforcement learning (See Figure 1). These techniques enable a pre-trained A-MDM to be efficiently adapted for a variety of new downstream tasks. We conduct a comprehensive suite of experiments to demonstrate the effectiveness of A-MDM, and compare its performance against state-of-the-art auto-regressive methods.	https://dl.acm.org/doi/abs/10.1145/3658140	Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, Xue Bin Peng
Interactive Design of Stylized Walking Gaits for Robotic Characters	Procedural animation has seen widespread use in the design of expressive walking gaits for virtual characters. While similar tools could breathe life into robotic characters, existing techniques are largely unaware of the kinematic and dynamic constraints imposed by physical robots. In this paper, we propose a system for the artist-directed authoring of stylized bipedal walking gaits, tailored for execution on robotic characters. The artist interfaces with an interactive editing tool that generates the desired character motion in realtime, either on the physical or simulated robot, using a model-based control stack. Each walking style is encoded as a set of sample parameters which are translated into whole-body reference trajectories using the proposed procedural animation technique. In order to generalize the stylized gait over a continuous range of input velocities, we employ a phase-space blending strategy that interpolates a set of example walk cycles authored by the animator while preserving contact constraints. To demonstrate the utility of our approach, we animate gaits for a custom, free-walking robotic character, and show, with two additional in-simulation examples, how our procedural animation technique generalizes to bipeds with different degrees of freedom, proportions, and mass distributions.	https://dl.acm.org/doi/abs/10.1145/3658227	Michael A. Hopkins, Georg Wiedebach, Kyle Cesare, Jared Bishop, Espen Knoop, Moritz Bächer
Interactive Invigoration: Volumetric Modeling of Trees with Strands	Generating realistic models of trees and plants is a complex problem because of the vast variety of shapes trees can form. Procedural modeling algorithms are popular for defining branching structures and steadily increasing their expressive power by considering more biological findings. Most existing methods focus on defining the branching structure of trees based on skeletal graphs, while the surface mesh of branches is most commonly defined as simple cylinders. One critical open problem is defining and controlling the complex details observed in real trees. This paper aims to advance tree modeling by proposing a strand-based volumetric representation for tree models. Strands are fixed-size volumetric pipes that define the branching structure. By leveraging strands, our approach captures the lateral development of trees. We combine the strands with a novel branch development formulation that allows us to locally inject vigor and reshape the tree model. Moreover, we define a set of editing operators for tree primary and lateral development that enables users to interactively generate complex tree models with unprecedented detail with minimal effort.	https://dl.acm.org/doi/abs/10.1145/3658206	Bosheng Li, Nikolas Alexander Schwarz, Wojtek Pałubicki, Sören Pirk, Bedrich Benes
Into the Portal: Directable Fractal Self-Similarity	"We present a novel, directable method for introducing fractal self-similarity into arbitrary shapes. Our method allows a user to directly specify the locations of self-similarities in a Julia set, and is general enough to reproduce other well-known fractals such as the Koch snowflake. Ours is the first algorithm to enable this level of general artistic control while also maintaining the character of the original fractal shape. We introduce the notion of placing ""portals"" in the iteration space of a dynamical system, bridging the aesthetics of iterated maps with the fine-grained control of iterated function systems (IFS). Our method is effective in both 2D and 3D."	https://dl.acm.org/doi/abs/10.1145/3641519.3657466	Alexa Schor, Theodore Kim
Intractable Live Free-Viewpoint Video with Haptic Feedback	We present a novel interactable free-viewpoint video (FVV), which generates photo-realistic and editable volumetric content with high degree of freedom. On the one hand, an enhanced visual hull guided neural representation with higher performance is proposed, an-easy-to-use sparse multi-camera capture system is used, as well as a high performance VH-NeRF is proposed for fast generation of photo-realistic FVV results for live streaming. On the other hand, for monocular panorama video input, high-precision depth and coarse dynamic mesh are calculated, which are used for novel view synthesis of right perspective views with motion parallax on the fly. For better interaction with high realism, dual-channel stereohaptics is implemented and attached on VR headsets to obtain haptic feedback. Last, our FVV solution can do effective compression and transmission on both multi-perspective videos and panorama video with depth data, as well as real-time rendering on consumer-grade hardware. To the best of our knowledge, our work is the first interactable FVV solution with high visual quality and real-time haptic feedback. It is established that the user can do intuitive manipulation for immersive experiences in VR/AR applications.	https://dl.acm.org/doi/abs/10.1145/3641521.3664412	Shaohui Jiao, Dehao Zhao, Yuzhong Chen, Shengdong Wang, Cong Xie, Bo Zhu, Ali Israr, Li Zhang
IntrinsicDiffusion: Joint Intrinsic Layers from Latent Diffusion Models	Reasoning about the intrinsic properties of an image, such as albedo, illumination, and surface geometry, is a long-standing problem with many applications in image editing and compositing. Existing solutions to this ill-posed problem either heavily rely on manually designed priors or learn priors from limited datasets that lack diversity. Hence, they fall short in generalizing to in-the-wild test scenarios. In this paper, we show that a large-scale text-to-image generation model trained on a massive amount of visual data can implicitly learn intrinsic image priors. In particular, we introduce a novel conditioning mechanism built on top of a pre-trained foundational image generation model to jointly predict multiple intrinsic modalities from an input image. We demonstrate that predicting different modalities in a collaborative manner improves the overall quality. This design also enables mixing datasets with annotations of only a subset of the modalities during training, contributing to the generalizability of our approach. Our method achieves state-of-the-art performance in intrinsic image decomposition, both qualitatively and quantitatively. We also demonstrate downstream image editing applications, such as relighting and retexturing.	https://dl.acm.org/doi/abs/10.1145/3641519.3657472	Jundan Luo, Duygu Ceylan, Jae Shin Yoon, Nanxuan Zhao, Julien Philip, Anna Frühstück, Wenbin Li, Christian Richardt, Tuanfeng Wang
Introduction to Real-Time Ray Tracing	GPUs have long been built for rasterization-based rendering. As such, graphics APIs like OpenGL, Vulkan, Metal, and Direct3D 12 have been designed to support rasterization-focused rendering pipelines. Now that modern GPUs support ray tracing, new APIs are necessary to fully exploit these new hardware capabilities.	https://dl.acm.org/doi/abs/10.1145/3664475.3665157	Chris Cascioli, Laura Reznikov
Introduction to Scripting in Blender3D: Computational Geometry Algorithms	In this course students will be extending the powerful Blender 3D toolsuite through the scripting ecosystem. The goal is to get users who are familiar or otherwise just starting Blender3D to start scripting, creating add-ons, and experimenting by implementing various simple algorithms from computational geometry. Some concrete activities include installation, setup, navigating the scripting user interface, understanding how to rapidly prototype scripts, how to create larger script projects, and how to debug. Some example algorithms that will be implemented include generating geometry, computing a bounding box, and computing a convex hull with a custom user interface. This course is targeted towards artists with minimal programming experience, or programmers who want to write tools that integrate into the Blender 3D ecosystem. Attendees will benefit by learning otherwise how Blender3D can be utilized as a sandbox for research experiments, app development, rapid prototyping, or otherwise more industrial uses like building a content pipeline.	https://dl.acm.org/doi/abs/10.1145/3664475.3664525	Michael D. Shah
InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars	While high fidelity and efficiency are central to the creation of digital head avatars, recent methods relying on 2D or 3D generative models often experience limitations such as shape distortion, expression inaccuracy, and identity flickering. Additionally, existing one-shot inversion techniques fail to fully leverage multiple input images for detailed feature extraction. We propose a novel framework, Incremental 3D GAN Inversion, that enhances avatar reconstruction performance using an algorithm designed to increase the fidelity from multiple frames, resulting in improved reconstruction quality proportional to frame count. Our method introduces a unique animatable 3D GAN prior with two crucial modifications for enhanced expression controllability alongside an innovative neural texture encoder that categorizes texture feature spaces based on UV parameterization. Differentiating from traditional techniques, our architecture emphasizes pixel-aligned image-to-image translation, mitigating the need to learn correspondences between observation and canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent networks for temporal data aggregation from multiple frames, boosting geometry and texture detail reconstruction. The proposed paradigm demonstrates state-of-the-art performance on one-shot and few-shot avatar animation tasks. The code will be available at https://github.com/XChenZ/invertAvatar.	https://dl.acm.org/doi/abs/10.1145/3641519.3657478	Xiaochen Zhao, Jingxiang Sun, Lizhen Wang, Jinli Suo, Yebin Liu
Iterative Motion Editing with Natural Language	Text-to-motion diffusion models can generate realistic animations from text prompts, but do not support fine-grained motion editing controls. In this paper, we present a method for using natural language to iteratively specify local edits to existing character animations, a task that is common in most computer animation workflows. Our key idea is to represent a space of motion edits using a set of kinematic motion editing operators (MEOs) whose effects on the source motion is well-aligned with user expectations. We provide an algorithm that leverages pre-existing language models to translate textual descriptions of motion edits into source code for programs that define and execute sequences of MEOs on a source animation. We execute MEOs by first translating them into keyframe constraints, and then use diffusion-based motion models to generate output motions that respect these constraints. Through a user study and quantitative evaluation, we demonstrate that our system can perform motion edits that respect the animator's editing intent, remain faithful to the original animation (it edits the original animation, but does not dramatically change it), and yield realistic character animation results.	https://dl.acm.org/doi/abs/10.1145/3641519.3657447	Purvi Goel, Kuan-Chieh Wang, C. Karen Liu, Kayvon Fatahalian
Kandinsky As You Preferred	Due to the significant generative capabilities of GenAI (generative artificial intelligence), the art community has actively embraced it to create painterly content. Large text-to-image models can quickly generate aesthetically pleasing outcomes. However, the process can be non-deterministic and often involves tedious trial-and-error as users struggle to formulate effective prompts to achieve their desired results. This paper describes a generative approach that empowers users to easily work with a large text-to-image (TTI) model to create their preferred painterly content. The authors propose a large model personalization method, namely Semantic Injection, to personalize a large TTI model in a given specific artistic style, i.e., Kandinsky's paintings in Bauhaus era, as the Artist Model. Through working with a Kandinsky expert, the authors first establish a semantic descriptive guideline and a TTI dataset of Kandinsky style and then apply the Semantic Injection method to obtain an Artist Model of Kandinsky, empowering users to create preferred Kandinsky content in a deterministically controllable manner.	https://dl.acm.org/doi/abs/10.1145/3641234.3671061	Aven-Le Zhou, Yu-Ao Wang, Wei Wu, Kang Zhang
KineSway: Simulating Terrain Shaking Sensation by Alternating Tendon Vibration to Ankles	Advancements of multimodal video content, interactive gaming, and virtual reality experiences frequently employ motion platforms to authentically replicate the dynamics of ground swaying and tilting. However, they are expensive to install and require significant space. In response to these constraints, we propose a method that utilizes a kinesthetic illusion. This technique involves the selective application of vibratory stimuli to the ventral and dorsal tendons of the ankle, thus engendering a perceptual sensation of ground sway. Additionally, integration of these vibratory stimuli with continuous visual inputs facilitates the simulation of ground tilt. The generation of these sensations is exclusively through vibratory means without any actual ground movement. This feature significantly enhances the safety and ease of installation, thereby presenting a viable and efficient alternative to conventional motion platform setup.	https://dl.acm.org/doi/abs/10.1145/3641517.3664391	Eifu Narita, Keigo Ushiyama, Izumi Mizoguchi, Hiroyuki Kajimoto
Kinetic Simulation of Turbulent Multifluid Flows	Despite its visual appeal, the simulation of separated multiphase flows (i.e., streams of fluids separated by interfaces) faces numerous challenges in accurately reproducing complex behaviors such as guggling, wetting, or bubbling. These difficulties are especially pronounced for high Reynolds numbers and large density variations between fluids, most likely explaining why they have received comparatively little attention in Computer Graphics compared to single- or two-phase flows. In this paper, we present a full LBM solver for multifluid simulation. We derive a conservative phase field model with which the spatial presence of each fluid or phase is encoded to allow for the simulation of miscible, immiscible and even partially-miscible fluids, while the temporal evolution of the phases is performed using a D3Q7 lattice-Boltzmann discretization. The velocity field, handled through the recent high-order moment-encoded LBM (HOME-LBM) framework to minimize its memory footprint, is simulated via a velocity-based distribution stored on a D3Q27 or D3Q19 discretization to offer accuracy and stability to large density ratios even in turbulent scenarios, while coupling with the phases through pressure, viscosity, and interfacial forces is achieved by leveraging the diffuse encoding of interfaces. The resulting solver addresses a number of limitations of kinetic methods in both computational fluid dynamics and computer graphics: it offers a fast, accurate, and low-memory fluid solver enabling efficient turbulent multiphase simulations free of the typical oscillatory pressure behavior near boundaries. We present several numerical benchmarks, examples and comparisons of multiphase flows to demonstrate our solver's visual complexity, accuracy, and realism.	https://dl.acm.org/doi/abs/10.1145/3658178	Wei Li, Kui Wu, Mathieu Desbrun
L'Animal Sauce Ail	Gooseville, close to nothing and far away from everything. The place where Goosevillers lead themselves to destruction. The inhabitants live off the farming of species that they overexploit each time, from geese to tadpoles. Each new managing and self consuming environment is told throughout low budget homemade TV shows. Advertising, teleshopping and others show us the devolution of a small French village rushing towards its end.	https://dl.acm.org/doi/abs/10.1145/3641230.3648632	Ysaline Debut, Isa Cotte, Aurélien Duchez, Camille Rostan, Clément Mouchel, Diane Mazella
LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model	In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation. LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation. Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts. To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment. Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence. Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications. Code and data for this paper are available at https://github.com/L-Sun/LGTM	https://dl.acm.org/doi/abs/10.1145/3641519.3657422	Haowen Sun, Ruikun Zheng, Haibin Huang, Chongyang Ma, Hui Huang, Ruizhen Hu
LOOSECONTROL: Lifting ControlNet for Generalized Depth Conditioning	We present LooseControl to allow generalized depth conditioning for diffusion-based image generation. ControlNet, the SOTA for depth conditioned image generation, produces remarkable results but relies on having access to detailed depth maps for guidance. Creating such exact depth maps, in many scenarios, is challenging. This paper introduces a generalized version of depth conditioning that enables new content creation workflows. Specifically, we allow (C1) scene boundary control for loosely specifying scenes with only boundary conditions, and (C2) 3D box control for specifying the target objects' layout locations rather than the objects' exact shape and appearance. Using LooseControl, along with text guidance, users can create complex environments (e.g., rooms, street views, etc.) by specifying only scene boundaries and locations of primary objects. Further, we provide two editing mechanisms to refine the results: (E1) 3D box editing enables the user to refine images by changing, adding, or removing boxes while freezing the image style. This yields minimal changes apart from changes induced by the edited boxes. (E2) Attribute editing proposes possible editing directions to change one particular aspect of the scene, such as the overall object density or a particular object. Tests and comparisons with baselines demonstrate the generality of our method. We believe that LooseControl can become an important design tool for easily creating complex environments and be extended to other forms of guidance channels. The project page can be found at https://shariqfarooq123.github.io/loose-control/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657525	Shariq Farooq Bhat, Niloy Mitra, Peter Wonka
LUKI & the Lights	LUKi, a charming and upbeat robot known for living life to the fullest, is diagnosed with the life-altering disease ALS. He must choose how to face life going forward.	https://dl.acm.org/doi/abs/10.1145/3641230.3652552	Toby Cochran
Labeled Black: english	When her brother fell victim to police profiling, Banchi couldn't have foreseen the family tragedy that would unfold. Later, she became a leading force in the fight against police brutality, striving for justice for her brother. Following Banchi through 3D animated environments, the user becomes immersed in her haunted world.	https://dl.acm.org/doi/abs/10.1145/3641231.3649233	Irit Dolev, Alon Marom
Lagrangian Covector Fluid with Free Surface	This paper introduces a novel Lagrangian fluid solver based on covector flow maps. We aim to address the challenges of establishing a robust flow-map solver for incompressible fluids under complex boundary conditions. Our key idea is to use particle trajectories to establish precise flow maps and tailor path integrals of physical quantities along these trajectories to reformulate the Poisson problem during the projection step. We devise a decoupling mechanism based on path-integral identities from flow-map theory. This mechanism integrates long-range flow maps for the main fluid body into a short-range projection framework, ensuring a robust treatment of free boundaries. We show that our method can effectively transform a long-range projection problem with integral boundaries into a Poisson problem with standard boundary conditions — specifically, zero Dirichlet on the free surface and zero Neumann on solid boundaries. This transformation significantly enhances robustness and accuracy, extending the applicability of flow-map methods to complex free-surface problems.	https://dl.acm.org/doi/abs/10.1145/3641519.3657514	Zhiqi Li, Barnabás Börcsök, Duowen Chen, Yutong Sun, Bo Zhu, Greg Turk
Large Scale Sand Simulations on Under The Boardwalk	In the 2023 Feature Animation Under the Boardwalk a Romeo and Juliet meets West Side Story themed musical comedy; we enter the miniature world of the Hermit Crab. Set on the Jersey Shore, the rivalry between the sea crabs descending on the home of the resident land crabs during spring break gave us lots of sand interaction shots. Our workflows had to handle closeup macro shots with one or two crab characters, full battle sequences with hundreds of crowd crabs, and human scale characters. With hundreds of FX shots featuring fully simulated sand interaction and many beach environments needing sculpting and shaping to match dressed assets, we had to turn to automation techniques to help us tackle this number of shots efficiently. Our envfinishing step was used to preprocess the shot beach geometry to add detail, deform the sand around the static set objects and add footprints. The FX team used this geometry as a basis for the grain simulations and include any water interaction with rain or waves. These caches had to inherit the large scale textural detail and the fine, per grain variations provided by the surfacing department to seamlessly blend the sand and beach geometry.	https://dl.acm.org/doi/abs/10.1145/3641233.3664318	Jonathan Davies, Cheten Sharma, Nicholas New
LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer	Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html.	https://dl.acm.org/doi/abs/10.1145/3641519.3657501	Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu
Learning Images Across Scales Using Adversarial Training	The real world exhibits rich structure and detail across many scales of observation. It is difficult, however, to capture and represent a broad spectrum of scales using ordinary images. We devise a novel paradigm for learning a representation that captures an orders-of-magnitude variety of scales from an unstructured collection of ordinary images. We treat this collection as a distribution of scale-space slices to be learned using adversarial training, and additionally enforce coherency across slices. Our approach relies on a multiscale generator with carefully injected procedural frequency content, which allows to interactively explore the emerging continuous scale space. Training across vastly different scales poses challenges regarding stability, which we tackle using a supervision scheme that involves careful sampling of scales. We show that our generator can be used as a multiscale generative model, and for reconstructions of scale spaces from unstructured patches. Significantly outperforming the state of the art, we demonstrate zoom-in factors of up to 256x at high quality and scale consistency.	https://dl.acm.org/doi/abs/10.1145/3658190	Krzysztof Wolski, Adarsh Djeacoumar, Alireza Javanmardi, Hans-Peter Seidel, Christian Theobalt, Guillaume Cordonnier, Karol Myszkowski, George Drettakis, Xingang Pan, Thomas Leimkühler
Learning a Generalized Physical Face Model From Data	Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today's methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry. Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.	https://dl.acm.org/doi/abs/10.1145/3658189	Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley
Level-Up Logics: Leveraging Three Game Design Platforms to Teach Coding	Coding has become an integral tool for designers, but the logical structures of programming can be intimidating for many students in the arts. To make learning the fundamentals of coding less stressful and more appealing, I revamped a Creative Coding elective in our Media Arts, Design and Technology (MADTech) program to progress students through three increasingly technical game design platforms. Students were taught how to apply the player-centric MDA (Mechanics, Dynamics, Aesthetics) game design framework to Fortnite Creative, a popular 3D platform that doesn't require any coding but instead has students configure devices in the game world to implement logical interaction. Students used a library of game assets to build a complex prototype that required collectibles to open a door and progress in the game. These core game mechanics were carried through to similar projects in Unreal Engine and Unity, while the MDA framework enabled students to creatively vary aesthetics and dynamics to make each game look and feel unique. By applying the same logical structures in three platforms, progressing from graphical node-based Blueprints to C# scripting, students gained a better grasp of fundamental coding concepts and showed a preference for scripting by the end of the semester. Though this curriculum was primarily targeted at students in the College of Design, students from more technical backgrounds like Computer Science were still challenged by the more aesthetic features of this design-centric approach to game development.	https://dl.acm.org/doi/abs/10.1145/3641235.3664434	Topher Maraffi
Lifting Directional Fields to Minimal Sections	Directional fields, including unit vector, line, and cross fields, are essential tools in the geometry processing toolkit. The topology of directional fields is characterized by their singularities. While singularities play an important role in downstream applications such as meshing, existing methods for computing directional fields either require them to be specified in advance, ignore them altogether, or treat them as zeros of a relaxed field. While fields are ill-defined at their singularities, the graphs of directional fields with singularities are well-defined surfaces in a circle bundle. By lifting optimization of fields to optimization over their graphs, we can exploit a natural convex relaxation to a problem over the space of currents in the bundle. This relaxation treats singularities as first-class citizens, expressing the relationship between fields and singularities as an explicit boundary condition. As curvature frustrates finite element discretization of the bundle, we devise a hybrid spectral method for representing and optimizing minimal sections. Our method supports field optimization on both flat and curved domains and enables more precise control over singularity placement.	https://dl.acm.org/doi/abs/10.1145/3658198	David Palmer, Albert Chern, Justin Solomon
LightFormer: Light-Oriented Global Neural Rendering in Dynamic Scene	The generation of global illumination in real time has been a long-standing challenge in the graphics community, particularly in dynamic scenes with complex illumination. Recent neural rendering techniques have shown great promise by utilizing neural networks to represent the illumination of scenes and then decoding the final radiance. However, incorporating object parameters into the representation may limit their effectiveness in handling fully dynamic scenes. This work presents a neural rendering approach, dubbed , that can generate realistic global illumination for fully dynamic scenes, including dynamic lighting, materials, cameras, and animated objects, in real time. Inspired by classic many-lights methods, the proposed approach focuses on the neural representation of light sources in the scene rather than the entire scene, leading to the overall better generalizability. The neural prediction is achieved by leveraging the virtual point lights and shading clues for each light. Specifically, two stages are explored. In the light encoding stage, each light generates a set of virtual point lights in the scene, which are then encoded into an implicit neural light representation, along with screen-space shading clues like visibility. In the light gathering stage, a pixel-light attention mechanism composites all light representations for each shading point. Given the geometry and material representation, in tandem with the composed light representations of all lights, a lightweight neural network predicts the final radiance. Experimental results demonstrate that the proposed LightFormer can yield reasonable and realistic global illumination in fully dynamic scenes with real-time performance.	https://dl.acm.org/doi/abs/10.1145/3658229	Haocheng Ren, Yuchi Huo, Yifan Peng, Hongtao Sheng, Weidong Xue, Hongxiang Huang, Jingzhen Lan, Rui Wang, Hujun Bao
Lightning-fast Method of Fundamental Solutions	The method of fundamental solutions (MFS) and its associated boundary element method (BEM) have gained popularity in computer graphics due to the reduced dimensionality they offer: for three-dimensional linear problems, they only require variables on the domain boundary to solve and evaluate the solution throughout space, making them a valuable tool in a wide variety of applications. However, MFS and BEM have poor computational scalability and huge memory requirements for large-scale problems, limiting their applicability and efficiency in practice. By leveraging connections with Gaussian Processes and exploiting the sparse structure of the inverses of boundary integral matrices, we introduce a variational preconditioner that can be computed via a sparse inverse-Cholesky factorization in a massively parallel manner. We show that applying our preconditioner to the Preconditioned Conjugate Gradient algorithm greatly improves the efficiency of MFS or BEM solves, up to four orders of magnitude in our series of tests.	https://dl.acm.org/doi/abs/10.1145/3658199	Jiong Chen, Florian Schaefer, Mathieu Desbrun
Lite2Relight: 3D-aware Single Image Portrait Relighting	Achieving photorealistic 3D view synthesis and relighting of human portraits is pivotal for advancing AR/VR applications. Existing methodologies in portrait relighting demonstrate substantial limitations in terms of generalization and 3D consistency, coupled with inaccuracies in physically realistic lighting and identity preservation. Furthermore, personalization from a single view is difficult to achieve and often requires multiview images during the testing phase or involves slow optimization processes. This paper introduces Lite2Relight , a novel technique that can predict 3D consistent head poses of portraits while performing physically plausible light editing at interactive speed. Our method uniquely extends the generative capabilities and efficient volumetric representation of EG3D, leveraging a lightstage dataset to implicitly disentangle face reflectance and perform relighting under target HDRI environment maps. By utilizing a pre-trained geometry-aware encoder and a feature alignment module, we map input images into a relightable 3D space, enhancing them with a strong face geometry and reflectance prior. Through extensive quantitative and qualitative evaluations, we show that our method outperforms the state-of-the-art methods in terms of efficacy, photorealism, and practical application. This includes producing 3D-consistent results of the full head, including hair, eyes, and expressions. Lite2Relight paves the way for large-scale adoption of photorealistic portrait editing in various domains, offering a robust, interactive solution to a previously constrained problem.	https://dl.acm.org/doi/abs/10.1145/3641519.3657470	Pramod Rao, Gereon Fox, Abhimitra Meka, Mallikarjun B R, Fangneng Zhan, Tim Weyrich, Bernd Bickel, Hanspeter Pfister, Wojciech Matusik, Mohamed Elgharib, Christian Theobalt
Look, Ma, No Matrices!	"This talk discusses some of the pitfalls and surprises revealed by the ""Look, Ma, No Matrices!"" [Keninck 2024] project that implements an industry standard forward renderer exclusively using PGA (Projective or Plane-based Geometric Algebra). We will briefly go over some of the challenges and reveal how and insightful application of PGA leads to substantial savings for a typical tangent space normal-mapping setup."	https://dl.acm.org/doi/abs/10.1145/3641233.3665801	Steven De Keninck
Loss of Sonnet 18	This project is an explorative digital art installation that delves into the phenomenon of generation loss in digital signal processing - the inevitable degradation of information (e.g., words) quality through copying and propagation. It transcends the technical realm to probe how this concept applies to the transmission and transformation of words and meanings through time and technology. It consists of a series of videos illustrating the loss of words themselves and their meaning, combined with real-time interpretation from both humans and AI. The project allows the audience to find a border of loss, beyond which, information of words might change.	https://dl.acm.org/doi/abs/10.1145/3641523.3669939	Junxiu Tang, Jiayi Zhou, Yifang Wang, Xinhuan Shu, Peiquan Xia, Xiaojiao Chen, Tan Tang, Yingcai Wu
Love letters without the recipient	An artificial intelligence writing machine by the seaside inscribes lines of love letters, yet the water-written words disappear without a trace as time passes. From day to night, its writing is never completed, and the letters remain unsent. Perhaps many have swallowed their unexpressed emotions, harboring a myriad of words in their hearts but never daring to write the recipient's name or send them out.	https://dl.acm.org/doi/abs/10.1145/3641523.3665168	Ziyao Lin
MOFA: Multiplayer Omnipresent Fighting Arena	"As Mixed Reality (MR) head-mounted displays (HMDs) become more widely used, the context of collocated MR gaming is shifting from controlled, private environments to spontaneous, public settings in-the-wild. This shift transforms everyday locations into potential gaming stages, blurring the line between designated play areas and the public sphere. We term this phenomenon Spontaneous Collocated Mixed Reality (SCMR). This transformation necessitates game designs that accommodate both HMD wearers (""Wizards"") and non-HMD wearers (""Muggles""). To explore the design space of synchronous social bodily interplay in SCMR, we developed the Multiplayer Omnipresent Fighting Arena (MOFA) framework. This framework leverages device asymmetry and role diversification to actively engage all participants. Using this framework, we created five game prototypes—""The Duel"", ""The Dragon"", ""The Ghost"", ""The Training"", and ""The Duck"". These games, inspired by scenarios from fantastic fiction such as Harry Potter, Game of Thrones, and Ghostbusters, demonstrate that strategically involving ""Muggles"" enhances social engagement and acceptance. This helps us to speculate a shared mixed reality future that is more inclusive and entertaining in a ubiquitous and spontaneous manner."	https://dl.acm.org/doi/abs/10.1145/3641521.3664414	Botao Amber Hu, Yuchen Zhang, Sizheng Hao, Yilan Tao
MOVIN TRACIN' : Move Outside the Box	We introduce MOVIN TRACIN', an advanced AI-driven motion capture system that leverages a single LiDAR sensor for precise, real-time, full-body tracking. TRACIN' streamlines the user experience by eliminating the need for traditional suit markers or body-mounted sensors, while delivering motion capture accuracy comparable to conventional systems. The system captures full-body motion at 60 frames per second by upscaling from a 20 fps LiDAR point cloud sequence, making it particularly effective for applications demanding high responsiveness, such as live streaming, metaverse platforms, gaming, and interactive media art. Unlike traditional motion capture systems that often require multiple sensors or high-quality cameras with extensive calibration, our approach uses a single LiDAR sensor with instant calibration. This significantly reduces setup costs, time, and effort, making the system accessible to general users without the need for expert mocap engineers. To address the limitations of a single LiDAR setup, such as self-occlusion, we utilize a generative AI model that generates the most plausible output pose from the learned motion distribution, even with sparse or incomplete input point clouds. Our model maintains real-time performance with a fast inference time of around 50 ms, ensuring suitability for real-time applications. MOVIN TRACIN' also supports an API-SDK for developers, enabling easy integration and streaming to game engines such as Unity and Unreal. In summary, MOVIN TRACIN' represents a significant advancement in motion capture technology, offering:	https://dl.acm.org/doi/abs/10.1145/3641520.3665302	Byeoli Choi, Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang
MVD^2: Efficient Multiview 3D Reconstruction for Multiview Diffusion	Multiview diffusion (MVD) has emerged as a prominent 3D generation technique, acclaimed for its generalizability, quality, and efficiency. MVD models finetune image diffusion models with 3D data to generate multiple views of a 3D object from an image or text prompt, followed by a multiview 3D reconstruction process. However, the sparsity of views and inconsistent details in the generated multiview images pose challenges for 3D reconstruction. We present MVD2, an efficient 3D reconstruction method tailored for MVD images. MVD2 integrates multiview image features into a 3D feature volume, then transforms this volume into a textureless 3D mesh, onto which the MVD images are mapped as textures. It employs a simple-yet-efficient view-dependent training scheme to mitigate discrepancies between MVD images and ground-truth views of 3D shapes, effectively improving 3D generation quality and robustness. MVD2 is trained with 3D collections and MVD images, and the trained MVD2 efficiently reconstructs 3D meshes from multiview images within one second and exhibits great model generalizability in dealing with images generated by various MVD methods. Our code and the pretrained model are available at: https://zhengxinyang.github.io/projects/MVD_square.html.	https://dl.acm.org/doi/abs/10.1145/3641519.3657403	Xin-Yang Zheng, Hao Pan, Yu-Xiao Guo, Xin Tong, Yang Liu
MaPa: Text-driven Photorealistic Material Painting for 3D Shapes	This paper aims to generate materials for 3D meshes from text descriptions. Unlike existing methods that synthesize texture maps, we propose to generate segment-wise procedural material graphs as the appearance representation, which supports high-quality rendering and provides substantial flexibility in editing. Instead of relying on extensive paired data, i.e., 3D meshes with material graphs and corresponding text descriptions, to train a material graph generative model, we propose to leverage the pre-trained 2D diffusion model as a bridge to connect the text and material graphs. Specifically, our approach decomposes a shape into a set of segments and designs a segment-controlled diffusion model to synthesize 2D images that are aligned with mesh parts. Based on generated images, we initialize parameters of material graphs and fine-tune them through the differentiable rendering module to produce materials in accordance with the textual description. Extensive experiments demonstrate the superior performance of our framework in photorealism, resolution, and editability over existing methods.	https://dl.acm.org/doi/abs/10.1145/3641519.3657504	Shangzhan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, Xiaowei Zhou
Making Magic with 3D Volume Style Transfer	"In order to achieve the unique look of a hand-drawn, 2D visual style blended with 3D effects elements in Walt Disney Animation Studios' film ""Wish"", effects artists leveraged new applications of Volumetric Neural Style Transfer (VNST) techniques. VNST, a method for stylizing 3D volume simulations by extracting the textures and shapes from a 2D image, has been increasingly more effective in assisting the stylization of effects through a number of the studio's previous projects. For ""Wish"", the effects artists took advantage of new advancements to inform and accelerate the visual development process, increase the reuse and efficiency of VNST throughout production, and achieve levels of style with detail unachievable with previous methods."	https://dl.acm.org/doi/abs/10.1145/3641233.3664340	Marie Tollec, Mike Navarro
Making STEAM: Build Your Custom Electric Guitar	By teaching the Science, Technology, Engineering, Art, and Math embedded in the objects they create, STEAM-based digital fabrication projects equip the next generation of leaders and problem solvers with the tools to make change and build the future. The Make Your Electric Guitar project has been tested successfully with undergraduate (Figure 1), middle, and high school students (Figure 2) since 2019. As students design and develop their guitar, they not only build a tool for self-expression but also learn iterative design, fabrication, and the physics of sound and circuitry and leave with a working electric guitar.	https://dl.acm.org/doi/abs/10.1145/3641235.3664444	Sara J Sanders
Making of Chameleon Transformation FX in KFP4	In the world of Kung Fu Panda 4, the transformation FX emerges as one of the central narrative elements, showcasing the primary power of the film's key villain, the Chameleon. This effect imbues the character with a scary, unsettling, ability to morph into different forms, varying significantly in size and shape. It enhances the storytelling by allowing the chameleon to be ubiquitously present in various guises and places. Our FX team embarked on a journey to develop a robust transformation system capable of handling numerous characters. It had to be flexible enough to cater to the needs of a large group of artists and adaptable across a wide range of shots from dynamic kung fu action to close-ups with subtle camera movement. Additionally, this work involved cooperative efforts with multiple departments, such as animation, CFX and lighting, resulting in the creation of cross-department workflows.	https://dl.acm.org/doi/abs/10.1145/3641233.3664347	Zachary Glynn, Jinguang Huang, Landon Gray, Hamid Shahsavari, Zhao Wang
Matting by Generation	This paper introduces an innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge. Our method harnesses the capabilities of latent diffusion models, enriched with extensive pre-trained knowledge, to regularize the matting process. We present novel architectural innovations that empower our model to produce mattes with superior resolution and detail. The proposed method is versatile and can perform both guidance-free and guidance-based image matting, accommodating a variety of additional cues. Our comprehensive evaluation across three benchmark datasets demonstrates the superior performance of our approach, both quantitatively and qualitatively. The results not only reflect our method's robust effectiveness but also highlight its ability to generate visually compelling mattes that approach photorealistic quality. The code for this paper is available at https://github.com/lightChaserX/alphaLDM.	https://dl.acm.org/doi/abs/10.1145/3641519.3657519	Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, Shin'Ichi Satoh
Measurement of the Imperceptible Threshold for Color Vibration Pairs Selected by using MacAdam Ellipse	We propose an efficient method for searching for color vibration pairs that are imperceptible to the human eye based on the MacAdam ellipse, an experimentally determined color-difference range that is indistinguishable to the human eye. We created color pairs by selecting eight colors within the sRGB color space specified by the ellipse, and conducted experiments to confirm the threshold of the amplitude of color vibration amplitude at which flicker becomes imperceptible to the human eye. The experimental results indicate a general guideline for acceptable amplitudes for pair selection.	https://dl.acm.org/doi/abs/10.1145/3641234.3671041	Shingo Hattori, Yuichi Hiroi, Takefumi Hiraki
Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance	The synthesis of 3D facial animations from speech has garnered considerable attention. Due to the scarcity of high-quality 4D facial data and well-annotated abundant multi-modality labels, previous methods often suffer from limited realism and a lack of flexible conditioning. We address this challenge through a trilogy. We first introduce Generalized Neural Parametric Facial Asset (GNPFA), an efficient variational auto-encoder mapping facial geometry and images to a highly generalized expression latent space, decoupling expressions and identities. Then, we utilize GNPFA to extract high-quality expressions and accurate head poses from a large array of videos. This presents the M2F-D dataset, a large, diverse, and scan-level co-speech 3D facial animation dataset with well-annotated emotion and style labels. Finally, we propose Media2Face, a diffusion model in GNPFA latent space for co-speech facial animation generation, accepting rich multi-modality guidances from audio, text, and image. Extensive experiments demonstrate that our model not only achieves high fidelity in facial animation synthesis but also broadens the scope of expressiveness and style adaptability in 3D facial animation.	https://dl.acm.org/doi/abs/10.1145/3641519.3657413	Qingcheng Zhao, Pengyu Long, Qixuan Zhang, Dafei Qin, Han Liang, Longwen Zhang, Yingliang Zhang, Jingyi Yu, Lan Xu
Mesh Mortal Kombat: Real-time voxelized soft-body destruction	Mesh Mortal Kombat uses a new non-physical voxel constraint based on Gram-Schmidt orthonormalization within a position-based dynamics (PBD) framework. After the input mesh is voxelized, we create 8 PBD particles which are constrained to approximate each voxel. Breakable face-to-face constraints between voxels, which are also based on Gram-Schmidt, are created for pairs of neighboring voxels. The regular voxel grid allows efficient parallel processing in a compute shader using Gauss-Seidel iterations with only 1 voxel constraint partition and 3 face constraint partitions. This allows us to minimize the number of compute shader dispatches and memory barriers per frame. Destruction is achieved by disabling face-to-face voxel constraints when the strain limit is exceeded. Anisotropic materials can be created by setting different strain limits on x-, y-, and z- faces. This allows meshes to be peeled into sheets, shredded into strips, or fractured into voxels.	https://dl.acm.org/doi/abs/10.1145/3641520.3665307	Tim McGraw
Mesh Neural Cellular Automata	Texture modeling and synthesis are essential for enhancing the realism of virtual environments. Methods that directly synthesize textures in 3D offer distinct advantages to the UV-mapping-based methods as they can create seamless textures and align more closely with the ways textures form in nature. We propose Mesh Neural Cellular Automata (MeshNCA), a method that directly synthesizes dynamic textures on 3D meshes without requiring any UV maps. MeshNCA is a generalized type of cellular automata that can operate on a set of cells arranged on non-grid structures such as the vertices of a 3D mesh. MeshNCA accommodates multi-modal supervision and can be trained using different targets such as images, text prompts, and motion vector fields. Only trained on an Icosphere mesh, MeshNCA shows remarkable test-time generalization and can synthesize textures on unseen meshes in real time. We conduct qualitative and quantitative comparisons to demonstrate that MeshNCA outperforms other 3D texture synthesis methods in terms of generalization and producing high-quality textures. Moreover, we introduce a way of grafting trained MeshNCA instances, enabling interpolation between textures. MeshNCA allows several user interactions including texture density/orientation controls, grafting/regenerate brushes, and motion speed/direction controls. Finally, we implement the forward pass of our MeshNCA model using the WebGL shading language and showcase our trained models in an online interactive demo, which is accessible on personal computers and smartphones and is available at https://meshnca.github.io/.	https://dl.acm.org/doi/abs/10.1145/3658127	Ehsan Pajouheshgar, Yitao Xu, Alexander Mordvintsev, Eyvind Niklasson, Tong Zhang, Sabine Süsstrunk
Meta Quest | Asgard’s Wrath 2 Cinematic Launch	Meta 'Asgard's Wrath 2' covers a trailer and a film which sees a glowing portal interrupt one of the most anticipated NBA moments of 2023. The trailer sends the protagonist into the eye of the game, dodging monsters in a faraway desert land with the trickster Loki looming over them.	https://dl.acm.org/doi/abs/10.1145/3641230.3653524	Ivan Guerrero, John Likens
Metahuman Theatre: Teaching Photogrammetry and Mocap as a Performing Arts Process	I have been designing a real-time Metahuman animation pipeline in our Media Arts, Design and Technology (MADTech) program at NCSU that emphasizes motion capture as a performing arts process to create dramatic scenes with digital doubles. Students developed a 3D self-portrait using photogrammetry in Epic Games Reality Capture software and Mesh to Metahuman plugin in Unreal Engine, and then performed their double using our inertial mocap suits and control rigs in Autodesk Maya and UE5 sequencer. Facial performance was captured using real-time software like Epic's Livelink Face app and Metahuman Animator, completing a virtual production process that emphasized embodied acting and improvisation over scripts and storyboarding. By framing the Metahuman as a performing object like a puppet and adapting design frameworks like Disney's 12 Principles of Animation to performance capture, students learned a technical animation process in the context of a performing arts approach. This produced aesthetics that are different and arguably more performative than the traditional keyframed approach taught in many animation programs.	https://dl.acm.org/doi/abs/10.1145/3641235.3664436	Topher Maraffi
Metapunch X: Combing Multidisplay and Exertion Interaction for Watching and Playing E-sports in Multiverse	E-sports events have become more popular. In the Olympics Esports Series, ten items were included in the official competition; four were exergames, and only one implemented extended reality (XR). However, there are no games that apply the encountered-type haptic feedback with exertion interaction. Also, these esports games are only broadcast on screen, resulting in a lack of sense of presence in the virtual world. In our work, we introduce Metapunch X, an encountered-type haptic feedback esports game that integrates exertion interaction in XR with multi-screen spectating. The game is designed as an asymmetric competition, utilizing an XR head-mounted display (HMD) and a substitutional reality robot to create an esports experience with encountered-type haptic feedback. To enhance the audiences' experience, in addition to the third-person perspective broadcast, we provide 360-degree live streaming available on mobile devices and virtual reality (VR) HMD, allowing audiences to immerse themselves in the virtual environment and experience the competition as if they were personally present at the venue and feel like engaging in a multiverse.	https://dl.acm.org/doi/abs/10.1145/3641521.3664403	Kuan-Ning Chang, Chia-Hui Lin, Yu-Hsiang Weng, Yu-Hin Chan, Po-Hsun Chen, Pei-Cih Zeng, Chien-Hsing Chou, Wen-Hsin Chiu, Ping-Hsuan Han
Minkowski Penalties: Robust Differentiable Constraint Enforcement for Vector Graphics	"This paper describes an optimization-based framework for finding arrangements of 2D shapes subject to pairwise constraints. Such arrangements naturally arise in tasks such as vector illustration and diagram generation, but enforcing these criteria robustly is surprisingly challenging. We approach this problem through the minimization of novel energetic penalties, derived from the signed distance function of the Minkowski difference between interacting shapes. This formulation provides useful gradients even when initialized from a wildly infeasible state, and, unlike many common collision penalties, can handle open curves that do not have a well-defined inside and outside. Moreover, it supports rich features beyond the basic no-overlap condition, such as tangency, containment, and precise padding, which are especially valuable in the vector illustration context. We develop closed-form expressions and efficient approximations of our penalty for standard vector graphics primitives, yielding efficient evaluation and easy implementation within existing automatic differentiation pipelines. The method has already been ""battle-tested"" as a component of public-facing open source software; we demonstrate the utility of the framework via examples from illustration, data visualization, diagram generation, and geometry processing."	https://dl.acm.org/doi/abs/10.1145/3641519.3657495	Jiří Minarčík, Sam Estep, Wode Ni, Keenan Crane
Mirror-Transcending Aerial Imaging System for Multiple Users	We present a system that enables multiple users to share the viewing experience of mirror-transcending aerial imaging, in which virtual objects continuously move between mirrored and physical spaces. The system consists of an aerial-imaging optical system with a wide viewing range and a control system that synchronizes multiple display devices' position and image rendering. We implemented a prototype to display the mirror-transcending aerial image for multiple users. The prototype looks like a three-sided mirror, allowing four to five users to participate in the viewing experience and share their impressions.	https://dl.acm.org/doi/abs/10.1145/3641517.3664380	Motohiro Makiguchi, Ayami Hoshi, Ayaka Sano, Takahiro Kusabuka, Hiroshi Chigira, Takayoshi Mochizuki
Miruoto : Sports event atmosphere visual rendering through real-time image and sound processing system	Did you already imagine how would it be to watch a sport match without sounds? You would miss all this specific sport related sounds but also mostly miss a big part of the atmosphere present in the stadium, that is particular to live events. This is what happens to most Deaf and Hard of Hearing persons. Towards Tokyo 2025 Deaflympics, we developed an AI-based system able to recognize sounds and players motion to render in real time sound related Onomatopoeia over the match video as one could see in Comics or Manga.	https://dl.acm.org/doi/abs/10.1145/3641517.3664393	Guillaume Gourmelen, Shutaro Toriya, Eiko Miya, Naohisa Shioura, Hiroyasu Iwata
MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations	In this work, we present MoConVQ, a novel unified framework for physics-based motion control leveraging scalable discrete representations. Building upon vector quantized variational autoencoders (VQ-VAE) and model-based reinforcement learning, our approach effectively learns motion embeddings from a large, unstructured dataset spanning tens of hours of motion examples. The resultant motion representation not only captures diverse motion skills but also offers a robust and intuitive interface for various applications. We demonstrate the versatility of MoConVQ through several applications: universal tracking control from various motion sources, interactive character control with latent motion representations using supervised learning, physics-based motion generation from natural language descriptions using the GPT framework, and, most interestingly, seamless integration with large language models (LLMs) with in-context learning to tackle complex and abstract tasks.	https://dl.acm.org/doi/abs/10.1145/3658137	Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, Libin Liu
Mob-FGSR: Frame Generation and Super Resolution for Mobile Real-Time Rendering	Recent advances in supersampling for frame generation and super-resolution improve real-time rendering performance significantly. However, because these methods rely heavily on the most recent features of high-end GPUs, they are impractical for mobile platforms, which are limited by lower GPU capabilities and a lack of dedicated optical flow estimation hardware. We propose Mob-FGSR, a novel lightweight supersampling framework tailored for mobile devices that integrates frame generation with super resolution to effectively improve real-time rendering performance. Our method introduces a splat-based motion vectors reconstruction method, which allows for accurate pixel-level motion estimation for both interpolation and extrapolation at desired times without the need for high-end GPUs or rendering data from generated frames. Subsequently, fast image generation models are designed to construct interpolated or extrapolated frames and improve resolution, providing users with a plethora of options. Our runtime models operate without the use of neural networks, ensuring their applicability to mobile devices. Extensive testing shows that our framework outperforms other lightweight solutions and rivals the performance of algorithms designed specifically for high-end GPUs. Our model's minimal runtime is confirmed by on-device testing, demonstrating its potential to benefit a wide range of mobile real-time rendering applications. More information and an Android demo can be found at: https://mob-fgsr.github.io/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657424	Sipeng Yang, Qingchuan Zhu, Junhao Zhuge, Qiang Qiu, Chen Li, Yuzhong Yan, Huihui Xu, Ling-Qi Yan, Xiaogang Jin
Mobile GPU 101	Hello everyone. I'm Ralph Potter. I'm a GPU engineer at Samsung Mobile, where I act as Samsung's primary representative to Khronos and particularly the Vulkan Working Group for the past 5 years. Prior to that I've been a GPU driver engineer and GPU compiler/programming model researcher.	https://dl.acm.org/doi/abs/10.1145/3664475.3664528	Ralph Potter
Modal Folding: Discovering Smooth Folding Patterns for Sheet Materials using Strain-Space Modes	Folding can transform mundane objects such as napkins into stunning works of art. However, finding new folding transformations for sheet materials is a challenging problem that requires expertise and real-world experimentation. In this paper, we present Modal Folding—an automated approach for discovering energetically optimal folding transformations, i.e., large deformations that require little mechanical work. For small deformations, minimizing internal energy for fixed displacement magnitudes leads to the well-known elastic eigenmodes. While linear modes provide promising directions for bending, they cannot capture the rotational motion required for folding. To overcome this limitation, we introduce strain-space modes—nonlinear analogues of elastic eigenmodes that operate on per-element curvatures instead of vertices. Using strain-space modes to determine target curvatures for bending elements, we can generate complex nonlinear folding motions by simply minimizing the sheet's internal energy. Our modal folding approach offers a systematic and automated way to create complex designs. We demonstrate the effectiveness of our method with simulation results for a range of shapes and materials, and validate our designs with physical prototypes.	https://dl.acm.org/doi/abs/10.1145/3641519.3657401	Pengbin Tang, Ronan Hinchet, Roi Poranne, Bernhard Thomaszewski, Stelian Coros
Modeling Ambient Scene Dynamics for Free-view Synthesis	We introduce a novel method for dynamic free-view synthesis of an ambient scenes from a monocular capture bringing a immersive quality to the viewing experience. Our method builds upon the recent advancements in 3D Gaussian Splatting (3DGS) that can faithfully reconstruct complex static scenes. Previous attempts to extend 3DGS to represent dynamics have been confined to bounded scenes or require multi-camera captures, and often fail to generalize to unseen motions, limiting their practical application. Our approach overcomes these constraints by leveraging the periodicity of ambient motions to learn the motion trajectory model, coupled with careful regularization. We also propose important practical strategies to improve the visual quality of the baseline 3DGS static reconstructions and to improve memory efficiency critical for GPU-memory intensive learning. We demonstrate high-quality photorealistic novel view synthesis of several ambient natural scenes with intricate textures and fine structural elements. We show that our method significantly outperforms prior methods both qualitatively and quantitatively. Project page: https://ambientgaussian.github.io/	https://dl.acm.org/doi/abs/10.1145/3641519.3657488	Meng-Li Shih, Jia-Bin Huang, Changil Kim, Rajvi Shah, Johannes Kopf, Chen Gao
Modeling Hair Strands with Roving Capsules	Hair strands can be modeled by sweeping spheres with varying radii along Bézier curves. We ray-trace such shapes by finding intersections of a given ray with a set of capsules dynamically defined at runtime. A substantial performance boost is achieved by systematically eliminating parts of the shape that are guaranteed not to intersect with the given ray. The new intersector is more than twice faster than the previously leading phantom algorithm [Reshetov and Luebke 2018]. This improvement results in a 30% overall performance increase, which includes traversal, shading, and the rendering system overhead. In addition, we derive a parametric form of the swept sphere shapes. This provides a deeper understanding of the properties of such objects compared to the offset surfaces obtained by sweeping circles orthogonal to a given curve. The complete WebGL implementation of our algorithm is available at https://www.shadertoy.com/view/4ffXWs.	https://dl.acm.org/doi/abs/10.1145/3641519.3657450	Alexander Reshetov, David Hart
Modelling a Feather as a Strongly Anisotropic Elastic Shell	Feathers exhibit a highly anisotropic behaviour, governed by their complex hierarchical microstructure composed of individual hairs (barbs) clamped onto a spine (rachis) and attached to each other through tiny hooks (barbules). Previous methods in computer graphics have approximated feathers as strips of cloth, thus failing to capture the particular macroscopic nonlinear behaviour of the feather surface (vane). To investigate the anisotropic properties of a feather vane, we design precise measurement protocols on real feather samples. Our experimental results suggest a linear strain-stress relationship of the feather membrane with orientation-dependent coefficients, as well as an extreme ratio of stiffnesses in the barb and barbule direction, of the order of 104. From these findings we build a simple continuum model for the feather vane, where the vane is represented as a three-parameter anisotropic elastic shell. However, implementing the model numerically reveals severe locking and ill-conditioning issues, due to the extreme stiffness ratio between the barb and the barbule directions. To resolve these issues, we align the mesh along the barb directions and replace the stiffest modes with an inextensibility constraint. We extensively validate our membrane model against real-world laboratory measurements, by using an intermediary microscale model that allows us to limit the number of required lab experiments. Finally, we enrich our membrane model with anisotropic bending, and show its practicality in graphics-like scenarios like a full feather and a larger-scale bird. Code and data for this paper are available at https://gitlab.inria.fr/elan-public-code/feather-shell/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657503	Jean Jouve, Victor Romero, Rahul Narain, Laurence Boissieux, Theodore Kim, Florence Bertails-Descoubes
Monkeys, Chimps & Gorillas: Wētā FX’s (r)Evolutionary Work with Primates	Wētā FX is known for many things in the VFX world, but one of their hallmarks is pretty bananas... From Peter Jackson's King Kong, to Caesar in Rise, Dawn, and War of the Planet of the Apes, through to The Umbrella Academy's loveable and intelligent chimpanzee, Pogo, the team has done it all.	https://dl.acm.org/doi/abs/10.1145/3641232.3649252	Erik Winquist, Kevin Smith, Phillip Leonhardt, Aidan Martin, Paul Story
MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar	The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods. Code and data can be found at https://github.com/aipixel/MonoGaussianAvatar.	https://dl.acm.org/doi/abs/10.1145/3641519.3657499	Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu
Monsters, Family, and Shenanigans, OH MY!  Our CG approach to representation at Nickelodeon Animation	"Telling amazing stories involving beloved characters is at the forefront of what we do at Nickelodeon Animation Studio. It is imperative to tell these stories with characters from diverse backgrounds and experiences, as it is important to see the world through their eyes and perspective. We pride ourselves in our history of demonstrating diversity, equity, and inclusion in our shows, and continue to innovate our CG processes and workflows to keep DEI representation relevant in the broadcast space. Recent productions such as ""Big Nate"", ""Transformers: EarthSpark"", and ""Monster High"" exhibit our commitment to DEI. This panel will demonstrate the efforts, focus, and resoluteness we pursue on being inclusive and authentically representing our characters in our productions."	https://dl.acm.org/doi/abs/10.1145/3641232.3649339	Michael Launder, Areeba Rikhan, Samuel Hale, Rommel Calderon, Leslie Wishnevski
More Than Killmonger Locs: A Style Guide for Black Hair (in Computer Graphics)	We will cover recent advances and ongoing challenges in the depiction of Black hair, otherwise known as kinky, or Afro-textured hair. In computer graphics, the majority hair research has been in the depiction straight or wavy hair. As a result, many aspects of the aesthetics and mechanics of Black hair remain poorly understood. To help fill this gap, we will present , a free guide to creating Black digital hairstyles that we co-authored in collaboration with a community of game artists and Dove. We also cover styling guidelines for 3D models in the , and present , our strand simulation technique specifically designed for Afro-textured hair. Finally, we will suggest future directions for hair research.	https://dl.acm.org/doi/abs/10.1145/3664475.3664535	A. M. Darke, Isaac Olander, Theodore Kim
Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling	We introduce Motion-I2V, a novel framework for consistent and controllable text-guided image-to-video generation (I2V). In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into two stages with explicit motion modeling. For the first stage, we propose a diffusion-based motion field predictor, which focuses on deducing the trajectories of the reference image's pixels. For the second stage, we propose motion-augmented temporal attention to enhance the limited 1-D temporal attention in video latent diffusion models. This module can effectively propagate reference image features to synthesized frames with the guidance of predicted trajectories from the first stage. Compared with existing methods, Motion-I2V can generate more consistent videos even in the presence of large motion and viewpoint variation. By training a sparse trajectory ControlNet for the first stage, Motion-I2V can support users to precisely control motion trajectories and motion regions with sparse trajectory and region. This offers more controllability of the I2V process than solely relying on textual instructions. Additionally, Motion-I2V's second stage naturally supports zero-shot video-to-video translation. Both qualitative and quantitative comparisons demonstrate the advantages of Motion-I2V over prior approaches in consistent and controllable image-to-video generation. Please see our project page at https://xiaoyushi97.github.io/Motion-I2V/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657497	Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li
MotionCtrl: A Unified and Flexible Motion Controller for Video Generation	Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement. Accurate control of both camera and object motion is essential for video generation. However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity. Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion. The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data. Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion. 2) Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos. 3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods. Project page: https://wzhouxiff.github.io/projects/MotionCtrl/ .	https://dl.acm.org/doi/abs/10.1145/3641519.3657518	Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan
Multi-Material Mesh-Based Surface Tracking with Implicit Topology Changes	We introduce a multi-material non-manifold mesh-based surface tracking algorithm that converts self-intersections into topological changes. Our algorithm generalizes prior work on manifold surface tracking with topological changes: it preserves surface features like mesh-based methods, and it robustly handles topological changes like level set methods. Our method also offers improved efficiency and robustness over the state of the art. We demonstrate the effectiveness of the approach on a range of examples, including complex soap film simulations with thousands of interacting bubbles, and boolean unions of non-manifold meshes consisting of millions of triangles.	https://dl.acm.org/doi/abs/10.1145/3658223	Peter Heiss-Synak, Aleksei Kalinov, Malina Strugaru, Arian Etemadi, Huidong Yang, Chris Wojtan
Multi-Role VR Training System for Film Production: Enhancing Collaboration with MetaCrew	Film production within studios has become an industry standard in mature professional filmmaking. However, most training institutions cannot afford the expensive film production equipment essential for providing beginners with collaborative training alongside directors, cinematographers, cameramen, and other film professionals. To address these challenges, we first propose the Film Production Virtual Reality Collaborative Learning (VRCL) system — MetaCrew. We then developed an interface featuring visual symbols for film operations to facilitate smooth communication and learning among beginners using professional terminology in film production. In an experiment with 24 participants, we further explored the impact of the film operation graphical user interface (GUI) design on social interaction factors within our system. Results show that our system and GUI enhance social interaction and learning outcomes among learners. Moreover, our system presents a smaller financial burden and requires less physical space.	https://dl.acm.org/doi/abs/10.1145/3641234.3671022	Zheng Wei, Shan Jin, Wai Tong, David Kei Man Yip, Pan Hui, Xian Xu
N-BVH: Neural ray queries with bounding volume hierarchies	"Neural representations have shown spectacular ability to compress complex signals in a fraction of the raw data size. In 3D computer graphics, the bulk of a scene's memory usage is due to polygons and textures, making them ideal candidates for neural compression. Here, the main challenge lies in finding good trade-offs between efficient compression and cheap inference while minimizing training time. In the context of rendering, we adopt a ray-centric approach to this problem and devise <Formula format=""inline""><TexMath><?TeX $\mathcal {N}$?></TexMath><AltText>Math 1</AltText><File name=""siggraphconferencepapers24-70-inline1"" type=""svg""/></Formula>-BVH, a neural compression architecture designed to answer arbitrary ray queries in 3D. Our compact model is learned from the input geometry and substituted for it whenever a ray intersection is queried by a path-tracing engine. While prior neural compression methods have focused on point queries, ours proposes neural ray queries that integrate seamlessly into standard ray-tracing pipelines. At the core of our method, we employ an adaptive BVH-driven probing scheme to optimize the parameters of a multi-resolution hash grid, focusing its neural capacity on the sparse 3D occupancy swept by the original surfaces. As a result, our <Formula format=""inline""><TexMath><?TeX $\mathcal {N}$?></TexMath><AltText>Math 2</AltText><File name=""siggraphconferencepapers24-70-inline2"" type=""svg""/></Formula>-BVH can serve accurate ray queries from a representation that is more than an order of magnitude more compact, providing faithful approximations of visibility, depth, and appearance attributes. The flexibility of our method allows us to combine and overlap neural and non-neural entities within the same 3D scene and extends to appearance level of detail."	https://dl.acm.org/doi/abs/10.1145/3641519.3657464	Philippe Weier, Alexander Rath, Élie Michel, Iliyan Georgiev, Philipp Slusallek, Tamy Boubekeur
N-Dimensional Gaussians for Fitting of High Dimensional Functions	In the wake of many new ML-inspired approaches for reconstructing and representing high-quality 3D content, recent hybrid and explicitly learned representations exhibit promising performance and quality characteristics. However, their scaling to higher dimensions is challenging, e.g. when accounting for dynamic content with respect to additional parameters such as material properties, illumination, or time. In this paper, we tackle these challenges for an explicit representations based on Gaussian mixture models. With our solutions, we arrive at efficient fitting of compact N-dimensional Gaussian mixtures and enable efficient evaluation at render time: For fast fitting and evaluation, we introduce a high-dimensional culling scheme that efficiently bounds N-D Gaussians, inspired by Locality Sensitive Hashing. For adaptive refinement yet compact representation, we introduce a loss-adaptive density control scheme that incrementally guides the use of additional capacity towards missing details. With these tools we can for the first time represent complex appearance that depends on many input dimensions beyond position or viewing angle within a compact, explicit representation optimized in minutes and rendered in milliseconds.	https://dl.acm.org/doi/abs/10.1145/3641519.3657502	Stavros Diolatzis, Tobias Zirr, Alexander Kuznetsov, Georgios Kopanas, Anton Kaplanyan
NICER: A New and Improved Consumed Endurance and Recovery Metric to Quantify Muscle Fatigue of Mid-Air Interactions	Natural gestures are crucial for mid-air interaction, but predicting and managing muscle fatigue is challenging. Existing torque-based models are limited in their ability to model above-shoulder interactions and to account for fatigue recovery. We introduce a new hybrid model, , which combines a torque-based approach with a new term derived from the empirical measurement of muscle contraction and a recovery factor to account for decreasing fatigue during rest. We evaluated NICER in a mid-air selection task using two interaction methods with different degrees of perceived fatigue. Results show that NICER can accurately model above-shoulder interactions as well as reflect fatigue recovery during rest periods. Moreover, both interaction methods show a stronger correlation with subjective fatigue measurement ( = 0.978/0.976) than a previous model, Cumulative Fatigue ( = 0.966/0.923), confirming that NICER is a powerful analytical tool to predict fatigue across a variety of gesture-based interactive applications.	https://dl.acm.org/doi/abs/10.1145/3658230	Yi Li, Benjamin Tag, Shaozhang Dai, Robert Crowther, Tim Dwyer, Pourang Irani, Barrett Ens
NIMEW - New Interface for Musical Expression on a piece of Wood	A workshop is proposed in which participants will build an electronic circuit on a piece of wood that creates a musical instrument. Creativity and non-conformity are encouraged. Participants will keep their creation with which they can entertain or annoy their family, friends, and colleagues. No prior skills are required; all materials will be provided.	https://dl.acm.org/doi/abs/10.1145/3641236.3664416	Michael Shiloh
Napkinmatic App as a Ubiquitous Pocket AGI: Utility-Context-Sensitive 3D AR XR HCI for Vision-to-LLM	"Napkinmatic Ubiquitous is a seamless spatial computing app platform connecting AI to the real world – and back again. Simply take a picture of anything (or import an image), to load an AI Swiss army knife menu that lets you ""chat with/narrate the image"", load relevant 3D models, chain models, or AI3D create from image to 3D [Chang 2023e], ControlNet [Zhang et al. 2023] transform the image into a masterpiece painting [Chang 2023b], add interior design to an empty room, and more - and augment it back to the real world."	https://dl.acm.org/doi/abs/10.1145/3664294.3664364	Yosun Chang
Navigation-Driven Approximate Convex Decomposition	Approximate convex decomposition – approximating a shape by a set of convex hulls – is a popular approach to creating efficient collision representations for games and simulations. Existing algorithms to construct such decompositions are typically driven by general surface- or volume-based error metrics that can't ignore unreachable internal surfaces nor provide local control over the results. We introduce the problem of navigable approximate convex decomposition: First, define a navigable space for the input shape which other objects in the game or simulation must be able to move through, then find a decomposition which does not overlap that space. We show how to automatically find such navigable space, how to customize it, and we introduce an approximate convex decomposition algorithm that protects it. Our results demonstrate that this approach can generate decompositions that meet application requirements faster and with fewer convex hulls than previous methods, while providing a new level of flexibility in defining what those requirements are.	https://dl.acm.org/doi/abs/10.1145/3641519.3657479	James Andrews
NeRF as a Non-Distant Environment Emitter in Physics-based Inverse Rendering	Physics-based inverse rendering enables joint optimization of shape, material, and lighting based on captured 2D images. To ensure accurate reconstruction, using a light model that closely resembles the captured environment is essential. Although the widely adopted distant environmental lighting model is adequate in many cases, we demonstrate that its inability to capture spatially varying illumination can lead to inaccurate reconstructions in many real-world inverse rendering scenarios. To address this limitation, we incorporate NeRF as a non-distant environment emitter into the inverse rendering pipeline. Additionally, we introduce an emitter importance sampling technique for NeRF to reduce the rendering variance. Through comparisons on both real and synthetic datasets, our results demonstrate that our NeRF-based emitter offers a more precise representation of scene lighting, thereby improving the accuracy of inverse rendering.	https://dl.acm.org/doi/abs/10.1145/3641519.3657404	Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao
NeurCADRecon: Neural Representation for Reconstructing CAD Surfaces by Enforcing Zero Gaussian Curvature	Despite recent advances in reconstructing an organic model with the neural signed distance function (SDF), the high-fidelity reconstruction of a CAD model directly from low-quality unoriented point clouds remains a significant challenge. In this paper, we address this challenge based on the prior observation that the surface of a CAD model is generally composed of piecewise surface patches, each approximately developable even around the feature line. Our approach, named , is self-supervised, and its loss includes a developability term to encourage the Gaussian curvature toward 0 while ensuring fidelity to the input points (see the teaser figure). Noticing that the Gaussian curvature is non-zero at tip points, we introduce a double-trough curve to tolerate the existence of these tip points. Furthermore, we develop a dynamic sampling strategy to deal with situations where the given points are incomplete or too sparse. Since our resulting neural SDFs can clearly manifest sharp feature points/lines, one can easily extract the feature-aligned triangle mesh from the SDF and then decompose it into smooth surface patches, greatly reducing the difficulty of recovering the parametric CAD design. A comprehensive comparison with existing state-of-the-art methods shows the significant advantage of our approach in reconstructing faithful CAD shapes.	https://dl.acm.org/doi/abs/10.1145/3658171	Qiujie Dong, Rui Xu, Pengfei Wang, Shuangmin Chen, Shiqing Xin, Xiaohong Jia, Wenping Wang, Changhe Tu
Neural Bounding	Bounding volumes are an established concept in computer graphics and vision tasks but have seen little change since their early inception. In this work, we study the use of neural networks as bounding volumes. Our key observation is that bounding, which so far has primarily been considered a problem of computational geometry, can be redefined as a problem of learning to classify space into free or occupied. This learning-based approach is particularly advantageous in high-dimensional spaces, such as animated scenes with complex queries, where neural networks are known to excel. However, unlocking neural bounding requires a twist: allowing – but also limiting – false positives, while ensuring that the number of false negatives is strictly zero. We enable such tight and conservative results using a dynamically-weighted asymmetric loss function. Our results show that our neural bounding produces up to an order of magnitude fewer false positives than traditional methods. In addition, we propose an extension of our bounding method using early exits that accelerates query speeds by 25 %. We also demonstrate that our approach is applicable to non-deep learning models that train within seconds. Our project page is at https://wenxin-liu.github.io/neural_bounding/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657442	Stephanie Wenxin Liu, Michael Fischer, Paul D. Yoo, Tobias Ritschel
Neural Control Variates with Automatic Integration	This paper presents a method to leverage arbitrary neural network architecture for control variates. Control variates are crucial in reducing the variance of Monte Carlo integration, but they hinge on finding a function that both correlates with the integrand and has a known analytical integral. Traditional approaches rely on heuristics to choose this function, which might not be expressive enough to correlate well with the integrand. Recent research alleviates this issue by modeling the integrands with a learnable parametric model, such as a neural network. However, the challenge remains in creating an expressive parametric model with a known analytical integral. This paper proposes a novel approach to construct learnable parametric control variates functions from arbitrary neural network architectures. Instead of using a network to approximate the integrand directly, we employ the network to approximate the anti-derivative of the integrand. This allows us to use automatic differentiation to create a function whose integration can be constructed by the antiderivative network. We apply our method to solve partial differential equations using the Walk-on-sphere algorithm [Sawhney and Crane 2020]. Our results indicate that this approach is unbiased using various network architectures and achieves lower variance than other control variate methods.	https://dl.acm.org/doi/abs/10.1145/3641519.3657395	Zilu Li, Guandao Yang, Qingqing Zhao, Xi Deng, Leonidas Guibas, Bharath Hariharan, Gordon Wetzstein
Neural Gaussian Scale-Space Fields	Gaussian scale spaces are a cornerstone of signal representation and processing, with applications in filtering, multiscale analysis, anti-aliasing, and many more. However, obtaining such a scale space is costly and cumbersome, in particular for continuous representations such as neural fields. We present an efficient and lightweight method to learn the fully continuous, anisotropic Gaussian scale space of an arbitrary signal. Based on Fourier feature modulation and Lipschitz bounding, our approach is trained self-supervised, i.e., training does not require any manual filtering. Our neural Gaussian scale-space fields faithfully capture multiscale representations across a broad range of modalities, and support a diverse set of applications. These include images, geometry, light-stage data, texture anti-aliasing, and multiscale optimization.	https://dl.acm.org/doi/abs/10.1145/3658163	Felix Mujkanovic, Ntumba Elie Nsampi, Christian Theobalt, Hans-Peter Seidel, Thomas Leimkühler
Neural Geometry Fields For Meshes	Recent work on using neural fields to represent surfaces has resulted in significant improvements in representational capability and computational efficiency. However, to our knowledge, most existing work has focused on implicit representations such as signed distance fields or volumes, and little work has explored their application to discrete surface geometry, i.e., 3D meshes, limiting the applicability of neural surface representations. We present Neural Geometry Fields, a neural representation for discrete surface geometry represented by triangle meshes. Our idea is to represent the target surface using a coarse set of quadrangular patches, and add surface details using coordinate neural networks by displacing the patches. We then extract a traditional triangular mesh from a neural geometry field instance by sampling the displacement. We show that our representation excels in mesh compression, where it significantly reduces the memory footprint of meshes without compromising on surface details.	https://dl.acm.org/doi/abs/10.1145/3641519.3657399	Venkataram Edavamadathil Sivaram, Tzu-Mao Li, Ravi Ramamoorthi
Neural Monte Carlo Fluid Simulation	The idea of using a neural network to represent continuous vector fields (i.e., neural fields) has become popular for solving PDEs arising from physics simulations. Here, the classical spatial discretization (e.g., finite difference) of PDE solvers is replaced with a neural network that models a differentiable function, so the spatial gradients of the PDEs can be readily computed via autodifferentiation. When used in fluid simulation, however, neural fields fail to capture many important phenomena, such as the vortex shedding experienced in the von Kármán vortex street experiment. We present a novel neural network representation for fluid simulation that augments neural fields with explicitly enforced boundary conditions as well as a Monte Carlo pressure solver to get rid of all weakly enforced boundary conditions. Our method, the Neural Monte Carlo method (NMC), is completely mesh-free, i.e., it doesn't depend on any grid-based discretization. While NMC does not achieve the state-of-the-art accuracy of the well-established grid-based methods, it significantly outperforms previous mesh-free neural fluid methods on fluid flows involving intricate boundaries and turbulence regimes.	https://dl.acm.org/doi/abs/10.1145/3641519.3657438	Pranav Jain, Ziyin Qu, Peter Yichen Chen, Oded Stein
Neural Slicer for Multi-Axis 3D Printing	We introduce a novel neural network-based computational pipeline as a representation-agnostic slicer for multi-axis 3D printing. This advanced slicer can work on models with diverse representations and intricate topology. The approach involves employing neural networks to establish a deformation mapping, defining a scalar field in the space surrounding an input model. Isosurfaces are subsequently extracted from this field to generate curved layers for 3D printing. Creating a differentiable pipeline enables us to optimize the mapping through loss functions directly defined on the field gradients as the local printing directions. New loss functions have been introduced to meet the manufacturing objectives of support-free and strength reinforcement. Our new computation pipeline relies less on the initial values of the field and can generate slicing results with significantly improved performance.	https://dl.acm.org/doi/abs/10.1145/3658212	Tao Liu, Tianyu Zhang, Yongxue Chen, Yuming Huang, Charlie C. L. Wang
Neural-Assisted Homogenization of Yarn-Level Cloth	Real-world fabrics, composed of threads and yarns, often display complex stress-strain relationships, making their homogenization a challenging task for fast simulation by continuum-based models. Consequently, existing homogenized yarn-level models frequently struggle with numerical stability without line search at large time steps, forcing a trade-off between model accuracy and stability. In this paper, we propose a neural-assisted homogenized constitutive model for simulating yarn-level cloth. Unlike analytic models, a neural model is advantageous in adapting to complex dynamic behaviors, and its inherent smoothness naturally mitigates stability issues. We also introduce a sector-based warm-start strategy to accelerate the data collection process in homogenization. This model is trained using collected strain energy datasets and its accuracy is validated through both qualitative and quantitative experiments. Thanks to our model's stability, our simulator can now achieve two-orders-of-magnitude speedups with large time steps compared to previous models.	https://dl.acm.org/doi/abs/10.1145/3641519.3657411	Xudong Feng, Huamin Wang, Yin Yang, Weiwei Xu
NeuralTO: Neural Reconstruction and View Synthesis of Translucent Objects	Learning from multi-view images using neural implicit signed distance functions shows impressive performance on 3D Reconstruction of opaque objects. However, existing methods struggle to reconstruct accurate geometry when applied to translucent objects due to the non-negligible bias in their rendering function. To address the inaccuracies in the existing model, we have reparameterized the density function of the neural radiance field by incorporating an estimated constant extinction coefficient. This modification forms the basis of our innovative framework, which is geared towards highfidelity surface reconstruction and the novel-view synthesis of translucent objects. Our framework contains two stages. In the reconstruction stage, we introduce a novel weight function to achieve accurate surface geometry reconstruction. Following the recovery of geometry, the second phase involves learning the distinct scattering properties of the participating media to enhance rendering. A comprehensive dataset, comprising both synthetic and real translucent objects, has been built for conducting extensive experiments. Experiments reveal that our method outperforms existing approaches in terms of reconstruction and novel-view synthesis.	https://dl.acm.org/doi/abs/10.1145/3658186	Yuxiang Cai, Jiaxiong Qiu, Zhong Li, Bo Ren
Neuro-Symbolic Transformation of Architectural Facades into Their Procedural Representations	We introduce a neuro-symbolic transformer model that converts flat, segmented facade structures into procedural definitions using a custom-designed split grammar. To facilitate this, we first develop a split grammar tailored for architectural facades and generate a dataset of facades alongside their procedural representations. This dataset is used to train our transformer model to convert segmented, flat facades into the procedural language of our grammar.	https://dl.acm.org/doi/abs/10.1145/3641234.3671063	Aleksander Płocharski, Jan Swidzinski, Joanna Porter-Sobieraj, Przemyslaw Musialski
Neutral Tone Mapping for PBR Color Accuracy	"On e-commerce websites, interactive 3D product models are side-by-side with sRGB product photos that have been carefully color graded in post to achieve the desired marketing look. For color consistency, e-commerce production requires a tone mapper that faithfully reproduces 3D model material colors on the screen under neutral (grayscale) lighting to match the photos. This allows artists to build 3D models using marking-approved sRGB color swatches without needing to later tweak the material values to make the output align to existing product images. A neutral tone mapper has been developed at the Khronos 3D Commerce working group precisely to address this need. The goal is to standardize it and make it an available option across authoring tools and renderers as an improved alternative to disabling tone mapping entirely when no ""look"" is desired."	https://dl.acm.org/doi/abs/10.1145/3641233.3664313	Emmett Lalish
Non-Hermitian Absorbing Layers for Schrödinger's Smoke	This paper proposes novel open boundary conditions for incompressible Schrödinger flow (ISF) by introducing a new non-Hermitian term to the original Schrödinger's equation, effective only within narrow-banded layers. Unlike previous work that explicitly requires auxiliary variables, our method achieves the same objective without such complexity, facilitating implementation. We demonstrate that our method retains the benefits of the original ISF while robustly dissipating velocity without noticeable reflections.	https://dl.acm.org/doi/abs/10.1145/3641234.3671033	Naoyuki Hirasawa, Takashi Kanai, Ryoichi Ando
Non-Line-of-Sight Imaging based on Dual Photography using Leaked EM Waves	In this study, we propose a novel method for Non-Line-of-Sight (NLoS) imaging utilizing leaked electromagnetic (EM) waves. Our approach leverages the ability to estimate images from projected by a hidden light source (projector) from leaked EM waves. By estimating the projected pattern from a hidden light source and capturing the diffuse surface of a wall illuminated by the reflected light of the hidden object, we demonstrate the reconstruction images of objects occluded from direct view. We show that leaked EM waves can be exploited to enhance NLoS imaging techniques, opening up new possibilities for reconstructing scenes beyond the direct field of view.	https://dl.acm.org/doi/abs/10.1145/3641234.3671030	Masaya Oishi, Taiki Kitazawa, Yuichi Hayashi, Hiroyuki Kubo
Nukabot: design of human-microbe-computer entanglement	We present the latest development of Nukabot, a human-computer interaction mediation system that connects the fermented food microbiome with its human carer through voice communication. A nukadoko is a traditional Japanese fermentation technique that involves rice bran mixed with salt and water (nuka) put in a wooden, enamel, or plastic container (doko). The current version of Nukabot is a porcelain nukadoko equipped with chemical sensors to track the fermentation status, voice recognition, and vocal synthesis engines that enable humans to talk and ask questions about the ferment.	https://dl.acm.org/doi/abs/10.1145/3641517.3665786	Dominique Chen, Youngah Seong, Kazuhiro Jo
O, What an Iridescent Web We Weave: Rendering Physically Inspired Spider Webs for Visual Effects	The presence of realistic spider webs can be used to establish plausible, visually appealing environments, providing a sense of immersion and authenticity to computer generated worlds. When lit, cobwebs transform into displays of iridescent colors and patterns, resulting from the interaction between the light and the structure of the silk fibers. We propose a straightforward, physically inspired method for rendering cobwebs using common VFX production software, replicating the iridescent visual appearance of spider webs for added realism in virtual environments.	https://dl.acm.org/doi/abs/10.1145/3641234.3671069	Vaya Simeonova, Eike Falk Anderson
Object-level Scene Deocclusion	Deoccluding the hidden portions of objects in a scene is a formidable task, particularly when addressing real-world scenes. In this paper, we present a new self-supervised PArallel visible-to-COmplete diffusion framework, named PACO, a foundation model for object-level scene deocclusion. Leveraging the rich prior of pre-trained models, we first design the parallel variational autoencoder, which produces a full-view feature map that simultaneously encodes multiple complete objects, and the visible-to-complete latent generator, which learns to implicitly predict the full-view feature map from partial-view feature map and text prompts extracted from the incomplete objects in the input image. To train PACO, we create a large-scale dataset with 500k samples to enable self-supervised learning, avoiding tedious annotations of the amodal masks and occluded regions. At inference, we devise a layer-wise deocclusion strategy to improve efficiency while maintaining the deocclusion quality. Extensive experiments on COCOA and various real-world scenes demonstrate the superior capability of PACO for scene deocclusion, surpassing the state of the arts by a large margin. Our method can also be extended to cross-domain scenes and novel categories that are not covered by the training set. Further, we demonstrate the deocclusion applicability of PACO in single-view 3D scene reconstruction and object recomposition. Project page: https://liuzhengzhe.github.io/Deocclude-Any-Object.github.io/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657409	Zhengzhe Liu, Qing Liu, Chirui Chang, Jianming Zhang, Daniil Pakhomov, Haitian Zheng, Zhe Lin, Daniel Cohen-Or, Chi-Wing Fu
On the 8th Day	It took 7 days to create the world, it only took one to disrupt its balance.	https://dl.acm.org/doi/abs/10.1145/3641230.3649285	Agathe Sénéchal, Alicia Massez, Elise Debruyne, Flavie Carin, Théo Duhautois
One Noise to Rule Them All: Learning a Unified Model of Spatially-Varying Noise Patterns	"Procedural noise is a fundamental component of computer graphics pipelines, offering a flexible way to generate textures that exhibit ""natural"" random variation. Many different types of noise exist, each produced by a separate algorithm. In this paper, we present a single generative model which can learn to generate multiple types of noise as well as blend between them. In addition, it is capable of producing spatially-varying noise blends despite not having access to such data for training. These features are enabled by training a denoising diffusion model using a novel combination of data augmentation and network conditioning techniques. Like procedural noise generators, the model's behavior is controllable via interpretable parameters plus a source of randomness. We use our model to produce a variety of visually compelling noise textures. We also present an application of our model to improving inverse procedural material design; using our model in place of fixed-type noise nodes in a procedural material graph results in higher-fidelity material reconstructions without needing to know the type of noise in advance. Open-sourced materials can be found at https://armanmaesumi.github.io/onenoise/"	https://dl.acm.org/doi/abs/10.1145/3658195	Arman Maesumi, Dylan Hu, Krishi Saripalli, Vladimir Kim, Matthew Fisher, Soeren Pirk, Daniel Ritchie
Oozing Hand Crafted 2D Illustration Into a 3D Ninja Turtle World	TMNT: Mutant Mayhem took on the monumental challenge of modernizing a very well-known and beloved franchise. It was important to the director, Jeff Rowe, that the audience believe the turtles were real teenagers... from their awkward bodies to how they acted (including their voices which were recordings of actual teenagers). The modeling also had to reveal all the imperfections related to their mutant nature, serving the idea of a teenager's insecure life. In line with this teenager vibe, the director and the production designer, Yashar Kassai, established the idea that the film design would suggest that it was created by an immature teenager drawing this movie in his sketchbook, with all of his naivety and energy.	https://dl.acm.org/doi/abs/10.1145/3641232.3649243	Jacques Daigle, Brian Gossart, Olivier Mitonneau
Optimization in Computer Graphics and Interactive Techniques	Like a semester long graduate seminar on Optimization in Computer Graphics and Interactive Techniques, this course looks at Optimization through the lens of 13 technical papers selected by the lecturers. The lecturers will highlight trends, similarities, differences, and historical threads through the papers. The papers will cover a range of topics including numerical solutions, objective functions, discrete and continuous optimization, dimensionality reduction, and frictional contact. Applications will range from image segmentation to truss structures to real-time rendering. The course will also serve as a retrospective on the selected papers, placing them in historical perspective and highlighting significant contributions as well as forgotten gems. The lecturers have broad expertise across computer graphics and interactive techniques and have co-led the VANGOGH lab meeting at UMBC since 2015.	https://dl.acm.org/doi/abs/10.1145/3664475.3664562	Adam W. Bargteil, Marc Olano
Optimizing Assets for Authoring and Consumption in USD	"Walt Disney Animation Studios has used Universal Scene Description (USD) [Pixar 2016] as the backbone of its production pipeline since ""Encanto"" in 2021[Miller et al. 2022]. In this talk, we introduce a new asset structure that addresses speed issues from our initial asset structure design and vastly simplifies asset authorship. The new asset structure helped to (1) streamline and decouple asset authorship and shot consumption, (2) enable new authoring workflows that better take advantage of USD's multi-stage model and (3) open the door for shot focused asset-based optimizations."	https://dl.acm.org/doi/abs/10.1145/3641233.3664335	Harmony Li, George Rieckenberg, Neelima Karanam, Emily Vo, Kelsey Hurley
Origami	Just like in life, being born from the earth, a square paper is transformed into various origami creatures and brought to life.	https://dl.acm.org/doi/abs/10.1145/3641230.3653525	Kei Kanamori
PEA-PODs: Perceptual Evaluation of Algorithms for Power Optimization in XR Displays	Display power consumption is an emerging concern for untethered devices. This goes double for augmented and virtual extended reality (XR) displays, which target high refresh rates and high resolutions while conforming to an ergonomically light form factor. A number of image mapping techniques have been proposed to extend battery usage. However, there is currently no comprehensive quantitative understanding of how the power savings provided by these methods compare to their impact on visual quality. We set out to answer this question. To this end, we present a perceptual evaluation of algorithms (PEA) for power optimization in XR displays (PODs). Consolidating a portfolio of six power-saving display mapping approaches, we begin by performing a large-scale perceptual study to understand the impact of each method on perceived quality in the wild. This results in a unified quality score for each technique, scaled in just-objectionable-difference (JOD) units. In parallel, each technique is analyzed using hardware-accurate power models. The resulting JOD-to-Milliwatt transfer function provides a first-of-its-kind look into tradeoffs offered by display mapping techniques, and can be directly employed to make architectural decisions for power budgets on XR displays. Finally, we leverage our study data and power models to address important display power applications like the choice of display primary, power implications of eye tracking, and more .	https://dl.acm.org/doi/abs/10.1145/3658126	Kenneth Chen, Thomas Wan, Nathan Matsuda, Ajit Ninan, Alexandre Chapiro, Qi Sun
Papa Bouteille	We follow an 8 years old child's daily life. From their eyes, we see their dad's alcoholism.	https://dl.acm.org/doi/abs/10.1145/3641230.3649107	Lucie Carraz, Eleonore Deprun, Lalita Bruguiere, Fred Marcassoli, Clément Bretou, Pierre Étienne Mazet
Part123: Part-aware 3D Reconstruction from a Single-view Image	Recently, the emergence of diffusion models has opened up new opportunities for single-view reconstruction. However, all the existing methods represent the target object as a closed mesh devoid of any structural information, thus neglecting the part-based structure, which is crucial for many downstream applications, of the reconstructed shape. Moreover, the generated meshes usually suffer from large noises, unsmooth surfaces, and blurry textures, making it challenging to obtain satisfactory part segments using 3D segmentation techniques. In this paper, we present Part123, a novel framework for part-aware 3D reconstruction from a single-view image. We first use diffusion models to generate multiview-consistent images from a given image, and then leverage Segment Anything Model (SAM), which demonstrates powerful generalization ability on arbitrary objects, to generate multiview segmentation masks. To effectively incorporate 2D part-based information into 3D reconstruction and handle inconsistency, we introduce contrastive learning into a neural rendering framework to learn a part-aware feature space based on the multiview segmentation masks. A clustering-based algorithm is also developed to automatically derive 3D part segmentation results from the reconstructed models. Experiments show that our method can generate 3D models with high-quality segmented parts on various objects. Compared to existing unstructured reconstruction methods, the part-aware 3D models from our method benefit some important applications, including feature-preserving reconstruction, primitive fitting, and 3D shape editing.	https://dl.acm.org/doi/abs/10.1145/3641519.3657482	Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, Wenping Wang
Path-Space Differentiable Rendering of Implicit Surfaces	Physics-based differentiable rendering is a key ingredient for integrating forward rendering into probabilistic inference and machine learning pipelines. As a state-of-the-art formulation for differentiable rendering, differential path integrals have enabled the development of efficient Monte Carlo estimators for both interior and boundary integrals. Unfortunately, this formulation has been designed mostly for explicit geometries like polygonal meshes. In this paper, we generalize the theory of differential path integrals to support implicit geometries like level sets and signed-distance functions (SDFs). In addition, we introduce new Monte Carlo estimators for efficiently sampling discontinuity boundaries that are also implicitly specified. We demonstrate the effectiveness of our theory and algorithms using several differentiable-rendering and inverse-rendering examples.	https://dl.acm.org/doi/abs/10.1145/3641519.3657473	Siwei Zhou, Youngha Chang, Nobuhiko Mukai, Hiroaki Santo, Fumio Okura, Yasuyuki Matsushita, Shuang Zhao
Patterns	Struck by a barrage of intrusive encounters on his weekly commute, a perceptive traveler struggles to uphold the positive mindset that wards against the ever-lurking grayness.	https://dl.acm.org/doi/abs/10.1145/3641230.3646969	Alex Glawion
Paumo D'amour	Instead of giving his wife the attention she deserves, Dédé sends all his love to his vegetable garden. One day, all his delicious tomatoes mysteriously disappear.	https://dl.acm.org/doi/abs/10.1145/3641230.3648635	Ian Halley, Nathan Hermouet, Luka Croisez, Laurène Vaubourdolle, Jade Van De Veire
Perceptual Evaluation of Steered Retinal Projection	Steered retinal projection (SRP) is an emerging display technology that combines retinal projection and pupil steering to achieve exceptional light efficiency and a consistent viewing experience. Retinal projection enables most photons from a display projector to reach the retina, and pupil steering dynamically aligns the narrow viewing window of the retinal projection with the eye. While SRP holds considerable promise, its development has been stagnant due to a lack of understanding how human vision reacts to the dynamic steering movement of the viewing window. To delve into these areas, this study introduces the first SRP system testbed specifically designed for perceptual studies on the viewing experience of pupil steering. The testbed replicates the SRP viewing experience and offers the flexibility in adjusting several parameters including steering resolution, accuracy, and latency. We conducted two perceptual studies utilizing the testbed. The first study investigates the impact of saccadic suppression, a phenomenon that reduces visual sensitivity during rapid eye movements, on the SRP viewing experience. The second study explores the trade space between eye-tracking and pupil steering performance, providing insights into the optimal balance between these factors. Additionally, we introduce a numerical model to predict the detection probability for SRP artifacts considering the temporal characteristics of global luminance and the human vision system. This model enables a more comprehensive interpretation of user study and provides preliminary hardware requirements for SRP systems. The findings from this study offer invaluable research directions that may help determine component-level development milestones for SRP development, paving the way for the practical implementation of this promising technology.	https://dl.acm.org/doi/abs/10.1145/3641519.3657486	Seungjae Lee, Seung-Woo Nam, Kevin Rio, Renate Landig, Hsien-Hui Cheng, Lu Lu, Barry Silverstein
Performance driven Character Effects in “The Garfield Movie”	"The stylised animation in the ""The Garfield Movie"" presented a number of technical challenges especially for the fur which included detached limbs sliding over the surface of the body which required the groom to maintain its styling and interact reliably and predictably. Additionally limbs and body regions could be stretched drastically to support the extreme poses and stylised facial expressions requiring the fur to dynamically maintain density in those regions."	https://dl.acm.org/doi/abs/10.1145/3641233.3664306	Kristin Farrensteiner, Freddy Chaleur, Christopher Lorrimar Kilshaw, Gunjan Kathale, Francesca Galluzzi, Francesco DelFuoco
Phases: Augmenting a Live Audio-Visual Performance with AR	Our performance combines stage visuals with Augmented Reality (AR) to demonstrate a novel way of experiencing live events. Our goal is to enhance the sociability of such events by showcasing the potential of AR glasses over smartphone technology, which forces the audience to look at their phones and isolates them from their environment. Our project explores how the latest AR and visual technology enhance the spectator experience while retaining the social and inclusive environment of live performances.	https://dl.acm.org/doi/abs/10.1145/3641520.3665310	Dávid Maruscsák, Brett Bolton, Bent Stamnes, Matt Swoboda, Christian Sandor
Physical Non-inertial Poser (PNP): Modeling Non-inertial Effects in Sparse-inertial Human Motion Capture	Existing inertial motion capture techniques use the human root coordinate frame to estimate local poses and treat it as an inertial frame by default. We argue that when the root has linear acceleration or rotation, the root frame should be considered non-inertial theoretically. In this paper, we model the fictitious forces that are non-neglectable in a non-inertial frame by an auto-regressive estimator delicately designed following physics. With the fictitious forces, the force-related IMU measurement (accelerations) can be correctly compensated in the non-inertial frame and thus Newton's laws of motion are satisfied. In this case, the relationship between the accelerations and body motions is deterministic and learnable, and we train a neural network to model it for better motion capture. Furthermore, to train the neural network with synthetic data, we develop an IMU synthesis by simulation strategy to better model the noise model of IMU hardware and allow parameter tuning to fit different hardware. This strategy not only establishes the network training with synthetic data but also enables calibration error modeling to handle bad motion capture calibration, increasing the robustness of the system. Code is available at https://xinyu-yi.github.io/PNP/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657436	Xinyu Yi, Yuxiao Zhou, Feng Xu
Physics-Informed Learning of Characteristic Trajectories for Smoke Reconstruction	We delve into the physics-informed neural reconstruction of smoke and obstacles through sparse-view RGB videos, tackling challenges arising from limited observation of complex dynamics. Existing physics-informed neural networks often emphasize short-term physics constraints, leaving the proper preservation of long-term conservation less explored. We introduce Neural Characteristic Trajectory Fields, a novel representation utilizing Eulerian neural fields to implicitly model Lagrangian fluid trajectories. This topology-free, auto-differentiable representation facilitates efficient flow map calculations between arbitrary frames as well as efficient velocity extraction via auto-differentiation. Consequently, it enables end-to-end supervision covering long-term conservation and short-term physics priors. Building on the representation, we propose physics-informed trajectory learning and integration into NeRF-based scene reconstruction. We enable advanced obstacle handling through self-supervised scene decomposition and seamless integrated boundary constraints. Our results showcase the ability to overcome challenges like occlusion uncertainty, density-color ambiguity, and static-dynamic entanglements. Code and sample tests are at https://github.com/19reborn/PICT_smoke.	https://dl.acm.org/doi/abs/10.1145/3641519.3657483	Yiming Wang, Siyu Tang, Mengyu Chu
Physics-based Scene Layout Generation from Human Motion	Creating scenes for captured motions that achieve realistic human-scene interaction is crucial for 3D animation in movies or video games. As character motion is often captured in a blue-screened studio without real furniture or objects in place, there may be a discrepancy between the planned motion and the captured one. This gives rise to the need for automatic scene layout generation to relieve the burdens of selecting and positioning furniture and objects. Previous approaches cannot avoid artifacts like penetration and floating due to the lack of physical constraints. Furthermore, some heavily rely on specific data to learn the contact affordances, restricting the generalization ability to different motions. In this work, we present a physics-based approach that simultaneously optimizes a scene layout generator and simulates a moving human in a physics simulator. To attain plausible and realistic interaction motions, our method explicitly introduces physical constraints. To automatically recover and generate the scene layout, we minimize the motion tracking errors to identify the objects that can afford interaction. We use reinforcement learning to perform a dual-optimization of both the character motion imitation controller and the scene layout generator. To facilitate the optimization, we reshape the tracking rewards and devise pose prior guidance obtained from our estimated pseudo-contact labels. We evaluate our method using motions from SAMP and PROX, and demonstrate physically plausible scene layout reconstruction compared with the previous kinematics-based method.	https://dl.acm.org/doi/abs/10.1145/3641519.3657517	Jianan Li, Tao Huang, Qingxu Zhu, Tien-Tsin Wong
PictorialAttributes: Depicting Multiple Attributes with Realistic Imaging	Traditional visualizations often use abstract graphics, limiting understanding and memorability. Existing methods for pictorial visualization are more engaging, but often create disjointed compositions. To address this, we propose PictorialAttributes, a technique utilizing LLMs and diffusion models to depict data attributes. Examples show its promise for compelling and informative pictorial visualizations.	https://dl.acm.org/doi/abs/10.1145/3641234.3671072	Omer Dahary, Min Lu, Or Patashnik, Daniel Cohen-Or
Picture (Im-) Perfect - Exploring Imperfections in Computer Generated Rendering	Computer graphics advances rapidly advance new innovations in animation practices aiming for ever more realism - calculating exact symmetry, perfect lines and flawless shadows that follow the physicality of reality. However, these technical advancements present a departure from traditional drawing techniques and physical artistry and handcraft, which often embrace originality, experimentation, and individual authorship in animation practices. This project explores hand-drawing and hand-painting techniques in the context of 3D shading and texturing, reintroducing a much-needed return to traditional artistic skills through a mixture of hand-painting and procedural texturing to3D shading. Reaching beyond hyper-realism, this illustrates a new synthesis between 2D drawing and 3D shading techniques.	https://dl.acm.org/doi/abs/10.1145/3641234.3671088	Jolie Che, Oliver Gingrich
Pixar's Inside Out 2: Character Rig Challenges & Techniques	The characters team on Pixar's Inside Out 2 shares some of the technical & design challenges on our character rigs and presents the techniques used to solve them.	https://dl.acm.org/doi/abs/10.1145/3641233.3664342	Jacob Speirs, Christian Hoffman, Michael Nieves, Brenda Lin Zhang
Pixar's Win or Lose - Stylized FX in an Animated Series	Win or Lose, Pixar's first original venture into episodic long-form storytelling, features a variety of stylized looks and visual effects as diverse as its cast and set of perspectives. To face these challenges, the effects team was formed from a group of multidisciplinary artists from typically separate groups across the studio. We worked under a philosophical mindset focused on collaboration across departments, lightweight and nimble experimentation, and an eye for global impact over polished detail. Each episode provided its own unique set of challenges and opportunities to exercise these philosophies, as well as successes and failures. We discuss examples from our production that highlight the process of creating these stylized effects.	https://dl.acm.org/doi/abs/10.1145/3641233.3664333	Edwin Chang, Eric Lacroix, Aimei Kutt
Polymorph, a minimal input procedural modeling tool for rapid prototyping of stop motion puppets at LAIKA	Polymorph is a minimal input voxel-poly toolset for interactive creation of stop motion puppet head and neck assemblies. At its core it generates procedural 3D print-ready geometry for replacement facial animation.	https://dl.acm.org/doi/abs/10.1145/3641233.3664348	Tyler Fermelis, Kingman Gallagher
Portrait3D: Text-Guided High-Quality 3D Portrait Generation Using Pyramid Representation and GANs Prior	"Existing neural rendering-based text-to-3D-portrait generation methods typically make use of human geometry prior and diffusion models to obtain guidance. However, relying solely on geometry information introduces issues such as the Janus problem, over-saturation, and over-smoothing. We present , a novel neural rendering-based framework with a novel joint geometry-appearance prior to achieve text-to-3D-portrait generation that overcomes the aforementioned issues. To accomplish this, we train a 3D portrait generator, 3DPortraitGAN, as a robust prior. This generator is capable of producing 360° canonical 3D portraits, serving as a starting point for the subsequent diffusion-based generation process. To mitigate the ""grid-like"" artifact caused by the high-frequency information in the feature-map-based 3D representation commonly used by most 3D-aware GANs, we integrate a novel 3D representation into 3DPortraitGAN. To generate 3D portraits from text, we first project a randomly generated image aligned with the given prompt into the pre-trained 3DPortraitGAN's latent space. The resulting latent code is then used to synthesize a Beginning with the obtained , we use score distillation sampling to distill the diffusion model's knowledge into the Following that, we utilize the diffusion model to refine the rendered images of the 3D portrait and then use these refined images as training data to further optimize the , effectively eliminating issues with unrealistic color and unnatural artifacts. Our experimental results show that Portrait3D can produce realistic, high-quality, and canonical 3D portraits that align with the prompt."	https://dl.acm.org/doi/abs/10.1145/3658162	Yiqian Wu, Hao Xu, Xiangjun Tang, Xien Chen, Siyu Tang, Zhebin Zhang, Chen Li, Xiaogang Jin
Position-Based Nonlinear Gauss-Seidel for Quasistatic Hyperelasticity	Position based dynamics [Müller et al. 2007] is a powerful technique for simulating a variety of materials. Its primary strength is its robustness when run with limited computational budget. Even though PBD is based on the projection of static constraints, it does not work well for quasistatic problems. This is particularly relevant since the efficient creation of large data sets of plausible, but not necessarily accurate elastic equilibria is of increasing importance with the emergence of quasistatic neural networks [Bailey et al. 2018; Chentanez et al. 2020; Jin et al. 2022; Luo et al. 2020]. Recent work [Macklin et al. 2016] has shown that PBD can be related to the Gauss-Seidel approximation of a Lagrange multiplier formulation of backward Euler time stepping, where each constraint is solved/projected independently of the others in an iterative fashion. We show that a position-based, rather than constraint-based nonlinear Gauss-Seidel approach resolves a number of issues with PBD, particularly in the quasistatic setting. Our approach retains the essential PBD feature of stable behavior with constrained computational budgets, but also allows for convergent behavior with expanded budgets. We demonstrate the efficacy of our method on a variety of representative hyperelastic problems and show that both successive over relaxation (SOR), Chebyshev and multiresolution-based acceleration can be easily applied.	https://dl.acm.org/doi/abs/10.1145/3658154	Yizhou Chen, Yushan Han, Jingyu Chen, Zhan Zhang, Alex Mcadams, Joseph Teran
Practical Error Estimation for Denoised Monte Carlo Image Synthesis	We present a practical global error estimation technique for Monte Carlo ray tracing combined with deep learning based denoising. Our method uses aggregated estimates of bias and variance to determine the squared error distribution of the pixels. Unlike unbiased estimates for classical Monte Carlo ray tracing, this distribution follows a noncentral chi-squared distribution, under reasonable assumptions. Based on this, we develop a stopping criterion for denoised Monte Carlo image synthesis that terminates rendering once a user specified error threshold has been achieved. Our results demonstrate that our error estimate and stopping criterion work well on a variety of scenes, and that we are able to achieve a given error threshold without the user specifying the number of samples needed.	https://dl.acm.org/doi/abs/10.1145/3641519.3657511	Arthur Firmino, Ravi Ramamoorthi, Jeppe Revall Frisvad, Henrik Wann Jensen
Preceding Emptiness: Alternative Arabic Typographic Technologies	Throughout its history, the formal evolution of written language has been driven by the technology used to render that language---from cuneiform to stone-carved Roman capitals, and up to letterpress and pixels. This phenomenon has accelerated rapidly in the last century with typography adapting to new display possibilities: such as segmented LED displays in early electronics, to more contemporary storage and rendering standards, such as postscript. The locality of these innovations has been primarily situated in the West, where the Latin typographic script is the predominant form of written language. The opportunities presented by these technologies generated pressure for alternative scripts, such as written Arabic, to conform to the formal constraints of these Latin-based technologies, which, in some cases, have undermined the traditional formal structures of the Arabic script.	https://dl.acm.org/doi/abs/10.1145/3641523.3665175	Levi Hammett, Mohammad Suleiman, Hind Al Saad, Fatima Abbas
Preconditioned Nonlinear Conjugate Gradient Method for Real-time Interior-point Hyperelasticity	The linear conjugate gradient method is widely used in physical simulation, particularly for solving large-scale linear systems derived from Newton's method. The nonlinear conjugate gradient method generalizes the conjugate gradient method to nonlinear optimization, which is extensively utilized in solving practical large-scale unconstrained optimization problems. However, it is rarely discussed in physical simulation due to the requirement of multiple vector-vector dot products. Fortunately, with the advancement of GPU-parallel acceleration techniques, it is no longer a bottleneck. In this paper, we propose a Jacobi preconditioned nonlinear conjugate gradient method for elastic deformation using interior-point methods. Our method is straightforward, GPU-parallelizable, and exhibits fast convergence and robustness against large time steps. The employment of the barrier function in interior-point methods necessitates continuous collision detection per iteration to obtain a penetration-free step size, which is computationally expensive and challenging to parallelize on GPUs. To address this issue, we introduce a line search strategy that deduces an appropriate step size in a single pass, eliminating the need for additional collision detection. Furthermore, we simplify and accelerate the computations of Jacobi preconditioning and Hessian-vector product for hyperelasticity and barrier function. Our method can accurately simulate objects comprising over 100,000 tetrahedra in complex self-collision scenarios at real-time speeds.	https://dl.acm.org/doi/abs/10.1145/3641519.3657490	Xing Shen, Runyuan Cai, Mengxiao Bi, Tangjie Lv
Primal-Dual Non-Smooth Friction for Rigid Body Animation	Current numerical algorithms for simulating friction fall in one of two camps: smooth solvers sacrifice the stable treatment of static friction in exchange for fast convergence, and non-smooth solvers accurately compute friction at convergence rates that are often prohibitive for large graphics applications. We introduce a novel bridge between these two ideas that computes static and dynamic friction stably and efficiently. Our key idea is to convert the highly constrained non-smooth problem into an unconstrained smooth problem using logarithmic barriers that converges to the exact solution as accuracy increases. We phrase the problem as an interior point primal-dual problem that can be solved efficiently with Newton iteration. We observe quadratic convergence despite the non-smooth nature of the original problem, and our method is well-suited for large systems of tightly packed objects with many contact points. We demonstrate the efficacy of our method with stable piles of grains and stacks of objects, complex granular flows, and robust interlocking assemblies of rigid bodies.	https://dl.acm.org/doi/abs/10.1145/3641519.3657485	Yi-Lu Chen, Mickaël Ly, Chris Wojtan
Progressive Dynamics for Cloth and Shell Animation	We propose Progressive Dynamics, a coarse-to-fine, level-of-detail simulation method for the physics-based animation of complex frictionally contacting thin shell and cloth dynamics. Progressive Dynamics provides tight-matching consistency and progressive improvement across levels, with comparable quality and realism to high-fidelity, IPC-based shell simulations [Li et al. 2021] at finest resolutions. Together these features enable an efficient animation-design pipeline with predictive coarse-resolution previews providing rapid design iterations for a final, to-be-generated, high-resolution animation. In contrast, previously, to design such scenes with comparable dynamics would require prohibitively slow design iterations via repeated direct simulations on high-resolution meshes. We evaluate and demonstrate Progressive Dynamics's features over a wide range of challenging stress-tests, benchmarks, and animation design tasks. Here Progressive Dynamics efficiently computes consistent previews at costs comparable to coarsest-level direct simulations. Its matching progressive refinements across levels then generate rich, high-resolution animations with high-speed dynamics, impacts, and the complex detailing of the dynamic wrinkling, folding, and sliding of frictionally contacting thin shells and fabrics.	https://dl.acm.org/doi/abs/10.1145/3658214	Jiayi Eris Zhang, Doug James, Danny M. Kaufman
Project Grand Path: An Inclusive, Interdisciplinary, Collaborative, Project Based Learning Model	Project Grand Path (PGP) is leading the way to re-envision higher education in the 21st century by 'mobilizing learners to be the architects of their education'. Located in the University's FuturEDlab, a resource for the campus at large, this interdisciplinary, collaborative, student centered. innovation hub is infused with a series of high impact learning experiences, accessible through a curated studio-based internship program. Utilizing design thinking, cutting-edge computer graphics and a mentorship program, PGP fosters an environment of inclusion and equity providing the opportunity for the next generation to dream up a new paradigm for the college experience.	https://dl.acm.org/doi/abs/10.1145/3641235.3664440	Julie Goldstein
Project Starline: A high-fidelity telepresence system	Experience Project Starline, the first photorealistic telepresence system that demonstrably outperforms 2D videoconferencing systems, as measured by participant ratings, meeting recall, and non-verbal behaviors [Lawrence et al. 2021]. Our prototype communication endstations combine state-of-the-art face tracking, real-time neural view synthesis, spatial audio, multi-stream compression, and a high-resolution auto-stereoscopic display to produce a striking and natural sense of co-presence between two meeting participants that is not possible with traditional videoconferencing.	https://dl.acm.org/doi/abs/10.1145/3641517.3664381	Jason Lawrence, Ryan Overbeck, Todd Prives, Tommy Fortes, Nikki Roth, Brett Newman
Projecting Radiance Fields to Mesh Surfaces	Radiance fields produce high fidelity images with high rendering speed, but are difficult to manipulate. We effectively perform avatar texture transfer across different appearances by combining benefits from radiance fields and mesh surfaces. We represent the source as a radiance field using 3D Gaussian Splatter, then project the Gaussians on the target mesh. Our pipeline consists of Source Preconditioning, Target Vectorization and Texture Projection. The projection completes in 1.12s in a pure CPU compute, compared to baselines techniques of Per Face Texture Projection and Ray Casting (31s, 4.1min). This method lowers the computational requirements, which makes it applicable to a broader range of devices from low-end mobiles to high end computers.	https://dl.acm.org/doi/abs/10.1145/3641234.3671036	Adrian Xuan Wei Lim, Lynnette Hui Xian Ng, Nicholas Kyger, Tomo Michigami, Faraz Baghernezhad
Prompt to Anything?: Exploring Generative AI's Iterative Potential in Mapping Show Production.	"Incorporating Generative AI (Gen-AI) into our content creation processes at Moment Factory is an adventure driven by curiosity full of excitement and challenges. We have embarked on this journey to not only understand the capabilities of Gen-AI but also to experiment with how to effectively utilize and adapt its creations capabilities to suit our project needs, particularly in the contexts of location-based experiences and 3D mapping shows. A location-based experience denotes a multimedia interactive and immersive journey meticulously crafted for a specific physical location. As we orchestrate these experiences from inception to execution, we must develop content tailored precisely to each unique setting. These unusual canvases, often vast in scale, are called ""mega-canvases"". They can encompass a diverse expansive array of display surfaces integrated into scenography or architectural designs, demanding visual content with hyper-resolutions exceeding 16k. Our talk aims to unfold our visual and technical explorations, highlighting the innovative integration of Gen-AI into our process, and sharing the significant accomplishments and lessons learned from conceptualization to execution, and the unique challenges they present. Attendees will leave with a comprehensive understanding of the potential of Gen-AI in enhancing location-based experiences and 3D mapping shows, equipped with knowledge and inspiration to explore these technologies in their own creative endeavors. Through sharing our experiments and outcomes, we aim to foster a dialogue on the future of immersive content creation, inviting others to join us in pushing the boundaries of what is possible with Gen-AI in multimedia experiences."	https://dl.acm.org/doi/abs/10.1145/3641233.3664729	Guillaume Borgomano
Props and Rocks: Passive Haptic Mixed Reality for Navigating Far-off Worlds	Our project immerses participants in a world blending virtual and physical realities. Users navigate a divided society, interacting with props via haptic feedback. Employing innovative techniques, including redirected walking, our project seamlessly transitions between virtual worlds, advancing VR integration.	https://dl.acm.org/doi/abs/10.1145/3641521.3664404	Michael Vincenty, Joshua Grebler, Cindy Piza, Ozara Dalgo, Taro Narahara
Proxy Asset Generation for Cloth Simulation in Games	Simulating high-resolution cloth poses computational challenges in real-time applications. In the gaming industry, the proxy mesh technique offers an alternative, simulating a simplified low-resolution cloth geometry, This proxy mesh's dynamics drive the detailed high-resolution geometry, , through Linear Blended Skinning (LBS). However, generating a suitable proxy mesh with appropriate skinning weights from a given visual mesh is non-trivial, often requiring skilled artists several days for fine-tuning. This paper presents an automatic pipeline to convert an ill-conditioned highresolution visual mesh into a single-layer low-poly proxy mesh. Given that the input visual mesh may not be simulation-ready, our approach then simulates the proxy mesh based on specific use scenarios and optimizes the skinning weights, relying on differential skinning with several well-designed loss functions to ensure the skinned visual mesh appears plausible in the final simulation. We have tested our method on various challenging cloth models, demonstrating its robustness and effectiveness.	https://dl.acm.org/doi/abs/10.1145/3658177	Zhongtian Zheng, Tongtong Wang, Qijia Feng, Zherong Pan, Xifeng Gao, Kui Wu
Proxy Tracing: Unbiased Reciprocal Estimation for Optimized Sampling in BDPT	Robust light transport algorithms, particularly bidirectional path tracing (BDPT), face significant challenges when dealing with specular or highly glossy involved paths. BDPT constructs the full path by connecting sub-paths traced individually from the light source and camera. However, it remains difficult to sample by connecting vertices on specular and glossy surfaces with narrow-lobed BSDF, as it poses severe constraints on sampling in the feasible direction. To address this issue, we propose a novel approach, called , that enables efficient sub-path connection of these challenging paths. When a low-contribution specular/glossy connection occurs, we drop out the problematic neighboring vertex next to this specular/glossy vertex from the original path, then retrace an alternative sub-path as a proxy to complement this incomplete path. This newly constructed complete path ensures that the connection adheres to the constraint of the narrow lobe within the BSDF of the specular/glossy surface. Unbiased reciprocal estimation is the key to our method to obtain a probability density function (PDF) reciprocal to ensure unbiased rendering. We derive the reciprocal estimation method and provide an efficiency-optimized setting for efficient sampling and connection. Our method provides a robust tool for substituting problematic paths with favorable alternatives while ensuring unbiasedness. We validate this approach in the probabilistic connections BDPT for addressing specular-involved difficult paths. Experimental results have proved the effectiveness and efficiency of our approach, showcasing high-performance rendering capabilities across diverse settings.	https://dl.acm.org/doi/abs/10.1145/3658216	Fujia Su, Bingxuan Li, Qingyang Yin, Yanchen Zhang, Sheng Li
Pushing the Limits: Crafting an Immersive Mega-Canvas for Phish’s Music Shows at Sphere™	"Moment Factory is excited to invite attendees to a production session at SIGGRAPH 2024, where we'll delve into the innovative production of ""a famous band"" concert series at a novel immersive venue. This venue, a multimedia megacanvas, is unlike any other, and our presentation will cover the collaborative effort, creative challenges, and technical breakthroughs achieved within a demanding three-month timeline."	https://dl.acm.org/doi/abs/10.1145/3641232.3649396	Guillaume Borgomano, Zane Kozak, Arnaud Grosjean, Jörgen Steinheim
QT-Font: High-efficiency Font Synthesis via Quadtree-based Diffusion Models	Few-shot font generation (FFG) aims to streamline the manual aspects of the font design process. Existing models are capable of generating glyph images in the same style of a few input reference glyphs. However, mainly due to their inefficient glyph representations, these existing FFG methods are limited to generating low-resolution glyph images. To address this problem, we introduce QT-Font, an efficient quadtree-based diffusion model specifically designed for FFG. More specifically, we design a sparse quadtree-based glyph representation to reduce the complexity of the representation space, exhibiting linear complexity and uniqueness. Concurrently, to reduce computational complexity, we propose a U-net model based on the dual quadtree graph network and the discrete diffusion model. Furthermore, a content-aware pooling module is also adopted to lessen the computational demands of the diffusion process. Qualitative and quantitative experiments have been conducted to demonstrate that our QT-Font, compared to existing approaches, can generate high-resolution glyph images with superior quality and more visually pleasing details, meanwhile significantly reducing both parameter sizes and computational costs.	https://dl.acm.org/doi/abs/10.1145/3641519.3657451	Yitian Liu, Zhouhui Lian
Quad-Optimized Low-Discrepancy Sequences	The convergence of Monte Carlo integration is given by the uniformity of samples as well as the regularity of the integrand. Despite much effort dedicated to producing excellent, extremely uniform, sampling patterns, the Sobol' sampler remains unchallenged in production rendering systems. This is not only due to its reasonable quality, but also because it allows for integration in (almost) arbitrary dimension, with arbitrary sample count, while actually producing sequences thus allowing for progressive rendering, with fast sample generation and small memory footprint. We improve over Sobol' sequences in terms of sample uniformity in consecutive 2-d and 4-d projections, while providing similar practical benefits – sequences, high dimensionality, speed and compactness. We base our contribution on a base-3 Sobol' construction, involving a search over irreducible polynomials and generator matrices, that produce (1, 4)-sequences or (2,4)-sequences in all consecutive quadruplets of dimensions, and (0, 2)-sequence in all consecutive pairs of dimensions. We provide these polynomials and matrices that may be used as a replacement of Joe & Kuo's widely used ones, with computational overhead, for moderate-dimensional problems.	https://dl.acm.org/doi/abs/10.1145/3641519.3657431	Victor Ostromoukhov, Nicolas Bonneel, David Coeurjolly, Jean-Claude Iehl
RGB↔X: Image decomposition and synthesis using material- and lighting-aware diffusion models	The three areas of realistic forward rendering, per-pixel inverse rendering, and generative image synthesis may seem like separate and unrelated sub-fields of graphics and vision. However, recent work has demonstrated improved estimation of per-pixel intrinsic channels (albedo, roughness, metallicity) based on a diffusion architecture; we call this the RGB → X problem. We further show that the reverse problem of synthesizing realistic images given intrinsic channels, X → RGB, can also be addressed in a diffusion framework. Focusing on the image domain of interior scenes, we introduce an improved diffusion model for RGB → X, which also estimates lighting, as well as the first diffusion X → RGB model capable of synthesizing realistic images from (full or partial) intrinsic channels. Our X → RGB model explores a middle ground between traditional rendering and generative models: We can specify only certain appearance properties that should be followed, and give freedom to the model to hallucinate a plausible version of the rest. This flexibility allows using a mix of heterogeneous training datasets that differ in the available channels. We use multiple existing datasets and extend them with our own synthetic and real data, resulting in a model capable of extracting scene properties better than previous work and of generating highly realistic images of interior scenes.	https://dl.acm.org/doi/abs/10.1145/3641519.3657445	Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, Miloš Hašan
RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting	We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.	https://dl.acm.org/doi/abs/10.1145/3641519.3657455	Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou
Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar	Neural fields have been broadly investigated as scene representations for the reproduction and novel generation of diverse outdoor scenes, including those autonomous vehicles and robots must handle. While successful approaches for RGB and LiDAR data exist, neural reconstruction methods for radar as a sensing modality have been largely unexplored. Operating at millimeter wavelengths, radar sensors are robust to scattering in fog and rain, and, as such, offer a complementary modality to active and passive optical sensing techniques. Moreover, existing radar sensors are highly cost-effective and deployed broadly in robots and vehicles that operate outdoors. We introduce Radar Fields –- a neural scene reconstruction method designed for active radar imagers. Our approach unites an explicit, physics-informed sensor model with an implicit neural geometry and reflectance model to directly synthesize raw radar measurements and extract scene occupancy. The proposed method does not rely on volume rendering. Instead, we learn fields in Fourier frequency space, supervised with raw radar data. We validate our method's effectiveness across diverse outdoor scenarios, including urban scenes with dense vehicles and infrastructure, and harsh weather scenarios, where mm-wavelength sensing is favorable.	https://dl.acm.org/doi/abs/10.1145/3641519.3657510	David Borts, Erich Liang, Tim Broedermann, Andrea Ramazzina, Stefanie Walz, Edoardo Palladin, Jipeng Sun, David Brueggemann, Christos Sakaridis, Luc Van Gool, Mario Bijelic, Felix Heide
Rage Against the Archive: Institutional Critique through New Media Art	Rage Against the Archive is an artivist project comprising experimental video, performance, and new media art that scrutinizes how the New York Public Library's (NYPL) digital archives catalog and display dehumanizing ethnographic images from colonial India. This work critically probes whether institutional archives inadvertently perpetuate the cycle of colonial exploitation and visual violence. My artistic approach is anchored in a critical paradigm intended to underscore how technology still commodifies the bodies of people of color and how we, as a more conscientious society, should consume certain images online.	https://dl.acm.org/doi/abs/10.1145/3641523.3665170	Anshul Roy
Ray Tracing Harmonic Functions	"is a fast and high-quality method for visualizing surfaces encoded by signed distance functions (SDFs). We introduce a similar method for a completely different class of surfaces encoded by , opening up rich new possibilities for visual computing. Our starting point is similar in spirit to sphere tracing: using conservative on the growth of harmonic functions, we develop a algorithm for visualizing level sets of harmonic functions, including those that are angle-valued and exhibit singularities. The method takes much larger steps than naïve ray marching, avoids numerical issues common to generic root finding methods and, like sphere tracing, needs only perform pointwise evaluation of the function at each step. For many use cases, the method is fast enough to run real time in a shader program. We use it to visualize smooth surfaces directly from point clouds (via Poisson surface reconstruction) or polygon soup (via generalized winding numbers) without linear solves or mesh extraction. We also use it to visualize nonplanar polygons (possibly with holes), surfaces from architectural geometry, mesh ""exoskeletons"", and key mathematical objects including knots, links, spherical harmonics, and Riemann surfaces. Finally we show that, at least in theory, Harnack tracing provides an alternative mechanism for visualizing arbitrary implicit surfaces."	https://dl.acm.org/doi/abs/10.1145/3658201	Mark Gillespie, Denise Yang, Mario Botsch, Keenan Crane
ReFiNe: Recursive Field Networks for Cross-Modal Multi-Scene Representation	"The common trade-offs of state-of-the-art methods for multi-shape representation (a single model ""packing"" multiple objects) involve trading modeling accuracy against memory and storage. We show how to encode multiple shapes represented as continuous neural fields with a higher degree of precision than previously possible and with low memory usage. Key to our approach is a recursive hierarchical formulation that exploits object self-similarity, leading to a highly compressed and efficient shape latent space. Thanks to the recursive formulation, our method supports spatial and global-to-local latent feature fusion without needing to initialize and maintain auxiliary data structures, while still allowing for continuous field queries to enable applications such as raytracing. In experiments on a set of diverse datasets, we provide compelling qualitative results and demonstrate state-of-the-art multi-scene reconstruction and compression results with a single network per dataset."	https://dl.acm.org/doi/abs/10.1145/3641519.3657526	Sergey Zakharov, Katherine Liu, Adrien Gaidon, Rares Ambrus
ReVerie	ReVerie is an Interactive AI installation that collects and visualizes textual dream data from the artist and the audience and creates a collective dream-reliving experience. Within Dream science, 'Dreamwork' encompasses techniques such as dream analysis, interpretation, and exploration aimed at uncovering insights into the subconscious mind. Central to 'Dreamwork' is the concept of re-experiencing dreams—immersing oneself in the recollection of dream memories, and emotions. Through a 3D generative diffusion model, the 'ReVerie' system translates the whispering of dream objects into 3D immersive visualization in real time to facilitate dream re-experiencing and a collective dream fly-through experience.	https://dl.acm.org/doi/abs/10.1145/3641521.3664410	Pinyao Liu, Keon Ju Lee
Reach for the Arcs: Reconstructing Surfaces from SDFs via Tangent Points	We introduce an algorithm to reconstruct a mesh from discrete samples of a shape's Signed Distance Function (SDF). A simple geometric reinterpretation of the SDF lets us formulate the problem through a point cloud, from which a surface can be extracted with existing techniques. We extract all possible information from the SDF data, outperforming commonly used algorithms and imposing no topological or geometric restrictions.	https://dl.acm.org/doi/abs/10.1145/3641519.3657419	Silvia Sellán, Yingying Ren, Christopher Batty, Oded Stein
Real-Time 3D Graphics for Health Impact: Interactive 3D IUD Insertion	BioDigital harnesses web-based real-time graphics to revolutionize health education, facilitating over 100 million health conversations yearly. Our content is built to be accessible to anyone with an internet connection, making health literacy available to individuals regardless of their background or location. Our Lead 3D Graphics Engineer and Content Directors will share how we built an optimized 3D interactive and enhanced engine features to expand access to reproductive healthcare in regions with unknown internet speeds and hardware specifications. We will present a live demo of our interactive 3D visualization of an IUD Insertion procedure and detailed anatomy interactives, sharing custom rendering and asset-creation techniques developed to meet the strict technical constraints of a global audience.	https://dl.acm.org/doi/abs/10.1145/3641233.3665160	Natalie Doolittle, Krystle Quinn, Brian Chirls
Real-Time Hair Rendering with Hair Meshes	Hair meshes are known to be effective for modeling and animating hair in computer graphics. We present how the hair mesh structure can be used for efficiently rendering strand-based hair models on the GPU with on-the-fly geometry generation that provides orders of magnitude reduction in storage and memory bandwidth. We use mesh shaders to carefully distribute the computation and a custom texture layout for offloading a part of the computation to the hardware texture units. We also present a set of procedural styling operations to achieve hair strand variations for a wide range of hairstyles and a consistent coordinate-frame generation approach to attach these variations to an animating/deforming hair mesh. Finally, we describe level-of-detail techniques for improving the performance of rendering distant hair models. Our results show an unprecedented level of performance with strand-based hair rendering, achieving hundreds of full hair models animated and rendered at real-time frame rates on a consumer GPU.	https://dl.acm.org/doi/abs/10.1145/3641519.3657521	Gaurav Bhokare, Eisen Montalvo, Elie Diaz, Cem Yuksel
Real-Time Path Guiding Using Bounding Voxel Sampling	We propose a real-time path guiding method, Voxel Path Guiding (VXPG), that significantly improves fitting efficiency under limited sampling budget. Our key idea is to use a spatial irradiance voxel data structure across all shading points to guide the location of path vertices. For each frame, we first populate the voxel data structure with irradiance and geometry information. To sample from the data structure for a shading point, we need to select a voxel with high contribution to that point. To importance sample the voxels while taking visibility into consideration, we adapt techniques from offline many-lights rendering by clustering pairs of shading points and voxels. Finally, we unbiasedly sample within the selected voxel while taking the geometry inside into consideration. Our experiments show that VXPG achieves significantly lower perceptual error compared to other real-time path guiding and virtual point light methods under equal-time comparison. Furthermore, our method does not rely on temporal information, but can be used together with other temporal reuse sampling techniques such as ReSTIR to further improve sampling efficiency.	https://dl.acm.org/doi/abs/10.1145/3658203	Haolin Lu, Wesley Chang, Trevor Hedstrom, Tzu-Mao Li
Real-Time Refraction Shader for Animation	We present Animal Logic's solution to simulate light refraction in deformable characters' eye corneas in Autodesk® Maya®'s viewport, with a result close to the final render, significantly reducing iterations for facial animation workflow. Our approach is tightly integrated with Animal Logic's GPU-based deformation engine for a minimal impact on playback. The refraction is generated automatically from the scene's geometries and a simplified shading definition exported by the lookdev department using Pixar® Universal Scene Description. It has proven to give reliable results, allowing for its adoption in production for all current and upcoming shows.	https://dl.acm.org/doi/abs/10.1145/3641233.3664328	Antoine Domon, Ankit Sinha, Zhicheng Ye, Valerie Bernard
Real-time AI and the Future of Creative Tools	"KREA is a novel platform that integrates AI into the creative process, aiming to enhance how artists interact with artificial intelligence. This system combines recent advances in AI research, distributed systems, and human-computer interaction to facilitate real-time visual content creation. KREA's architecture, which we term an ""inverted videogame"" system, enables AI-powered creation through web browsers, addressing challenges in latency, state management, and reliability in dynamic GPU compute environments. This paper presents the key innovations of KREA, including its unique distributed architecture, real-time AI generation capabilities, and user interface designed for intuitive AI-assisted creativity."	https://dl.acm.org/doi/abs/10.1145/3641520.3665303	Diego Rodriguez Prado, Victor Perez, Mihai Petrescu, Titus Ebbecke, Erwann Millon, Tianpei Gu
Real-time Neural Woven Fabric Rendering	Woven fabrics are widely used in applications of realistic rendering, where real-time capability is also essential. However, rendering realistic woven fabrics in real time is challenging due to their complex structure and optical appearance, which cause aliasing and noise without many samples. The core of this issue is a multi-scale representation of the fabric shading model, which allows for a fast range query. Some previous neural methods deal with the issue at the cost of training on each material, which limits their practicality. In this paper, we propose a lightweight neural network to represent different types of woven fabrics at different scales. Thanks to the regularity and repetitiveness of woven fabric patterns, our network can encode fabric patterns and parameters as a small latent vector, which is later interpreted by a small decoder, enabling the representation of different types of fabrics. By applying the pixel's footprint as input, our network achieves multi-scale representation. Moreover, our network is fast and occupies little storage because of its lightweight structure. As a result, our method achieves rendering and editing woven fabrics at nearly 60 frames per second on an RTX 3090, showing a quality close to the ground truth and being free from visible aliasing and noise.	https://dl.acm.org/doi/abs/10.1145/3641519.3657496	Xiang Chen, Lu Wang, Beibei Wang
Real-time Physically Guided Hair Interpolation	Strand-based hair simulations have recently become increasingly popular for a range of real-time applications. However, accurately simulating the full number of hair strands remains challenging. A commonly employed technique involves simulating a subset of guide hairs to capture the overall behavior of the hairstyle. Details are then enriched by interpolation using linear skinning. Hair interpolation enables fast real-time simulations but frequently leads to various artifacts during runtime. As the skinning weights are often pre-computed, substantial variations between the initial and deformed shapes of the hair can cause severe deviations in fine hair geometry. Straight hairs may become kinked, and curly hairs may become zigzags. This work introduces a novel physical-driven hair interpolation scheme that utilizes existing simulated guide hair data. Instead of directly operating on positions, we interpolate the internal forces from the guide hairs before efficiently reconstructing the rendered hairs based on their material model. We formulate our problem as a constraint satisfaction problem for which we present an efficient solution. Further practical considerations are addressed using regularization terms that regulate penetration avoidance and drift correction. We have tested various hairstyles to illustrate that our approach can generate visually plausible rendered hairs with only a few guide hairs and minimal computational overhead, amounting to only about 20% of conventional linear hair interpolation. This efficiency underscores the practical viability of our method for real-time applications.	https://dl.acm.org/doi/abs/10.1145/3658176	Jerry Hsu, Tongtong Wang, Zherong Pan, Xifeng Gao, Cem Yuksel, Kui Wu
Real-time Wing Deformation Simulations for Flying Insects	Realistic simulation of the intricate wing deformations seen in flying insects not only deepens our comprehension of insect flight mechanics but also opens up numerous applications in fields such as computer animation and virtual reality. Despite its importance, this research area has been relatively underexplored due to the complex and diverse wing structures and the intricate patterns of deformation. This paper presents an efficient skeleton-driven model specifically designed to real-time simulate realistic wing deformations across a wide range of flying insects. Our approach begins with the construction of a virtual skeleton that accurately reflects the distinct morphological characteristics of individual insect species. This skeleton serves as the foundation for the simulation of the intricate deformation wave propagation often observed in wing deformations. To faithfully reproduce the bending effect seen in these deformations, we introduce both internal and external forces that act on the wing joints, drawing on periodic wing-beat motion and a simplified aerodynamics model. Additionally, we utilize mass-spring algorithms to simulate the inherent elasticity of the wings, helping to prevent excessive twisting. Through various simulation experiments, comparisons, and user studies, we demonstrate the effectiveness, robustness, and adaptability of our model.	https://dl.acm.org/doi/abs/10.1145/3641519.3657434	Qiang Chen, Zhigang Deng, Feng Li, Yuming Fang, Tingsong Lu, Yang Tong, Yifan Zuo
RealFill: Reference-Driven Generation for Authentic Image Completion	Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions. However, the content these models hallucinate is necessarily inauthentic, since they are unaware of the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging scenarios, and find that it outperforms existing approaches by a large margin. Project page: https://realfill.github.io.	https://dl.acm.org/doi/abs/10.1145/3658237	Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David E. Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, Michael Rubinstein
Recipes for Creating Digital Game Prototypes	This lab introduces aspiring or current game designers with limited programming knowledge, to concepts that can be used to create digital game prototypes. Four basic concepts are presented as ingredients (vectors, time, collisions, and lines and rays), each with explanations and examples demonstrated in Unreal and Unity. Then examples are provided on how the ingredients can be combined into recipes to create common gameplay mechanics for prototyping. This is not an exhaustive list of everything someone needs to develop digital game prototypes, however, it serves as a good starting point for beginners who may want to start designing and prototyping games with limited knowledge of programming in game engines. This lab assumes a basic understanding of Unreal, Unity, or a comparable game engine, but programming experience is not required. Upon completion of this lab, participants should have several recipes they can use, build on, and modify for creating simple game play mechanics for prototyping. The example projects used in the lab will be available for download.	https://dl.acm.org/doi/abs/10.1145/3641236.3665165	Justin Johnson
Recompose Grammars for Procedural Architecture	We present the novel grammar language Recomp for the procedural modeling of architecture. In grammar-based approaches, the procedural refinement process is based on shape subdivisions. This process of decomposition results in disconnected subparts, which not only restricts the geometric expressiveness but also limits the control over an appropriate shape granularity needed to coordinate design decisions. Recomp overcomes these limitations by extending grammar languages with the recomposition ability. Fundamental is the concept of rule inlining, allowing for the topological recomposition of edited subparts by collapsing a shape subtree into one single shape on which derivation can continue. This is completed with a versatile geometry tagging system, allowing authors to compile and transport context information at any level of detail and gain full control over the geometry independent of the structure of the shape tree. Through various examples, we demonstrate the power of Recomp in procedural layout and mass modeling, as well as its capabilities in facilitating context-sensitive design.	https://dl.acm.org/doi/abs/10.1145/3641519.3657400	Niklaus Houska, Cheryl Lau, Matthias Specht
Recreating the Sodium Vapor Matting Process	We present a Sodium Vapor Matting (SVM) system built from off-the-shelf components allowing simultaneous capture of a full-color foreground subject and an accurate holdout matte. We use a beamsplitter cube and two filters to image the sodium vapor wavelength independently of the rest of the visible spectrum. We build custom sodium vapor light sources for illuminating the background, and aim synchronized digital cinema cameras into the filtered prism to record the foreground and the matte. We process several composite shots using the system, using the matte and a clean plate to subtract visible light in the subject's background and to hold out the alpha region from the new background.	https://dl.acm.org/doi/abs/10.1145/3641234.3671046	Paul Debevec, Nikolas Pueringer
Reframe: Recording and Editing Character Motion in Virtual Reality	Creating lifelike 3D character animations is traditionally complex and requires substantial skill and effort. To overcome this challenge, we introduce Reframe, a Virtual Reality animation authoring interface that allows users to record and edit motion. Reframe utilizes tracking technology in Virtual Reality headsets to capture the user's full-body motion, facial expressions, and hand gestures. To facilitate the editing process, we have developed an immersive motion editing interface that combines spatial and temporal control for character animation. This system extracts keyposes from the 3D character animation and displays them along a timeline, connecting the joints through 3D trajectories to depict the character's movement. We have created a proof-of-concept prototype that demonstrates how a single user can select and animate multiple characters in a scene. This system offers an interactive experience that explores the possibilities of future immersive animation technologies.	https://dl.acm.org/doi/abs/10.1145/3641521.3664413	Qian Zhou, Aniruddha Prithul, Hans Kellner, Brian Pene, David Ledo, Sebastian Herrera Urrutia, Hilmar Koch, George Fitzmaurice, Fraser Anderson
Reimagined Volume III: Young Thang	Young Thang is a human living within The Community, a place where humans are forbidden. To blend in, Young Thang's guardians urge her to wear a second skin. After the pressure to conform pushes Young Thang to the brink, she unknowingly puts her beloved guardians and The Community in peril.	https://dl.acm.org/doi/abs/10.1145/3641231.3649299	Melissa Joyner, Julie Cavaliere, Michaela Ternasky-Holland
Remembrance	An old woman with Alzheimer's goes on a journey through her childhood memories to find something she once lost.	https://dl.acm.org/doi/abs/10.1145/3641230.3648598	Isaac Gazmararian
Repulsive Shells	This paper develops a shape space framework for collision-aware geometric modeling, where basic geometric operations automatically avoid inter-penetration. Shape spaces are a powerful tool for surface modeling, shape analysis, nonrigid motion planning, and animation, but past formulations permit nonphysical intersections. Our framework augments an existing shape space using a repulsive energy such that collision avoidance becomes a first-class property, encoded in the Riemannian metric itself. In turn, tasks like intersection-free shape interpolation or motion extrapolation amount to simply computing geodesic paths via standard numerical algorithms. To make optimization practical, we develop an adaptive collision penalty that prevents mesh self-intersection, and converges to a meaningful limit energy under refinement. The final algorithms apply to any category of shape, and do not require a dataset of examples, training, rigging, nor any other prior information. For instance, to interpolate between two shapes we need only a single pair of meshes with the same connectivity. We evaluate our method on a variety of challenging examples from modeling and animation.	https://dl.acm.org/doi/abs/10.1145/3658174	Josua Sassen, Henrik Schumacher, Martin Rumpf, Keenan Crane
Return to Arrakis: The VFX of ‘Dune: Part Two’	Join the DNEG VFX team for an in-depth behind-the-scenes look at the visual effects that brought the most hotlyanticipated sequel of the year to life! DNEG was proud to build on the success of their award-winning work for `Dune' by acting as the main VFX partner for `Dune: Part Two'. The sequel expands on the first film in all ways - bigger, bolder, and with much more action. As such, DNEG's work pushed the creative and technical boundaries to create something new, exhilarating and awe-inspiring. A fully global team of over 1700 crew came together to craft an expanded world, more detailed creatures, and even more explosive and technically complex FX sequences to build on the aesthetic created for `Dune' and continue the epic journey for `Dune: Part 2'.	https://dl.acm.org/doi/abs/10.1145/3641232.3649325	Stephen James, Carina Kaiser
Revolutionizing VFX Production with Real-Time Volumetric Effects	Zibra AI introduces a unified approach, integrating both the creation and rendering of VFX directly within game engines, aiming to streamline and enhance the production process. The new workflow is empowered by two major innovations: ZibraVDB — an OpenVDB compression tool that enables real-time rendering of volumetric VFX, and Zibra Effects — a highly performant real-time VFX simulation technology. Both tools support in-engine workflows for Unreal Engine, Unity, or any other custom engine. ZibraVDB achieves up to a 150x compression rate while maintaining the high visual fidelity required for advanced production workflows such as virtual production and AAA game development. Combined with a highly efficient custom render pass, it achieves significant rendering performance. Zibra Effects enables the physics-based VFX simulation of liquids, smoke, and fire directly inside a game engine. It leverages proprietary AI-based SDF compression technology that reduces collider memory footprint by 40x on average. In combination with ZibraVDB, it covers the VFX creation workflow end-to-end: users can choose between real-time interactive effects or cache them into ZibraVDB format for efficiency	https://dl.acm.org/doi/abs/10.1145/3641520.3665311	Oleksandr Petrenko, Oleksandr Puchka, Alex Klimenko
Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic Solids	Despite significant advancements in Neural Radiance Fields (NeRFs), the renderings may still suffer from aliasing and blurring artifacts, since it remains a fundamental challenge to effectively and efficiently characterize anisotropic areas induced by the cone-casting procedure. This paper introduces a Ripmap-Encoded Platonic Solid representation to precisely and efficiently featurize 3D anisotropic areas, achieving high-fidelity anti-aliased renderings. Central to our approach are two key components: Platonic Solid Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D space onto the unparalleled faces of a certain Platonic solid, such that the anisotropic 3D areas can be projected onto planes with distinguishable characterization. Meanwhile, each face of the Platonic solid is encoded by the Ripmap encoding, which is constructed by anisotropically pre-filtering a learnable feature grid, to enable featurzing the projected anisotropic areas both precisely and efficiently by the anisotropic area-sampling. Extensive experiments on both well-established synthetic datasets and a newly captured real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art rendering quality, particularly excelling in the fine details of repetitive structures and textures, while maintaining relatively swift training times, as shown in Fig. 1. The source code and data for this paper are at https://github.com/JunchenLiu77/Rip-NeRF.	https://dl.acm.org/doi/abs/10.1145/3641519.3657402	Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao
RobotSketch: An Interactive Showcase of Superfast Design of Legged Robots	Robots consisting of many articulated parts performing complex movements are challenging to design. We showcase an interactive system for exploring shapes and structures of robots through 3D sketching, generating plausible movements of robots through AI, and reviewing and refining them in VR. Such immersive prototyping in the early stages can help reduce the time and cost associated with trial and error in later stages, contributing to shortening and streamlining of the robot development process.	https://dl.acm.org/doi/abs/10.1145/3641517.3664382	Joon Hyub Lee, Hyunsik Oh, Junwoo Yoon, Seung-Jun Lee, Taegyu Jin, Jemin Hwangbo, Seok-Hyung Bae
Robust Containment Queries over Collections of Rational Parametric Curves via Generalized Winding Numbers	Point containment queries for regions bound by watertight geometric surfaces, i.e., closed and without self-intersections, can be evaluated straightforwardly with a number of well-studied algorithms. When this assumption on domain geometry is not met, such methods are either unusable, or prone to misclassifications that can lead to cascading errors in downstream applications. More robust point classification schemes based on generalized winding numbers have been proposed, as they are indifferent to these imperfections. However, existing algorithms are limited to point clouds and collections of linear elements. We extend this methodology to encompass more general curved shapes with an algorithm that evaluates the winding number scalar field over unstructured collections of rational parametric curves. In particular, we evaluate the winding number for each curve independently, making the derived containment query robust to how the curves are arranged. We ensure geometric fidelity in our queries by treating each curve as equivalent to an adaptively constructed polyline that provably has the same generalized winding number at the point of interest. Our algorithm is numerically stable for points that are arbitrarily close to the model, and explicitly treats points that are coincident with curves. We demonstrate the improvements in computational performance granted by this method over conventional techniques as well as the robustness induced by its application.	https://dl.acm.org/doi/abs/10.1145/3658228	Jacob Spainhour, David Gunderman, Kenneth Weiss
Rodin: 3D Asset Creation with a Text/Image/3D-Conditioned Large Generative Model for Creative Frontier	"""Rodin"" leverages a Latent Diffusion Transformer and a 3D ConditionNet to facilitate the rapid generation of 3D assets from text, images, and direct inputs. This system streamlines the creation process, producing production-ready models optimized for real-time engines. This ""Real-Time Live!"" highlights Rodin's innovative framework, its seamless integration into professional workflows, and its transformative impact on the 3D content creation industry."	https://dl.acm.org/doi/abs/10.1145/3641520.3665304	Qixuan Zhang, Longwen Zhang
S3: Speech, Script and Scene driven Head and Eye Animation	We present , a novel approach to generating expressive, animator-centric 3D head and eye animation of characters in conversation. Given audio, a Directorial and a cinematographic 3D as input, we automatically output the animated 3D rotation of each character's head and eyes. distills animation and psycho-linguistic insights into a novel modular framework for conversational gaze capturing: audio-driven rhythmic head motion; narrative script-driven emblematic head and eye gestures; and gaze trajectories computed from audio-driven gaze focus/aversion and 3D visual scene salience. Our evaluation is four-fold: we quantitatively validate our algorithm against ground truth data and baseline alternatives; we conduct a perceptual study showing our results to compare favourably to prior art; we present examples of animator control and critique of output; and present a large number of compelling and varied animations of conversational gaze.	https://dl.acm.org/doi/abs/10.1145/3658172	Yifang Pan, Rishabh Agrawal, Karan Singh
SMEAR: Stylized Motion Exaggeration with ARt-direction	Smear frames are routinely used by artists for the expressive depiction of motion in animations. In this paper, we present an automatic, yet art-directable method for the generation of smear frames in 3D, with a focus on elongated in-betweens where an object is stretched along its trajectory. It takes as input a key-framed animation of a 3D mesh, and outputs a deformed version of this mesh for each frame of the animation, while providing for artistic refinement at the end of the animation process and prior to rendering. Our approach works in two steps. We first compute spatially and temporally coherent motion offsets that describe to which extent parts of the input mesh should be leading in front or trailing behind. We then describe a framework to stylize these motion offsets in order to produce elongated in-betweens at interactive rates, which we extend to the other two common smear frame effects: multiple in-betweens and motion lines. Novice users may rely on preset stylization functions for fast and easy prototyping, while more complex custom-made stylization functions may be designed by experienced artists through our geometry node implementation in Blender.	https://dl.acm.org/doi/abs/10.1145/3641519.3657457	Jean Basset, Pierre Bénard, Pascal Barla
SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration	Recent techniques for real-time view synthesis have rapidly advanced in fidelity and speed, and modern methods are capable of rendering near-photorealistic scenes at interactive frame rates. At the same time, a tension has arisen between explicit scene representations amenable to rasterization and neural fields built on ray marching, with state-of-the-art instances of the latter surpassing the former in quality while being prohibitively expensive for real-time applications. We introduce SMERF, a view synthesis approach that achieves state-of-the-art accuracy among real-time methods on large scenes with footprints up to 300 m at a volumetric resolution of 3.5 mm . Our method is built upon two primary contributions: a hierarchical model partitioning scheme, which increases model capacity while constraining compute and memory consumption, and a distillation training strategy that simultaneously yields high fidelity and internal consistency. Our method enables full six degrees of freedom navigation in a web browser and renders in real-time on commodity smartphones and laptops. Extensive experiments show that our method exceeds the state-of-the-art in real-time novel view synthesis by 0.78 dB on standard benchmarks and 1.78 dB on large scenes, renders frames three orders of magnitude faster than state-of-the-art radiance field models, and achieves real-time performance across a wide variety of commodity devices, including smartphones. We encourage readers to explore these models interactively at our project website: https://smerf-3d.github.io.	https://dl.acm.org/doi/abs/10.1145/3658193	Daniel Duckworth, Peter Hedman, Christian Reiser, Peter Zhizhin, Jean-François Thibert, Mario Lučić, Richard Szeliski, Jonathan T. Barron
ST-4DGS: Spatial-Temporally Consistent 4D Gaussian Splatting for Efficient Dynamic Scene Rendering	Dynamic scene rendering at any novel view continues to be a difficult but important task, especially for high-fidelity rendering quality with efficient rendering speed. The recent 3D Gaussian Splatting, i.e., 3DGS, shows great success for static scene rendering with impressive quality at a very efficient speed. However, the extension of 3DGS from static scene to dynamic 4DGS is still challenging, even for scenes with modest amounts of foreground object movement (such as a human moving an object). This paper proposes a novel spatial-temporally 4D Gaussian Splatting, i.e., ST-4DGS, which aims at the spatial-temporally persistent dynamic rendering quality and maintains real-time rendering efficiency. The key ideas of ST-4DGS are two novel mechanisms: (1) a novel spatial-temporal 4D Gaussian Splatting with a motion-aware shape regularization, and (2) a spatial-temporal joint density control mechanism. The proposed mechanisms efficiently prevent the compactness degeneration of the 4D Gaussian representation during dynamic scene learning, thus leading to spatial-temporally consistent dynamic rendering quality. With extensive evaluation on public datasets, our ST-4DGS can achieve much better dynamic rendering quality than previous approaches, such as 4DGS, HexPlane, K-Plane, 4K4D, etc, and in a more efficient rendering speed for persistent dynamic rendering. To our best knowledge, ST-4DGS is a new state-of-the-art 4D Gaussian Splatting for high-fidelity dynamic rendering, especially ensuring the spatial-temporally consistent rendering quality in scenes with modest movement. The code is available at https://github.com/wanglids/ST-4DGS.	https://dl.acm.org/doi/abs/10.1145/3641519.3657520	Deqi Li, Shi-Sheng Huang, Zhiyuan Lu, Xinran Duan, Hua Huang
Saccade-Contingent Rendering	Battery-constrained power consumption, compute limitations, and high frame rate requirements in head-mounted displays present unique challenges in the drive to present increasingly immersive and comfortable imagery in virtual reality. However, humans are not equally sensitive to all regions of the visual field, and perceptually-optimized rendering techniques are increasingly utilized to address these bottlenecks. Many of these techniques are gaze-contingent and often render reduced detail away from a user's fixation. Such techniques are dependent on spatio-temporally-accurate gaze tracking and can result in obvious visual artifacts when eye tracking is inaccurate. In this work we present a gaze-contingent rendering technique which only requires saccade detection, bypassing the need for highly-accurate eye tracking. In our first experiment, we show that visual acuity is reduced for several hundred milliseconds after a saccade. In our second experiment, we use these results to reduce the rendered image resolution after saccades in a controlled psychophysical setup, and find that observers cannot discriminate between saccade-contingent, reduced-resolution rendering and full-resolution rendering under certain conditions identified in Experiment 1. Finally, in our third experiment, we introduce a 90 pixels per degree headset and validate our saccade-contingent rendering method under typical VR viewing conditions.	https://dl.acm.org/doi/abs/10.1145/3641519.3657420	Yuna Kwak, Eric Penner, Xuan Wang, Mohammad R. Saeedpour-Parizi, Olivier Mercier, Xiuyun Wu, Scott Murdison, Phillip Guan
Scale-Invariant Monocular Depth Estimation via SSI Depth	Existing methods for scale-invariant monocular depth estimation (SI MDE) often struggle due to the complexity of the task, and limited and non-diverse datasets, hindering generalizability in real-world scenarios. This is while shift-and-scale-invariant (SSI) depth estimation, simplifying the task and enabling training with abundant stereo datasets achieves high performance. We present a novel approach that leverages SSI inputs to enhance SI depth estimation, streamlining the network's role and facilitating in-the-wild generalization for SI depth estimation while only using a synthetic dataset for training. Emphasizing the generation of high-resolution details, we introduce a novel sparse ordinal loss that substantially improves detail generation in SSI MDE, addressing critical limitations in existing approaches. Through in-the-wild qualitative examples and zero-shot evaluation we substantiate the practical utility of our approach in computational photography applications, showcasing its ability to generate highly detailed SI depth maps and achieve generalization in diverse scenarios.	https://dl.acm.org/doi/abs/10.1145/3641519.3657523	S. Mahdi H. Miangoleh, Mahesh Reddy, Yağız Aksoy
Scintilla: Simulating Combustible Vegetation for Wildfires	Wildfires are a complex physical phenomenon that involves the combustion of a variety of flammable materials ranging from fallen leaves and dried twigs to decomposing organic material and living flora. All these materials can potentially act as fuel with different properties that determine the progress and severity of a wildfire. In this paper, we propose a novel approach for simulating the dynamic interaction between the varying components of a wildfire, including processes of convection, combustion and heat transfer between vegetation, soil and atmosphere. We propose a novel representation of vegetation that includes detailed branch geometry, fuel moisture, and distribution of grass, fine fuel, and duff. Furthermore, we model the ignition, generation, and transport of fire by firebrands and embers. This allows simulating and rendering virtual 3D wildfires that realistically capture key aspects of the process, such as progressions from ground to crown fires, the impact of embers carried by wind, and the effects of fire barriers and other human intervention methods. We evaluate our approach through numerous experiments and based on comparisons to real-world wildfire data.	https://dl.acm.org/doi/abs/10.1145/3658192	Andrzej Kokosza, Helge Wrede, Daniel Gonzalez Esparza, Milosz Makowski, Daoming Liu, Dominik L. Michels, Soren Pirk, Wojtek Palubicki
Seamless Parametrization in Penner Coordinates	We introduce a conceptually simple and efficient algorithm for seamless parametrization, a key element in constructing quad layouts and texture charts on surfaces. More specifically, we consider the construction of parametrizations with prescribed i.e., a set of angles at singularities, and rotations along homology loops, preserving which is essential for constructing parametrizations following an input field, as well as for user control of the parametrization structure. Our algorithm performs exceptionally well on a large dataset based on Thingi10k [Zhou and Jacobson 2016], (16156 meshes) as well as on a challenging smaller dataset of [Myles et al. 2014], converging, on average, in 9 iterations. Although the algorithm lacks a formal mathematical guarantee, presented empirical evidence and the connections between convex optimization and closely related algorithms, suggest that a similar formulation can be found for this algorithm in the future.	https://dl.acm.org/doi/abs/10.1145/3658202	Ryan Capouellez, Denis Zorin
Seiler's Interpolation for Evaluating Polynomial Curves	Seiler's interpolation allows evaluating polynomial curves, such as Bézier curves, with a small number of linear interpolations. It is particularly effective with hardware linear interpolation used in GPU texture filtering. We compare it to the popular alternatives, such as de Casteljau's algorithm, and present how it extends to higher-degree polynomials.	https://dl.acm.org/doi/abs/10.1145/3641233.3664331	Cem Yuksel
Self	A wooden doll strives to fit in and ends up sparking a journey of self discovery.	https://dl.acm.org/doi/abs/10.1145/3641230.3649145	Searit Huluf, Eric Rosales
Self Examination: Pixar's Adventures in Stop Motion	Self is the story of a wooden doll who desperately wants to fit in and makes an ill-fated wish upon a star, sparking a journey of self discovery, leading her down a harmful path, and challenging her perspective of both who she is and where she belongs. It is also the most recent of Pixar's SparkShorts, and represented a number of firsts for Pixar: The first use of stop motion animation, the first collaboration with another studio, and the first use of a live-action visual effects workflow in Pixar's animation-centric pipeline. These presented some unique challenges and required us to restructure much of how we work in order to incorporate physical puppet fabrication and stop motion animation.	https://dl.acm.org/doi/abs/10.1145/3641233.3664724	Nathan Fariss
Self-Supervised Video Defocus Deblurring with Atlas Learning	Misfocus is ubiquitous for almost all video producers, degrading video quality and often causing expensive delays and reshoots. Current autofocus (AF) systems are vulnerable to sudden disturbances such as subject movement or lighting changes commonly present in real-world and on-set conditions. Single image defocus deblurring methods are temporally unstable when applied to videos and cannot recover details obscured by temporally varying defocus blur. In this paper, we present an end-to-end solution that allows users to correct misfocus during post-processing. Our method generates and parameterizes defocused videos into sharp layered neural atlases and propagates consistent focus tracking back to the video frames. We introduce a novel differentiable disk blur layer for more accurate point spread function (PSF) simulation, coupled with a circle of confusion (COC) map estimation module with knowledge transferred from the current single image defocus deblurring (SIDD) networks. Our pipeline offers consistent, sharp video reconstruction and effective subject-focus correction and tracking directly on the generated atlases. Furthermore, by adopting our approach, we achieve comparable results to the state-of-the-art optical flow estimation approach from defocus videos.	https://dl.acm.org/doi/abs/10.1145/3641519.3657524	Lingyan Ruan, Martin Bálint, Mojtaba Bemana, Krzysztof Wolski, Hans-Peter Seidel, Karol Myszkowski, Bin Chen
Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis	In this work, we present , a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT's output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin. We will release the code and dataset for academic research.	https://dl.acm.org/doi/abs/10.1145/3658134	Zeyi Zhang, Tenglong Ao, Yuyao Zhang, Qingzhe Gao, Chuan Lin, Baoquan Chen, Libin Liu
Semantic Shape Editing with Parametric Implicit Templates	We propose a semantic shape editing method to edit 3D triangle meshes using parametric implicit surface templates, benefiting from the many advantages offered by analytical implicit representations, such as infinite resolution and boolean or blending operations. We propose first a template fitting method to optimize its parameters to best capture the input mesh. For subsequent template edits, our novel mesh deformation method allows tracking the template's 0-set even when featuring anisotropic stretch and/or local volume change. We make few assumptions on the template implicit fields and only strictly require continuity. We demonstrate applications to interactive semantic shape editing and semantic mesh retargeting.	https://dl.acm.org/doi/abs/10.1145/3641519.3657421	Uday Kusupati, Mathieu Gaillard, Jean-Marc Thiery, Adrien Kaiser
Separate-and-Enhance: Compositional Finetuning for Text-to-Image Diffusion Models	Despite recent significant strides achieved by diffusion-based Text-to-Image (T2I) models, current systems are still less capable of ensuring decent compositional generation aligned with text prompts, particularly for the multi-object generation. In this work, we first show the fundamental reasons for such misalignment by identifying issues related to low attention activation and mask overlaps. Then we propose a compositional finetuning framework with two novel objectives, the Separate loss and the Enhance loss, that reduce object mask overlaps and maximize attention scores, respectively. Unlike conventional test-time adaptation methods, our model, once finetuned on critical parameters, is able to directly perform inference given an arbitrary multi-object prompt, which enhances the scalability and generalizability. Through comprehensive evaluations, our model demonstrates superior performance in image realism, text-image alignment, and adaptability, significantly surpassing established baselines. Furthermore, we show that training our model with a diverse range of concepts enables it to generalize effectively to novel concepts, exhibiting enhanced performance compared to models trained on individual concept pairs.	https://dl.acm.org/doi/abs/10.1145/3641519.3657527	Zhipeng Bao, Yijun Li, Krishna Kumar Singh, Yu-Xiong Wang, Martial Hebert
Shadow of HyperPose: New Animation System	"This presentation details HyperPose, an innovative animation system that represents motion through strategically chosen samples (""principal dynamic poses"") integrated into a procedural state machine. State navigation is achieved via a cost function, optimizing the system's efficiency and realism in animation. Data is represented as a four-dimensional hyperpose."	https://dl.acm.org/doi/abs/10.1145/3641233.3664311	Alex Bereznyak
Shellfish	In a French rocky inlet, Bernard, a small and selfish hermit crab loses his shell. He will has to face dangers of the beach to find an other one. In the company of a weird little crab which helps protecting him, both will lives multiples adventures that will befriend them.	https://dl.acm.org/doi/abs/10.1145/3641230.3652556	Justine Aubert, Cassandra Bouton, Grégoire Callies, Maud Chesneau, Anna Danton, Loic Girault, Gatien Peyrude, Justine Raux
Silhouette	Silhouette tells the story of Claire, a young woman who has just moved to a big city. She soon finds her body gradually disappearing.	https://dl.acm.org/doi/abs/10.1145/3641230.3653235	Alexis Lafuente, Marc Forest, Antoni Nicolaï, Elliot Dreuille, Baptiste Gueusquin, Chloé Stricher
Simplicits: Mesh-Free, Geometry-Agnostic Elastic Simulation	The proliferation of 3D representations, from explicit meshes to implicit neural fields and more, motivates the need for simulators agnostic to representation. We present a data-, mesh-, and grid-free solution for elastic simulation for any object in any geometric representation undergoing large, nonlinear deformations. We note that every standard geometric representation can be reduced to an occupancy function queried at any point in space, and we define a simulator atop this common interface. For each object, we fit a small implicit neural network encoding spatially varying weights that act as a reduced deformation basis. These weights are trained to learn physically significant motions in the object via random perturbations. Our loss ensures we find a weight-space basis that best minimizes deformation energy by stochastically evaluating elastic energies through Monte Carlo sampling of the deformation volume. At runtime, we simulate in the reduced basis and sample the deformations back to the original domain. Our experiments demonstrate the versatility, accuracy, and speed of this approach on data including signed distance functions, point clouds, neural primitives, tomography scans, radiance fields, Gaussian splats, surface meshes, and volume meshes, as well as showing a variety of material energies, contact models, and time integration schemes.	https://dl.acm.org/doi/abs/10.1145/3658184	Vismay Modi, Nicholas Sharp, Or Perel, Shinjiro Sueda, David I. W. Levin
Simulation and Representation of Topology-Changing Rolling Waves for Massive Open Ocean Games	Rendering close-shore water oceanic phenomena, e.g. rolling waves, for real-time applications is a challenging problem, especially in a massive open-world game where variability, artistic control and performance are crucial. While using a form of animated height displacement maps is a common technique in open-world games (e.g. Assassin's Creed Odyssey (2018), Death Stranding (2019)), it is limited to height data and is unable to deal with overhangs and changing topology of collapsing oceanic waves, we are proposing a novel method of capturing wave simulation data with a set of approximation curves that can deal with overhangs, changing topology and strict budget requirements.	https://dl.acm.org/doi/abs/10.1145/3641233.3664308	Vladimir Vladimirovich Lopatin, Elco Vossers, Hugh Malan-Revell
Singapore GeAR	The GEAR.sg App displays 3D models of The GEAR, a recently built research lab in Singapore. Using the smartphone camera, the app presents the building's models and onsite photos on AR picture cards, which illustrate floor plans and allow users to move, flip, and combine the virtual models on them.	https://dl.acm.org/doi/abs/10.1145/3664294.3664730	Takehiko Nagakura, Han Tu, Wenzhe Peng, Chili Cheng, Jenny Zhang, Xiaoyun Zhang, Woongki Sung, Katsuyoshi Ishikawa, Shineng Luke Wu, Jia Hui Joey Ong, Nobuo Iwashita, Hisakata Kobayashi
Singular Foliations for Knit Graph Design	"We build upon the stripes-based knit planning framework of [Mitra et al. 2023], and view the resultant stripe pattern through the lens of singular foliations. This perspective views the stripes, and thus the candidate course rows or wale columns, as integral curves of a vector field specified by the spinning form of [Knöppel et al. 2015]. We show how to tightly control the topological structure of this vector field with linear level set constraints, preventing helicing of any integral curve. Practically speaking, this obviates the stripe placement constraints of [Mitra et al. 2023] and allows for shifting and variation of the stripe frequency without introducing additional helices. En route, we make the first explicit algebraic characterization of spinning form level set structure within singular triangles, and replace the standard interpolant with an ""effective"" one that improves the robustness of knit graph generation. We also extend the model of [Mitra et al. 2023] to surfaces with genus, via a Morse-based cylindrical decomposition, and implement automatic singularity pairing on the resulting components."	https://dl.acm.org/doi/abs/10.1145/3641519.3657487	Rahul Mitra, Erick Jimenez Berumen, Megan Hofmann, Edward Chien
SketchDream: Sketch-based Text-To-3D Generation and Editing	Existing text-based 3D generation methods generate attractive results but lack detailed geometry control. Sketches, known for their conciseness and expressiveness, have contributed to intuitive 3D modeling but are confined to producing texture-less mesh models within predefined categories. Integrating sketch and text simultaneously for 3D generation promises enhanced control over geometry and appearance but faces challenges from 2D-to-3D translation ambiguity and multi-modal condition integration. Moreover, further editing of 3D models in arbitrary views will give users more freedom to customize their models. However, it is difficult to achieve high generation quality, preserve unedited regions, and manage proper interactions between shape components. To solve the above issues, we propose a text-driven 3D content generation and editing method, SketchDream, which supports NeRF generation from given hand-drawn sketches and achieves free-view sketch-based local editing. To tackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view image generation diffusion model, which leverages depth guidance to establish spatial correspondence. A 3D ControlNet with a 3D attention module is utilized to control multi-view images and ensure their 3D consistency. To support local editing, we further propose a coarse-to-fine editing approach: the coarse phase analyzes component interactions and provides 3D masks to label edited regions, while the fine stage generates realistic results with refined details by local enhancement. Extensive experiments validate that our method generates higher-quality results compared with a combination of 2D ControlNet and image-to-3D generation techniques and achieves detailed control compared with existing diffusion-based 3D editing approaches.	https://dl.acm.org/doi/abs/10.1145/3658120	Feng-Lin Liu, Hongbo Fu, Yu-Kun Lai, Lin Gao
SkillPicker: Tweezers for Recording and Training Dexterous Operations	Tweezers are used to carry out crucial functions in various research fields, most notably biology. Thus, proficiency in using tweezers impacts experimental outcomes. However, becoming proficient in using tweezers under expert guidance is not always easy for beginners because the force applied to the tip of a tweezer is virtually imperceptible. To help learners, we propose SkillPicker, which provides the user with images taken at the tip of their tweezers and measures the pinch force simultaneously. Thus, experts can record their technique using SkillPicker, enabling beginners to practice autonomously. SkillPicker is equipped with a small magnifying camera at the fulcrum of the tweezers and a pressure sensor to record the pinch force applied by the user's thumb. We establish an empirical correlation between the pressure applied and the pinch force at the tip, thereby enabling the estimation of the pinch force applied at the tip of the tweezers. SkillPicker also enables pressure values to be visualized or converted into musical notes on a scale to promote instant comprehension. This system facilitates the recording and sharing of tweezer operation, enabling beginners to learn dexterity both visually and physically.	https://dl.acm.org/doi/abs/10.1145/3641517.3664392	Noriyasu Obushi, Shohei Oshiro, Yusei Imai, Chikayoshi Matsudaira, Saku Kijima, Takehiko Kanazawa, Hirokazu Tsukaya, Masahiko Inami
Skin Wrinkle	Skin Wrinkle is a custom deformer tool at DreamWorks Animation that produces wrinkles in the animated skin mesh geometry solely based on the incoming animated skin mesh, rest skin mesh, and artist specified parameters. This wrinkle effect on characters that exhibit dynamic folds enhances their look and animation performance. The deformer minimizes loss in body volume as the skin wrinkles and also performs continuous self collision detection and resolution to handle the tight wrinkles. Skin Wrinkle is time-independent, fast, robust, and controllable, thus making it very artist-friendly.	https://dl.acm.org/doi/abs/10.1145/3641233.3664327	Arunachalam Somasundaram, Chris De St. Jeor
Smooth Bijective Projection in a High-order Shell	We propose a new structure called a higher-order shell, which is composed of a set of triangular prisms. Each triangular prism is enveloped by three Bézier triangles (top, middle, and bottom) and three side surfaces, each of which is trimmed from a bilinear surface. Moreover, we define a continuous vector field to smoothly and bijectively transfer attributes between two surfaces inside the shell. Since the higher-order shell has several hard construction constraints, we apply an interior-point strategy to robustly and automatically construct a high-order shell for an input mesh. Specifically, the strategy starts from a valid linear shell with a small thickness. Then, the shell is optimized until the specified thickness is reached, where explicit checks ensure that the constraints are always satisfied. We extensively test our method on more than 8300 models, demonstrating its robustness and performance. Compared to state-of-the-art methods, our bijective projection is smoother, and the space between the shell and input mesh is more uniform.	https://dl.acm.org/doi/abs/10.1145/3658207	Shibo Liu, Yang Ji, Jia-Peng Guo, Ligang Liu, Xiao-Ming Fu
Soft Pneumatic Actuator Design using Differentiable Simulation	We propose a computational design pipeline for pneumatically-actuated soft robots interacting with their environment through contact. We optimize the shape of the robot with a shape optimization approach, using a physically-accurate high-order finite element model for the forward simulation. Our approach enables fine-grained control over both deformation and contact forces by optimizing the shape of internal cavities, which we exploit to design pneumatically-actuated robots that can assume user-prescribed poses, or apply user-controlled forces. We demonstrate the efficacy of our method on two artistic and two functional examples.	https://dl.acm.org/doi/abs/10.1145/3641519.3657467	Arvi Gjoka, Espen Knoop, Moritz Bächer, Denis Zorin, Daniele Panozzo
Solid Knitting	We introduce solid knitting, a new fabrication technique that combines the layer-by-layer volumetric approach of 3D printing with the topologically-entwined stitch structure of knitting to produce solid 3D objects. We define the basic building blocks of solid knitting and demonstrate a working prototype of a solid knitting machine controlled by a low-level instruction language, along with a volumetric design tool for creating machine-knittable patterns. Solid knitting uses a course-wale-layer structure, where every loop in a solid-knit object passes through both a loop from the previous layer and a loop from the previous course. Our machine uses two beds of latch needles to create stitches like a conventional V-bed knitting machine, but augments these needles with a pair of rotating hook arrays to provide storage locations for all of the loops in one layer of the object. It can autonomously produce solid-knit prisms of arbitrary length, although it requires manual intervention to cast on the first layer and bind off the final row. Our design tool allows users to create solid knitting patterns by connecting elementary stitches; objects designed in our interface can---after basic topological checks and constraint propagation---be exported as a sequence of instructions for fabrication on the solid knitting machine. We validate our solid knitting hardware and software on prism examples, detail the mechanical errors which we have encountered, and discuss potential extensions to the capability of our solid knitting machine.	https://dl.acm.org/doi/abs/10.1145/3658123	Yuichi Hirose, Mark Gillespie, Angelica M. Bonilla Fominaya, James McCann
Sony Imageworks Animation Layout Workflow with Unreal Engine and OpenUSD	This is an overview of the new rough layout pipeline at Sony Pictures Imageworks. In a notable departure from the legacy pipeline, sequence and shot-based work now begin in Unreal Engine. By exporting USD data out of Unreal Engine to share with other DCCs, we were able to reinvent the early stages of feature film production.	https://dl.acm.org/doi/abs/10.1145/3641233.3664722	Jonghwan Hwang, Tzung-Da Tsai, Dan Ziegler
Spambots	Spambots explores questions around AI and industrial farming. AI is increasingly used to generate spam content. Spambots explores what would happen if, instead, machine learning was used to empower a group of robotic Spam cans to tell their tales. Each Spambot has a small keyboard with four letters on it and when they collaborate they are able type the whole alphabet along with some punctuation. The text they are typing is generated by a neural network fine tuned on a version of Aldous Huxley's Brave New World where occasional words have been swapped out for pig-related ones. In this novel each character is born into a role in much the same way industrial farm animals, such as those that end up in Spam, are born into their fate.	https://dl.acm.org/doi/abs/10.1145/3641523.3665167	Neil Mendoza
Spark: Milky Way Evolution	"The visualization-based Spark: The Universe in Us planetarium show includes an immersive journey through a threedimensional model of the Milky Way, merging observed and computational data. Astrophysical simulations also ""turn back the clock"" on our galaxy's evolution to reveal the origin of atoms that formed the Solar System."	https://dl.acm.org/doi/abs/10.1145/3641230.3653475	Ryan Wyatt, Mike Schmitt
Spatial and Surface Correspondence Field for Interaction Transfer	In this paper, we introduce a new method for the task of interaction transfer. Given an example interaction between a source object and an agent, our method can automatically infer both surface and spatial relationships for the agent and target objects within the same category, yielding more accurate and valid transfers. Specifically, our method characterizes the example interaction using a combined spatial and surface representation. We correspond the agent points and object points related to the representation to the target object space using a learned spatial and surface correspondence field, which represents objects as deformed and rotated signed distance fields. With the corresponded points, an optimization is performed under the constraints of our spatial and surface interaction representation and additional regularization. Experiments conducted on human-chair and hand-mug interaction transfer tasks show that our approach can handle larger geometry and topology variations between source and target shapes, significantly outperforming state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3658169	Zeyu Huang, Honghao Xu, Haibin Huang, Chongyang Ma, Hui Huang, Ruizhen Hu
Spatial storytelling in the Newsroom: Reconstructing news events in 3D	When we create stories in 3-D space, we can go beyond words to help the reader feel they were there to experience and understand it. This talk shows how three visual stories in the New York Times create a sense of presence, using computer vision techniques, spatial audio, and 3-D tiles.	https://dl.acm.org/doi/abs/10.1145/3641233.3664727	Malika Khurana, Karthik Patanjali, Nick Bartzokas
Specular Polynomials	Finding valid light paths that involve specular vertices in Monte Carlo rendering requires solving many non-linear, transcendental equations in high-dimensional space. Existing approaches heavily rely on Newton iterations in path space, which are limited to obtaining at most a single solution each time and easily diverge when initialized with improper seeds. We propose , a Newton iteration-free methodology for finding a complete set of admissible specular paths connecting two arbitrary endpoints in a scene. The core is a reformulation of specular constraints into polynomial systems, which makes it possible to reduce the task to a univariate root-finding problem. We first derive bivariate systems utilizing rational coordinate mapping between the coordinates of consecutive vertices. Subsequently, we adopt the hidden variable resultant method for variable elimination, converting the problem into finding zeros of the determinant of univariate matrix polynomials. This can be effectively solved through Laplacian expansion for one bounce and a bisection solver for more bounces. Our solution is generic, completely deterministic, accurate for the case of one bounce, and GPU-friendly. We develop efficient CPU and GPU implementations and apply them to challenging glints and caustic rendering. Experiments on various scenarios demonstrate the superiority of specular polynomial-based solutions compared to Newton iteration-based counterparts. Our implementation is available at https://github.com/mollnn/spoly.	https://dl.acm.org/doi/abs/10.1145/3658132	Zhimin Fan, Jie Guo, Yiming Wang, Tianyu Xiao, Hao Zhang, Chenxi Zhou, Zhenyu Chen, Pengpei Hong, Yanwen Guo, Ling-Qi Yan
Spice·E: Structural Priors in 3D Diffusion using Cross-Entity Attention	We are witnessing rapid progress in automatically generating and manipulating 3D assets due to the availability of pretrained text-to-image diffusion models. However, time-consuming optimization procedures are required for synthesizing each sample, hindering their potential for democratizing 3D content creation. Conversely, 3D diffusion models now train on million-scale 3D datasets, yielding high-quality text-conditional 3D samples within seconds. In this work, we present Spice · E – a neural network that adds structural guidance to 3D diffusion models, extending their usage beyond text-conditional generation. At its core, our framework introduces a cross-entity attention mechanism that allows for multiple entities—in particular, paired input and guidance 3D shapes—to interact via their internal representations within the denoising network. We utilize this mechanism for learning task-specific structural priors in 3D diffusion models from auxiliary guidance shapes. We show that our approach supports a variety of applications, including 3D stylization, semantic shape editing and text-conditional abstraction-to-3D, which transforms primitive-based abstractions into highly-expressive shapes. Extensive experiments demonstrate that Spice · E achieves SOTA performance over these tasks while often being considerably faster than alternative methods. Importantly, this is accomplished without tailoring our approach for any specific task. We will release our code and trained models.	https://dl.acm.org/doi/abs/10.1145/3641519.3657461	Etai Sella, Gal Fiebelman, Noam Atia, Hadar Averbuch-Elor
Spin-It Faster: Quadrics Solve All Topology Optimization Problems That Depend Only On Mass Moments	The behavior of a rigid body primarily depends on its mass moments, which consist of the mass, center of mass, and moments of inertia. It is possible to manipulate these quantities without altering the geometric appearance of an object by introducing cavities in its interior. Algorithms that find cavities of suitable shapes and sizes have enabled the computational design of spinning tops, yo-yos, wheels, buoys, and statically balanced objects. Previous work is based, for example, on topology optimization on voxel grids, which introduces a large number of optimization variables and box constraints, or offset surface computation, which cannot guarantee that solutions to a feasible problem will always be found. In this work, we provide a mathematical analysis of constrained topology optimization problems that depend only on mass moments. This class of problems covers, among others, all applications mentioned above. Our main result is to show that no matter the outer shape of the rigid body to be optimized or the optimization objective and constraints considered, the optimal solution always features a quadric-shaped interface between material and cavities. This proves that optimal interfaces are always ellipsoids, hyperboloids, paraboloids, or one of a few degenerate cases, such as planes. This insight lets us replace a difficult topology optimization problem with a provably equivalent non-linear equation system in a small number (<10) of variables, which represent the coefficients of the quadric. This system can be solved in a few seconds for most examples, provides insights into the geometric structure of many specific applications, and lets us describe their solution properties. Finally, our method integrates seamlessly into modern fabrication workflows because our solutions are analytical surfaces that are native to the CAD domain.	https://dl.acm.org/doi/abs/10.1145/3658194	Christian Hafner, Mickaël Ly, Chris Wojtan
Spin-Weighted Spherical Harmonics for Polarized Light Transport	The objective of polarization rendering is to simulate the interaction of light with materials exhibiting polarization-dependent behavior. However, integrating polarization into rendering is challenging and increases computational costs significantly. The primary difficulty lies in efficiently modeling and computing the complex reflection phenomena associated with polarized light. Specifically, frequency-domain analysis, essential for efficient environment lighting and storage of complex light interactions, is lacking. To efficiently simulate and reproduce polarized light interactions using frequency-domain techniques, we address the challenge of maintaining continuity in polarized light transport represented by Stokes vectors within angular domains. The conventional spherical harmonics method cannot effectively handle continuity and rotation invariance for Stokes vectors. To overcome this, we develop a new method called polarized spherical harmonics (PSH) based on the spin-weighted spherical harmonics theory. Our method provides a rotation-invariant representation of Stokes vector fields. Furthermore, we introduce frequency domain formulations of polarized rendering equations and spherical convolution based on PSH. We first define spherical convolution on Stokes vector fields in the angular domain, and it also provides efficient computation of polarized light transport, nearly on an entry-wise product in the frequency domain. Our frequency domain formulation, including spherical convolution, led to the development of the first real-time polarization rendering technique under polarized environmental illumination, named precomputed polarized radiance transfer, using our polarized spherical harmonics. Results demonstrate that our method can effectively and accurately simulate and reproduce polarized light interactions in complex reflection phenomena, including polarized environmental illumination and soft shadows.	https://dl.acm.org/doi/abs/10.1145/3658139	Shinyoung Yi, Donggun Kim, Jiwoong Na, Xin Tong, Min H. Kim
Split-Aperture 2-in-1 Computational Cameras	While conventional cameras offer versatility for applications ranging from amateur photography to autonomous driving, computational cameras allow for domain-specific adaption. Cameras with co-designed optics and image processing algorithms enable high-dynamic-range image recovery, depth estimation, and hyperspectral imaging through optically encoding scene information that is otherwise undetected by conventional cameras. However, this optical encoding creates a challenging inverse reconstruction problem for conventional image recovery, and often lowers the overall photographic quality. Thus computational cameras with domain-specific optics have only been adopted in a few specialized applications where the captured information cannot be acquired in other ways. In this work, we investigate a method that combines two optical systems into one to tackle this challenge. We split the aperture of a conventional camera into two halves: one which applies an application-specific modulation to the incident light via a diffractive optical element to produce a coded image capture, and one which applies no modulation to produce a conventional image capture. Co-designing the phase modulation of the split aperture with a dual-pixel sensor allows us to simultaneously capture these coded and uncoded images without increasing physical or computational footprint. With an uncoded conventional image alongside the optically coded image in hand, we investigate image reconstruction methods that are conditioned on the conventional image, making it possible to eliminate artifacts and compute costs that existing methods struggle with. We assess the proposed method with 2-in-1 cameras for optical high-dynamic-range reconstruction, monocular depth estimation, and hyperspectral imaging, comparing favorably to all tested methods in all applications.	https://dl.acm.org/doi/abs/10.1145/3658225	Zheng Shi, Ilya Chugunov, Mario Bijelic, Geoffroi Côté, Jiwoon Yeom, Qiang Fu, Hadi Amata, Wolfgang Heidrich, Felix Heide
Split-and-Fit: Learning B-Reps via Structure-Aware Voronoi Partitioning	"We introduce a novel method for acquiring boundary representations (B-Reps) of 3D CAD models which involves a two-step process: it first applies a , referred to as the ""split"", followed by a ""fit"" operation to derive a single primitive within each partition. Specifically, our partitioning aims to produce the classical of the set of ground-truth (GT) B-Rep primitives. In contrast to prior B-Rep constructions which were bottom-up, either via direct primitive fitting or point clustering, our Split-and-Fit approach is and , since a Voronoi partition explicitly reveals both the number of and the connections between the primitives. We design a neural network to predict the Voronoi diagram from an input point cloud or distance field via a binary classification. We show that our network, coined NVD-Net for neural Voronoi diagrams, can effectively learn Voronoi partitions for CAD models from training data and exhibits superior generalization capabilities. Extensive experiments and evaluation demonstrate that the resulting B-Reps, consisting of parametric surfaces, curves, and vertices, are more plausible than those obtained by existing alternatives, with significant improvements in reconstruction quality. Code will be released on https://github.com/yilinliu77/NVDNet."	https://dl.acm.org/doi/abs/10.1145/3658155	Yilin Liu, Jiale Chen, Shanshan Pan, Daniel Cohen-Or, Hao Zhang, Hui Huang
Spot the Differences	"Lottie, a nine-year-old perfect little girl, discovers a sumptuous dollhouse in her attic. This dollhouse turns out to be the exact replica of the imposing Victorian house Lottie lives in. More than a simple toy, this dollhouse is a ""spot the differences"" game with very strict rules. Lottie begins her quest to find the differences between her house and its miniature replica."	https://dl.acm.org/doi/abs/10.1145/3641230.3648639	Brune De Cerval, Fleur Larcena, Clémence Beauprez, Aurélie Lequien, Yohan Gameiro, Samantha Liu
Spots of Light	Dan Layan tragically lost his sight during an Israeli military intervention in Lebanon in 1982. After 25 years in the dark, an experimental surgical procedure brings him the miraculous opportunity of recovering his sight. This VR experience uses personal archives to allow you to relive this introspective journey.	https://dl.acm.org/doi/abs/10.1145/3641231.3644814	Sara Fatucci, Adam Weingrod, Kobi Mizrahi, Sean Thomas Evans
Stabler Neo-Hookean Simulation: Absolute Eigenvalue Filtering for Projected Newton	Volume-preserving hyperelastic materials are widely used to model near-incompressible materials such as rubber and soft tissues. However, the numerical simulation of volume-preserving hyperelastic materials is notoriously challenging within this regime due to the non-convexity of the energy function. In this work, we identify the pitfalls of the popular eigenvalue clamping strategy for projecting Hessian matrices to positive semi-definiteness during Newton's method. We introduce a novel eigenvalue filtering strategy for projected Newton's method to stabilize the optimization of Neo-Hookean energy and other volume-preserving variants under high Poisson's ratio (near 0.5) and large initial volume change. Our method only requires a single line of code change in the existing projected Newton framework, while achieving significant improvement in both stability and convergence speed. We demonstrate the effectiveness and efficiency of our eigenvalue projection scheme on a variety of challenging examples and over different deformations on a large dataset.	https://dl.acm.org/doi/abs/10.1145/3641519.3657433	Honglin Chen, Hsueh-Ti Derek Liu, David I.W. Levin, Changxi Zheng, Alec Jacobson
Starry Starry Night: An Interactive Narrative Visualization of Astrology Imagery in Tang Poetry	Astrology emerges as a distinctive and mystical facet of traditional Chinese culture, imparting a unique charm when interwoven into ancient literature such as Tang poetry. We designed an interactive narrative visualization, Starry Starry Night, to present the application of astrology imagery in Tang poems. The visualization combines author-driven storytelling and reader-driven exploration, hoping to effectively enhance readers' understanding of astrology-related poetry and to promote the dissemination of traditional Chinese culture.	https://dl.acm.org/doi/abs/10.1145/3641234.3671059	Yechun Peng, Jingxin Ye, Dianheng Jiang, Qing Chen, Yang Shi, Nan Cao
Still Here	"""Still Here"" est la cinématique de la saison 2024, de League of Legends : la saison 14. Cette dernière met en scène plusieurs visages familiers parmi la liste des champions de LoL, tels que Kayle, Morgana, Aatrox, Yasuo, Tryndamere, Kindred et Ashe. On découvre des batailles épiques de chacun de ces champions affrontant la mort, dans le passé, le présent et le futur."	https://dl.acm.org/doi/abs/10.1145/3641230.3652893	Remi Kozyra
Stochastic Computation of Barycentric Coordinates	This paper presents a practical and general approach for computing barycentric coordinates through stochastic sampling. Our key insight is a reformulation of the kernel integral defining barycentric coordinates into a weighted least-squares minimization that enables Monte Carlo integration without sacrificing linear precision. Our method can thus compute barycentric coordinates directly at the points of interest, both inside and outside the cage, using just proximity queries to the cage such as closest points and ray intersections. As a result, we can evaluate barycentric coordinates for a large variety of cage representations (from quadrangulated surface meshes to parametric curves) seamlessly, bypassing any volumetric discretization or custom solves. To address the archetypal noise induced by sample-based estimates, we also introduce a denoising scheme tailored to barycentric coordinates. We demonstrate the efficiency and flexibility of our formulation by implementing a stochastic generation of harmonic coordinates, mean-value coordinates, and positive mean-value coordinates.	https://dl.acm.org/doi/abs/10.1145/3658131	Fernando de Goes, Mathieu Desbrun
StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering	Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single viewspace depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements. Our renderer is publicly available at https://github.com/r4dl/StopThePop.	https://dl.acm.org/doi/abs/10.1145/3658187	Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger
Strategy and Skill Learning for Physics-based Table Tennis Animation	Recent advancements in physics-based character animation leverage deep learning to generate agile and natural motion, enabling characters to execute movements such as backflips, boxing, and tennis. However, reproducing the selection and use of diverse motor skills in dynamic environments to solve complex tasks, as humans do, still remains a challenge. We present a strategy and skill learning approach for physics-based table tennis animation. Our method addresses the issue of mode collapse, where the characters do not fully utilize the motor skills they need to perform to execute complex tasks. More specifically, we demonstrate a hierarchical control system for diversified skill learning and a strategy learning framework for effective decision-making. We showcase the efficacy of our method through comparative analysis with state-of-the-art methods, demonstrating its capabilities in executing various skills for table tennis. Our strategy learning framework is validated through both agent-agent interaction and human-agent interaction in Virtual Reality, handling both competitive and cooperative tasks.	https://dl.acm.org/doi/abs/10.1145/3641519.3657437	Jiashun Wang, Jessica Hodgins, Jungdam Won
Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion	We present a method for generating Streetscapes—long sequences of views through an on-the-fly synthesized city-scale scene. Our generation is conditioned by language input (e.g., city name, weather), as well as an underlying map/layout hosting the desired trajectory. Compared to recent models for video generation or 3D view synthesis, our method can scale to much longer-range camera trajectories, spanning several city blocks, while maintaining visual quality and consistency. To achieve this goal, we build on recent work on video diffusion, used within an autoregressive framework that can easily scale to long sequences. In particular, we introduce a new temporal imputation method that prevents our autoregressive approach from drifting from the distribution of realistic city imagery. We train our Streetscapes system on a compelling source of data—posed imagery from Google Street View, along with contextual map data—which allows users to generate city views conditioned on any desired city layout, with controllable camera pose.	https://dl.acm.org/doi/abs/10.1145/3641519.3657513	Boyang Deng, Richard Tucker, Zhengqi Li, Leonidas Guibas, Noah Snavely, Gordon Wetzstein
Student / Futures: Creative Careers in Animation, Computer Graphics, and Interactive Techniques	Industry panelists share perspectives and insights for students, educators, and creative professionals who are considering careers in animation, computer graphics, creative technologies, and interactive techniques. Within rapid and ever-expanding fields of technological change, many creative industries are reaching critical inflection points. Changing workplace cultures, advances in machine learning and artificial intelligence, real-time graphics and virtual production systems, edge computing and ever-faster network communications are all radically transforming the forms and capacities of creative industries and cultural production. A variety of opportunities and unforeseen challenges are exposed along the way. In this panel, creative industry representatives discuss the general and specific states of their fields, providing insights into changing career paradigms. Discussion includes advice for educators to help prepare students to meet the challenges and opportunities that currently face creative industries, as well as the preparation and training needed to anticipate change. Panelists consider what qualities make for desirable applicants in their respective fields, with insights for individuals who are preparing to enter creative industries, as well as for those considering career transitions. Represented industry segments include animation and VFX, computer graphics and information systems, themed entertainment, and interactive educational technologies. Questions considered include how pedagogy can help prepare and empower students for successful creative careers; what entry-level applicants should have (and should not have) on resumes, portfolios, and demo reels; and what can creative talents do to proactively acquire requisite credentials. Discussion will expose fresh outlooks on the futures of creative fields in animation, computer graphics, and interactive techniques.	https://dl.acm.org/doi/abs/10.1145/3641235.3664452	Johannes DeYoung
Stylized Rendering as a Function of Expectation	We propose a generalization of the rendering equation that captures both the realistic light transport of physically-based rendering (PBR) and a subset of non-photorealistic rendering (NPR) stylizations in a principled manner. The proposed formulation is based on the key observation that both classical transport and certain NPR stylizations can be modeled as a Given this observation, we generalize the recursive integrals of the rendering equation to As estimating functions of expectation can be challenging, especially recursive ones, we provide a toolkit for unbiased and biased estimation comprising prior work, general strategies, and a novel build-your-own strategy for constructing more complex unbiased estimators from simpler unbiased estimators. We then use this toolkit to construct a complete estimator for the proposed recursive formulation, and implement a sampling algorithm that is both conceptually simple and leverages many of the components of an ordinary path tracer. To demonstrate the practicality of the proposed method we showcase how it captures several existing stylizations like color mapping, cel shading, and cross-hatching, fuses NPR and PBR visuals, and allows us to explore visuals that were previously challenging under existing formulations.	https://dl.acm.org/doi/abs/10.1145/3658161	Rex West, Sayan Mukherjee
Subject-Diffusion: Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning	Recent progress in personalized image generation using diffusion models has been significant. However, development in the area of open-domain and test-time fine-tuning-free personalized image generation is proceeding rather slowly. In this paper, we propose Subject-Diffusion, a novel open-domain personalized image generation model that, in addition to not requiring test-time fine-tuning, also only requires a single reference image to support personalized generation of single- or two-subjects in any domain. Firstly, we construct an automatic data labeling tool and use the LAION-Aesthetics dataset to construct a large-scale dataset consisting of 76M images and their corresponding subject detection bounding boxes, segmentation masks, and text descriptions. Secondly, we design a new unified framework that combines text and image semantics by incorporating coarse location and fine-grained reference image control to maximize subject fidelity and generalization. Furthermore, we also adopt an attention control mechanism to support two-subject generation. Extensive qualitative and quantitative results demonstrate that our method have certain advantages over other frameworks in single, multiple, and human-customized image generation.	https://dl.acm.org/doi/abs/10.1145/3641519.3657469	Jian Ma, Junhao Liang, Chen Chen, Haonan Lu
Suit Up! Sony Pictures Imageworks Presents Ghostbusters: Frozen Empire	Who you gonna call when the world freezes over? Sony Pictures Imageworks! Join artists for an exclusive behind-thescenes sneak peek at the making of Ghostbusters: Frozen Empire. This session will focus on the development of detailed environments, creation of unique characters, and the artistic and technological innovations the production has developed for this highly-anticipated feature.	https://dl.acm.org/doi/abs/10.1145/3641232.3649393	Jason Greenblum, Benjamin Hendricks, Ben Aguillon, Christopher Messineo, Andrew Bain, Steven Argula
Super-Resolution Cloth Animation with Spatial and Temporal Coherence	Creating super-resolution cloth animations, which refine coarse cloth meshes with fine wrinkle details, faces challenges in preserving spatial consistency and temporal coherence across frames. In this paper, we introduce a general framework to address these issues, leveraging two core modules. The first module interleaves a simulator and a corrector. The simulator handles cloth dynamics, while the corrector rectifies differences in low-frequency features across various resolutions. This interleaving ensures prompt correction of spatial errors from the coarse simulation, effectively preventing their temporal propagation. The second module performs mesh-based super-resolution for detailed wrinkle enhancements. We decompose garment meshes into overlapping patches for adaptability to various styles and geometric continuity. Our method achieves an 8× improvement in resolution for cloth animations. We showcase the effectiveness of our method through diverse animation examples, including simple cloth pieces and intricate garments.	https://dl.acm.org/doi/abs/10.1145/3658143	Jiawang Yu, Zhendong Wang
SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation	Physically-simulated models for human motion can generate high-quality responsive character animations, often in real-time. Natural language serves as a flexible interface for controlling these models, allowing expert and non-expert users to quickly create and edit their animations. Many recent physics-based animation methods, including those that use text interfaces, train control policies using reinforcement learning (RL). However, scaling these methods beyond several hundred motions has remained challenging. Meanwhile, kinematic animation models are able to successfully learn from thousands of diverse motions by leveraging supervised learning methods. Inspired by these successes, in this work we introduce SuperPADL, a scalable framework for physics-based text-to-motion that leverages both RL and supervised learning to train controllers on thousands of diverse motion clips. SuperPADL is trained in stages using progressive distillation, starting with a large number of specialized experts using RL. These experts are then iteratively distilled into larger, more robust policies using a combination of reinforcement learning and supervised learning. Our final SuperPADL controller is trained on a dataset containing over 5000 skills and runs in real time on a consumer GPU. Moreover, our policy can naturally transition between skills, allowing for users to interactively craft multi-stage animations. We experimentally demonstrate that SuperPADL significantly outperforms RL-based baselines at this large data scale.	https://dl.acm.org/doi/abs/10.1145/3641519.3657492	Jordan Juravsky, Yunrong Guo, Sanja Fidler, Xue Bin Peng
Surface-Filling Curve Flows via Implicit Medial Axes	We introduce a fast, robust, and user-controllable algorithm to generate surface-filling curves. We compute these curves through the gradient flow of a simple sparse energy, making our method several orders of magnitude faster than previous works. Our algorithm makes minimal assumptions on the topology and resolution of the input surface, achieving improved robustness. Our framework provides tuneable parameters that guide the shape of the output curve, making it ideal for interactive design applications.	https://dl.acm.org/doi/abs/10.1145/3658158	Yuta Noma, Silvia Sellán, Nicholas Sharp, Karan Singh, Alec Jacobson
Surrounded Serenity	Surrounded Serenity is a VR experience that takes the user inside a symbolic womb and gives them a feeling of comfort, disconnect from reality and its anxieties, and a chance at symbolic rebirth. It is a creative interpretation and imitation of the experience within the womb in interactive VR.	https://dl.acm.org/doi/abs/10.1145/3641231.3646557	Malak Quota
T2DyVec: Leveraging Sparse Images and Controllable Text for Dynamic SVG	In this work, we introduce a controlled dynamic vector graphic generation method. While existing work mostly focuses on text-based generation of single-frame images, dynamic images, or single-frame vectors, there is a lack of research on generating dynamic vectors with complex elements and diverse styles. This is due to the unique challenges posed by dynamic vectors, which require coherent and seamless transitions of vector parameters between frames. To address these challenges, we propose T2DyVec, which leverages text prompts and sparse images as a control for vector generation. It incorporates Vector Consistency, Semantic Tracking, and VPSD to optimize the diffusion model for vector parameters, enabling the generation of multi-frame dynamic coherent vectors. This approach can significantly optimize creative workers' workflow, facilitating generation and further editing.	https://dl.acm.org/doi/abs/10.1145/3641234.3671020	Jiakai Wu, Jun Xiao, Haiyong Jiang
TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis	The gradual nature of a diffusion process that synthesizes samples in small increments constitutes a key ingredient of Denoising Diffusion Probabilistic Models (DDPM), which have presented unprecedented quality in image synthesis and been recently explored in the motion domain. In this work, we propose to adapt the gradual diffusion concept (operating along a diffusion time-axis) into the temporal-axis of the motion sequence. Our key idea is to extend the DDPM framework to support temporally varying denoising, thereby entangling the two axes. Using our special formulation, we iteratively denoise a motion buffer that contains a set of increasingly-noised poses, which auto-regressively produces an arbitrarily long stream of frames. With a stationary diffusion time-axis, in each diffusion step we increment only the temporal-axis of the motion such that the framework produces a new, clean frame which is removed from the beginning of the buffer, followed by a newly drawn noise vector that is appended to it. This new mechanism paves the way towards a new framework for long-term motion synthesis with applications to character animation and other domains.	https://dl.acm.org/doi/abs/10.1145/3641519.3657515	Zihan Zhang, Richard Liu, Rana Hanocka, Kfir Aberman
TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts	Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIP-Editor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIP-Editor utilizes explicit and flexible 3D Gaussian splatting (GS) as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.	https://dl.acm.org/doi/abs/10.1145/3658205	Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan
Taming Diffusion Probabilistic Models for Character Control	We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control. To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs. These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control. As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model. We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers. The code and model are available at  https://aiganimation.github.io/CAMDM/.	https://dl.acm.org/doi/abs/10.1145/3641519.3657440	Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, Xuelin Chen
Tangible Data: Immersive Data Exploration with Touch	We demonstrate an interactive 3D data visualization tool called TangibleData. TangibleData adapts hand gestures and mid-air haptics to provide 3D data exploration and interaction in VR using hand gestures and ultrasound mid-air haptic feedback. We showcase different types of 3D visualization datasets with different data encoding methods that convert data into visual and haptic representations for data interaction. We focus on an intuitive approach for each dataset, using mid-air haptics to improve the user's understanding.	https://dl.acm.org/doi/abs/10.1145/3641517.3664398	Ayush Bhardwaj, Jin Ryong Kim
Target-Aware Image Denoising for Inverse Monte Carlo Rendering	Physically based differentiable rendering allows an accurate light transport simulation to be differentiated with respect to the rendering input, i.e., scene parameters, and it enables inferring scene parameters from target images, e.g., photos or synthetic images, via an iterative optimization. However, this inverse Monte Carlo rendering inherits the fundamental problem of the Monte Carlo integration, i.e., noise, resulting in a slow optimization convergence. An appealing approach to addressing such noise is exploiting an image denoiser to improve optimization convergence. Unfortunately, the direct adoption of existing image denoisers designed for ordinary rendering scenarios can drive the optimization into undesirable local minima due to denoising bias. It motivates us to reformulate a new image denoiser specialized for inverse rendering. Unlike existing image denoisers, we conduct our denoising by considering the target images, i.e., specific information in inverse rendering. For our target-aware denoising, we determine our denoising weights via a linear regression technique using the target. We demonstrate that our denoiser enables inverse rendering optimization to infer scene parameters robustly through a diverse set of tests.	https://dl.acm.org/doi/abs/10.1145/3658182	Jeongmin Gu, Jonghee Back, Sung-Eui Yoon, Bochang Moon
Teaching Design with Consumer-ready Artificial Intelligence and Virtual Reality: A Case Study	Design disciplines that require intensive ideation and demand spatial perception (e.g., architecture, interior, and industrial design) benefit from advanced computer graphics, interactive techniques, and human computer interactions. However, design students without a background in Computer Science (CS) face a steep learning curve and difficulties in experimenting with such advancements. Consumer-ready technologies, like Artificial Intelligence (AI) and Virtual Reality (VR), can leverage design skills such as risk-taking, iterating, and visualizing. Interior design students can translate their proficiency in concept writing and computer-aided design (CAD) software into AI prompt writing and VR digital modeling. This Engaging Education Technique and Assignment (EETA) features a nine-week lighting design project. Students use AI to ideate in 2 weeks, VR to visualize in 4 weeks, and 3D print to prototype in 3 weeks. Assessments for the 2022 class showed significant statistical evidence for creativity improvements with the use of consumer-ready AI and VR (but not with 3D printing).	https://dl.acm.org/doi/abs/10.1145/3641235.3664437	Hoa Vo
Teenage Mutant Ninja Turtles: Mutant Mayhem - Cinesite	Cinesite contributed 21 minutes of Teenage Mutant Ninja Turtles, adding its distinctive visual style to match the rest of the film. Full of exaggerated strokes and effects lines, director Jeff Rowe wanted it to resemble school notebook sketches he made as a teenager, imperfect and with a rough brush feel.	https://dl.acm.org/doi/abs/10.1145/3641230.3653329	Chris Kazmier, Ezequiel Mastrasso, Eric Cheung, Chris Springfield
Tele-Aloha: A Telepresence System with Low-budget and High-authenticity Using Sparse RGB Cameras	In this paper, we present a low-budget and high-authenticity bidirectional telepresence system, Tele-Aloha, targeting peer-to-peer communication scenarios. Compared to previous systems, Tele-Aloha utilizes only four sparse RGB cameras, one consumer-grade GPU, and one autostereoscopic screen to achieve high-resolution (2048x2048), real-time (30 fps), low-latency (less than 150ms) and robust distant communication. As the core of Tele-Aloha, we propose an efficient novel view synthesis algorithm for upper-body. Firstly, we design a cascaded disparity estimator for obtaining a robust geometry cue. Additionally a neural rasterizer via Gaussian Splatting is introduced to project latent features onto target view and to decode them into a reduced resolution. Further, given the high-quality captured data, we leverage weighted blending mechanism to refine the decoded image into the final resolution of 2K. Exploiting world-leading autostereoscopic display and low-latency iris tracking, users are able to experience a strong three-dimensional sense even without any wearable head-mounted display device. Altogether, our telepresence system demonstrates the sense of co-presence in real-life experiments, inspiring the next generation of communication.	https://dl.acm.org/doi/abs/10.1145/3641519.3657491	Hanzhang Tu, Ruizhi Shao, Xue Dong, Shunyuan Zheng, Hao Zhang, Lili Chen, Meili Wang, Wenyu Li, Siyan Ma, Shengping Zhang, Boyao Zhou, Yebin Liu
Temporal acoustic point holography	Holographic acoustic levitation using phased arrays of transducers has facilitated innovative mid-air particle displays. However, current time-invariant methods for computing acoustic holograms often induce high phase changes and thus transducer amplitude fluctuations, severely limiting possible dynamic displays. In this work, we develop a wide range of temporal phase retrieval algorithms that suppress phase change and trade off computational time and solution optimality. We base our hardware implementation on Gerchberg-Saxton (GS), which we identify as gradient descent for maximizing focal amplitudes and thus acoustic trap quality. Following this, we adapt GS to additionally constrain transducers' (and points') phase changes from the previous time frame, and enable multiparticle animations without prior knowledge of particles' paths and interactions. We experimentally showcase a series of levitated character animations depicting human motion and believe our work paves the way for natural dynamic multi-particle displays and unencumbered 3D delivery of other modalities such as haptics and audio.	https://dl.acm.org/doi/abs/10.1145/3641519.3657443	Giorgos Christopoulos, Lei Gao, Diego Martinez Plasencia, Marta Betcke, Ryuji Hirayama, Sriram Subramanian
Temporally Stable Metropolis Light Transport Denoising using Recurrent Transformer Blocks	Metropolis Light Transport (MLT) is a global illumination algorithm that is well-known for rendering challenging scenes with intricate light paths. However, MLT methods tend to produce unpredictable correlation artifacts in images, which can introduce visual inconsistencies for animation rendering. This drawback also makes it challenging to denoise MLT renderings while maintaining temporal stability. We tackle this issue with modern learning-based methods and build a sequence denoiser combining the recurrent connections with the cutting-edge vision transformer architecture. We demonstrate that our sophisticated denoiser can consistently improve the quality and temporal stability of MLT renderings with difficult light paths. Our method is efficient and scalable for complex scene renderings that require high sample counts.	https://dl.acm.org/doi/abs/10.1145/3658218	Chuhao Chen, Yuze He, Tzu-Mao Li
TensoSDF: Roughness-aware Tensorial Representation for Robust Geometry and Material Reconstruction	Reconstructing objects with realistic materials from multi-view images is problematic, since it is highly ill-posed. Although the neural reconstruction approaches have exhibited impressive reconstruction ability, they are designed for objects with specific materials (e.g., diffuse or specular materials). To this end, we propose a novel framework for robust geometry and material reconstruction, where the geometry is expressed with the implicit signed distance field (SDF) encoded by a tensorial representation, namely At the core of our method is the roughness-aware incorporation of the radiance and reflectance fields, which enables a robust reconstruction of objects with arbitrary reflective materials. Furthermore, the tensorial representation enhances geometry details in the reconstructed surface and reduces the training time. Finally, we estimate the materials using an explicit mesh for efficient intersection computation and an implicit SDF for accurate representation. Consequently, our method can achieve more robust geometry reconstruction, outperform the previous works in terms of relighting quality, and reduce 50% training times and 70% inference time. Codes and datasets are available at https://github.com/Riga2/TensoSDF.	https://dl.acm.org/doi/abs/10.1145/3658211	Jia Li, Lu Wang, Lei Zhang, Beibei Wang
Terrain Amplification using Multi Scale Erosion	Modeling high-resolution terrains is a perennial challenge in the creation of virtual worlds. In this paper, we focus on the amplification of a low-resolution input terrain into a high-resolution, hydrologically consistent terrain featuring complex patterns by a multi-scale approach. Our framework combines the best of both worlds, relying on physics-inspired erosion models producing consistent erosion landmarks and introducing control at different scales, thus bridging the gap between physics-based erosion simulations and multi-scale procedural modeling. The method uses a fast and accurate approximation of different simulations, including thermal, stream power erosion and deposition performed at different scales to obtain a range of effects. Our approach provides landscape designers with tools for amplifying mountain ranges and valleys with consistent details.	https://dl.acm.org/doi/abs/10.1145/3658200	Hugo Schott, Eric Galin, Eric Guérin, Axel Paris, Adrien Peytavie
TexPainter: Generative Mesh Texturing with Multi-view Consistency	The recent success of pre-trained diffusion models unlocks the possibility of the automatic generation of textures for arbitrary 3D meshes in the wild. However, these models are trained in the screen space, while converting them to a multi-view consistent texture image poses a major obstacle to the output quality. In this paper, we propose a novel method to enforce multi-view consistency. Our method is based on the observation that latent space in a pre-trained diffusion model is noised separately for each camera view, making it difficult to achieve multi-view consistency by directly manipulating the latent codes. Based on the celebrated Denoising Diffusion Implicit Models (DDIM) scheme, we propose to use an optimization-based color-fusion to enforce consistency and indirectly modify the latent codes by gradient back-propagation. Our method further relaxes the sequential dependency assumption among the camera views. By evaluating on a series of general 3D models, we find our simple approach improves consistency and overall quality of the generated textures as compared to competing state-of-the-arts. Our implementation is available at: https://github.com/Quantuman134/TexPainter	https://dl.acm.org/doi/abs/10.1145/3641519.3657494	Hongkun Zhang, Zherong Pan, Congyi Zhang, Lifeng Zhu, Xifeng Gao
TexSliders: Diffusion-Based Texture Editing in CLIP Space	"Generative models have enabled intuitive image creation and manipulation using natural language. In particular, diffusion models have recently shown remarkable results for natural image editing. In this work, we propose to apply diffusion techniques to edit textures, a specific class of images that are an essential part of 3D content creation pipelines. We analyze existing editing methods and show that they are not directly applicable to textures, since their common underlying approach, manipulating attention maps, is unsuitable for the texture domain. To address this, we propose a novel approach that instead manipulates CLIP image embeddings to condition the diffusion generation. We define editing directions using simple text prompts (e.g., ""aged wood"" to ""new wood"") and map these to CLIP image embedding space using a texture prior, with a sampling-based approach that gives us identity-preserving directions in CLIP space. To further improve identity preservation, we project these directions to a CLIP subspace that minimizes identity variations resulting from entangled texture attributes. Our editing pipeline facilitates the creation of arbitrary sliders using natural language prompts only, with no ground-truth annotated data necessary."	https://dl.acm.org/doi/abs/10.1145/3641519.3657444	Julia Guerrero-Viu, Milos Hasan, Arthur Roullier, Midhun Harikumar, Yiwei Hu, Paul Guerrero, Diego Gutiérrez, Belen Masia, Valentin Deschaintre
Text-Guided Synthesis of Crowd Animation	Creating vivid crowd animations is core to immersive virtual environments in digital games. This work focuses on tackling the challenges of the crowd behavior generation problem. Existing approaches are labor-intensive, relying on practitioners to manually craft the complex behavior systems. We propose a machine learning approach to synthesize diversified dynamic crowd animation scenarios for a given environment based on a text description input. We first train two conditional diffusion models that generate text-guided agent distribution fields and velocity fields. Assisted by local navigation algorithms, the fields are then used to control multiple groups of agents. We further employ Large-Language Model (LLM) to canonicalize the general script into a structured sentence for more stable training and better scalability. To train our diffusion models, we devise a constructive method to generate random environments and crowd animations. We show that our trained diffusion models can generate crowd animations for both unseen environments and novel scenario descriptions. Our method paves the way towards automatic generating of crowd behaviors for virtual environments. Code and data for this paper are available at: https://github.com/MLZG/Text-Crowd.git.	https://dl.acm.org/doi/abs/10.1145/3641519.3657516	Xuebo Ji, Zherong Pan, Xifeng Gao, Jia Pan
Text-to-Vector Generation with Neural Path Representation	Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties. However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task. Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible. However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints. To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities. By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs. Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs. In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process. In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure. We demonstrate the effectiveness of our method through extensive experiments and showcase various applications. The project page is https://intchous.github.io/T2V-NPR.	https://dl.acm.org/doi/abs/10.1145/3658204	Peiying Zhang, Nanxuan Zhao, Jing Liao
"The ""How-To"" as a Post-AI Assessment Tool"	"This paper introduces a simple but innovative assignment technique, the ""How-To"" as a way to simplify the relationship with AI in assessment events. As an assignment, the ""How-To"" guide merges reflective practice with technical skill development in technical or creative fields, inviting students to author a guide based on insights from prior coursework, personal projects or specific directed challenges. This method emphasizes reciprocal learning by engaging students in a reflective process where they must articulate and analyze their technical learning, thus reinforcing their understanding and evidencing their mastery of the given learning outcome. This technique could be more broadly applicable, building on the large success of online how-to tutorials; but in flipping the emphasis from passive consumption to productive engagement, the discussed assignment encourages a blend of narrative, technical detail, and creative expression, culminating in a resource that benefits both the author, their peers and potentially course development itself. This presentation will explore how the assignment's framework develops as an extension of the principle of 'ako', a Māori concept emphasizing reciprocal learning. The discussion will focus on its application within the specific educational context of Aotearoa New Zealand and the broader value it offers to creative and technical education."	https://dl.acm.org/doi/abs/10.1145/3641235.3664442	Roman Mitch
The Art of Weightlessness	The Art of Weightlessness is an animated short film documentary that chronicles the evolution of Artist and Performer Bill Shannon. Born with a degenerative hip condition, Bill developed new ways to express himself through dance and skateboarding on crutches.	https://dl.acm.org/doi/abs/10.1145/3641230.3647466	Moshe Mahler
The Chosen One: Consistent Characters in Text-to-Image Diffusion Models	Recent advances in text-to-image generation models have unlocked vast potential for visual creativity. However, the users that use these models struggle with the generation of consistent characters, a crucial aspect for numerous real-world applications such as story visualization, game development, asset design, advertising, and more. Current methods typically rely on multiple pre-existing images of the target character or involve labor-intensive manual processes. In this work, we propose a fully automated solution for consistent character generation, with the sole input being a text prompt. We introduce an iterative procedure that, at each stage, identifies a coherent set of images sharing a similar identity and extracts a more consistent identity from this set. Our quantitative analysis demonstrates that our method strikes a better balance between prompt alignment and identity consistency compared to the baseline methods, and these findings are reinforced by a user study. To conclude, we showcase several practical applications of our approach.	https://dl.acm.org/doi/abs/10.1145/3641519.3657430	Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, Dani Lischinski
The Future is Here: Verbal Interaction with NPCs on Mobile.	Applying large Language Models (LM) to unlock natural language based interactions with game NPCs is an emerging frontier. But the LMs of today are prohibitively expensive to run locally, especially on mobile platforms due to limited runtime resources, thermal issues and strong FPS requirements. In this work, we identified a series of mobile-friendly small LMs called TinyStories[Eldan and Li 2023] that can generate simple but coherent English. We repurposed this model to generate NPC dialogues. We then combined it with Unity ML-Agents, creating an NPC that can perform complex actions and interact verbally with the player. Our implementation runs on current-generation mobile platforms, at the level of performance required for real-time gaming. With this work, we offer a glimpse into the future of gaming: interacting with complex NPCs using natural speech. We also demonstrate it is possible to run LMs alongside Unity ML-Agents on mobile with great results. This will significantly enhance the accessibility and interactivity in games. Lastly, we want to attract academia and industry attention to the area of mobile / edge friendly LMs, as we believe this will be a fertile ground for exploration.	https://dl.acm.org/doi/abs/10.1145/3664294.3664365	Roberto Lopez Mendez, Sicong Li, Hong Ji Liu
The Future of Interaction with Mobile Game Characters	This poster showcases the future of player-NPC interaction in gaming: more natural, based on speech. The poster explains the implementation of verbal interaction with an NPC in a mobile game. Large Language Models (LLMs) open new ways of interacting in gaming, but the large size and memory footprint make deploying them on mobile a challenge. We have identified a 33M-parameter small Language Model (LM), TinyStories [Eldan and Li 2023], repurposed it to be conversational, and combined it with the standard Unity Multi-Layer Perceptron (MLP) ML-Agents [Juliani et al. 2020] model. The LM drives the interaction with the user, while the MLP performs the actions. A Sentence Similarity model [Reimers and Gurevych 2019] is then responsible for filtering user input to decide whether the NPC needs to communicate with the player or to perform an action. Running all these models locally on mobile inside a game engine like Unity can be very challenging in terms of optimization, but this poster demonstrates how it is possible.	https://dl.acm.org/doi/abs/10.1145/3641234.3671019	Roberto Lopez Mendez, Sicong Li, Hong Ji Liu
The Human-like Non-human Series: Data_Revoc(The precursor to the Electrotactile Asura Series)	"As a form of electrotactile artificial organ, what kind of immersive experience can MR technology provide? Revoc is ""cover"" spelled backward, symbolizing the liberation of electro-tactility. Data.Revoc liberates the audience's perceptual abilities through microcurrents, visualizing the relationship between electrotactile sensations and sound using MR technology."	https://dl.acm.org/doi/abs/10.1145/3641521.3665162	Lai Jiun Ting
The Last of Us	As a main VFX partner for HBO's hit series 'The Last of Us', DNEG was tasked with recreating the dystopian world from Naughty Dog's acclaimed video game. The DNEG team delivered award-winning VFX, including designing overgrown environments, constructing skylines, building mountains, and destroying buildings, to showcase the postapocalyptic world.	https://dl.acm.org/doi/abs/10.1145/3641230.3653288	Stephen James
The Life and Legacy of Bui Tuong Phong	We examine the life and legacy of pioneering Vietnamese American computer scientist Bùi Tường Phong, whose shading and lighting models turned 50 last year. We trace the trajectory of his life through Vietnam, France, and the United States, and its intersections with global conflicts. Crucially, we present evidence that his name has been cited incorrectly over the last five decades. His family name is Bùi Tường, not Phong. By presenting these facts at SIGGRAPH, we hope to collect more information about his life, and ensure that his name is remembered correctly in the future.	https://dl.acm.org/doi/abs/10.1145/3641233.3664315	Theodore Kim, Yoehan Oh, Jacinda Tran
The Malleable-Self Experience: Transforming Body Image by Integrating Visual and Whole-Body Haptic Stimuli	The Malleable-Self Experience comprises the integration of the visual element of virtual reality (VR) with the whole-body haptic sensations of the Synesthesia X1 haptic chair. The goal is to induce a provocative experience that expands one's understanding of the self by creating a malleable perception of the body image. We explore the effects of visual and whole-body haptic integration on augmenting body image during dynamic transformations of visual representations of the body in VR. We design the plausibility of these perceptual augmentations using a specific sequence of multisensory events: (1) establishing body ownership of a virtual body anchored in the same self-located space as the participant, (2) separating the virtual body to hover above the participant's physical body, enhanced by accompanying haptic stimuli to increase proprioceptive uncertainty, and (3) transforming the virtual body with integrated visuo-haptic stimuli to sustain perceptual congruency.	https://dl.acm.org/doi/abs/10.1145/3641517.3664385	Tanner Person, Nobuhisa Hanamitsu, Danny Hynds, Sohei Wakisaka, Kota Isobe, Leonard Mochizuki, Tetsuya Mizuguchi, Kouta Minamizawa
The Real Breadboard: Creating an Electronic Sandwich	"""The Real Breadboard: Creating an Electronic Sandwich"" is a hands-on class that allows participants to experiment with electronic circuit techniques while exploring concepts surrounding electronic technology's pervasive invasion into all aspects of public and private life. Participants use electronic circuit prototyping techniques to create a satirical object that uses bread as a prototyping breadboard, exposing the absurdity accompanying certain technological ""advancements"" and our media-rich diets that creep into everything, including your sandwich. Participants will take away their creations and increase their appetite for conceptual/technical experimentation. First developed in the isolation of an art studio, the expansion of this project into a participatory workshop conduces collaboration and broadens the possible interpretations of the inherent concepts while inviting the integration of new ideas from a larger and more diverse audience. This hands-on class introduces participants to the project's concept and prototyping using breadboards. The class provides electronic components such as microcontrollers and LEDs, from which participants build and code circuits to control an LED matrix. This circuit is built on actual bread to create the electronic sandwich with the components inside the sandwich as ingredients and the LEDs visible outside the sandwich through the bread. Participants, provided with sample code and libraries, will write the code to make creative animations on the LEDs, which will appear on top of the sandwich. The result might be a pattern, a graphic drawing, or text. Participants combine their LED designs with laser-engraved images of their creation toasted into the bread, which also appears on the top slice of bread of the sandwich. The creative combination of these processes allows participants to explore exciting concepts. Inserting electronic consumerism into something meant for human internal consumption provides a thoughtful platform for participants to be creative and have fun. In a world where a sandwich with grill marks that loosely resemble a religious icon sells online for a ludicrous amount, maybe it's not so far-fetched to have electronic components in our food. Maybe your electronic sandwich's LED matrix informs you of its ingredients or nutritional content. Perhaps your electronic sandwich displays an attractive design, so you choose it over those boring sandwiches to its left and right that only show their price and expiration date. Possibly, there's also a dark side. Is your electronic sandwich a data harvesting device relaying the contents of your bowels to advertisers and insurance agents? Or is it spewing misinformation and claiming a hotdog isn't a sandwich? As society continues to inundate every human experience and interaction with digital technology and media, we must reflect on these choices. Although this project is superfluous on its surface, the deeper goal is contemplation."	https://dl.acm.org/doi/abs/10.1145/3641236.3664421	Jeremiah Teipen
The Real-Time Frontier: Stylized Feature Quality Storytelling in Unreal Engine	"This paper presents Steamroller Animation's innovative method for merging 2D artistic styling within a 3D real-time environment for ""Spice Frontier"", leveraging Unreal Engine for streamlined long-form production. Focusing on a novel shot setup process that addresses the challenges of transitioning to a real-time workflow. We introduce solutions like custom character setups, rim shaders, rig versioning, gobos and FX sprite sheets. This approach not only aligns artistic vision with advanced technology but also advances animation workflow standards, emphasizing efficiency and creative integrity."	https://dl.acm.org/doi/abs/10.1145/3641233.3664305	David Alve, Josh Carroll
The Sun is Bad: English	Set in Hong Kong during the late 80s, a temperamental girl tries to destroy the sun and stop it from melting her city using toys and whatever she has.	https://dl.acm.org/doi/abs/10.1145/3641230.3653063	Rachel Mow, Ivan Chui, Laura Correal, Quinn Marsh, Liren Sun, Chenrui Lan, Elle Yeung, Hazel Wong, Henry Ni, Rielle Ong, Samson Flanagan
The Swineherd	"Hans Christian Andersen's ""The Swineherd"" as you have never seen it before! In this entirely goofy and funny version of H.C. Andersen's famous fairy tale ""The Swineherd"", we meet the young swineherd who sits and looks after the king's pigs. Even though it is Christmas, the boy and the pigs are bored in the pigsty. But whoa! Suddenly the boy remembers his magic pot. It can always start the antics. Soon he finds it, and then there is a party and trouble in the pigsty. When the Princess, who is just as bored, discovers the magic pot and the party it's throwing, she HAS to get hold of it. But the Swineherd doesn't just want to get rid of it, because maybe you could get a little kiss in return... Will the Swineherd get his kiss, will the Princess stop being bored, and will it ever be Christmas in the pigsty?"	https://dl.acm.org/doi/abs/10.1145/3641230.3650727	Magnus Igland Møller, Peter Smith
The Tent: Towards a Future of Spatial Entertainment: Movies you can walk around inside of, with or without a headset.	"The dominant form of popular entertainment for the last 125 years, the ""rectangle"" of film and television, is waning and the emerging medium of Spatial Entertainment is in its infancy. A misplaced focus on the form factor of Virtual Reality headsets has obfuscated this fundamental technological advancement, while creating a bottleneck that has prevented the general population from experiencing this important new medium. The Tent is an Augmented Reality Narrative, 22 minutes long, built for iPadPro. It allows the audience to explore a 3D world on their tabletop, experiencing the story from any perspective and getting as close to the actor as they wish. Photogrammetry creates photo-real environments and props and a live actor captured with Volumetric Video can convey a more nuanced humanity than a mo-cap or animated actor. (Figure 1) Together they are a step towards ""movies you can walk around inside of."""	https://dl.acm.org/doi/abs/10.1145/3641521.3664411	Rory Coleman Mitchell, Sébastien Paul Hameline, Logan Hamilton Davis, Miller Jacob Klitsner
The War Within Announce Cinematic | World of Warcraft	Our heroes Anduin and Thrall find themselves drawn together by a radiant vision emanating from within the heart of the world. Find out what comes next in the opening chapter of The Worldsoul Saga story arc with the newest expansion World of Warcraft: The War Within: coming in 2024.	https://dl.acm.org/doi/abs/10.1145/3641230.3652004	Anna Morgan
ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars	Real-world applications often require a large gallery of 3D assets that share a consistent theme. While remarkable advances have been made in general 3D content creation from text or image, synthesizing customized 3D assets following the shared theme of input 3D exemplars remains an open and challenging problem. In this work, we present ThemeStation, a novel approach for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D assets based on given few exemplars with two goals: 1) unity for generating 3D assets that thematically align with the given exemplars and 2) diversity for generating 3D assets with a high degree of variations. To this end, we design a two-stage framework that draws a concept image first, followed by a reference-informed 3D modeling stage. We propose a novel dual score distillation (DSD) loss to jointly leverage priors from both the input exemplars and the synthesized concept image. Extensive experiments and a user study confirm that ThemeStation surpasses prior works in producing diverse theme-aware 3D models with impressive quality. ThemeStation also enables various applications such as controllable 3D-to-3D generation.	https://dl.acm.org/doi/abs/10.1145/3641519.3657471	Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, Rynson W.H. Lau
Theory of Human Tetrachromatic Color Experience and Printing	Genetic studies indicate that more than 50% of women are genetically tetrachromatic, expressing four distinct types of color photoreceptors (cone cells) in the retina. At least one functional tetrachromat has been identified in laboratory tests. We hypothesize that there is a large latent group in the population capable of fundamentally richer color experience, but we are not yet aware of this group because of a lack of tetrachromatic colors in the visual environment. This paper develops theory and engineering practice for fabricating tetrachromatic colors and potentially identifying tetrachromatic color vision in the wild. First, we apply general -dimensional color theory to derive and compute all the key color structures of human tetrachromacy for the first time, including its 4D space of possible object colors, 3D space of chromaticities, and yielding a predicted 2D sphere of tetrachromatic hues. We compare this predicted hue sphere to the familiar hue circle of trichromatic color, extending the theory to predict how the higher dimensional topology produces an expanded color experience for tetrachromats. Second, we derive the four reflectance functions for the ideal tetrachromatic inkset, analogous to the well-known CMY printing basis for trichromacy. Third, we develop a method for prototyping tetrachromatic printers using a library of fountain pen inks and a multi-pass inkjet printing platform. Fourth, we generalize existing color tests - sensitive hue ordering tests and rapid isochromatic plate screening tests - to higher-dimensional vision, and prototype variants of these tests for identifying and characterizing tetrachromacy in the wild.	https://dl.acm.org/doi/abs/10.1145/3658232	Jessica Lee, Nicholas Jennings, Varun Srivastava, Ren Ng
There Can Be (at least) Two Introductory Graphics Courses: Teaching Introduction to Non-Interactive Computer Graphics	For decades many colleges and universities offering an introductory to computer graphics course have been teaching computer graphics through a single course focusing on real-time computer graphics. Many of these real-time or interactive computer graphics courses have a focus on graphics in the gaming domain, and also require students to be proficient in at least one instructor-selected language (e.g. C++, Java, JavaScript) through the schools introductory sequence. In this work, we describe motivations to a 'second' alternative and complementary introductory graphics course focused on non-interactive computer graphics (i.e. rendering images for movies) that can be taken at nearly any point in the curriculum after the first programming course. We have observed that this additional course may better foster a more diverse pool of students to garner interest in computer graphics. This non-interactive graphics course is taught with a focus on rendering (using path tracing), uses free resources, and is programming language-agnostic.	https://dl.acm.org/doi/abs/10.1145/3641235.3664429	Michael Shah
Tonii: Customizable Amulets that Activate Augmented Reality Suits	This work introduces Tonii, an integration of augmented reality (AR) technology, artistic expression, community engagement and social empowerment. Tonii empowers participants to craft unique amulets adorned with symbols that encapsulate fundamental values for cultivating safe and inclusive digital realms. Utilizing advanced machine learning techniques, these symbols serve as keys to unlock AR suits, offering users a tangible connection between virtual experiences and real-world identities. This endeavor not only narrows the divide between digital and tangible realms but also underscores the critical roles of technological inclusivity and societal empowerment.	https://dl.acm.org/doi/abs/10.1145/3641236.3670866	Yingxi Adelle Lin, Matthew Pinner
Toonify3D: StyleGAN-based 3D Stylized Face Generator	Recent advances in generative models enable high-quality facial image stylization. Toonify is a popular StyleGAN-based framework that has been widely used for facial image stylization. Our goal is to create expressive 3D faces by turning Toonify into a 3D stylized face generator. Toonify is fine-tuned with a few gradient descent steps from StyleGAN trained for standard faces, and its features would carry semantic and visual information aligned with the features of the original StyleGAN model. Based on this observation, we design a versatile 3D-lifting method for StyleGAN, StyleNormal, that regresses a surface normal map of a StyleGAN-generated face using StyleGAN features. Due to the feature alignment between Toonify and StyleGAN, although StyleNormal is trained for regular faces, it can be applied for various stylized faces without additional fine-tuning. To learn local geometry of faces under various illuminations, we introduce a novel regularization term, the normal consistency loss, based on lighting manipulation in the GAN latent space. Finally, we present Toonify3D, a fully automated framework based on StyleNormal, that can generate full-head 3D stylized avatars and support GAN-based 3D facial expression editing.	https://dl.acm.org/doi/abs/10.1145/3641519.3657480	Wonjong Jang, Yucheol Jung, Hyomin Kim, Gwangjin Ju, Chaewon Son, Jooeun Son, Seungyong Lee
Towards Motion Metamers for Foveated Rendering	Foveated rendering takes advantage of the reduced spatial sensitivity in peripheral vision to greatly reduce rendering cost without noticeable spatial quality degradation. Due to its benefits, it has emerged as a key enabler for real-time high-quality virtual and augmented realities. Interestingly though, a large body of work advocates that a key role of peripheral vision may be motion detection, yet foveated rendering lowers the image quality in these regions, which may impact our ability to detect and quantify motion. The problem is critical for immersive simulations where the ability to detect and quantify movement drives actions and decisions. In this work, we diverge from the contemporary approach towards the goal of foveated graphics, and demonstrate that a loss of high-frequency spatial details in the periphery inhibits motion perception, leading to underestimating motion cues such as velocity. Furthermore, inspired by an interesting visual illusion, we design a perceptually motivated real-time technique that synthesizes controlled spatio-temporal motion energy to offset the loss in motion perception. Finally, we perform user experiments demonstrating our method's effectiveness in recovering motion cues without introducing objectionable quality degradation.	https://dl.acm.org/doi/abs/10.1145/3658141	Taimoor Tariq, Piotr Didyk
Towards Unstructured Unlabeled Optical Mocap: A Video Helps!	Optical motion capture (mocap) requires accurately reconstructing the human body from retroreflective markers, including pose and shape. In a typical mocap setting, marker labeling is an important but tedious and error-prone step. Previous work has shown that marker labeling can be automated by using a structured template defining specific marker placements, but this places additional recording constraints. We propose to relax these constraints and solve for Unstructured Unlabeled Optical (UUO) mocap. Compared to the typical mocap setting that either labels markers or places them w.r.t a structured layout, markers in UUO mocap can be placed anywhere on the body and even on one specific limb (e.g., right leg for biomechanics research), hence it is of more practical significance. It is also more challenging. To solve UUO mocap, we exploit a monocular video captured by a single RGB camera, which does not require camera calibration. On this video, we run an off-the-shelf method to reconstruct and track a human individual, giving strong visual priors of human body pose and shape. With both the video and UUO markers, we propose an optimization pipeline towards marker identification, marker labeling, human pose estimation, and human body reconstruction. Our technical novelties include multiple hypothesis testing to optimize global orientation, and marker localization and marker-part matching to better optimize for body surface. We conduct extensive experiments to quantitatively compare our method against state-of-the-art approaches, including marker-only mocap and video-only human body/shape reconstruction. Experiments demonstrate that our method resoundingly outperforms existing methods on three established benchmark datasets for both full-body and partial-body reconstruction.	https://dl.acm.org/doi/abs/10.1145/3641519.3657522	Nicholas Milef, John Keyser, Shu Kong
Training-Free Consistent Text-to-Image Generation	Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present , a approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, can naturally extend to multi-subject scenarios, and even enable training-free for common objects.	https://dl.acm.org/doi/abs/10.1145/3658157	Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon
Transparent Image Layer Diffusion using Latent Transparency	"We present an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a ""latent transparency"" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock."	https://dl.acm.org/doi/abs/10.1145/3658150	Lvmin Zhang, Maneesh Agrawala
Traversing the Mist	Inside a surreal gay sauna, you find yourself in the body of a young Taiwanese man. You venture into this dreamscape, woven by desires, excitements and fears, crossing eyes with the other identical sauna-goers. As you walk along every floor, corridor and room, each layer of the maze unravels.	https://dl.acm.org/doi/abs/10.1145/3641231.3672087	Tung-Yen Chou
True Detective: Night Country - Cinesite VFX and Animation	The unsettling visual effects set the tone for this suspenseful HBO series, which opens with an almost entirely CG sequence showing the mysterious death of a herd of caribou.	https://dl.acm.org/doi/abs/10.1145/3641230.3653287	Simon Stanley-Clamp, Simon Wottge, Manuel Reyes, Sabine Janetzka
USD in Production	I'm here today to talk about how we utilise Universal Scene Description at Animal Logic, with a particular focus on our Asset structure. This overview will primarily encompass the traditional CG departments of Modelling, Surfacing & Rigging but can introduce some non-standard elements as we'll come across shortly. The structure of USD assets is a foundational puzzle piece of the pipeline which can too often be overlooked. The construction of these assets can influence the make-up and design of all downstream departments so is vital to get right. Luckily, by working with the core concepts of what USD can provide, it becomes possible to identify inefficiencies in the workflow. In a continuously evolving process, we can take a modular approach to seamlessly improve technological stability and user experience. One final quick note before we get started: All production images shown here come from the ALab, a publicly available playground provided by Animal Logic to help get to grips and explore how we use USD in our pipeline.	https://dl.acm.org/doi/abs/10.1145/3664475.3664568	Reuben Bloom
Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging	While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity. Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy. In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances. We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging—dynamically and without the need for stationary reference anchors. Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor. Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation. To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS. For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB). Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from 13.62 to 10.65 cm (22% better) and lowering jitter from 1.56 to 0.055 km/s3 (a reduction of 97%). UIP code, UIP-DB dataset, and hardware design: https://github.com/eth-siplab/UltraInertialPoser	https://dl.acm.org/doi/abs/10.1145/3641519.3657465	Rayan Armani, Changlin Qian, Jiaxi Jiang, Christian Holz
Universal Facial Encoding of Codec Avatars from VR Headsets	Faithful real-time facial animation is essential for avatar-mediated telepresence in Virtual Reality (VR). To emulate authentic communication, avatar animation needs to be efficient and accurate: able to capture both extreme and subtle expressions within a few milliseconds to sustain the rhythm of natural conversations. The oblique and incomplete views of the face, variability in the donning of headsets, and illumination variation due to the environment are some of the unique challenges in generalization to unseen faces. In this paper, we present a method that can animate a photorealistic avatar in realtime from head-mounted cameras (HMCs) on a consumer VR headset. We present a self-supervised learning approach, based on a cross-view reconstruction objective, that enables generalization to unseen users. We present a lightweight expression calibration mechanism that increases accuracy with minimal additional cost to run-time efficiency. We present an improved parameterization for precise ground-truth generation that provides robustness to environmental variation. The resulting system produces accurate facial animation for unseen users wearing VR headsets in realtime. We compare our approach to prior face-encoding methods demonstrating significant improvements in both quantitative metrics and qualitative results.	https://dl.acm.org/doi/abs/10.1145/3658234	Shaojie Bai, Te-Li Wang, Chenghui Li, Akshay Venkatesh, Tomas Simon, Chen Cao, Gabriel Schwartz, Jason Saragih, Yaser Sheikh, Shih-En Wei
Unveiling the Invisible: Interactive Spatial Sensing Transforms Air Flow Measurement	This engaging experience transforms participants into aerodynamicists, enabling them to explore air flow fields firsthand. Utilizing groundbreaking augmented reality visualizations alongside an active learning measurement system, individuals wield a sensor with inside-out tracking, allowing them to navigate and directly uncover the dynamics of air flow. The system generates real-time visualizations of measured air currents, making the usually complex task of evaluating flow fields more accessible. Assisted by a companion artificial intelligence, users are directed to subsequent measurement points, effectively integrating them into the algorithm's process. This innovative approach not only simplifies the examination of air flow fields but also enriches the investigative experience by making participants active contributors to the exploration.	https://dl.acm.org/doi/abs/10.1145/3641521.3664408	Julian Humml, Thomas Rösgen, Morteza Gharib
VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality	As 3D content becomes increasingly prevalent, there's a growing focus on the development of engagements with 3D virtual content. Unfortunately, traditional techniques for creating, editing, and interacting with this content are fraught with difficulties. They tend to be not only engineering-intensive but also require extensive expertise, which adds to the frustration and inefficiency in virtual object manipulation. Our proposed VR-GS system represents a leap forward in human-centered 3D content interaction, offering a seamless and intuitive user experience. By developing a physical dynamics-aware interactive Gaussian Splatting (GS) in a Virtual Reality (VR) setting, and constructing a highly efficient two-level embedding strategy alongside deformable body simulations, VR-GS ensures real-time execution with highly realistic dynamic responses. The components of our system are designed for high efficiency and effectiveness, starting from detailed scene reconstruction and object segmentation, advancing through multi-view image in-painting, and extending to interactive physics-based editing. The system also incorporates real-time deformation embedding and dynamic shadow casting, ensuring a comprehensive and engaging virtual experience.	https://dl.acm.org/doi/abs/10.1145/3641519.3657448	Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, Chenfanfu Jiang
VRMM: A Volumetric Relightable Morphable Head Model	In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables high-quality 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.	https://dl.acm.org/doi/abs/10.1145/3641519.3657406	Haotian Yang, Mingwu Zheng, Chongyang Ma, Yu-Kun Lai, Pengfei Wan, Haibin Huang
Variational Feature Extraction in Scientific Visualization	Across many scientific disciplines, the pursuit of even higher grid resolutions leads to a severe scalability problem in scientific computing. Feature extraction is a commonly chosen approach to reduce the amount of information from dense fields down to geometric primitives that further enable a quantitative analysis. Examples of common features are isolines, extremal lines, or vortex corelines. Due to the rising complexity of the observed phenomena, or in the event of discretization issues with the data, a straightforward application of textbook feature definitions is unfortunately insufficient. Thus, feature extraction from spatial data often requires substantial pre- or post-processing to either clean up the results or to include additional domain knowledge about the feature in question. Such a separate pre- or post-processing of features not only leads to suboptimal and incomparable solutions, it also results in many specialized feature extraction algorithms arising in the different application domains. In this paper, we establish a mathematical language that not only encompasses commonly used feature definitions, it also provides a set of regularizers that can be applied across the bounds of individual application domains. By using the language of variational calculus, we treat features as variational minimizers, which can be combined and regularized as needed. Our formulation not only encompasses existing feature definitions as special case, it also opens the path to novel feature definitions. This work lays the foundations for many new research directions regarding formal definitions, data representations, and numerical extraction algorithms.	https://dl.acm.org/doi/abs/10.1145/3658219	Nico Daßler, Tobias Günther
Velocity-Based Monte Carlo Fluids	We present a velocity-based Monte Carlo fluid solver that overcomes the limitations of its existing vorticity-based counterpart. Because the velocity-based formulation is more commonly used in graphics, our Monte Carlo solver can be readily extended with various techniques from the fluid simulation literature. We derive our method by solving the Navier-Stokes equations via operator splitting and designing a pointwise Monte Carlo estimator for each substep. We reformulate the projection and diffusion steps as integration problems based on the recently introduced walk-on-boundary technique [Sugimoto et al. 2023]. We transform the volume integral arising from the source term of the pressure Poisson equation into a form more amenable to practical numerical evaluation. Our resulting velocity-based formulation allows for the proper simulation of scenes that the prior vorticity-based Monte Carlo method [Rioux-Lavoie et al. 2022] either simulates incorrectly or cannot support. We demonstrate that our method can easily incorporate advancements drawn from conventional non-Monte Carlo methods by showing how one can straightforwardly add buoyancy effects, divergence control capabilities, and numerical dissipation reduction methods, such as advection-reflection and PIC/FLIP methods.	https://dl.acm.org/doi/abs/10.1145/3641519.3657405	Ryusuke Sugimoto, Christopher Batty, Toshiya Hachisuka
Versatile Vision Foundation Model for Image and Video Colorization	Image and video colorization are among the most common problems in image restoration. This is an ill-posed problem and a wide variety of methods have been proposed, ranging from more traditional computer vision strategies to most recent development with transformer-based or generative neural network models. In this work we show how a latent diffusion model, pre-trained on text-to-image synthesis, can be finetuned for image colorization and provide a flexible solution for a wide variety of scenarios: high quality direct colorization with diverse results, user guided colorization through colors hints, text prompts or reference image and finally video colorization. Some works already investigated using diffusion models for colorization, however the proposed solutions are often more complex and require training a side model guiding the denoising process (à la ControlNet). Not only is this approach increasing the number of parameters and compute time, it also results in sub optimal colorization as we show. Our evaluation demonstrates that our model is the only approach that offers a wide flexibility while either matching or outperforming existing methods specialized in each sub-task, by proposing a group of universal, architecture-agnostic mechanisms which could be applied to any pre-trained diffusion model.	https://dl.acm.org/doi/abs/10.1145/3641519.3657509	Vukasin Bozic, Abdelaziz Djelouah, Yang Zhang, Radu Timofte, Markus Gross, Christopher Schroers
Vertex Block Descent	We introduce vertex block descent, a block coordinate descent solution for the variational form of implicit Euler through vertex-level Gauss-Seidel iterations. It operates with local vertex position updates that achieve reductions in global variational energy with maximized parallelism. This forms a physics solver that can achieve numerical convergence with unconditional stability and exceptional computation performance. It can also fit in a given computation budget by simply limiting the iteration count while maintaining its stability and superior convergence rate. We present and evaluate our method in the context of elastic body dynamics, providing details of all essential components and showing that it outperforms alternative techniques. In addition, we discuss and show examples of how our method can be used for other simulation systems, including particle-based simulations and rigid bodies.	https://dl.acm.org/doi/abs/10.1145/3658179	Anka He Chen, Ziheng Liu, Yin Yang, Cem Yuksel
Vidalia	In a massacred village, a part of the inhabitants is kidnapped by strange creatures. Only survivors, Cal and Vidalia decide to search for them.	https://dl.acm.org/doi/abs/10.1145/3641230.3652559	Camille Borg, Constance Cartaut, Diane Hamel, Tabatha Hugues, Morgane Jolivel, Emile Lachkar, Bastien L'Hourre, Enzo Regardin, Céline Soubrane
Virtual Reality Application That Leverages the Proteus Effect to Make Blood Donation a Positive Experience	In Japan, the reluctance of young people to donate blood has become a social issue. In this study, for the purpose of deterring the trend among young people not to donate blood, we propose a virtual reality (VR) application that changes blood donation into a positive experience by gradually changing the appearance of an avatar from a normal state to a state full of energy according to the elapsed time of blood donation. As a result of a comparison experiment between the proposed method to change blood donation into a positive experience and a method to distract attention by stimulation, it was clarified that the pain of blood donation was significantly alleviated through the proposed method.	https://dl.acm.org/doi/abs/10.1145/3641234.3671024	Hitoshi Usui, Kei Kobayashi
Virtual Reality for inward contemplation	"When performed at a high level, meditation and contemplative practices can give rise to altered states of consciousness, the study of which is key for cognitive neurosciences. Those effects indeed can help to better understand the underlying mechanisms of the self and its relation with the body. But reaching those states is rare and thus hard to observe. They require intense practice and dedication, even among regular meditators. To make those specific states more accessible, we propose a new approach of a ""neuro-engineered meditation"", bringing technologies such as virtual reality (VR) and biofeedback to give practitioners new perspectives over their practice, by targeting their own body and internal sensations. We expect this approach to help participants better understand complex concepts and develop deeper results, in comparison with a regular practice."	https://dl.acm.org/doi/abs/10.1145/3641233.3664341	Loup Vuarnesson, Bruno Herbelin, Olaf Blanke
Visual Data Stories for Climate Action: The Making of NASA’s Earth Information Center Public Exhibits	This production session presents the What, How, and Why of bringing to life the physical installations and data-driven visualizations featured at the Earth Information Center (earth.gov). The session shares with the SIGGRAPH community the challenges of, approaches to, and lessons learned from visualizing data for climate action and sheds light on research agendas and upcoming efforts.	https://dl.acm.org/doi/abs/10.1145/3641232.3649304	Helen-Nicole Kostis, Dan Goods, Joseph Ardizzone, Ryan Boller, Benjamin Bach, Fanny Chevalier
Visualization and the Science/Art of Making the Complex Simple	Visualization, whether in 2D or 3D, is frequently faced with the challenge of navigating complexity, be it in the realm of data, design, or broader concept. This panel aims to delve into the intricacies of effectively communicating ideas through visualization, drawing upon the diverse experiences and expertise of panelists representing esteemed institutions such as NASA, NCSA, and The New York Times. By exploring the challenges encountered and the solutions devised by these industry leaders, the panel aims to offer valuable insights into the art and science of conveying complex ideas through visualization.	https://dl.acm.org/doi/abs/10.1145/3641516.3664592	Kalina Borkiewicz, Lars Riedemann, Karthik Patanjali, Kel Elkins
Visualization of Flow Direction using Polarization Angle Changes of Cellulose Nanofiber Suspension	In this study, we propose a novel method for visualizing flow direction by exploiting the polarization angle changes of cellulose nanofiber suspensions. Utilizing a polarization camera, we calculate the polarization angle, enabling pixel-wise visualization of flow direction. We demonstrate that the boat's shape significantly influences the flow patterns generated in its wake. Furthermore, we verify the correspondence between polarization angle and flow direction, and investigate the effects of illumination multiplexing and de-multiplexing on the proposed method. This study presents a promising technique for high-resolution flow direction visualization, with potential applications in fluid dynamics research and industrial settings.	https://dl.acm.org/doi/abs/10.1145/3641234.3671029	Ryusei Okamoto, Shogo Yamashita, Takuya Kato, Hiroyuki Kubo
Visualizing the Invisible Universe with the James Webb Space Telescope	"This course will be an exploration of the techniques used to create the iconic imagery from NASA's James Webb Space Telescope. If you're interested in working with real data from one of NASA's space-based observatories, this course is for you! We'll provide some background on the telescope and its launch as well as some general philosophy of our image processing methods. We'll then jump into image processing techniques that bring the universe into rich and vibrant detail. We'll cover how to navigate the data archives to find a suitable observation and download data files. Next, we'll take a close look at the process of ""stretching"" which essentially compresses the enormous dynamic range of the data to reveal hidden details. Following a successful stretch of the data, we'll go in depth on the technique known as ""Chromatic Color"" which is how we apply color to several different layers and blend them together to create an initial color composite. Finally, we'll look at some post-processing techniques which are used to clean up image artifacts and produce the final composite image. The techniques covered will demonstrate how this work straddles the line between art and science, with an eye towards maintaining the integrity of the source data while extracting the most information possible. As with so many artistic endeavors, there are different methods to achieve similar results and Alyssa and I will both provide examples of how we work with data to produce our images."	https://dl.acm.org/doi/abs/10.1145/3664475.3670770	Joseph DePasquale, Alyssa Pagan
VolumeMatic and the AI3D.foundation: Text to interactive volume app spatial computing creator app	"VolumeMatic is an Apple Vision Pro natural language text to interactive volume app creator app, utilizing the ""chat to create"" motif. The user can easily create 3D using various AI3D creation methods, such as text to 3D and image to 3D from different models and providers, utilizing an abstractified AI3D multimodal API. We utilize object detected and semantic relations among different HCI elements to enable natural language interactive spatial computing app creation. The app hopes to launch an AI3D Foundation to help accelerate the advancement and impact of AI for 3D and interactive content (AI3D). We also present the AI3D Benchmark Card to quickly summarize the results from different models, with a ground truth mesh."	https://dl.acm.org/doi/abs/10.1145/3664294.3664361	Yosun Chang
Volumetric Display with Dual-Path Holographic Laser Rendering	We have developed a display system that achieves to render palm-sized volumetric contents in real space. The system consisted of two optical paths for laser rendering, in which femtosecond-laser-excited emission points were scanned by a liquid-crystal spatial light modulator and a 3D laser scanner. A single volume was formed by the cooperative dual-path laser rendering based on the optimal design of the optical system and scanning path. Volumetric contents such as volumetric graphics and animations with well-filled voxels were demonstrated in a centimeter-sized volume.	https://dl.acm.org/doi/abs/10.1145/3641517.3664387	Kota Kumagai, Hisashi Oka, Kazuki Horikiri, Tetsuji Suzuki, Yoshio Hayasaki
Voxelizing Google Earth: A Pipeline for New Virtual Worlds	This paper presents Voxel Earth, a novel pipeline for automatically converting Google Maps Photorealistic 3D Tiles into voxels, enabling the creation of interactive virtual worlds in Minecraft, web browsers, and VR environments. Our approach addresses the challenge of representing the Earth's geography in voxel-based platforms, offering a scalable and dynamic solution that preserves detail and accuracy. We discuss the technical challenges, applications, and future directions of this technology, highlighting its potential to revolutionize digital earth representation and enhance our understanding of the planet's diverse landscapes.	https://dl.acm.org/doi/abs/10.1145/3641236.3664423	Ryan Hardesty Lewis
WAR IS OVER! Wētā FX Gets Animated	"WAR IS OVER! brings the classic John Lennon and Yoko Ono song to life in this highly stylized, Oscar-winning animated film. Entrusted with the director's vision of creating a world akin to being inside a ""living, breathing concept painting"", the Wētā FX team went to work crafting the unique aesthetic, utilising Unreal Engine. In this session, Creative Director and VFX Supervisor Keith Miller will be joined by CG Supervisor, Ross McWhannell, Animation Supervisor, Aidan Martin, Digital Cinematographer, David Scott, and VFX Producer, Sophie Cherry to explore the art and technology employed across production."	https://dl.acm.org/doi/abs/10.1145/3641232.3649258	Keith Miller, Sophie Cherry, Aidan Martin, David Scott, Ross McWhannell
WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds	We present a new approach for understanding the periodicity structure and semantics of motion datasets, independently of the morphology and skeletal structure of characters. Unlike existing methods using an overly sparse high-dimensional latent, we propose a phase manifold consisting of multiple closed curves, each corresponding to a latent amplitude. With our proposed vector quantized periodic autoencoder, we learn a shared phase manifold for multiple characters, such as a human and a dog, without any supervision. This is achieved by exploiting the discrete structure and a shallow network as bottlenecks, such that semantically similar motions are clustered into the same curve of the manifold, and the motions within the same component are aligned temporally by the phase variable. In combination with an improved motion matching framework, we demonstrate the manifold's capability of timing and semantics alignment in several applications, including motion retrieval, transfer and stylization. Code and pre-trained models for this paper are available at peizhuoli.github.io/walkthedog.	https://dl.acm.org/doi/abs/10.1145/3641519.3657508	Peizhuo Li, Sebastian Starke, Yuting Ye, Olga Sorkine-Hornung
Walkin’ Robin: Walk on Stars with Robin Boundary Conditions	Numerous scientific and engineering applications require solutions to boundary value problems (BVPs) involving elliptic partial differential equations, such as the Laplace or Poisson equations, on geometrically intricate domains. We develop a Monte Carlo method for solving such BVPs with arbitrary first-order linear boundary conditions---Dirichlet, Neumann, and Robin. Our method directly generalizes the algorithm, which previously tackled only the first two types of boundary conditions, with a few simple modifications. Unlike conventional numerical methods, WoSt does not need finite element meshing or global solves. Similar to Monte Carlo rendering, it instead computes pointwise solution estimates by simulating random walks along star-shaped regions inside the BVP domain, using efficient ray-intersection and distance queries. To ensure WoSt produces estimates in the presence of Robin boundary conditions, we show that it is sufficient to modify how WoSt selects the size of these star-shaped regions. Our generalized WoSt algorithm reduces estimation error by orders of magnitude relative to alternative grid-free methods such as the algorithm. We also develop and strategies to further reduce estimation error. Our algorithm is trivial to parallelize, scales sublinearly with increasing geometric detail, and enables progressive and view-dependent evaluation.	https://dl.acm.org/doi/abs/10.1145/3658153	Bailey Miller, Rohan Sawhney, Keenan Crane, Ioannis Gkioulekas
Warudo: Interactive and Accessible Live Performance Capture	Warudo innovates virtual performances (VTubing) using accessible tracking technologies, procedurally-generated avatar motions, and a visual programming system to customize audience interactions. Its seamless approach enables creators with no technical background to design unique, collaborative live performances. We demonstrate an immersive and real-time VTubing performance driven by the SIGGRAPH audience.	https://dl.acm.org/doi/abs/10.1145/3641520.3665308	Man To Tang, Jesse Thompson
WaterForm: Altering the Liquid to Generate Multisensory Feedback for Enhancing Immersive Environment	With virtual reality and haptic technologies, we can create a compelling, immersive experience. Thus, researchers have been exploring integrating different feedback modules to provide more abundantly hybrid feedback systems. Alternatively, they want to use the same module to generate multiple feedback using different interaction techniques. However, simultaneously providing multiple sensory feedbacks in virtual environments presents challenges beyond system integration. Ensuring that users can experience multiple stimuli without compromising the overall immersive experience, as in a physical world, is still challenging. Therefore, we present WaterForm, a liquid-transformation system that utilizes the liquid to generate multisensory feedback, including water splash, water flow, gravity, wind, resistance, buoyant force, mechanical energy, and mist, to enhance the immersive environment. In our demonstration, we developed a VR excursion in the virtual East landscape to explore self-awareness via immersive storytelling.	https://dl.acm.org/doi/abs/10.1145/3641521.3665161	I-Chin Chen, Hsin-Chia Chang, Chia-Yi Hung, Chi-Yu Lin, Kuan-Ning Chang, Ping-Hsuan Han
We Forge the Chains We Wear in Life	"""We forge the chains we wear in life"" is an aphorism by Charles Dickens. This chain is a metaphor for heavy and unbreakable things that bind our ideas and emotions. That is, words that tend to possess our minds and souls can also become chains. This inspired me to create a device that forms letters with chains."	https://dl.acm.org/doi/abs/10.1145/3641523.3665173	Yuichiro Katsumoto
Wig Refitting in Pixar?s Inside Out 2	In Pixar's feature animation Inside Out 2 (2024), emotion characters are identified with their corresponding human characters by exhibiting similar wigs. To achieve this look, we developed a custom rig that assists the sharing and reuse of hair grooms between characters of different shapes, feature proportions, and mesh connectivities. Our approach starts by adopting curvenets as a light-weighted representation of scalp surfaces that eases the registration from human to emotion models by detaching the groom setup from the underlying mesh discretization. We then implemented a mix of surface-based and volumetric deformations that warp hair shells and guide curves onto the new character's scalp defined by the refit curvenet. At last, we incorporated a shaping tool for editing the wig layout controlled by additional curvenets that profile each hair shell.	https://dl.acm.org/doi/abs/10.1145/3641233.3664319	Ben Porter, Jacob Speirs, Fernando de Goes
Wing It!	An uptight engineer gets an unwelcome visit from a enthusiastic wannabe-pilot, causing both of them to be launched into the air inside an out-of-control space shuttle.	https://dl.acm.org/doi/abs/10.1145/3641230.3644711	Rik Schutte, Fiona Cohen, Francesco Siddi, Vivien Lulkowski
Wondermom	During her pregnancy, Alice learns from her super mom handbook that nothing is more wonderful than having a child. But when the baby is born, she fonds herself overwhelmed by the idealized expectations she had of motherhood.	https://dl.acm.org/doi/abs/10.1145/3641230.3652590	Clémence Provost, Lisa Tardieu, Maud Grainger, Mariam Ulmasova, Manon Benet
Woven Fabric Capture with a Reflection-Transmission Photo Pair	Digitizing woven fabrics would be valuable for many applications, from digital humans to interior design. Previous work introduces a lightweight woven fabric acquisition approach by capturing a single reflection image and estimating the fabric parameters with a differentiable geometric and shading model. The renderings of the estimated fabric parameters can closely match the photo; however, the captured reflection image is insufficient to fully characterize the fabric sample reflectance. For instance, fabrics with different thicknesses might have similar reflection images but lead to significantly different transmission. We propose to recover the woven fabric parameters from two captured images: reflection and transmission. At the core of our method is a differentiable bidirectional scattering distribution function (BSDF) model, handling reflection and transmission, including single and multiple scattering. We propose a two-layer model, where the single scattering uses an SGGX phase function as in previous work, and multiple scattering uses a new azimuthally-invariant microflake definition, which we term ASGGX. This new fabric BSDF model closely matches real woven fabrics in both reflection and transmission. We use a simple setup for capturing reflection and transmission photos with a cell phone camera and two point lights, and estimate the fabric parameters via a lightweight network, together with a differentiable optimization. We also model the out-of-focus effects explicitly with a simple solution to match the thin-lens camera better. As a result, the renderings of the estimated parameters can agree with the input images on both reflection and transmission for the first time. The code for this paper is at https://github.com/lxtyin/FabricBTDF-Recovery.	https://dl.acm.org/doi/abs/10.1145/3641519.3657410	Yingjie Tang, Zixuan Li, Milos Hasan, Jian Yang, Beibei Wang
X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention	We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.	https://dl.acm.org/doi/abs/10.1145/3641519.3657459	You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo
X-SLAM: Scalable Dense SLAM for Task-aware Optimization using CSFD	We present X-SLAM, a real-time dense differentiable SLAM system that leverages the complex-step finite difference (CSFD) method for efficient calculation of numerical derivatives, bypassing the need for a large-scale computational graph. The key to our approach is treating the SLAM process as a differentiable function, enabling the calculation of the derivatives of important SLAM parameters through Taylor series expansion within the complex domain. Our system allows for the real-time calculation of not just the gradient, but also higher-order differentiation. This facilitates the use of high-order optimizers to achieve better accuracy and faster convergence. Building on X-SLAM, we implemented end-to-end optimization frameworks for two important tasks: camera relocalization in wide outdoor scenes and active robotic scanning in complex indoor environments. Comprehensive evaluations on public benchmarks and intricate real scenes underscore the improvements in the accuracy of camera relocalization and the efficiency of robotic navigation achieved through our task-aware optimization. The code and data are available at https://gapszju.github.io/X-SLAM.	https://dl.acm.org/doi/abs/10.1145/3658233	Zhexi Peng, Yin Yang, Tianjia Shao, Chenfanfu Jiang, Kun Zhou
XRweld: An In-Situ Extended Reality Platform for Welding Education	Craft-based manufacturing trades such as metal welding require extensive hands-on training and mentorship to gain embodied knowledge, muscle memory, and experiential learning. As such, there is an opportunity to leverage spatial computing platforms to monitor welding behaviors, provide feedback, and provide assistive value through an extended reality (XR) system. Our demo leverages an XR headset and controller that have been integrated into a fully-functional welding helmet and torch, allowing users to receive in-situ, real-time feedback while actively welding. This system also allows us to explore other aspects of the embodied learning of welding, including the analysis of biometric data, performance analysis, and the inclusion of meditation to further augment the welding educational experience.	https://dl.acm.org/doi/abs/10.1145/3641521.3664409	Tate Johnson, Ann Li, Andrew Knowles, Zhenfang Chen, Semina Yi, Yumeng Zhuang, Man To Tang, Daragh Byrne, Dina El-Zanfaly
Yellowbird	While living in New York, a young Bulgarian immigrant receives tragic news from overseas, and must choose between going home or staying in the US to maintain her legal status.	https://dl.acm.org/doi/abs/10.1145/3641230.3651622	Tsvetelina Zdraveva, Jerred North
Yin and Yang: The Balance of Animation in Kung Fu Panda 4	"At the end of the Kung Fu Panda 3, Oogway calls Po ""Both sides of the Yin and Yang. And my true successor."" Now as we enter the fourth installment in the Kung Fu Panda franchise, Po is faced with a new challenge of growth, his promotion to the spiritual leader of the Valley of Peace. We accompany Po as he visits both familiar and new locales where he meets new friends, including a thief named Zhen, as well as finding a dangerous villain with far reaching shape shifting powers, The Chameleon. But in order to become the spiritual leader, he must understand what it means to have the balance of Yin and Yang. This session will explore how the filmmakers kept balance at the forefront of their design and technology decisions; the innovative new FX techniques for transforming animated characters, the use of Real Time game engines to build out and discover new worlds, the various threads of the story - good vs evil, familiar vs new, heart vs action."	https://dl.acm.org/doi/abs/10.1145/3641232.3649255	Sean Sexton, Jason Mayer, Pablo Valle, Todd Zullo, Youxi Woo
Zeitgeist - A deep learning visualization of Social Flow	Zeitgeist is a participatory artwork that forms part of the AHRC-funded research project. The interface employs artificial intelligence (ai)-based algorithms to represent mental states of creative 'Flow': Flow, a concept of peak performance, low stress and heightened creative stimulation [Nakamura and Csikszentmihalyi, 2002] is measured and assessed using physiological data analysed in real-time using a deep-learning algorithms: Mapped onto 3D representations and displayed on a Pepper's ghost type hologram, participants jointly observe their individual and collective propensity of being in Flow. As a research project, the artefact is embedded into research on the effect of participatory art engagements on Flow, wellbeing and social connectedness. A pilot with two cohorts creatives (N=12) and scientists (N=16) demonstrates a significant effect of the participatory art intervention on Flow, mood and social connectedness, with facilitation and environmental context as confounding factors.	https://dl.acm.org/doi/abs/10.1145/3641234.3671070	Oliver Gingrich, Shama Rahman, Daniel Hignell-Tully
ZeroGrads: Learning Local Surrogates for Non-Differentiable Graphics	"Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a ""surrogate"" that has similar minima but is differentiable. Our proposed framework, , automates this process by learning a neural approximation of the objective function, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competitive performance at little overhead. We demonstrate optimizing diverse non-convex, non-differentiable black-box problems in graphics, such as visibility in rendering, discrete parameter spaces in procedural modelling or optimal control in physics-driven animation. In contrast to other derivative-free algorithms, our approach scales well to higher dimensions, which we demonstrate on problems with up to 35k interlinked variables."	https://dl.acm.org/doi/abs/10.1145/3658173	Michael Fischer, Tobias Ritschel
[sin-gí]:Data Weaving	"""[ sin-gí ] : Data Weaving"" connects digital language and cultural language, developing sound waves of real-time visual programming. It transcodes sound data of local narratives into woven visual patterns, becoming a touchable textile embedded with body artistries and generating local narratives in cross-cultural sound preservations of digital materials. The textiles of ""[ sin-gí ] : Data Weaving"" are stored, arranged, and indexed individual messages in a meaning materialized way, and become a trace of the producers' body marks, as well as a way of interaction in which personal language is memorized and cultural textiles are mutually mended."	https://dl.acm.org/doi/abs/10.1145/3641523.3665177	Hsin-I Lin
demoConstruct: Democratizing Scene Construction for Digital Twins through Progressive Reconstruction	We introduce demoConstruct, an open-source project aimed at developing an accessible collaborative scene authoring tool for immersive applications, catering to both professional and untrained users. The tool employs progressive reconstruction, enabling simultaneous near real-time scene acquisition and editing tasks, including editing in Virtual Reality (VR). And, can be applied to multiple use cases, such as reconstructing disaster struck areas in near real-time for remote responders to plan and simulate operations. Participants will have the opportunity to experience an alpha version of demoConstruct, allowing them to construct their immersive environments during the lab. This experience aims to foster academic discourse and stimulate discussions to collectively advance this research area through an open-source initiative.	https://dl.acm.org/doi/abs/10.1145/3641236.3664424	Leon Foo, Chek Tien Tan, Liuziyi Liu, Nirmal Sukumaran Nair, Songjia Shen, Jeannie Lee
fVDB : A Deep-Learning Framework for Sparse, Large Scale, and High Performance Spatial Intelligence	We present VDB, a novel GPU-optimized framework for deep learning on large-scale 3D data. VDB provides a complete set of differentiable primitives to build deep learning architectures for common tasks in 3D learning such as convolution, pooling, attention, ray-tracing, meshing, etc. VDB simultaneously provides a feature set (primitives and operators) than established frameworks with no loss in efficiency: our operators match or exceed the performance of other frameworks with narrower scope. Furthermore, VDB can process datasets with much larger footprint and spatial resolution than prior works, while providing a competitive memory footprint on small inputs. To achieve this combination of versatility and performance, VDB relies on a single novel VDB index grid acceleration structure paired with several key innovations including GPU accelerated sparse grid construction, convolution using tensorcores, fast ray tracing kernels using a Hierarchical Digital Differential Analyzer algorithm (HDDA), and jagged tensors. Our framework is fully integrated with PyTorch enabling interoperability with existing pipelines, and we demonstrate its effectiveness on a number of representative tasks such as large-scale point-cloud segmentation, high resolution 3D generative modeling, unbounded scale Neural Radiance Fields, and large-scale point cloud reconstruction.	https://dl.acm.org/doi/abs/10.1145/3658226	Francis Williams, Jiahui Huang, Jonathan Swartz, Gergely Klar, Vijay Thakkar, Matthew Cong, Xuanchi Ren, Ruilong Li, Clement Fuji-Tsang, Sanja Fidler, Eftychios Sifakis, Ken Museth
