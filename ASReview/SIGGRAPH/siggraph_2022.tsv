title	abstract	url	authors
-Functions Piecewise-linear Approximation from Noisy and Hermite Data	We introduce α-functions, providing piecewise linear approximation to given data as the difference of two convex functions. The parameter α controls the shape of a paraboloid that is probing the data and may be used to filter out noise in the data. The use of convex functions enables tools for efficient approximation to the data, adding robustness to outliers, and dealing with gradient information. It also allows using the approach in higher dimension. We show that α-functions can be efficiently computed and demonstrate their versatility at the example of surface reconstruction from noisy surface samples.	https://dl.acm.org/doi/abs/10.1145/3528233.3530743	Marc Alexa
3.5%	What might Europe look like if we continue to ignore the climate crisis? A glimpse.	https://dl.acm.org/doi/abs/10.1145/3512752.3517822	Lukas Bieri
A Clever Label: Multi-sensory VR data visualization for art, productivity and communication	"A Clever Label is an interactive immersive documentary pilot experience that introduces a novel data visualization mechanic (""Grapho"") for curating and presenting connected graph data as simply as a slide deck. Inside the XR experience the audience is guided by a presenter through volumetric video, voice-overs, haptics and subtitles to explore complex data."	https://dl.acm.org/doi/abs/10.1145/3532834.3536206	Michela Ledwidge
A Comparison of Zoom-In Transition Methods for Multiscale VR	When navigating within an unfamiliar virtual environment in VR, transitions between pre-defined viewpoints are known to facilitate spatial awareness of a user. Previously, different viewpoint transition techniques had been investigated, but mainly for single-scale environments. We present a comparative study of zoom-in transition techniques, where the viewpoint of a user is being smoothly transitioned from a large level of scale (LoS) to a smaller LoS in a multiscale virtual environment (MVE) with a nested structure. We identify that orbiting first before zooming in is preferred over other alternatives when transitioning to a viewpoint at a small LoS.	https://dl.acm.org/doi/abs/10.1145/3532719.3543237	Jong-In Lee, Paul Asente, Wolfgang Stuerzlinger
A Fast & Robust Solution for Cubic & Higher-Order Polynomials	We present a computationally-efficient and numerically-robust method for finding real roots of cubic and higher-order polynomials. It begins with determining the intervals where a given polynomial is monotonic. Then, the existence of a real root within each interval can be quickly identified. If one exists, we find the root using a stable variant of Newton iterations, providing fast and guaranteed convergence and satisfying the given error bound.	https://dl.acm.org/doi/abs/10.1145/3532836.3536266	Cem Yuksel
A Focused Animation Curriculum Model	"We offer this curriculum as a model for a focused animation program that does not require the infrastructure of an animation major, multiple instructors, or many courses. Students who have taken two courses are well-prepared for technical director positions. Many students report this experience as a college highlight. The success of the program is due to several key practices. Group critiques mimic ""dailies"" in industry and allow students to improve work in progress. Students are motivated to learn animation basics when they are applied to a final short film. We emphasize problem-solving over recipe-following so students become self-sufficient in learning new techniques. We provide a supportive and enthusiastic structure through in-class activities, TA support, and a well-attended final showcase. We discuss these principles along with course aims and lessons learned as we refined our approach."	https://dl.acm.org/doi/abs/10.1145/3532724.3535591	Barbara J. Meier
A GPU-based multilevel additive schwarz preconditioner for cloth and deformable body simulation	In this paper, we wish to push the limit of real-time cloth and deformable body simulation to a higher level with 50K to 500K vertices, based on the development of a novel GPU-based multilevel additive Schwarz (MAS) pre-conditioner. Similar to other preconditioners under the MAS framework, our preconditioner naturally adopts multilevel and domain decomposition concepts. But contrary to previous works, we advocate the use of small, non-overlapping domains that can well explore the parallel computing power on a GPU. Based on this idea, we investigate and invent a series of algorithms for our preconditioner, including multilevel domain construction using Morton codes, low-cost matrix precomputation by one-way Gauss-Jordan elimination, and conflict-free symmetric-matrix-vector multiplication in runtime preconditioning. The experiment shows that our preconditioner is effective, fast, cheap to precompute and scalable with respect to stiffness and problem size. It is compatible with many linear and nonlinear solvers used in cloth and deformable body simulation with dynamic contacts, such as PCG, accelerated gradient descent and L-BFGS. On a GPU, our preconditioner speeds up a PCG solver by approximately a factor of four, and its CPU version outperforms a number of competitors, including ILU0 and ILUT.	https://dl.acm.org/doi/abs/10.1145/3528223.3530085	Botao Wu, Zhendong Wang, Huamin Wang
A Game-Development Paradigm for Building Programming Intuition	A fundamental aspect of learning computer programming is the development of an intuition that can provide one with what needs to be done to solve a problem sans specific implementation detail. However, intuition cannot be taught using just lectures and/or readings. Students must be immersed in the process of programming, so relationships among the elements they learn become apparent. Often, short, discrete demonstration programs are used to introduce various programming schema. Given the disconnectedness of these short examples, they fail to help students gain a programming intuition. Using game development as the basis for an introductory programming class taps into students existing understanding of and intuition relating to image, sound, and games. It also demonstrates how various programming approaches can be used to the benefit of solving a large, meaningful problem, which, appropriately designed, provides ample space for the development of a student's programming intuition.	https://dl.acm.org/doi/abs/10.1145/3532724.3535601	William Joel
A Minimalist Social Robot Platform for Promoting Positive Behavior Change Among Children	We present the design of a minimalist interactive social robot named Haksh-E which integrates audio-visual perception with artificial intelligence to nurture positive behaviors in children. We co-designed the robot's embodiment with children acting as design informants. The robot features expressive interactions, real time conversation and human action detection capabilities. Haksh-E can assist children in learning new behaviors as a tutor, play a supportive role in health interventions and also pose as an objective assessor of various indicators of health and hygiene.	https://dl.acm.org/doi/abs/10.1145/3532724.3535597	Pranav Prabha, Sreejith Sasidharan, Devasena Pasupuleti, Anand Das, Gayathri Manikutty, Rajesh Sharma
A Motion Matching-based Framework for Controllable Gesture Synthesis from Speech	Recent deep learning-based approaches have shown promising results for synthesizing plausible 3D human gestures from speech input. However, these approaches typically offer limited freedom to incorporate user control. Furthermore, training such models in a supervised manner often does not capture the multi-modal nature of the data, particularly because the same audio input can produce different gesture outputs. To address these problems, we present an approach for generating controllable 3D gestures that combines the advantage of database matching and deep generative modeling. Our method predicts 3D body motion by sequentially searching for the most plausible audio-gesture clips from a database using a k-Nearest Neighbors (k-NN) algorithm that considers the similarity to both the input audio and the previous body pose information. To further improve the synthesis quality, we propose a conditional Generative Adversarial Network (cGAN) model to provide a data-driven refinement to the k-NN result by comparing its plausibility against the ground truth audio-gesture pairs. Our novel approach enables direct and more varied control manipulation that is not possible with prior learning-based counterparts. Our experiments show that our proposed approach outperforms recent models on control-based synthesis tasks using high-level signals such as motion statistics while enabling flexible and effective user control for lower-level signals. 1	https://dl.acm.org/doi/abs/10.1145/3528233.3530750	Ikhsanul Habibie, Mohamed Elgharib, Kripasindhu Sarkar, Ahsan Abdullah, Simbarashe Nyatsanga, Michael Neff, Christian Theobalt
A Showcase of Decima Engine in Horizon Forbidden West	Horizon Forbidden West's graphics are the product of many separate features working together to deliver the acclaimed visuals. Each has to run within strict performance and memory budgets to deliver a reliable framerate. In this presentation we'll break down some of those features, and the goals and challenges for each. The audience will get a behind the scenes look at some of the real-time systems that drive the rich dynamism of the world of Horizon.	https://dl.acm.org/doi/abs/10.1145/3532833.3538681	Hugh Malan, Maarten van der Gaag
A Theoretical Analysis of Compactness of the Light Transport Operator	Rendering photorealistic visuals of virtual scenes requires tractable models for the simulation of light. The rendering equation describes one such model using an integral equation, the crux of which is a continuous integral operator. A majority of rendering algorithms aim to approximate the effect of this light transport operator via discretization (using rays, particles, patches, etc.). Research spanning four decades has uncovered interesting properties and intuition surrounding this operator. In this paper we analyze compactness, a key property that is independent of its discretization and which characterizes the ability to approximate the operator uniformly by a sequence of finite rank operators. We conclusively prove lingering suspicions that this operator is not compact and therefore that any discretization that relies on a finite-rank or nonadaptive finite-bases is susceptible to unbounded error over arbitrary light distributions. Our result justifies the expectation for rendering algorithms to be evaluated using a variety of scenes and illumination conditions. We also discover that its lower dimensional counterpart (over purely diffuse scenes) is not compact except in special cases, and uncover connections with it being noninvertible and acting as a low-pass filter. We explain the relevance of our results in the context of previous work. We believe that our theoretical results will inform future rendering algorithms regarding practical choices.	https://dl.acm.org/doi/abs/10.1145/3528233.3530725	Cyril Soler, Ronak Molazem, Kartic Subr
A clebsch method for free-surface vortical flow simulation	We propose a novel Clebsch method to simulate the free-surface vortical flow. At the center of our approach lies a level-set method enhanced by a wave-function correction scheme and a wave-function extrapolation algorithm to tackle the Clebsch method's numerical instabilities near a dynamic interface. By combining the Clebsch wave function's expressiveness in representing vortical structures and the level-set function's ability on tracking interfacial dynamics, we can model complex vortex-interface interaction problems that exhibit rich free-surface flow details on a Cartesian grid. We showcase the efficacy of our approach by simulating a wide range of new free-surface flow phenomena that were impractical for previous methods, including horseshoe vortex, sink vortex, bubble rings, and free-surface wake vortices.	https://dl.acm.org/doi/abs/10.1145/3528223.3530150	Shiying Xiong, Zhecheng Wang, Mengdi Wang, Bo Zhu
A fast unsmoothed aggregation algebraic multigrid framework for the large-scale simulation of incompressible flow	Multigrid methods are quite efficient for solving the pressure Poisson equation in simulations of incompressible flow. However, for viscous liquids, geometric multigrid turned out to be less efficient for solving the variational viscosity equation. In this contribution, we present an Unsmoothed Aggregation Algebraic MultiGrid (UAAMG) method with a multi-color Gauss-Seidel smoother, which consistently solves the variational viscosity equation in a few iterations for various material parameters. Moreover, we augment the OpenVDB data structure with Intel SIMD intrinsic functions to perform sparse matrix-vector multiplications efficiently on all multigrid levels. Our framework is 2.0 to 14.6 times faster compared to the state-of-the-art adaptive octree solver in commercial software for the large-scale simulation of both non-viscous and viscous flow. The code is available at http://computationalsciences.org/publications/shao-2022-multigrid.html.	https://dl.acm.org/doi/abs/10.1145/3528223.3530109	Han Shao, Libo Huang, Dominik L. Michels
A general two-stage initialization for sag-free deformable simulations	Initializing simulations of deformable objects involves setting the rest state of all internal forces at the rest shape of the object. However, often times the rest shape is not explicitly provided. In its absence, it is common to initialize by treating the given initial shape as the rest shape. This leads to sagging, the undesirable deformation under gravity as soon as the simulation begins. Prior solutions to sagging are limited to specific simulation systems and material models, most of them cannot handle frictional contact, and they require solving expensive global nonlinear optimization problems. We introduce a novel solution to the sagging problem that can be applied to a variety of simulation systems and materials. The key feature of our approach is that we avoid solving a global nonlinear optimization problem by performing the initialization in two stages. First, we use a global linear optimization for static equilibrium. Any nonlinearity of the material definition is handled in the local stage, which solves many small local problems efficiently and in parallel. Notably, our method can properly handle frictional contact orders of magnitude faster than prior work. We show that our approach can be applied to various simulation systems by presenting examples with mass-spring systems, cloth simulations, the finite element method, the material point method, and position-based dynamics.	https://dl.acm.org/doi/abs/10.1145/3528223.3530165	Jerry Hsu, Nghia Truong, Cem Yuksel, Kui Wu
A moving eulerian-lagrangian particle method for thin film and foam simulation	We present the Moving Eulerian-Lagrangian Particles (MELP), a novel mesh-free method for simulating incompressible fluid on thin films and foams. Employing a bi-layer particle structure, MELP jointly simulates detailed, vigorous flow and large surface deformation at high stability and efficiency. In addition, we design multi-MELP: a mechanism that facilitates the physically-based interaction between multiple MELP systems, to simulate bubble clusters and foams with non-manifold topological evolution. We showcase the efficacy of our method with a broad range of challenging thin film phenomena, including the Rayleigh-Taylor instability across double-bubbles, foam fragmentation with rim surface tension, recovery of the Plateau borders, Newton black films, as well as cyclones on bubble clusters.	https://dl.acm.org/doi/abs/10.1145/3528223.3530174	Yitong Deng, Mengdi Wang, Xiangxin Kong, Shiying Xiong, Zangyueyang Xian, Bo Zhu
A unified newton barrier method for multibody dynamics	We present a simulation framework for multibody dynamics via a universal variational integration. Our method naturally supports mixed rigid-deformables and mixed codimensional geometries, while providing guaranteed numerical convergence and accurate resolution of contact, friction, and a wide range of articulation constraints. We unify (1) the treatment of simulation degrees of freedom for rigid and soft bodies by formulating them both in terms of Lagrangian nodal displacements, (2) the handling of general linear equality joint constraints through an efficient change-of-variable strategy, (3) the enforcement of nonlinear articulation constraints based on novel distance potential energies, (4) the resolution of frictional contact between mixed dimensions and bodies with a variational Incremental Potential Contact formulation, and (5) the modeling of generalized restitution through semi-implicit Rayleigh damping. We conduct extensive unit tests and benchmark studies to demonstrate the efficacy of our method.	https://dl.acm.org/doi/abs/10.1145/3528223.3530076	Yunuo Chen, Minchen Li, Lei Lan, Hao Su, Yin Yang, Chenfanfu Jiang
A web around asteroid bennu	Over the course of two-and-a-half years, NASA's OSIRIS-REx space-craft wrapped asteroid Bennu in a complex web of observations.	https://dl.acm.org/doi/abs/10.1145/3512752.3526118	Kel Elkins, Daniel Gallagher, Greg Shirah, Ernie Wright
ABBA Voyage: High Volume Facial Likeness and Performance Pipeline	For the ABBA: Voyage concert experience, Industrial Light & Magic (ILM) was tasked with digitally time traveling the iconic band′s members Agnetha, Anni-Frida, Björn and Benny back to their prime time appearances. For the duration of this fully computer graphics generated concert four continuous photo-real digital human facial performances had to be synthesised driven by their original current day counterparts and stand-in young actors. This talk will dive into the extensive research and development that was undertaken to cater for high volume facial capture and processing, true-to-likeness face-retargeting and additional techniques for breaking through the uncanny valley.	https://dl.acm.org/doi/abs/10.1145/3532836.3536260	Jo Plaete, Derek Bradley, Paige Warner, Anthony Zwartouw
ADOP: approximate differentiable one-pixel point rendering	In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/ADOP	https://dl.acm.org/doi/abs/10.1145/3528223.3530122	Darius Rückert, Linus Franke, Marc Stamminger
AI and Physics Assisted Character Pose Authoring	We present a tool that allows users to quickly author character poses. Authoring character poses is typically done by professional artists and is a time consuming process that involves a lot of user manipulations. Our tool leverages both machine learning and a physics engine to enable users with no artistic experience to author natural-looking poses in a few seconds. First, we trained a machine learning (ML) model to predict a full character pose, including individual fingers, from a set of sparse constraints. These constraints allow the user to control the final pose by specifying final joint positions, orientations or a target that they should face. Our ML architecture allows the constraints to be given in any order and number. The model was trained on a large set of motion capture data so that it predicts natural and realistic human poses. Second, we integrated our ML model with a physics solver so that the final pose also respects environmental constraints such as colliding with other objects. This allows the user to quickly pose a character interacting with the environment, another character or itself. Finally, we developed a user-friendly interface to control this tool. We believe that the combination of machine learning and physics lower the entry bar to character animation.	https://dl.acm.org/doi/abs/10.1145/3532833.3538680	Florent Bocquelet, Boris Oreshkin, Felix Harvey, Louis-Simon Ménard, Dominic Laflamme, Bay Raitt, Jeremy Cowles
ASE: large-scale reusable adversarial skill embeddings for physically simulated characters	The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.	https://dl.acm.org/doi/abs/10.1145/3528223.3530110	Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, Sanja Fidler
ASSET: autoregressive semantic scene editing with transformers at high resolutions	We present ASSET, a neural architecture for automatically modifying an input high-resolution image according to a user's edits on its semantic segmentation map. Our architecture is based on a transformer with a novel attention mechanism. Our key idea is to sparsify the transformer's attention matrix at high resolutions, guided by dense attention extracted at lower image resolutions. While previous attention mechanisms are computationally too expensive for handling high-resolution images or are overly constrained within specific image regions hampering long-range interactions, our novel attention mechanism is both computationally efficient and effective. Our sparsified attention mechanism is able to capture long-range interactions and context, leading to synthesizing interesting phenomena in scenes, such as reflections of landscapes onto water or fora consistent with the rest of the landscape, that were not possible to generate reliably with previous convnets and transformer approaches. We present qualitative and quantitative results, along with user studies, demonstrating the effectiveness of our method. Our code and dataset are available at our project page: https://github.com/DifanLiu/ASSET	https://dl.acm.org/doi/abs/10.1145/3528223.3530172	Difan Liu, Sandesh Shetty, Tobias Hinz, Matthew Fisher, Richard Zhang, Taesung Park, Evangelos Kalogerakis
Accelerating facial motion capture with video-driven animation transfer	We describe a hybrid pipeline that leverages: 1) video-driven animation transfer [Moser et al. 2021] for regressing high-quality animation under partially-controlled conditions from a single input image, and 2) a marker-based tracking approach [Moser et al. 2017] that, while more complex and slower, is capable of handling the most challenging scenarios seen in the capture set. By applying the most suited approach to each shot, we have an overall pipeline that, without loss of quality, is faster and has less user intervention. We also improve the prior work [Moser et al. 2021] with augmentations during training to make it more robust for the Head Mounted Camera (HMC) scenario. The new pipeline is currently being integrated into our offline and real-time workflows.	https://dl.acm.org/doi/abs/10.1145/3532836.3536237	Jose Serra, Mark Williams, Lucio Moser
Accommodative holography: improving accommodation response for perceptually realistic holographic displays	Holographic displays have gained unprecedented attention as next-generation virtual and augmented reality applications with recent achievements in the realization of a high-contrast image through computer-generated holograms (CGHs). However, these holograms show a high energy concentration in a limited angular spectrum, whereas the holograms with uniformly distributed angular spectrum suffer from a severe speckle noise in the reconstructed images. In this study, we claim that these two physical phenomena attributed to the existing CGHs significantly limit the support of accommodation cues, which is known as one of the biggest advantages of holographic displays. To support the statement, we analyze and evaluate various CGH algorithms with contrast gradients - a change of contrast over the change of the focal diopter of the eye - simulated based on the optical configuration of the display system and human visual perception models. We first introduce two approaches to improve monocular accommodation response in holographic viewing experience; optical and computational approaches to provide holographic images with sufficient contrast gradients. We design and conduct user experiments with our prototype of holographic near-eye displays, validating the deficient support of accommodation cues in the existing CGH algorithms and demonstrating the feasibility of the proposed solutions with significant improvements on accommodative gains.	https://dl.acm.org/doi/abs/10.1145/3528223.3530147	Dongyeon Kim, Seung-Woo Nam, Byounghyo Lee, Jong-Mo Seo, Byoungho Lee
Adaptive rigidification of elastic solids	We present a method for reducing the computational cost of elastic solid simulation by treating connected sets of non-deforming elements as rigid bodies. Non-deforming elements are identified as those where the strain rate squared Frobenius norm falls below a threshold for several frames. Rigidification uses a breadth first search to identify connected components while avoiding connections that would form hinges between rigid components. Rigid elements become elastic again when their approximate strain velocity rises above a threshold, which is fast to compute using a single iteration of conjugate gradient with a fixed Laplacian-based incomplete Cholesky preconditioner. With rigidification, the system size to solve at each time step can be greatly reduced, and if all elastic element become rigid, it reduces to solving the rigid body system. We demonstrate our results on a variety of 2D and 3D examples, and show that our method is likewise especially beneficial in contact rich examples.	https://dl.acm.org/doi/abs/10.1145/3528223.3530124	Alexandre Mercier-Aubin, Paul G. Kry, Alexandre Winter, David I. W. Levin
Adjoint nonlinear ray tracing	Reconstructing and designing media with continuously-varying refractive index fields remains a challenging problem in computer graphics. A core difficulty in trying to tackle this inverse problem is that light travels inside such media along curves, rather than straight lines. Existing techniques for this problem make strong assumptions on the shape of the ray inside the medium, and thus limit themselves to media where the ray deflection is relatively small. More recently, differentiable rendering techniques have relaxed this limitation, by making it possible to differentiably simulate curved light paths. However, the automatic differentiation algorithms underlying these techniques use large amounts of memory, restricting existing differentiable rendering techniques to relatively small media and low spatial resolutions. We present a method for optimizing refractive index fields that both accounts for curved light paths and has a small, constant memory footprint. We use the adjoint state method to derive a set of equations for computing derivatives with respect to the refractive index field of optimization objectives that are subject to nonlinear ray tracing constraints. We additionally introduce discretization schemes to numerically evaluate these equations, without the need to store nonlinear ray trajectories in memory, significantly reducing the memory requirements of our algorithm. We use our technique to optimize high-resolution refractive index fields for a variety of applications, including creating different types of displays (multiview, lightfield, caustic), designing gradient-index optics, and reconstructing gas flows.	https://dl.acm.org/doi/abs/10.1145/3528223.3530077	Arjun Teh, Matthew O'Toole, Ioannis Gkioulekas
Advances for Digital Humans in VFX Production at Goodbye Kansas Studios	Digital humans and digital doubles are core products at Goodbye Kansas Studios and in order to further improve their quality and related workflows, a number of new tools have been developed and integrated into our existing pipeline. This talk will cover some of these implementations, such as an offline generation of blendshapes based on facial scans, our take on the sticky lips problem and an efficient implementation of render time dynamic skin microstructure. Furthermore, we look into future usages and how Universal Scene Description (USD) [Pixar Animation Studios 2021] can serve as a helpful tool for facial rigs.	https://dl.acm.org/doi/abs/10.1145/3532836.3536270	Rasmus Haapaoja, Josefine Klintberg
Advances in Spatial Hashing: A Pragmatic Approach towards Robust, Real-time Light Transport Simulation	Spatial hashing is a battle-tested technique for efficiently storing sparse spatial data. Originally designed to optimize secondary light bounces in path tracing, it has been extended for real-time ambient occlusion and diffuse environment lighting. We complement spatial hashing by introducing support for view-dependent effects using world-space temporal filtering. Optimizing the hash key generation, we improve performance using a much better cache coherence and aliasing reduction. Finally, we enhance the sampling quality using methods including visibility-aware environment sampling.	https://dl.acm.org/doi/abs/10.1145/3532836.3536239	Pascal Gautron
Affine body dynamics: fast, stable and intersection-free simulation of stiff materials	Simulating stiff materials in applications where deformations are either not significant or else can safely be ignored is a fundamental task across fields. Rigid body modeling has thus long remained a critical tool and is, by far, the most popular simulation strategy currently employed for modeling stiff solids. At the same time, rigid body methods continue to pose a number of well known challenges and trade-offs including intersections, instabilities, inaccuracies, and/or slow performances that grow with contact-problem complexity. In this paper we revisit the stiff body problem and present ABD, a simple and highly effective affine body dynamics framework, which significantly improves state-of-the-art for simulating stiff-body dynamics. We trace the challenges in rigid-body methods to the necessity of linearizing piecewise-rigid trajectories and subsequent constraints. ABD instead relaxes the unnecessary (and unrealistic) constraint that each body's motion be exactly rigid with a stiff orthogonality potential, while preserving the rigid body model's key feature of a small coordinate representation. In doing so ABD replaces piecewise with piecewise trajectories. This, in turn, combines the best of both worlds: compact coordinates ensure small, sparse system solves, while piecewise-linear trajectories enable efficient and accurate constraint (contact and joint) evaluations. Beginning with this simple foundation, ABD preserves all guarantees of the underlying IPC model we build it upon, e.g., solution convergence, guaranteed non-intersection, and accurate frictional contact. Over a wide range and scale of simulation problems we demonstrate that ABD brings orders of magnitude performance gains (two- to three-orders on the CPU and an order more when utilizing the GPU, obtaining 10, 000× speedups) over prior IPC-based methods, while maintaining simulation quality and nonintersection of trajectories. At the same time ABD has comparable or faster timings when compared to state-of-the-art rigid body libraries optimized for performance without guarantees, and successfully and efficiently solves challenging simulation problems where both classes of prior rigid body simulation methods fail altogether.	https://dl.acm.org/doi/abs/10.1145/3528223.3530064	Lei Lan, Danny M. Kaufman, Minchen Li, Chenfanfu Jiang, Yin Yang
Alpha wrapping with an offset	Given an input 3D geometry such as a triangle soup or a point set, we address the problem of generating a watertight and orientable surface triangle mesh that strictly encloses the input. The output mesh is obtained by greedily refining and carving a 3D Delaunay triangulation on an offset surface of the input, while carving with empty balls of radius alpha. The proposed algorithm is controlled via two user-defined parameters: alpha and offset. Alpha controls the size of cavities or holes that cannot be traversed during carving, while offset controls the distance between the vertices of the output mesh and the input. Our algorithm is guaranteed to terminate and to yield a valid and strictly enclosing mesh, even for defect-laden inputs. Genericity is achieved using an abstract interface probing the input, enabling any geometry to be used, provided a few basic geometric queries can be answered. We benchmark the algorithm on large public datasets such as Thingi10k, and compare it to state-of-the-art approaches in terms of robustness, approximation, output complexity, speed, and peak memory consumption. Our implementation is available through the CGAL library.	https://dl.acm.org/doi/abs/10.1145/3528223.3530152	Cédric Portaneri, Mael Rouxel-Labbé, Michael Hemmer, David Cohen-Steiner, Pierre Alliez
Alternate mesozoic	In the Mesozoic area, Rex and his dinosaur friends are going to be disturbed in their peaceful life by a tribe of annoying humans. A meteor shower will crash on the Earth. But dinosaurs are strong and humans will beg them to protect them. Rex will accept and save the humans. Dinosaurs and humans will evolve and coexist through the ages: antiquity, the Middle Ages and contemporary times. Are dinosaurs going to cope with the annoying and invasive behavior of humans?	https://dl.acm.org/doi/abs/10.1145/3512752.3520186	Lucie Laudrin, Swann Boby, Marion Métivier, Léna Miguet, Sixtine Sanrame, Marie Schaeffer
Analytically Integratable Zero-restlength Springs for Capturing Dynamic Modes unrepresented by Quasistatic Neural Networks	We present a novel paradigm for modeling certain types of dynamic simulation in real-time with the aid of neural networks. In order to significantly reduce the requirements on data (especially time-dependent data), as well as decrease generalization error, our approach utilizes a data-driven neural network only to capture quasistatic information (instead of dynamic or time-dependent information). Subsequently, we augment our quasistatic neural network (QNN) inference with a (real-time) dynamic simulation layer. Our key insight is that the dynamic modes lost when using a QNN approximation can be captured with a quite simple (and decoupled) zero-restlength spring model, which can be integrated analytically (as opposed to numerically) and thus has no time-step stability restrictions. Additionally, we demonstrate that the spring constitutive parameters can be robustly learned from a surprisingly small amount of dynamic simulation data. Although we illustrate the efficacy of our approach by considering soft-tissue dynamics on animated human bodies, the paradigm is extensible to many different simulation frameworks.	https://dl.acm.org/doi/abs/10.1145/3528233.3530705	Yongxu Jin, Yushan Han, Zhenglin Geng, Joseph Teran, Ronald Fedkiw
Animating Portrait Line Drawings from a Single Face Photo and a Speech Signal	Animating a single face photo is an important research topic which receives considerable attention in computer vision and graphics. Yet line drawings for face portraits, which is a longstanding and popular art form, have not been explored much in this area. Simply concatenating a realistic talking face video generation model with a photo-to-drawing style transfer module suffers from severe inter-frame discontinuity issues. To address this new challenge, we propose a novel framework to generate artistic talking portrait-line-drawing video, given a single face photo and a speech signal. After predicting facial landmark movements from the input speech signal, we propose a novel GAN model to simultaneously handle domain transfer (from photo to drawing) and facial geometry change (according to the predicted facial landmarks). To address the inter-frame discontinuity issues, we propose two novel temporal coherence losses: one based on warping and the other based on a temporal coherence discriminator. Experiments show that our model produces high quality artistic talking portrait-line-drawing videos and outperforms baseline methods. We also show our method can be easily extended to other artistic styles and generate good results. The source code is available at https://github.com/AnimatePortrait/AnimatePortrait .	https://dl.acm.org/doi/abs/10.1145/3528233.3530720	Ran Yi, Zipeng Ye, Ruoyu Fan, Yezhi Shu, Yong-Jin Liu, Yu-Kun Lai, Paul L. Rosin
Approximate convex decomposition for 3D meshes with collision-aware concavity and tree search	Approximate convex decomposition aims to decompose a 3D shape into a set of almost convex components, whose convex hulls can then be used to represent the input shape. It thus enables efficient geometry processing algorithms specifically designed for convex shapes and has been widely used in game engines, physics simulations, and animation. While prior works can capture the global structure of input shapes, they may fail to preserve fine-grained details (e.g., filling a toaster's slots), which are critical for retaining the functionality of objects in interactive environments. In this paper, we propose a novel method that addresses the limitations of existing approaches from three perspectives: (a) We introduce a novel collision-aware concavity metric that examines the distance between a shape and its convex hull from both the boundary and the interior. The proposed concavity preserves collision conditions and is more robust to detect various approximation errors. (b) We decompose shapes by directly cutting meshes with 3D planes. It ensures generated convex hulls are intersection-free and avoids voxelization errors. (c) Instead of using a one-step greedy strategy, we propose employing a multi-step tree search to determine the cutting planes, which leads to a globally better solution and avoids unnecessary cuttings. Through extensive evaluation on a large-scale articulated object dataset, we show that our method generates decompositions closer to the original shape with fewer components. It thus supports delicate and efficient object interaction in downstream applications.	https://dl.acm.org/doi/abs/10.1145/3528223.3530103	Xinyue Wei, Minghua Liu, Zhan Ling, Hao Su
Arithmagical: Magical Platform Technology Towards a Strong AI Virtual Tutor	Arithmagical is an iOS/Android/Chrome app that lets you feed your real math homework to a virtual magical dragon, where AI turns correct answers into dragon food and incorrect answers into mathemagical adventures, with UX that exposes AR themes and seamless non-intrusive computer vision capture in a way accessible to toddlers (with shaky hands and short attention spans). Virtual pet emotional-connection interactivity, memory retention and additional open-ended AI-guided game creation generalize this towards a strong AI virtual tutor platform.	https://dl.acm.org/doi/abs/10.1145/3532723.3535464	Yosun Chang
Artemis: articulated neural pets with appearance and motion synthesis	We, humans, are entering into a virtual era and indeed want to bring animals to the virtual world as well for companion. Yet, computer-generated (CGI) furry animals are limited by tedious off-line rendering, let alone interactive motion control. In this paper, we present ARTEMIS, a novel neural modeling and rendering pipeline for generating ARTiculated neural pets with appEarance and Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time animation, and photo-realistic rendering of furry animals. The core of our ARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient octree-based representation for animal animation and fur rendering. The animation then becomes equivalent to voxel-level deformation based on explicit skeletal warping. We further use a fast octree indexing and efficient volumetric rendering scheme to generate appearance and density features maps. Finally, we propose a novel shading network to generate high-fidelity details of appearance and opacity under novel poses from appearance and density feature maps. For the motion control module in ARTEMIS, we combine state-of-the-art animal motion capture approach with recent neural character control scheme. We introduce an effective optimization scheme to reconstruct the skeletal motion of real animals captured by a multi-view RGB and Vicon camera array. We feed all the captured motion into a neural character control scheme to generate abstract control signals with motion styles. We further integrate ARTEMIS into existing engines that support VR headsets, providing an unprecedented immersive experience where a user can intimately interact with a variety of virtual animals with vivid movements and photo-realistic appearance. Extensive experiments and showcases demonstrate the effectiveness of our ARTEMIS system in achieving highly realistic rendering of NGI animals in real-time, providing daily immersive and interactive experiences with digital animals unseen before. We make available our ARTEMIS model and dynamic furry animal dataset at https://haiminluo.github.io/publication/artemis/.	https://dl.acm.org/doi/abs/10.1145/3528223.3530086	Haimin Luo, Teng Xu, Yuheng Jiang, Chenglin Zhou, Qiwei Qiu, Yingliang Zhang, Wei Yang, Lan Xu, Jingyi Yu
Artist Controlled Fracture Design Using Impurity Maps	When an object breaks, simulating evolution of fracture as per artist control while maintaining physical realism and plausibility is a challenging problem due to different complex material properties of real world objects. In this work, we present impurity maps as a way to guide fracture paths for both brittle and ductile fracture. We develop a novel probabilistic damage mechanics to model fracture in materials with impurities, using a random graph-based formulation in conjunction with graph-based FEM. An artist created map allows us to selectively distribute the impurities in the material, to weaken the object in those specific regions where the imperfections are added. During simulation, the presence of impurities guide the cracks that develop such that the fracture pattern closely follows the impurity map. We simulate artist-controlled fractures on different materials to demonstrate the potency of our method.	https://dl.acm.org/doi/abs/10.1145/3532719.3543202	Avirup Mandal, Parag Chaudhuri, Subhasis Chaudhuri
Artistically Directable Walk Generation	We present a framework for artistically directable walk generation. A generative network is trained using a motion capture dataset and a manually animated collection of walks. To accommodate an animator's workflow, each walk is presented as a sequence of key poses. The generative framework allows to specify a set of traits including gender, stride, velocity and weight. A generated walk is designed to be the starting point when blocking an animation: an animator can introduce new keys on the controls.	https://dl.acm.org/doi/abs/10.1145/3532836.3536241	Vladimir Ivanov, Parag Havaldar
As-locally-uniform-as-possible reshaping of vector clip-art	Vector clip-art images consist of regions bounded by a network of vector curves. Users often wish to , or rescale, existing clip-art images by changing the locations, proportions, or scales of different image elements. When reshaping images depicting synthetic content they seek to preserve global and local structures. These structures are best preserved when the gradient of the mapping between the original and the reshaped curve networks is locally as close as possible to a uniform scale; mappings that satisfy this property maximally preserve the input curve orientations and minimally change the shape of the input's geometric details, while allowing changes in the relative scales of the different features. The expectation of approximate scale uniformity is ; while reshaping operations are typically expected to change the of a subset of network regions, users expect the change to be minimal away from the directly impacted regions and expect such changes to be gradual and distributed as evenly as possible. Unfortunately, existing methods for editing 2D curve networks do not satisfy these criteria. We propose a targeted vector clip-art reshaping method that satisfies the properties above. We formulate the computation of the desired output network as the solution of a constrained variational optimization problem. We effectively compute the desired solution by casting this continuous problem as a minimization of a non-linear discrete energy function, and obtain the desired minimizer by using a custom iterative solver. We validate our method via perceptual studies comparing our results to those created via algorithmic alternatives and manually generated ones. Participants preferred our results over the closest alternative by a ratio of 6 to 1.	https://dl.acm.org/doi/abs/10.1145/3528223.3530098	Chrystiano Araújo, Nicholas Vining, Enrique Rosales, Giorgio Gori, Alla Sheffer
Atlas of a changing earth: visualizing the vavilov ice cap collapse	Data visualization breakdown, showing the data processing, data fusion, and design that went into creating a cinematic scientific visualization of a collapsing ice cap.	https://dl.acm.org/doi/abs/10.1145/3512752.3524121	Jeff Carpenter, Kalina Borkiewicz, Stuart Levy, Donna Cox, Robert Patterson, AJ Christensen
Attack on mutant plants	In a world where vegetation no longer exists, Hazel, a 14 year old girl and her robot friend Smoko, discover a plant and set out to protect it, things go wrong!	https://dl.acm.org/doi/abs/10.1145/3512752.3519545	Valentine Mercier, Laurie Pouille, Lea Pouille, Laurie Daunay
Audio-Driven Violin Performance Animation with Clear Fingering and Bowing	This paper presents an audio-to-animation synthesis method for violin performance. This new approach provides a fine-grained violin performance animation using information on playing procedure consisting of played string, finger number, position, and bow direction. We demonstrate that our method is capable of synthesizing natural violin performance animation with fine fingering and bowing through extensive evaluation.	https://dl.acm.org/doi/abs/10.1145/3532719.3543240	Asuka Hirata, Keitaro Tanaka, Masatoshi Hamanaka, Shigeo Morishima
Authentic volumetric avatars from a phone scan	Creating photorealistic avatars of existing people currently requires extensive person-specific data capture, which is usually only accessible to the VFX industry and not the general public. Our work aims to address this drawback by relying only on a short mobile phone capture to obtain a drivable 3D head avatar that matches a person's likeness faithfully. In contrast to existing approaches, our architecture avoids the complex task of directly modeling the entire manifold of human appearance, aiming instead to generate an avatar model that can be to novel identities using only small amounts of data. The model dispenses with low-dimensional latent spaces that are commonly employed for hallucinating novel identities, and instead, uses a conditional representation that can extract person-specific information at multiple scales from a high resolution registered neutral phone scan. We achieve high quality results through the use of a novel universal avatar prior that has been trained on high resolution multi-view video captures of facial performances of hundreds of human subjects. By fine-tuning the model using inverse rendering we achieve increased realism and personalize its range of motion. The output of our approach is not only a high-fidelity 3D head avatar that matches the person's facial shape and appearance, but one that can also be driven using a jointly discovered shared global expression space with disentangled controls for gaze direction. Via a series of experiments we demonstrate that our avatars are faithful representations of the subject's likeness. Compared to other state-of-the-art methods for lightweight avatar creation, our approach exhibits superior visual quality and animateability.	https://dl.acm.org/doi/abs/10.1145/3528223.3530143	Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shun-Suke Saito, Stephen Lombardi, Shih-En Wei, Danielle Belko, Shoou-I Yu, Yaser Sheikh, Jason Saragih
Automatic quantization for physics-based simulation	Quantization has proven effective in high-resolution and large-scale simulations, which benefit from bit-level memory saving. However, identifying a quantization scheme that meets the requirement of both precision and memory efficiency requires trial and error. In this paper, we propose a novel framework to allow users to obtain a quantization scheme by simply specifying either an error bound or a memory compression rate. Based on the error propagation theory, our method takes advantage of auto-diff to estimate the contributions of each quantization operation to the total error. We formulate the task as a constrained optimization problem, which can be efficiently solved with analytical formulas derived for the linearized objective function. Our workflow extends the Taichi compiler and introduces dithering to improve the precision of quantized simulations. We demonstrate the generality and efficiency of our method via several challenging examples of physics-based simulation, which achieves up to 2.5× memory compression without noticeable degradation of visual quality in the results. Our code and data are available at https://github.com/Hanke98/AutoQantizer.	https://dl.acm.org/doi/abs/10.1145/3528223.3530154	Jiafeng Liu, Haoyang Shi, Siyuan Zhang, Yin Yang, Chongyang Ma, Weiwei Xu
Automating the Creation and Placement of Objects in a Scene using Python Scripting in Autodesk Maya	This hands on lab will use Python scripting in Maya to automate the creation and placement of objects in a scene.	https://dl.acm.org/doi/abs/10.1145/3532725.3538518	Ann McNamara
AvatarCLIP: zero-shot text-driven generation and animation of 3D avatars	3D avatar creation plays a crucial role in the digital age. However, the whole production process is prohibitively time-consuming and labor-intensive. To democratize this technology to a larger audience, we propose AvatarCLIP, a zero-shot text-driven framework for 3D avatar generation and animation. Unlike professional software that requires expert knowledge, AvatarCLIP empowers layman users to customize a 3D avatar with the desired shape and texture, and drive the avatar with the described motions using solely natural languages. Our key insight is to take advantage of the powerful vision-language model CLIP for supervising neural human generation, in terms of 3D geometry, texture and animation. Specifically, driven by natural language descriptions, we initialize 3D human geometry generation with a shape VAE network. Based on the generated 3D human shapes, a volume rendering model is utilized to further facilitate geometry sculpting and texture generation. Moreover, by leveraging the priors learned in the motion VAE, a CLIP-guided reference-based motion synthesis method is proposed for the animation of the generated 3D avatar. Extensive qualitative and quantitative experiments validate the effectiveness and generalizability of AvatarCLIP on a wide range of avatars. Remarkably, AvatarCLIP can generate unseen 3D avatars with novel animations, achieving superior zero-shot capability. Codes are available at https://github.com/hongfz16/AvatarCLIP.	https://dl.acm.org/doi/abs/10.1145/3528223.3530094	Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, Ziwei Liu
Aδ: autodiff for discontinuous programs - applied to shaders	Over the last decade, automatic differentiation (AD) has profoundly impacted graphics and vision applications --- both broadly via deep learning and specifically for inverse rendering. Traditional AD methods ignore gradients at discontinuities, instead treating functions as continuous. Rendering algorithms intrinsically rely on discontinuities, crucial at object silhouettes and in general for any branching operation. Researchers have proposed automatic differentiation approaches for handling discontinuities by restricting to affine functions, or automatic processes restricted either to invertible functions or to specialized applications like vector graphics. This paper describes a compiler-based approach to extend reverse mode AD so as to accept arbitrary programs involving discontinuities. Our novel gradient rules generalize differentiation to work correctly, assuming there is a single discontinuity in a local neighborhood, by approximating the prefiltered gradient over a box kernel oriented along a 1D sampling axis. We describe when such approximation rules are first-order correct, and show that this correctness criterion applies to a relatively broad class of functions. Moreover, we show that the method is effective in practice for arbitrary programs, including features for which we cannot prove correctness. We evaluate this approach on procedural shader programs, where the task is to optimize unknown parameters in order to match a target image, and our method outperforms baselines in terms of both convergence and efficiency. Our compiler outputs gradient programs in TensorFlow, PyTorch (for quick prototypes) and Halide with an optional auto-scheduler (for efficiency). The compiler also outputs GLSL that renders the target image, allowing users to interactively modify and animate the shader, which would otherwise be cumbersome in other representations such as triangle meshes or vector art.	https://dl.acm.org/doi/abs/10.1145/3528223.3530125	Yuting Yang, Connelly Barnes, Andrew Adams, Adam Finkelstein
Becoming: An Interactive Musical Journey in VR	Becoming is an operatic VR piece based on a Persian poem by Mowlana Rumi. The critical content of the piece is about the spiritual evolution of humans on Earth. The artistic expression of the piece takes advantage of an advanced ray-tracing audio spatialization system (Space3D), which is capable of creating realistic spatial impressions within changing acoustic environments in real-time. In this piece the user can interact with the environment and influence the progression of the music by touching various elements and by changing the spatialization paths and speeds of various layers of the music. Two audience members can be connected through the network and interact with each other via haptic effects.	https://dl.acm.org/doi/abs/10.1145/3532834.3536209	Shahrokh Yadegari, John Burnett, Eito Murakami, Louis Pisha, Francesca Talenti, Juliette Regimbal, Yongjae Yoo
Blending camera and 77 GHz radar sensing for equitable, robust plethysmography	With the resurgence of non-contact vital sign sensing due to the COVID-19 pandemic, remote heart-rate monitoring has gained significant prominence. Many existing methods use cameras; however previous work shows a performance loss for darker skin tones. In this paper, we show through light transport analysis that the camera modality is fundamentally biased against darker skin tones. We propose to reduce this bias through multi-modal fusion with a complementary and fairer modality - radar. Through a novel debiasing oriented fusion framework, we achieve performance gains over all tested baselines and achieve skin tone fairness improvements over the RGB modality. That is, the associated Pareto frontier between performance and fairness is improved when compared to the RGB modality. In addition, performance improvements are obtained over the radar-based method, with small trade-offs in fairness. We also open-source the largest multi-modal remote heart-rate estimation dataset of paired camera and radar measurements with a focus on skin tone representation.	https://dl.acm.org/doi/abs/10.1145/3528223.3530161	Alexander Vilesov, Pradyumna Chari, Adnan Armouti, Anirudh Bindiganavale Harish, Kimaya Kulkarni, Ananya Deoghare, Laleh Jalilian, Achuta Kadambi
Boss Baby: Foamy Business	In a world run by babies, the most potent weapon from The Boss Baby: Family Business was, appropriately, the loveable, children's craft - foam. Yet needing to show foam as both a friendly, intimate plaything and a powerful, massively dangerous adversary, we extended our proprietary Material Point Method solver to represent close up interactions retaining the mass of the frothy material as well as creating large scale, explosive volumetric setups. To retain a consistent look, we created a robust geometric surface and textural shader that was able to hold its form whether fitting in a handheld glass or expanding at speeds and scales to fill an entire courtyard.	https://dl.acm.org/doi/abs/10.1145/3532836.3536243	Jinguang Huang, Andre Pradhana, David Chow, KC Ong, Youxi Woo
Building an Illustrated World in The Bad Guys	The artistic style of The Bad Guys is inspired by the strong and simplified details of 2D illustration with its hand-drawn imperfections. Similar to traditional artists, our film needed techniques to selectively apply detail & thoughtfully deconstruct objects, focusing on artistic flexibility while maintaining scalability.	https://dl.acm.org/doi/abs/10.1145/3532836.3536246	Jeff Budsberg, Pablo Valle, Paolo de Guzman
BusLine35A	One city bus, three passengers and a back seat scenario they fail to address.	https://dl.acm.org/doi/abs/10.1145/3512752.3527630	Elena Felici
CCP: Configurable Crowd Profiles	Diversity among agents' behaviors and heterogeneity in virtual crowds in general, is an important aspect of crowd simulation as it is crucial to the perceived realism and plausibility of the resulting simulations. Heterogeneous crowds constitute the pillar in creating numerous real-life scenarios such as museum exhibitions, which require variety in agent behaviors, from basic collision avoidance to more complex interactions both among agents and with environmental features. Most of the existing systems optimize for specific behaviors such as goal seeking, and neglect to take into account other behaviors and how these interact together to form diverse agent profiles. In this paper, we present a RL-based framework for learning multiple agent behaviors concurrently. We optimize the agent policy by varying the importance of the selected behaviors (goal seeking, collision avoidance, interaction with environment, and grouping) while training; essentially we have a reward function that changes dynamically during training. The importance of each separate sub-behavior is added as input to the policy, resulting in the development of a single model capable of capturing as well as enabling dynamic run-time manipulation of agent profiles; thus allowing configurable profiles. Through a series of experiments, we verify that our system provides users with the ability to design virtual scenes; control and mix agent behaviors thus creating personality profiles, and assign different profiles to groups of agents. Moreover, we demonstrate that interestingly the proposed model generalizes to situations not seen in the training data such as a) crowds with higher density, b) behavior weights that are outside the training intervals and c) to scenes with more intricate environment layouts. Code, data and trained policies for this paper are at https://github.com/veupnea/CCP.	https://dl.acm.org/doi/abs/10.1145/3528233.3530712	Andreas Panayiotou, Theodoros Kyriakou, Marilena Lemonari, Yiorgos Chrysanthou, Panayiotis Charalambous
CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions	The success of StyleGAN has enabled unprecedented semantic editing capabilities, on both synthesized and real images. However, such editing operations are either trained with semantic supervision or annotated manually by users. In another development, the CLIP architecture has been trained with internet-scale loose image and text pairings, and has been shown to be useful in several zero-shot learning settings. In this work, we investigate how to effectively link the pretrained latent spaces of StyleGAN and CLIP, which in turn allows us to automatically extract semantically-labeled edit directions from StyleGAN, finding and naming meaningful edit operations, in a fully unsupervised setup, without additional human guidance. Technically, we propose two novel building blocks; one for discovering interesting CLIP directions and one for semantically labeling arbitrary directions in CLIP latent space. The setup does not assume any pre-determined labels and hence we do not require any additional supervised text/attributes to build the editing framework. We evaluate the effectiveness of the proposed method and demonstrate that extraction of disentangled labeled StyleGAN edit directions is indeed possible, revealing interesting and non-trivial edit directions.	https://dl.acm.org/doi/abs/10.1145/3528233.3530747	Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, Peter Wonka
CLIPasso: semantically-aware object sketching	Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of Bézier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.	https://dl.acm.org/doi/abs/10.1145/3528223.3530068	Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, Ariel Shamir
Character articulation through profile curves	Computer animation relies heavily on rigging setups that articulate character surfaces through a broad range of poses. Although many deformation strategies have been proposed over the years, constructing character rigs is still a cumbersome process that involves repetitive authoring of point weights and corrective sculpts with limited and indirect shaping controls. This paper presents a new approach for character articulation that produces detail-preserving deformations fully controlled by 3D curves that profile the deforming surface. Our method starts with a spline-based rigging system in which artists can draw and articulate sparse curvenets that describe surface profiles. By analyzing the layout of the rigged curvenets, we quantify the deformation along each curve side independent of the mesh connectivity, thus separating the articulation controllers from the underlying surface representation. To propagate the curvenet articulation over the character surface, we formulate a deformation optimization that reconstructs surface details while conforming to the rigged curvenets. In this process, we introduce a cut-cell algorithm that binds the curvenet to the surface mesh by cutting mesh elements into smaller polygons possibly with cracks, and then derive a cut-aware numerical discretization that provides harmonic interpolations with curve discontinuities. We demonstrate the expressiveness and flexibility of our method using a series of animation clips.	https://dl.acm.org/doi/abs/10.1145/3528223.3530060	Fernando De Goes, William Sheffler, Kurt Fleischer
Closed-loop control of direct ink writing via reinforcement learning	Enabling additive manufacturing to employ a wide range of novel, functional materials can be a major boost to this technology. However, making such materials printable requires painstaking trial-and-error by an expert operator, as they typically tend to exhibit peculiar rheological or hysteresis properties. Even in the case of successfully finding the process parameters, there is no guarantee of print-to-print consistency due to material differences between batches. These challenges make closed-loop feedback an attractive option where the process parameters are adjusted on-the-fly. There are several challenges for designing an efficient controller: the deposition parameters are complex and highly coupled, artifacts occur after long time horizons, simulating the deposition is computationally costly, and learning on hardware is intractable. In this work, we demonstrate the feasibility of learning a closed-loop control policy for additive manufacturing using reinforcement learning. We show that approximate, but efficient, numerical simulation is sufficient as long as it allows learning the behavioral patterns of deposition that translate to real-world experiences. In combination with reinforcement learning, our model can be used to discover control policies that outperform baseline controllers. Furthermore, the recovered policies have a minimal sim-to-real gap. We showcase this by applying our control policy in-vivo on a single-layer printer using low and high viscosity materials.	https://dl.acm.org/doi/abs/10.1145/3528223.3530144	Michal Piovarči, Michael Foshey, Jie Xu, Timmothy Erps, Vahid Babaei, Piotr Didyk, Szymon Rusinkiewicz, Wojciech Matusik, Bernd Bickel
Clothing Suite: Interactively Design Complex Garments	In this talk, we discuss the proprietary toolset created to move our clothing look-design process from 2D into 3D. The Clothing Suite is an easy to use and interactive toolset to create complex, art-directed garments constructed from woven curves. The Clothing Suite allows artists to build and decorate textiles in real time through a three-dimensional sculptural approach while accommodating a highly deformed animation style. This allows the look-dev artist to design the clothing in 3D space in a fast and highly iterative process instead of relying on refined flat art or complex geometry manually sculpted in the model.	https://dl.acm.org/doi/abs/10.1145/3532836.3536248	Megan Walker, Rachele Bellini, Cory Sivek, Shyh-Chyuan Huang
Clustered vector textures	Repetitive vector patterns are common in a variety of applications but can be challenging and tedious to create. Existing automatic synthesis methods target relatively simple, unstructured patterns such as discrete elements and continuous Bézier curves. This paper proposes an algorithm for generating vector patterns with diverse shapes and structured local interactions via a sample-based representation. Our main idea is adding explicit clustering as part of neighborhood similarity and iterative sample optimization for more robust sample synthesis and pattern reconstruction. The results indicate that our method can outperform existing methods on synthesizing a variety of structured vector textures. Our project page is available at https://phtu-cs.github.io/cvt-sig22/.	https://dl.acm.org/doi/abs/10.1145/3528223.3530062	Peihan Tu, Li-Yi Wei, Matthias Zwicker
Colors and shapes	"In this music video for ""Colors and Shapes,"" Hornet Director Sam Mason pays tribute to Mac Miller with a surreal & stirring film commissioned by Miller's family. Following Mac Miller's dog Ralphie as he embarks on a quest into unconsciousness, the film is a dream-scape portrait with abstract yet realistic CG animation, ethereal transitions, and a genuine, heartfelt story."	https://dl.acm.org/doi/abs/10.1145/3512752.3528382	Sam Mason
Compact Poisson Filters for Fast Fluid Simulation	Poisson equations appear in many graphics settings including, but not limited to, physics-based fluid simulation. Numerical solvers for such problems strike context-specific memory, performance, stability and accuracy trade-offs. We propose a new Poisson filter-based solver that balances between the strengths of spectral and iterative methods. We derive universal Poisson kernels for forward and inverse Poisson problems, leveraging careful adaptive filter truncation to localize their extent, all while maintaining stability and accuracy. Iterative composition of our compact filters improves solver iteration time by orders-of-magnitude compared to optimized linear methods. While motivated by spectral formulations, we overcome important limitations of spectral methods while retaining many of their desirable properties. We focus on the application of our method to high-performance and high-fidelity fluid simulation, but we also demonstrate its broader applicability. We release our source code at https://github.com/Ubisoft-LaForge/CompactPoissonFilters .	https://dl.acm.org/doi/abs/10.1145/3528233.3530737	Amir Hossein Rabbani, Jean-Philippe Guertin, Damien Rioux-Lavoie, Arnaud Schoentgen, Kaitai Tong, Alexandre Sirois-Vigneux, Derek Nowrouzezahrai
Comparison of single image HDR reconstruction methods — the caveats of quality assessment	As the problem of reconstructing high dynamic range (HDR) images from a single exposure has attracted much research effort, it is essential to provide a robust protocol and clear guidelines on how to evaluate and compare new methods. In this work, we compared six recent single image HDR reconstruction (SI-HDR) methods in a subjective image quality experiment on an HDR display. We found that only two methods produced results that are, on average, more preferred than the unprocessed single exposure images. When the same methods are evaluated using image quality metrics, as typically done in papers, the metric predictions correlate poorly with subjective quality scores. The main reason is a significant tone and color difference between the reference and reconstructed HDR images. To improve the predictions of image quality metrics, we propose correcting for the inaccuracies of the estimated camera response curve before computing quality values. We further analyze the sources of prediction noise when evaluating SI-HDR methods and demonstrate that existing metrics can reliably predict only large quality differences.	https://dl.acm.org/doi/abs/10.1145/3528233.3530729	Param Hanji, Rafal Mantiuk, Gabriel Eilertsen, Saghi Hajisharif, Jonas Unger
Compatible intrinsic triangulations	Finding distortion-minimizing homeomorphisms between surfaces of arbitrary genus is a fundamental task in computer graphics and geometry processing. We propose a simple method utilizing intrinsic triangulations, operating directly on the original surfaces without going through any intermediate domains such as a plane or a sphere. Given two models A and B as triangle meshes, our algorithm constructs a (CIT), a pair of intrinsic triangulations over A and B with full correspondences in their vertices, edges and faces. Such a tessellation allows us to establish consistent images of edges and faces of A's input mesh over B (and vice versa) by tracing piecewise-geodesic paths over A and B. Our algorithm for constructing CITs, primarily consisting of carefully designed edge flipping schemes, is empirical in nature without any guarantee of success, but turns out to be robust enough to be used within a similar second-order optimization framework as was used previously in the literature. The utility of our method is demonstrated through comparisons and evaluation on a standard benchmark dataset.	https://dl.acm.org/doi/abs/10.1145/3528223.3530175	Kenshi Takayama
ComplexGen: CAD reconstruction by B-rep chain complex generation	We view the reconstruction of CAD models in the boundary representation (B-Rep) as the detection of geometric primitives of different orders, , vertices, edges and surface patches, and the correspondence of primitives, which are holistically modeled as a chain complex, and show that by modeling such comprehensive structures more complete and regularized reconstructions can be achieved. We solve the complex generation problem in two steps. First, we propose a novel neural framework that consists of a sparse CNN encoder for input point cloud processing and a tri-path transformer decoder for generating geometric primitives and their mutual relationships with estimated probabilities. Second, given the probabilistic structure predicted by the neural network, we recover a definite B-Rep chain complex by solving a global optimization maximizing the likelihood under structural validness constraints and applying geometric refinements. Extensive tests on large scale CAD datasets demonstrate that the modeling of B-Rep chain complex structure enables more accurate detection for learning and more constrained reconstruction for optimization, leading to structurally more faithful and complete CAD B-Rep models than previous results.	https://dl.acm.org/doi/abs/10.1145/3528223.3530078	Haoxiang Guo, Shilin Liu, Hao Pan, Yang Liu, Xin Tong, Baining Guo
Compression and Interactive Visualization of Terabyte Scale Volumetric RGBA Data with Voxel-scale Details	We present a compressed volumetric data structure and traversal algorithm that interactively visualizes complete terabyte-scale scientific data. Previous methods rely on heavy approximation and do not provide individual sample-level representation when going beyond gigabytes. We develop an extensible pipeline that makes the data streamable on GPU using compact pointers and a compression algorithm based on wavelet transform. The resulting approach renders high-resolution captures under varying sampling characteristics in real-time.	https://dl.acm.org/doi/abs/10.1145/3532719.3543256	Mehmet Oguz Derin, Takahiro Harada, Yusuke Takeda, Yasuhiro Iba
Computational design of high-level interlocking puzzles	Interlocking puzzles are intriguing geometric games where the puzzle pieces are held together based on their geometric arrangement, preventing the puzzle from falling apart. , or simply , interlocking puzzles are a subclass of interlocking puzzles that require multiple moves to take out the first subassembly from the puzzle. Solving a high-level interlocking puzzle is a challenging task since one has to explore many different configurations of the puzzle pieces until reaching a configuration where the first subassembly can be taken out. Designing a high-level interlocking puzzle with a user-specified level of difficulty is even harder since the puzzle pieces have to be interlocking in all the configurations before the first subassembly is taken out. In this paper, we present a computational approach to design high-level interlocking puzzles. The core idea is to represent all possible configurations of an interlocking puzzle as well as transitions among these configurations using a rooted, undirected graph called a and leverage this graph to find a disassembly plan that requires a minimal number of moves to take out the first subassembly from the puzzle. At the design stage, our algorithm iteratively constructs the geometry of each puzzle piece to expand the disassembly graph incrementally, aiming to achieve a user-specified level of difficulty. We show that our approach allows efficient generation of high-level interlocking puzzles of various shape complexities, including new solutions not attainable by state-of-the-art approaches.	https://dl.acm.org/doi/abs/10.1145/3528223.3530071	Rulin Chen, Ziqi Wang, Peng Song, Bernd Bickel
Computational design of passive grippers	This work proposes a novel generative design tool for passive grippers---robot end effectors that have no additional actuation and instead leverage the existing degrees of freedom in a robotic arm to perform grasping tasks. Passive grippers are used because they offer interesting trade-offs between cost and capabilities. However, existing designs are limited in the types of shapes that can be grasped. This work proposes to use rapid-manufacturing and design optimization to expand the space of shapes that can be passively grasped. Our novel generative design algorithm takes in an object and its positioning with respect to a robotic arm and generates a 3D printable passive gripper that can stably pick the object up. To achieve this, we address the key challenge of jointly optimizing the shape and the insert trajectory to ensure a passively stable grasp. We evaluate our method on a testing suite of 22 objects (23 experiments), all of which were evaluated with physical experiments to bridge the virtual-to-real gap. Code and data are at https://homes.cs.washington.edu/~milink/passive-gripper/	https://dl.acm.org/doi/abs/10.1145/3528223.3530162	Milin Kodnongbua, Ian Good, Yu Lou, Jeffrey Lipton, Adriana Schulz
Computational pattern making from 3D garment models	We propose a method for computing a sewing pattern of a given 3D garment model. Our algorithm segments an input 3D garment shape into patches and computes their 2D parameterization, resulting in pattern pieces that can be cut out of fabric and sewn together to manufacture the garment. Unlike the general state-of-the-art approaches for surface cutting and flattening, our method explicitly targets garment fabrication. It accounts for the unique properties and constraints of tailoring, such as seam symmetry, the usage of darts, fabric grain alignment, and a flattening distortion measure that models woven fabric deformation, respecting its anisotropic behavior. We bootstrap a recent patch layout approach developed for quadrilateral remeshing and adapt it to the purpose of computational pattern making, ensuring that the deformation of each pattern piece stays within prescribed bounds of cloth stress. While our algorithm can automatically produce the sewing patterns, it is fast enough to admit user input to creatively iterate on the pattern design. Our method can take several target poses of the 3D garment into account and integrate them into the sewing pattern design. We demonstrate results on both skintight and loose garments, showcasing the versatile application possibilities of our approach.	https://dl.acm.org/doi/abs/10.1145/3528223.3530145	Nico Pietroni, Corentin Dumery, Raphael Falque, Mark Liu, Teresa Vidal-Calleja, Olga Sorkine-Hornung
Computing sparse integer-constrained cones for conformal parameterizations	We propose a novel method to generate sparse integer-constrained cone singularities with low distortion constraints for conformal parameterizations. Inspired by [Fang et al. 2021; Soliman et al. 2018], the cone computation is formulated as a constrained optimization problem, where the objective is the number of cones measured by the -norm of Gaussian curvature of vertices, and the constraint is to restrict the cone angles to be multiples of /2 and control the distortion while ensuring that the Yamabe equation holds. Besides, the holonomy angles for the non-contractible homology loops are additionally required to be multiples of /2 for achieving rotationally seamless conformal parameterizations. The Douglas-Rachford (DR) splitting algorithm is used to solve this challenging optimization problem, and our success relies on two key components. First, replacing each integer constraint with the intersection of a box set and a sphere enables us to manage the subproblems in DR splitting update steps in the continuous domain. Second, a novel solver is developed to optimize the -norm without any approximation. We demonstrate the effectiveness and feasibility of our algorithm on a data set containing 3885 models. Compared to state-of-the-art methods, our method achieves a better tradeoff between the number of cones and the parameterization distortion.	https://dl.acm.org/doi/abs/10.1145/3528223.3530118	Mo Li, Qing Fang, Wenqing Ouyang, Ligang Liu, Xiao-Ming Fu
Contact and friction simulation for computer graphics	Efficient simulation of contact is of interest for numerous physics-based animation applications. For instance, virtual reality training, video games, rapid digital prototyping, and robotics simulation are all examples of applications that involve contact modeling and simulation. However, despite its extensive use in modern computer graphics, contact simulation remains one of the most challenging problems in physics-based animation. This course covers fundamental topics on the nature of contact modeling and simulation for computer graphics. Specifically, we provide mathematical details about formulating contact as a complementarity problem in rigid body and soft body animations. We briefly cover several approaches for contact generation using discrete collision detection. Then, we present a range of numerical techniques for solving the associated LCPs and NCPs. The advantages and disadvantages of each technique are further discussed in a practical manner, and best practices for implementation are discussed. Finally, we conclude the course with several advanced topics such as methods for soft body contact problems, barrier functions, and anisotropic friction modeling. Programming examples are provided in our appendix as well as on the course website to accompany the course notes.	https://dl.acm.org/doi/abs/10.1145/3532720.3535640	Sheldon Andrews, Kenny Erleben, Zachary Ferguson
Contact-centric deformation learning	We propose a novel method to machine-learn highly detailed, nonlinear contact deformations for real-time dynamic simulation. We depart from previous deformation-learning strategies, and model contact deformations in a contact-centric manner. This strategy shows excellent generalization with respect to the object's configuration space, and it allows for simple and accurate learning. We complement the contact-centric learning strategy with two additional key ingredients: learning a continuous vector field of contact deformations, instead of a discrete approximation; and sparsifying the mapping between the contact configuration and contact deformations. These two ingredients further contribute to the accuracy, efficiency, and generalization of the method. We integrate our learning-based contact deformation model with subspace dynamics, showing real-time dynamic simulations with fine contact deformation detail.	https://dl.acm.org/doi/abs/10.1145/3528223.3530182	Cristian Romero, Dan Casas, Maurizio M. Chiaramonte, Miguel A. Otaduy
Context-aware Risk Degree Prediction for Smartphone Zombies	"Using smartphones while walking is becoming a social problem. Recent works try to support this issue by different warning systems. However, most only focus on detecting obstacles, without considering the risk to the user. In this paper, we propose a deep learning-based context-aware risk prediction system using a built-in camera on smartphones, aiming to notify ""smombies"" by a risk-degree based algorithm. The proposed system both estimates the risk degree of a potential obstacle and the user's status, which can also be used for distracted driving or visually impaired people."	https://dl.acm.org/doi/abs/10.1145/3532719.3543197	Erwin Wu, Chen-Chieh Liao, Ruofan Liu, Hideki Koike
Course on virtual nature as a digital twin: botanically correct 3D AR and VR optimized low-polygon and photogrammetry high-polygon plant models	The student of this course should already know how to use 3D modeling software to create FBX files. This Course expands on a short overview presented in the Educators' Forum. It is different in several ways. First presented is the overview of the context and justification of why botanically accurate plants and landscapes are important for educational applications, such as for use in museums, arboretums, and field trip experiences to botanical gardens. Connected to that goal is the importance of accuracy in visualization of not only the plant, but the entire virtual model of the landscape using plant inventory data and plant population density geographical information system (GIS) data. This then touches on the important issues for digital twins, as models and simulations of reality. Educational applications are different than entertainment application in the dimensions of information fidelity, or the trustworthiness of the presentation, and also the graphical fidelity, or the photorealistic capacity of the rendering systems. These are not always the same. High graphical fidelity is a byproduct of high information fidelity; the reverse is not always true. High graphical fidelity enhances information fidelity, if used for that purpose. Two immersive informal learning applications use cases are presented, one for augmented reality (AR), and the other as a virtual reality (VR) example. Both models used the same design and development process, integrating domain expertise, the botanist and the ecologist, with the art and software team to enhance the accuracy. Co-design, highly iterative review process, removes errors in educational content, and representation, as well as usability and may be generalized to any domain when learning is a goal of a digital twin. In this work it is referred to as the Expert-Learner-User-Experience (ELUX) design process. Game engines, as general purpose visualization tools, make multimodal and interaction possible to enhance user experiences and to make semantic material accessible to the learner. The technical constrains on the application design demanded two production pipelines. The AR and VR pipeline required low-polygon models for performance, and the newly released Unreal Engine 5 and Reality Capture created an opportunity to increase the graphical fidelity and the information fidelity of the plants and models. Virtual nature construction methods are covered in two processes, first with low-polygon 3D plant models ideal for AR and VR, and the second with high-polygon 3D plant models using Unreal Engine 5 and Reality Capture. As highly accurate 3D plant models, combined with stat of the art rendering for photorealistic models, when combined with GIS geospatial datasets, and visualized in immersive devices, digital twins of the natural world become possible. Once these models are connected to mathematical models of the natural world, dynamics driven by real time data feeds, and forecasts, both back in time and forward into the future will enhance our understanding of the natural world, and how it interacts with that of the artificial man-made world.	https://dl.acm.org/doi/abs/10.1145/3532720.3535663	Maria C. R. Harrington, Chris Jones, Crissy Peters
Covector fluids	The animation of delicate vortical structures of gas and liquids has been of great interest in computer graphics. However, common velocity-based fluid solvers can damp the vortical flow, while vorticity-based fluid solvers suffer from performance drawbacks. We propose a new velocity-based fluid solver derived from a reformulated Euler equation using covectors. Our method generates rich vortex dynamics by an advection process that respects the The numerical algorithm requires only a small local adjustment to existing advection-projection methods and can easily leverage recent advances therein. The resulting solver emulates a vortex method without the expensive conversion between vortical variables and velocities. We demonstrate that our method preserves vorticity in both vortex filament dynamics and turbulent flows significantly better than previous methods, while also improving preservation of energy.	https://dl.acm.org/doi/abs/10.1145/3528223.3530120	Mohammad Sina Nabizadeh, Stephanie Wang, Ravi Ramamoorthi, Albert Chern
Cracking the Snake Code on The Bad Guys	A snake is simply a tube and doesn't have shoulders or hips that are traditionally complicated to rig on human characters. However, rigging a snake is complicated enough even just for slithering actions. Mr. Snake in the film The Bad Guys uses its tail like an arm and acts like a human, walks like a human, and even plays a guitar! This talk presents the wide variety of rigging techniques that were used to achieve the unique cartoony actions of Mr. Snake.	https://dl.acm.org/doi/abs/10.1145/3532836.3536249	Yukinori Inagaki, Evan Boucher, Min Hong, Ludovic Bouancheau, Fredrik Nilsson
Creating Life-like Autonomous Agents for Real-time Interactive Installations	This talk briefly describes the implementation of a complex virtual ecosystem of autonomous agents for the purpose of an art installation. The autonomous agents, called Aerobes, are inspired by the lifecycle of the Aurelia sp. jellyfish, and use artificial life techniques to simulate the behavior of two distinct types of organisms. We describe the process of using ethological research of organisms to design an artificial life system in a way that both creates a cohesive simulacrum of life-like behavior and allows for compelling interactions with audiences. We created complex behaviors for the Aerobes using low-level schemata that encapsulate individual goal-directed behaviors, and combined schemata to build behaviors that appear biomimetic. In order to give each agent the appearance of individuation, we mapped the underlying parameters of individual schemata and behaviors to personality traits to create a cohesive psychographic resource for autonomous agents that allowed for variance in decision-making and behaviors without additional computational complexity. The final artificial life system was then used to control the Aerobes in In Love With The World, a public art installation hosted at the Tate Modern's Turbine Hall for four months.	https://dl.acm.org/doi/abs/10.1145/3532836.3536255	Nathan Lachenmyer, Sadiya Akasha
Creating a Planet and Clouds Lightyears Away	For Lightyear, Space Ranger Buzz Lightyear's exploits take him on a journey around the planet, T'Kani Prime and its neighboring star. In order to create realistic planets as seen from his star ship, we built a new workflow for creating procedural planet terrains and volumetric clouds as seen from space. These techniques needed to produce realistic results but also be highly art directable to help the audience believe Buzz could get to Infinity and Beyond.	https://dl.acm.org/doi/abs/10.1145/3532836.3536268	Laura K. Murphy, Joshua Jenny, Michael O'Brien, Colin Thompson
Custom landmarkers: building location based AR with lens studio	Today we're excited to walk you through creating location based Augmented Reality (AR) content with Snap's Lens Studio - Snaps augmented reality content authoring tool. We'll discuss what Location Based AR is, the design challenges it presents and demonstrate how you can use Lens Studio to address and solve many of these challenges.	https://dl.acm.org/doi/abs/10.1145/3532720.3535644	Hammad Bashir, Chris Reilly, Callie Holderman
DCT-net: domain-calibrated translation for portrait stylization	"This paper introduces DCT-Net, a novel image translation architecture for few-shot portrait stylization. Given limited style exemplars (~100), the new architecture can produce high-quality style transfer results with advanced ability to synthesize high-fidelity contents and strong generality to handle complicated scenes (e.g., occlusions and accessories). Moreover, it enables full-body image translation via one elegant evaluation network trained by partial observations (i.e., stylized heads). Few-shot learning based style transfer is challenging since the learned model can easily become overfitted in the target domain, due to the biased distribution formed by only a few training examples. This paper aims to handle the challenge by adopting the key idea of ""calibration first, translation later"" and exploring the augmented global structure with locally-focused translation. Specifically, the proposed DCT-Net consists of three modules: a content adapter borrowing the powerful prior from source photos to calibrate the content distribution of target samples; a geometry expansion module using affine transformations to release spatially semantic constraints; and a texture translation module leveraging samples produced by the calibrated distribution to learn a fine-grained conversion. Experimental results demonstrate the proposed method's superiority over the state of the art in head stylization and its effectiveness on full image translation with adaptive deformations. Our code is publicly available at https://github.com/menyifang/DCT-Net."	https://dl.acm.org/doi/abs/10.1145/3528223.3530159	Yifang Men, Yuan Yao, Miaomiao Cui, Zhouhui Lian, Xuansong Xie
DEF: deep estimation of sharp geometric features in 3D shapes	We propose Deep Estimators of Features (DEFs), a learning-based framework for predicting sharp geometric features in sampled 3D shapes. Differently from existing data-driven methods, which reduce this problem to feature classification, we propose to representing the distance from point samples to the closest feature line on Our approach is the first that scales to massive point clouds by fusing distance-to-feature estimates obtained on individual patches. We extensively evaluate our approach against related state-of-the-art methods on newly proposed synthetic and real-world 3D CAD model benchmarks. Our approach not only outperforms these (with improvements in Recall and False Positives Rates), but generalizes to real-world scans after training our model on synthetic data and fine-tuning it on a small dataset of scanned data. We demonstrate a downstream application, where we reconstruct an explicit representation of straight and curved sharp feature lines from range scan data. We make code, pre-trained models, and our training and evaluation datasets available at https://github.com/artonson/def.	https://dl.acm.org/doi/abs/10.1145/3528223.3530140	Albert Matveev, Ruslan Rakhimov, Alexey Artemov, Gleb Bobrovskikh, Vage Egiazarian, Emil Bogomolov, Daniele Panozzo, Denis Zorin, Evgeny Burnaev
DR.JIT: a just-in-time compiler for differentiable rendering	DR.JIT is a new just-in-time compiler for physically based rendering and its derivative. DR.JIT expedites research on these topics in two ways: first, it traces high-level simulation code (e.g., written in Python) and aggressively simplifies and specializes the resulting program representation, producing data-parallel kernels with state-of-the-art performance on CPUs and GPUs. Second, it simplifies the development of differentiable rendering algorithms. Efficient methods in this area turn the derivative of a simulation into a simulation of the derivative. DR.JIT provides fine-grained control over the process of automatic differentiation to help with this transformation. Specialization is particularly helpful in the context of differentiation, since large parts of the simulation ultimately do not influence the computed gradients. DR.JIT tracks data dependencies globally to find and remove redundant computation.	https://dl.acm.org/doi/abs/10.1145/3528223.3530099	Wenzel Jakob, Sébastien Speierer, Nicolas Roussel, Delio Vicini
Dark stereo: improving depth perception under low luminance	It is often desirable or unavoidable to display Virtual Reality (VR) or stereoscopic content at low brightness. For example, a dimmer display reduces the flicker artefacts that are introduced by low-persistence VR headsets. It also saves power, prolongs battery life, and reduces the cost of a display or projection system. Additionally, stereo movies are usually displayed at relatively low luminance due to polarization filters or other optical elements necessary to separate two views. However, the binocular depth cues become less reliable at low luminance. In this paper, we propose a model of stereo constancy that predicts the precision of binocular depth cues for a given contrast and luminance. We use the model to design a novel contrast enhancement algorithm that compensates for the deteriorated depth perception to deliver good-quality stereoscopic images even for displays of very low brightness.	https://dl.acm.org/doi/abs/10.1145/3528223.3530136	Krzysztof Wolski, Fangcheng Zhong, Karol Myszkowski, Rafał K. Mantiuk
Deep Compliant Control	In many physical interactions such as opening doors and playing sports, humans act compliantly to move in various ways to avoid large impacts or to manipulate objects. This paper aims to build a framework for simulation and control of humanoids that creates physically compliant interactions with surroundings. We can generate a broad spectrum of movements ranging from passive reactions to external physical perturbations, to active manipulations with clear intentions. Technical challenges include defining compliance, reproducing physically reliable movements, and robustly controlling under-actuated dynamical systems. The key technical contribution is a two-level control architecture based on deep reinforcement learning that imitates human movements while adjusting their bodies to external perturbations. The controller minimizes the interaction forces and the control torques for imitation, and we demonstrate the effectiveness of the controller with various motor skills including opening doors, balancing a ball, and running hand in hand.	https://dl.acm.org/doi/abs/10.1145/3528233.3530719	Seunghwan Lee, Phil Sik Chang, Jehee Lee
Deep Deformable 3D Caricatures with Learned Shape Control	A 3D caricature is an exaggerated 3D depiction of a human face. The goal of this paper is to model the variations of 3D caricatures in a compact parameter space so that we can provide a useful data-driven toolkit for handling 3D caricature deformations. To achieve the goal, we propose an MLP-based framework for building a deformable surface model, which takes a latent code and produces a 3D surface. In the framework, a SIREN MLP models a function that takes a 3D position on a fixed template surface and returns a 3D displacement vector for the input position. We create variations of 3D surfaces by learning a hypernetwork that takes a latent code and produces the parameters of the MLP. Once learned, our deformable model provides a nice editing space for 3D caricatures, supporting label-based semantic editing and point-handle-based deformation, both of which produce highly exaggerated and natural 3D caricature shapes. We also demonstrate other applications of our deformable model, such as automatic 3D caricature creation. Our code and supplementary materials are available at https://github.com/ycjungSubhuman/DeepDeformable3DCaricatures.	https://dl.acm.org/doi/abs/10.1145/3528233.3530748	Yucheol Jung, Wonjong Jang, Soongjin Kim, Jiaolong Yang, Xin Tong, Seungyong Lee
Deep Learned Face Swapping in Feature Film Production	In visual effects for film, replacement of stunt performers' facial likeness for their doubled actor counterparts using traditional computer graphics methods is a multi-stage, labor intensive task. Recently, deep learning techniques have made a compelling argument to train neural networks to learn how to take an image of a person's face and convincingly infer a rendered image of a second person's face with a previously unseen perspective, pose and lighting environment. A novel method is discussed for bringing deep neural network face swapping to feature film production which utilizes facial recognition for the discovery of training data. Our method further innovates in the area of utilizing traditional CG assets for informing some of the shortcomings of ML techniques. Connected with a technique for feature engineering during training dataset assembly, our Face Fabrication System enables Wētā FX to deliver final picture quality for use in production.	https://dl.acm.org/doi/abs/10.1145/3532836.3536271	Jonathan Swartz, Sean Walker
DeepFaceVideoEditing: sketch-based deep editing of face videos	Sketches, which are simple and concise, have been used in recent deep image synthesis methods to allow intuitive generation and editing of facial images. However, it is nontrivial to extend such methods to video editing due to various challenges, ranging from appropriate manipulation propagation and fusion of multiple editing operations to ensure temporal coherence and visual quality. To address these issues, we propose a novel sketch-based facial video editing framework, in which we represent editing manipulations in latent space and propose specific propagation and fusion modules to generate high-quality video editing results based on StyleGAN3. Specifically, we first design an optimization approach to represent sketch editing manipulations by editing vectors, which are propagated to the whole video sequence using a proper strategy to cope with different editing needs. Specifically, input editing operations are classified into two categories: temporally consistent editing and temporally variant editing. The former (e.g., change of face shape) is applied to the whole video sequence directly, while the latter (e.g., change of facial expression or dynamics) is propagated with the guidance of expression or only affects adjacent frames in a given time window. Since users often perform different editing operations in multiple frames, we further present a region-aware fusion approach to fuse diverse editing effects. Our method supports video editing on facial structure and expression movement by sketch, which cannot be achieved by previous works. Both qualitative and quantitative evaluations show the superior editing ability of our system to existing and alternative solutions.	https://dl.acm.org/doi/abs/10.1145/3528223.3530056	Feng-Lin Liu, Shu-Yu Chen, Yu-Kun Lai, Chunpeng Li, Yue-Ren Jiang, Hongbo Fu, Lin Gao
DeepPhase: periodic autoencoders for learning motion phase manifolds	Learning the spatial-temporal structure of body movements is a fundamental problem for character motion synthesis. In this work, we propose a novel neural network architecture called the Periodic Autoencoder that can learn periodic features from large unstructured motion datasets in an unsupervised manner. The character movements are decomposed into multiple latent channels that capture the non-linear periodicity of different body segments while progressing forward in time. Our method extracts a multi-dimensional phase space from full-body motion data, which effectively clusters animations and produces a manifold in which computed feature distances provide a better similarity measure than in the original motion space to achieve better temporal and spatial alignment. We demonstrate that the learned periodic embedding can significantly help to improve neural motion synthesis in a number of tasks, including diverse locomotion skills, style-based movements, dance motion synthesis from music, synthesis of dribbling motions in football, and motion query for matching poses within large animation databases.	https://dl.acm.org/doi/abs/10.1145/3528223.3530178	Sebastian Starke, Ian Mason, Taku Komura
Delirious Departures	Delirious Departures is a VR experience in the form of an installation/performance. It questions the nature of travelling and our relationship to others. It explores the desire and impossibility of travelling, of leaving meaningfully in times of pandemic. The participant moves through impressive environments, based on iconic Belgian railway stations. An actor acts as guide, antagonist and director of the journey. The mocaped actor mixes with canned animations, intelligent avatars and figures in fixed poses, in various degrees of abstraction. Who is real, what is live? The experience of the participant is visualized in an experimental XR set-up which projects a mix of realtime video and computer graphics. The one-on-one performance builds on animation and crowd simulation technology developed by Inria and Cubic Motion (Epic Games).	https://dl.acm.org/doi/abs/10.1145/3532834.3536222	Eric Joris, Isjtar Vandebroeck
DeltaConv: anisotropic operators for geometric deep learning on point clouds	Learning from 3D point-cloud data has rapidly gained momentum, motivated by the success of deep learning on images and the increased availability of 3D data. In this paper, we aim to construct anisotropic convolution layers that work directly on the surface derived from a point cloud. This is challenging because of the lack of a global coordinate system for tangential directions on surfaces. We introduce DeltaConv, a convolution layer that combines geometric operators from vector calculus to enable the construction of anisotropic filters on point clouds. Because these operators are defined on scalar- and vector-fields, we separate the network into a scalar- and a vector-stream, which are connected by the operators. The vector stream enables the network to explicitly represent, evaluate, and process directional information. Our convolutions are robust and simple to implement and match or improve on state-of-the-art approaches on several benchmarks, while also speeding up training and inference.	https://dl.acm.org/doi/abs/10.1145/3528223.3530166	Ruben Wiersma, Ahmad Nasikun, Elmar Eisemann, Klaus Hildebrandt
Demonstrating poimo as Inflatable, Inclusive Mobility Devices with a Soft Input Interface	In this demo, we showcase poimo, a series of POrtable and Inflatable MObility devices made of a balloon-like soft material called drop-stitch fabric. Poimo has two merits: (1) inflatability of a soft and lightweight body allowing users to deflate and carry it on a small scale and (2) inclusiveness enabling made-to-order design and fabrication of mobility devices for each user. As an alternative way to conventional rigid controllers, we also introduce a soft and deformable input interface to navigate a sofa-type poimo. Finally, we report two types of field studies: public users riding on two types of poimos on a flat paved road, and the authors riding on a sofa-type poimo in a traditional European environment with inclined stone-paved streets.	https://dl.acm.org/doi/abs/10.1145/3532721.3544020	Hiroki Sato, Ryuma Niiyama, Koya Narumi, Young ah Seong, Keisuke Watanabe, Ryosuke Yamamura, Yasuaki Kakehi, Yoshihiro Kawahara
Demonstration of Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation	We demonstrate a novel interface concept in which interactive systems directly manipulate the user's head orientation. We implement this using electrical-muscle-stimulation (EMS) of the neck muscles, which turns the head around its yaw (left/right) and pitch (up/down) axis. At SIGGRAPH 2022 Emerging Techinologies, we will demonstrate how this technology enables novel interactions via two example applications: (1) finding different visual targets in mixed reality while the system actuates the user's head orientation to guide their point-of-view; (2) a VR roller coaster application where the user's head nods up as the ride accelerates.	https://dl.acm.org/doi/abs/10.1145/3532721.3535567	Yudai Tanaka, Jun Nishida, Pedro Lopes
Demystifying the Python-Processing Landscape: An Overview of Tools Combining Python and Processing	Processing is composed of a programming language and an editor for writing and compiling code, providing a collection of special commands to draw, animate, and handle user input using Java. Python Mode for Processing (also referred to as Processing.py) leverages Jython, a Java implementation of Python, to interface with Processing's Java core. One can install and activate Python Mode in Processing using a button in the IDE interface. Python Mode enables Python syntax in the IDE (instead of Java) but has its limitations: it is source-compatible with Python 2.7 (not 3+) and does not support CPython libraries (such as NumPy). Several promising new Python-Processing tools have emerged, but this proliferation of alternatives can confuse would-be users. This talk maps out the Python-Processing landscape, offering insight into the different options and providing direction for beginners, teachers, and more accomplished programmers keen to explore Python as a tool for creative coding projects.	https://dl.acm.org/doi/abs/10.1145/3532836.3536231	Tristan Alan Bunn, Taylor Carrasco
Denoising and Guided Upsampling of Monte Carlo Path Traced Low Resolution Renderings	Monte Carlo path tracing generates renderings by estimating the rendering equation using the Monte Carlo method. Studies focus on rendering a noisy image at the original resolution with a low sample per pixel count to decrease the rendering time. Image-space denoising is then applied to produce a visually appealing output. However, denoising process cannot handle the high variance of the noisy image accurately if the sample count is reduced harshly to finish the rendering in a shorter time. We propose a framework that renders the image at a reduced resolution to cast more samples than the harshly lowered sample count in the same time budget. The image is then robustly denoised, and the denoised result is upsampled using original resolution G-buffer of the scene as guidance.	https://dl.acm.org/doi/abs/10.1145/3532719.3543250	Kadir Cenk Alpay, Ahmet Oguz Akyuz
Designing Perceptual Puzzles by Differentiating Probabilistic Programs	"We design new visual illusions by finding ""adversarial examples"" for principled models of human perception — specifically, for probabilistic models, which treat vision as Bayesian inference. To perform this search efficiently, we design a differentiable probabilistic programming language, whose API exposes MCMC inference as a first-class differentiable function. We demonstrate our method by automatically creating illusions for three features of human vision: color constancy, size constancy, and face perception."	https://dl.acm.org/doi/abs/10.1145/3528233.3530715	Kartik Chandra, Tzu-Mao Li, Joshua Tenenbaum, Jonathan Ragan-Kelley
Detecting viewer-perceived intended vector sketch connectivity	Many sketch processing applications target precise vector drawings with accurately specified stroke intersections, yet free-form artist drawn sketches are typically inexact: strokes that are to intersect often stop short of doing so. While human observers easily perceive the artist intended stroke connectivity, manually, or even semi-manually, correcting drawings to generate correctly connected outputs is tedious and highly time consuming. We propose a novel, robust algorithm that extracts viewer-perceived stroke connectivity from inexact free-form vector drawings by leveraging observations about local and global factors that impact human perception of inter-stroke connectivity. We employ the identified local cues to train classifiers that assess the likelihood that pairs of strokes are perceived as forming end-to-end or T- junctions based on local context. We then use these classifiers within an incremental framework that combines classifier provided likelihoods with a more global, contextual and closure-based, analysis. We demonstrate our method on over 95 diversely sourced inputs, and validate it via a series of perceptual studies; participants prefer our outputs over the closest alternative by a factor of 9 to 1.	https://dl.acm.org/doi/abs/10.1145/3528223.3530097	Jerry Yin, Chenxi Liu, Rebecca Lin, Nicholas Vining, Helge Rhodin, Alla Sheffer
Determining the Orientation of Low Resolution Images of a De-Bruijn Tracking Pattern with a CNN	Inside-out optical 2D tracking of tangible objects on a surface oftentimes uses a high-resolution pattern printed on the surface. While De-Bruijn-torus patterns offer maximum information density, their orientation must be known to decode them. Determining the orientation is challenging for patterns with very fine details; traditional algorithms, such as Hough Lines, do not work reliably. We show that a convolutional neural network can reliably determine the orientation of quasi-random bitmaps with 6 × 6 pixels per block within 36 × 36 pixel images taken by a mouse sensor. Mean error rate is below 2°. Furthermore, our model outperformed Hough Lines in a test with arbitrarily rotated low-resolution rectangles. This implies that CNN-based rotation-detection might also be applicable for more general use cases.	https://dl.acm.org/doi/abs/10.1145/3532719.3543259	Andreas Schmid, Raphael Wimmer, Stefan Lippl
Developability-driven piecewise approximations for triangular meshes	We propose a novel method to compute a piecewise mesh with a few developable patches and a small approximation error for an input triangular mesh. Our key observation is that a deformed mesh after enforcing discrete developability is easily partitioned into nearly developable patches. To obtain the nearly developable mesh, we present a new edge-oriented notion of discrete developability to define a developability-encouraged deformation energy, which is further optimized by the block nonlinear Gauss-Seidel method. The key to successfully applying this optimizer is three types of auxiliary variables. Then, a coarse-to-fine segmentation technique is developed to partition the deformed mesh into a small set of nearly discrete developable patches. Finally, we refine the segmented mesh to reduce the discrete Gaussian curvature while keeping the patches smooth and the approximation error small. In practice, our algorithm achieves a favorable tradeoff between the number of developable patches and the approximation error. We demonstrate the feasibility and practicability of our method over various examples, including seventeen physical manufacturing models with paper.	https://dl.acm.org/doi/abs/10.1145/3528223.3530117	Zheng-Yu Zhao, Qing Fang, Wenqing Ouyang, Zheng Zhang, Ligang Liu, Xiao-Ming Fu
Development of Exergame to Resolve Deconditioning in Children with Orthostatic Dysregulation	In this study, we proposed an exergame to reduce resistance to exercise and maintain motivation in adolescents with orthostatic dysregulation. We created a 2D side-scrolling action game in Unity to be played on smartphones. The game is synchronized with exercises that can be done in a lying posture. We analyzed the effect of the game on the feelings of the participants. Our experiments showed that the use of the exergame has a positive effect on the participants' emotions during exercise.	https://dl.acm.org/doi/abs/10.1145/3532719.3543212	Hitomi Miyazaki, Wataru Kurihara, Xu Han, Saki Sakaguchi, Kumiko Kushiyama
Diffeomorphic Neural Surface Parameterization for 3D and Reflectance Acquisition	This paper proposes a simple method which solves the problem of multi-view 3D reconstruction for objects with unknown and generic surface materials, imaged by a freely moving camera and lit by a freely moving point light source. The object can have arbitrary (diffuse or specular) and spatially-varying surface reflectances. Our solution consists of two small-sized neural networks (dubbed the 'Shape-Net' and 'BRDF-Net'), used to parameterize the unknown shape and material map as functions on a canonical surface (e.g. unit sphere). Key to our method is a velocity field shape representation that drives the canonical surface to target shape through time. We show this parameterization can be implemented as a recurrent residual network that is guaranteed to be diffeomorphic and orientation-preserving. Our method yields an exceptionally clean formulation that can be optimized by standard gradient descent without initialization, and works with both near-field and distant light source. Synthetic and real experiments demonstrate the reliability and accuracy of our reconstructions, with extensions including novel-view-synthesis, relighting and material retouching done with ease. Our source codes are available at https://github.com/za-cheng/DNS.	https://dl.acm.org/doi/abs/10.1145/3528233.3530741	Ziang Cheng, Hongdong Li, Richard Hartley, Yinqiang Zheng, Imari Sato
Differentiable signed distance function rendering	Physically-based differentiable rendering has recently emerged as an attractive new technique for solving inverse problems that recover complete 3D scene representations from images. The inversion of shape parameters is of particular interest but also poses severe challenges: shapes are intertwined with visibility, whose discontinuous nature introduces severe bias in computed derivatives unless costly precautions are taken. Shape representations like triangle meshes suffer from additional difficulties, since the continuous optimization of mesh parameters cannot introduce topological changes. One common solution to these difficulties entails representing shapes using signed distance functions (SDFs) and gradually adapting their zero level set during optimization. Previous differentiable rendering of SDFs did not fully account for visibility gradients and required the use of mask or silhouette supervision, or discretization into a triangle mesh. In this article, we show how to extend the commonly used sphere tracing algorithm so that it additionally outputs a reparameterization that provides the means to compute accurate shape parameter derivatives. At a high level, this resembles techniques for differentiable mesh rendering, though we show that the SDF representation admits a particularly efficient reparameterization that outperforms prior work. Our experiments demonstrate the reconstruction of (synthetic) objects without complex regularization or priors, using only a per-pixel RGB loss.	https://dl.acm.org/doi/abs/10.1145/3528223.3530139	Delio Vicini, Sébastien Speierer, Wenzel Jakob
Disentangling random and cyclic effects in time-lapse sequences	"Time-lapse image sequences offer visually compelling insights into dynamic processes that are too slow to observe in real time. However, playing a long time-lapse sequence back as a video often results in distracting flicker due to random effects, such as weather, as well as cyclic effects, such as the day-night cycle. We introduce the problem of disentangling time-lapse sequences in a way that allows separate, after-the-fact control of overall trends, cyclic effects, and random effects in the images, and describe a technique based on data-driven generative models that achieves this goal. This enables us to ""re-render"" the sequences in ways that would not be possible with the input images alone. For example, we can stabilize a long sequence to focus on plant growth over many months, under selectable, consistent weather. Our approach is based on Generative Adversarial Networks (GAN) that are conditioned with the time coordinate of the time-lapse sequence. Our architecture and training procedure are designed so that the networks learn to model random variations, such as weather, using the GAN's latent space, and to disentangle overall trends and cyclic variations by feeding the conditioning time label to the model using Fourier features with specific frequencies. We show that our models are robust to defects in the training data, enabling us to amend some of the practical difficulties in capturing long time-lapse sequences, such as temporary occlusions, uneven frame spacing, and missing frames."	https://dl.acm.org/doi/abs/10.1145/3528223.3530170	Erik Härkönen, Miika Aittala, Tuomas Kynkäänniemi, Samuli Laine, Timo Aila, Jaakko Lehtinen
Do We Measure What We Perceive? Comparison of Perceptual and Computed Differences between Hand Animations	An increased interest in public motion capture data has allowed for the use of data-driven animation algorithms through neural networks. While motion capture data is increasingly accessible, data sets have become too large to sort through manually. Similarity metrics quantify how different two motions are and can be used to search databases much faster when compared to manual searches as well as to train neural networks. However, the most popular similarity metrics are not informed by human perception, resulting in the potential for data that is not perceptually similar being labeled as such by these metrics. We conducted an experiment with hand motions to identify how large the differences between human perception and common similarity metrics are. In this study, participants watched two animations of hand motions, one altered and the other unaltered, and scored their similarity on a 7-point Likert scale. In our comparisons, we found that none of the tested similarity metrics correlated with human judged scores of similarity.	https://dl.acm.org/doi/abs/10.1145/3532719.3543233	Jacob Justice, Alex Adkins, Tommy Dong, Sophie Jörg
Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning	In this work, we tackle the challenging problem of arbitrary image style transfer using a novel style feature representation learning method. A suitable style representation, as a key component in image stylization tasks, is essential to achieve satisfactory results. Existing deep neural network based approaches achieve reasonable results with the guidance from second-order statistics such as Gram matrix of content features. However, they do not leverage sufficient style information, which results in artifacts such as local distortions and style inconsistency. To address these issues, we propose to learn style representation directly from image features instead of their second-order statistics, by analyzing the similarities and differences between multiple styles and considering the style distribution. Specifically, we present Contrastive Arbitrary Style Transfer (CAST), which is a new style representation learning and style transfer method via contrastive learning. Our framework consists of three key components, i.e., a multi-layer style projector for style code encoding, a domain enhancement module for effective learning of style distribution, and a generative network for image style transfer. We conduct qualitative and quantitative evaluations comprehensively to demonstrate that our approach achieves significantly better results compared to those obtained via state-of-the-art methods. Code and models are available at https://github.com/zyxElsa/CAST_pytorch.	https://dl.acm.org/doi/abs/10.1145/3528233.3530736	Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Changsheng Xu
Drivable Volumetric Avatars using Texel-Aligned Features	Photorealistic telepresence requires both high-fidelity body modeling and faithful driving to enable dynamically synthesized appearance that is indistinguishable from reality. In this work, we propose an end-to-end framework that addresses two core challenges in modeling and driving full-body avatars of real people. One challenge is driving an avatar while staying faithful to details and dynamics that cannot be captured by a global low-dimensional parameterization such as body pose. Our approach supports driving of clothed avatars with wrinkles and motion that a real driving performer exhibits beyond the training corpus. Unlike existing global state representations or non-parametric screen-space approaches, we introduce texel-aligned features—a localised representation which can leverage both the structural prior of a skeleton-based parametric model and observed sparse image signals at the same time. Another challenge is modeling a temporally coherent clothed avatar, which typically requires precise surface tracking. To circumvent this, we propose a novel volumetric avatar representation by extending mixtures of volumetric primitives to articulated objects. By explicitly incorporating articulation, our approach naturally generalizes to unseen poses. We also introduce a localized viewpoint conditioning, which leads to a large improvement in generalization of view-dependent appearance. The proposed volumetric representation does not require high-quality mesh tracking as a prerequisite and brings significant quality improvements compared to mesh-based counterparts. In our experiments, we carefully examine our design choices and demonstrate the efficacy of our approach, outperforming the state-of-the-art methods on challenging driving scenarios.	https://dl.acm.org/doi/abs/10.1145/3528233.3530740	Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Chenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo, Zhe Cao, Fabian Prada, Jason Saragih, Yaser Sheikh
Dual Robot Avatar: Real-time Multispace Experience using Telepresence Robots and Walk Sensation Feedback including Viewpoint Sharing for Immersive Virtual Tours	Traveling to different places simultaneously is a dream for several people, but it is difficult to realize this aspiration because of our physical space limits. On one hand, virtual reality technologies can help alleviate such limits. According to the best of the authors' knowledge, there is no study attempt to operate multiple telepresence robots in remote places simultaneously, with presenting walk sensation feedback to the operator for an immersive multispace experience. In this study, we used autonomous mobile robots; a dog and wheel type one, where their movements' direction can be controlled by an operator (Fig. 1). The operator can alternatively choose/re-choose the space (or robot) to attend and can move the viewpoint using a head-mounted display (HMD) controller. A live video image with 4 K resolution is transmitted to the HMD via web real-time communication (WebRTC) network from a 360° camera placed to the top of each robot. The operator perceives viewpoint movement feedback as a visual cue and vestibular feeling via waist motion and proprioception on the legs. Our system also allows viewpoint sharing in which fifty users can enjoy omnidirectional viewing of the remote environments through the HMD without walk-like sensation feedback.	https://dl.acm.org/doi/abs/10.1145/3532721.3535570	Yusuke Kikuchi, Yukiya Ojima, Ryoto Kato, Minori Unno, Vibol Yem, Yukie Nagai, Yasushi Ikei
Dual octree graph networks for learning adaptive volumetric shape representations	We present an adaptive deep representation of volumetric fields of 3D shapes and an efficient approach to learn this deep representation for high-quality 3D shape reconstruction and auto-encoding. Our method encodes the volumetric field of a 3D shape with an adaptive feature volume organized by an octree and applies a compact multilayer perceptron network for mapping the features to the field value at each 3D position. An encoder-decoder network is designed to learn the adaptive feature volume based on the graph convolutions over the dual graph of octree nodes. The core of our network is a new graph convolution operator defined over a regular grid of features fused from irregular neighboring octree nodes at different levels, which not only reduces the computational and memory cost of the convolutions over irregular neighboring octree nodes, but also improves the performance of feature learning. Our method effectively encodes shape details, enables fast 3D shape reconstruction, and exhibits good generality for modeling 3D shapes out of training categories. We evaluate our method on a set of reconstruction tasks of 3D shapes and scenes and validate its superiority over other existing approaches. Our code, data, and trained models are available at https://wang-ps.github.io/dualocnn.	https://dl.acm.org/doi/abs/10.1145/3528223.3530087	Peng-Shuai Wang, Yang Liu, Xin Tong
Dual-Layer Light Source with Textured Lighting Gel	We propose a new light source representation to intuitively model complicated lighting effects with simple user interactions. Our representation uses two layers; an emissive layer which is a traditional diffuse area light source with a constant emission, and a lighting gel layer which introduces variations to the emission. The lighting gel layer is mapped with a texture for colored shadows without modeling a 3D scene. The two layers are transformed independently to cast the texture with different effects. To cast lighting on a planar canvas in 2D design, the proposed light source can be created and edited in the 2D canvas directly, without switching to 3D world space.	https://dl.acm.org/doi/abs/10.1145/3532719.3543227	Xin Sun, Nathan Carr, Sumit Dhingra, Vineet Batra, Ankit Phogat
DynaPix: Normal Map Pixelization for Dynamic Lighting	This work introduces DynaPix, a Krita extension that automatically generates pixelated images and surface normals from an input image. DynaPix is a tool that aids pixel artists and game developers more efficiently develop 8-bit style games and bring them to life with dynamic lighting through normal maps that can be used in modern game engines such as Unity. The extension offers artists a degree of flexibility as well as allows for further refinements to generated artwork. Powered by out of the box solutions, DynaPix is a tool that seamlessly integrates in the artistic workflow.	https://dl.acm.org/doi/abs/10.1145/3532719.3543238	Gerardo Gandeaga, Denys Iliash, Chris Careaga, Yağız Aksoy
Dynamic Vertex Hierarchies for Parallel View-Dependent Progressive Meshes	Triangles are one way we measure the complexity in a synthesized image. There often exists a trade-off between how realistic the synthesized image is based on the number of triangles and the overall rendering time. In real time applications, this trade-off becomes more important as we need to render detailed scenes at a fixed frame rate without sacrificing image quality. We propose a new view-dependent method that creates a more dynamic structure and is easy to parallelize. We also propose an amortized rebalancing operation to reduce long dependency lines in our data structure, prevent worst-case behavior, and in some instances improve the average case. Our implementation is still in-progress, but our new method is provably consistent and has potential to reduce the amount of storage needed for view-dependent methods and remove more geometry at higher fidelity, along with other performance improvements.	https://dl.acm.org/doi/abs/10.1145/3532719.3543217	Jonathan Merrin, Michael Shah
Dynamic deformables: implementation and production practicalities (now with code!)	Simulating dynamic deformation has been an integral component of Pixar's storytelling since Boo's shirt in (2001). Recently, several key transformations have been applied to Pixar's core simulator that improve its speed, robustness, and generality. Starting with (2017), improved collision detection and response were incorporated into the cloth solver, then with (2017) 3D solids were introduced, and in (2020) clothing is allowed to interact with a character's body with two-way coupling. The 3D solids are based on a fast, compact, and powerful new formulation that we have published over the last few years at SIGGRAPH. Under this formulation, the construction and eigendecomposition of the force gradient, long considered the most onerous part of the implementation, becomes fast and simple. We provide a detailed, self-contained, and unified treatment here that is not available in the technical papers. We also provide, for the first time, open-source C++ implementations of many of the described algorithms. This new formulation is only a starting point for creating a simulator that is up challenges of a production environment. One challenge is performance: we discuss our current best practices for accelerating system assembly and solver performance. Another challenge that requires considerable attention is robust collision detection and response. Much has been written about collision detection approaches such as proximity-queries, continuous collisions and global intersection analysis. We discuss our strategies for using these techniques, which provides us with valuable information that is needed to handle challenging scenarios.	https://dl.acm.org/doi/abs/10.1145/3532720.3535628	Theodore Kim, David Eberle
Dynamic optimal space partitioning for redirected walking in multi-user environment	In multi-user Redirected Walking (RDW), the space subdivision method divides a shared physical space into sub-spaces and allocates a sub-space to each user. While this approach has the advantage of precluding any collisions between users, the conventional space subdivision method suffers from frequent boundary resets due to the reduction of available space per user. To address this challenge, in this study, we propose a space subdivision method called Optimal Space Partitioning (OSP) that dynamically divides the shared physical space in real-time. By exploiting spatial information of the physical and virtual environment, OSP predicts the movement of users and divides the shared physical space into optimal sub-spaces separated with shutters. Our OSP framework is trained using deep reinforcement learning to allocate optimal sub-space to each user and provide optimal steering. Our experiments demonstrate that OSP provides higher sense of immersion to users by minimizing the total number of reset counts, while preserving the advantage of the existing space subdivision strategy: ensuring better safety to users by completely eliminating the possibility of any collisions between users beforehand. Our project is available at https://github.com/AppleParfait/OSP-Archive.	https://dl.acm.org/doi/abs/10.1145/3528223.3530113	Sang-Bin Jeon, Soon-Uk Kwon, June-Young Hwang, Yong-Hun Cho, Hayeon Kim, Jinhyung Park, In-Kwon Lee
EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model	Although significant progress has been made to audio-driven talking face generation, existing methods either neglect facial emotion or cannot be applied to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model (EAMM) to generate one-shot emotional talking faces by involving an emotion source video. Specifically, we first propose an Audio2Facial-Dynamics module, which renders talking faces from audio-driven unsupervised zero- and first-order key-points motion. Then through exploring the motion model's properties, we further propose an Implicit Emotion Displacement Learner to represent emotion-related facial dynamics as linearly additive displacements to the previously acquired motion representations. Comprehensive experiments demonstrate that by incorporating the results from both modules, our method can generate satisfactory talking face results on arbitrary subjects with realistic emotion patterns.	https://dl.acm.org/doi/abs/10.1145/3528233.3530745	Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne Wu, Feng Xu, Xun Cao
EARS: efficiency-aware russian roulette and splitting	Russian roulette and splitting are widely used techniques to increase the efficiency of Monte Carlo estimators. But, despite their popularity, there is little work on how to best apply them. Most existing approaches rely on simple heuristics based on, e.g., surface albedo and roughness. Their efficiency often hinges on user-controlled parameters. We instead iteratively learn optimal Russian roulette and splitting factors during rendering, using a simple and lightweight data structure. Given perfect estimates of variance and cost, our fixed-point iteration provably converges to the optimal Russian roulette and splitting factors that maximize the rendering efficiency. In our application to unidirectional path tracing, we achieve consistent and significant speed-ups over the state of the art.	https://dl.acm.org/doi/abs/10.1145/3528223.3530168	Alexander Rath, Pascal Grittmann, Sebastian Herholz, Philippe Weier, Philipp Slusallek
EMBER: exact mesh booleans via efficient & robust local arrangements	Boolean operators are an essential tool in a wide range of geometry processing and CAD/CAM tasks. We present a novel method, EMBER, to compute Boolean operations on polygon meshes which is exact, reliable, and highly performant at the same time. Exactness is guaranteed by using a plane-based representation for the input meshes along with recently introduced homogeneous integer coordinates. Reliability and robustness emerge from a formulation of the algorithm via generalized winding numbers and mesh arrangements. High performance is achieved by avoiding the (pre-)construction of a global acceleration structure. Instead, our algorithm performs an adaptive recursive subdivision of the scene's bounding box while generating and tracking all required data on the fly. By leveraging a number of early-out termination criteria, we can avoid the generation and inspection of regions that do not contribute to the output. With a careful implementation and a work-stealing multi-threading architecture, we are able to compute Boolean operations between meshes with millions of triangles at interactive rates. We run an extensive evaluation on the Thingi10K dataset to demonstrate that our method outperforms state-of-the-art algorithms, even inexact ones like QuickCSG, by orders of magnitude.	https://dl.acm.org/doi/abs/10.1145/3528223.3530181	Philip Trettner, Julius Nehring-Wirxel, Leif Kobbelt
Ecoclimates: climate-response modeling of vegetation	One of the greatest challenges to mankind is understanding the underlying principles of climate change. Over the last years, the role of forests in climate change has received increased attention. This is due to the observation that not only the atmosphere has a principal impact on vegetation growth but also that vegetation is contributing to local variations of weather resulting in diverse microclimates. The interconnection of plant ecosystems and weather is described and studied as ecoclimates. In this work we take steps towards simulating ecoclimates by modeling the feedback loops between vegetation, soil, and atmosphere. In contrast to existing methods that only describe the climate at a global scale, our model aims at simulating local variations of climate. Specifically, we model tree growth interactively in response to gradients of water, temperature and light. As a result, we are able to capture a range of ecoclimate phenomena that have not been modeled before, including geomorphic controls, forest edge effects, the Foehn effect and spatial vegetation patterning. To validate the plausibility of our method we conduct a comparative analysis to studies from ecology and climatology. Consequently, our method advances the state-of-the-art of generating highly realistic outdoor landscapes of vegetation.	https://dl.acm.org/doi/abs/10.1145/3528223.3530146	Wojtek Pałubicki, Miłosz Makowski, Weronika Gajda, Torsten Hädrich, Dominik L. Michels, Sören Pirk
Editorial Pipeline Conversion: Animal Logic’s Transition to OpenTimelineIO	We introduce Animal Logic's editorial pipeline refactor from a rigid and overly complex in-house solution, towards a more modern, flexible approach, based on the open source technologies OpenTimelineIO and Electron.js. This upgraded design greatly increases flexibility over the previous effort, enabling cross-platform user adoption and further decoupling our tools from the editorial software of choice. The new pipeline is now rolled out onto our most recent productions and we are already starting to see the benefits of its extensibility and ease of troubleshooting.	https://dl.acm.org/doi/abs/10.1145/3532836.3536278	Oliver Dunn
Efficiency-aware multiple importance sampling for bidirectional rendering algorithms	Multiple importance sampling (MIS) is an indispensable tool in light-transport simulation. It enables robust Monte Carlo integration by combining samples from several techniques. However, it is well understood that such a combination is not always more efficient than using a single sampling technique. Thus a major criticism of complex combined estimators, such as bidirectional path tracing, is that they can be significantly less efficient on common scenes than simpler algorithms like forward path tracing. We propose a general method to improve MIS efficiency: By cheaply estimating the efficiencies of various technique and sample-count combinations, we can pick the best one. The key ingredient is a numerically robust and efficient scheme that uses the samples of one MIS combination to compute the efficiency of multiple other combinations. For example, we can run forward path tracing and use its samples to decide which subset of VCM to enable, and at what sampling rates. The sample count for each technique can be controlled per-pixel or globally. Applied to VCM, our approach enables robust rendering of complex scenes with caustics, without compromising efficiency on simpler scenes.	https://dl.acm.org/doi/abs/10.1145/3528223.3530126	Pascal Grittmann, Ömercan Yazici, Iliyan Georgiev, Philipp Slusallek
Efficient estimation of boundary integrals for path-space differentiable rendering	Boundary integrals are unique to physics-based differentiable rendering and crucial for differentiating with respect to object geometry. Under the differential path integral framework---which has enabled the development of sophisticated differentiable rendering algorithms---the boundary components are themselves path integrals. Previously, although the mathematical formulation of boundary path integrals have been established, efficient estimation of these integrals remains challenging. In this paper, we introduce a new technique to efficiently estimate boundary path integrals. A key component of our technique is a primary-sample-space guiding step for importance sampling of boundary segments. Additionally, we show multiple importance sampling can be used to combine multiple guided samplings. Lastly, we introduce an optional edge sorting step to further improve the runtime performance. We evaluate the effectiveness of our method using several differentiable-rendering and inverse-rendering examples and provide comparisons with existing methods for reconstruction as well as gradient quality.	https://dl.acm.org/doi/abs/10.1145/3528223.3530080	Kai Yan, Christoph Lassner, Brian Budge, Zhao Dong, Shuang Zhao
Efficient kinetic simulation of two-phase flows	Real-life multiphase flows exhibit a number of complex and visually appealing behaviors, involving bubbling, wetting, splashing, and glugging. However, most state-of-the-art simulation techniques in graphics can only demonstrate a limited range of multiphase flow phenomena, due to their inability to handle the real water-air density ratio and to the large amount of numerical viscosity introduced in the flow simulation and its coupling with the interface. Recently, kinetic-based methods have achieved success in simulating large density ratios and high Reynolds numbers efficiently; but their memory overhead, limited stability, and numerically-intensive treatment of coupling with immersed solids remain enduring obstacles to their adoption in movie productions. In this paper, we propose a new kinetic solver to couple the incompressible Navier-Stokes equations with a conservative phase-field equation which remedies these major practical hurdles. The resulting two-phase immiscible fluid solver is shown to be efficient due to its massively-parallel nature and GPU implementation, as well as very versatile and reliable because of its enhanced stability to large density ratios, high Reynolds numbers, and complex solid boundaries. We highlight the advantages of our solver through various challenging simulation results that capture intricate and turbulent air-water interaction, including comparisons to previous work and real footage.	https://dl.acm.org/doi/abs/10.1145/3528223.3530132	Wei Li, Yihui Ma, Xiaopei Liu, Mathieu Desbrun
Egocentric scene reconstruction from an omnidirectional video	Omnidirectional videos capture environmental scenes effectively, but they have rarely been used for geometry reconstruction. In this work, we propose an egocentric 3D reconstruction method that can acquire scene geometry with high accuracy from a short egocentric omnidirectional video. To this end, we first estimate per-frame depth using a spherical disparity network. We then fuse per-frame depth estimates into a novel data structure that is specifically designed to tolerate spherical depth estimation errors. By subdividing the spherical space into binary tree and octree nodes that represent spherical frustums adaptively, the spherical binoctree effectively enables egocentric surface geometry reconstruction for environmental scenes while simultaneously assigning high-resolution nodes for closely observed surfaces. This allows to reconstruct an entire scene from a short video captured with a small camera trajectory. Experimental results validate the effectiveness and accuracy of our approach for reconstructing the 3D geometry of environmental scenes from short egocentric omnidirectional video inputs. We further demonstrate various applications using a conventional omnidirectional camera, including novel-view synthesis, object insertion, and relighting of scenes using reconstructed 3D models with texture.	https://dl.acm.org/doi/abs/10.1145/3528223.3530074	Hyeonjoong Jang, Andréas Meuleman, Dahyun Kang, Donggun Kim, Christian Richardt, Min H. Kim
Eikonal Fields for Refractive Novel-View Synthesis	We tackle the problem of generating novel-view images from collections of 2D images showing refractive and reflective objects. Current solutions assume opaque or transparent light transport along straight paths following the emission-absorption model. Instead, we optimize for a field of 3D-varying index of refraction (IoR) and trace light through it that bends toward the spatial gradients of said IoR according to the laws of eikonal light transport.	https://dl.acm.org/doi/abs/10.1145/3528233.3530706	Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad, Hans-Peter Seidel, Tobias Ritschel
Embroidery and Cloth Fiber Workflows on Disney’s ”Encanto”	"Walt Disney Animation Studios' ""Encanto"" tells the tale of an extraordinary family, the Madrigals, who live in the hidden mountains of Colombia. The garments are an important aspect of the characters' design and express their individual personalities. Accurate cloth looks have been difficult to achieve with our traditional look development workflow. We present the techniques utilized to create the varied and complex fiber-level cloth features in the film. In order to produce the desired level of geometric detail, we developed a new workflow that procedurally models each cloth fiber in Houdini and then binds the resulting curves to the clothing geometry via Disney's XGen. We also extended our embroidery workflow to support a wide variety of embroidery types and styles which are exemplified by Mirabel's outfit and include needle paint, knot, and rope stitching."	https://dl.acm.org/doi/abs/10.1145/3532836.3536250	Jose Velasquez, Alexander Alvarado, Ying Liu, Maryann Simmons
Emerging media and tech in a traditional company	Do you think that a company in the construction industry that manufactures aluminum doors and facades would have an internal animation, emerging-media, and tech R&D group? Well, here we are: the Virtual Construction Lab (VCL) of Schüco. In the span of six years, we have created diverse media in the fields of engineering visualizations, highly-technical assembly processes, training and marketing materials, amongst others; establishing ourselves as a creative and technologically innovative force inside a very traditional industry. Thanks to finding and understanding our own advantages and specific targets we managed to carve our own space inside the company, a position we leveraged to expand into more and more ambitious projects by carefully constructing captivating functional prototypes that address new needs and issues within the industry. This path has not been an easy one, but it is one we want to share in hopes others can learn from it and use it to establish their own teams. We will go in depth into not only how VCL was formed, but most importantly, how we have managed to carve a space for ourselves and planted the seeds to develop increasingly ambitious projects. Starting with simple -but highly technical- renders to Virtual Reality interactive showrooms and presentations enhanced with the Unreal Engine Live Production pipeline, this exciting journey has taught us much about fabricating our own opportunities to realize our creative ambitions, and carving out a niche as an innovative force within the global company. Thanks to a highly driven multidisciplinary and collaborative team, we have been able to address specific needs within the company with prototypes born out of our interests, expertise, and curiosity. Ultimately, we want to show that both successes and failures have helped us reassess our approaches, and constantly turn those small MVPs into bigger ongoing efforts.	https://dl.acm.org/doi/abs/10.1145/3532724.3535596	Nicolas Escarpentier
Encanto: creating a magical place	"Walt Disney Animation Studios' ""Encanto"" features an enchanted environment filled with unique, complex characters that required deep collaboration between artists, technical directors, and engineers. Embroidered, multi-layered fabrics, and the complex hair grooms of the large cast of characters, including a house, inhabit a town inspired by Colombian architecture and culture."	https://dl.acm.org/doi/abs/10.1145/3512752.3528363	Scott Kersavage, Bradford Simonsen, Thaddeus P. Miller, Ian Gooding
Enemies	In a spaceship approaching a planet, a mysterious woman is setting up the chessboard for yet another game, while pondering the nature of power.	https://dl.acm.org/doi/abs/10.1145/3512752.3528232	Veselin Efremov, Aleksander Karshikoff
Energetically consistent inelasticity for optimization time integration	In this paper, we propose Energetically Consistent Inelasticity (ECI), a new formulation for modeling and discretizing finite strain elastoplasticity/viscoelasticity in a way that is compatible with optimization-based time integrators. We provide an in-depth analysis for allowing plasticity to be implicitly integrated through an augmented strain energy density function. We develop ECI on the associative von-Mises J2 plasticity, the non-associative Drucker-Prager plasticity, and the finite strain viscoelasticity. We demonstrate the resulting scheme on both the Finite Element Method (FEM) and the Material Point Method (MPM). Combined with a custom Newton-type optimization integration scheme, our method enables simulating stiff and large-deformation inelastic dynamics of metal, sand, snow, and foam with larger time steps, improved stability, higher efficiency, and better accuracy than existing approaches.	https://dl.acm.org/doi/abs/10.1145/3528223.3530072	Xuan Li, Minchen Li, Chenfanfu Jiang
Estimation of yarn-level simulation models for production fabrics	This paper introduces a methodology for inverse-modeling of yarn-level mechanics of cloth, based on the mechanical response of fabrics in the real world. We compiled a database from physical tests of several different knitted fabrics used in the textile industry. These data span different types of complex knit patterns, yarn compositions, and fabric finishes, and the results demonstrate diverse physical properties like stiffness, nonlinearity, and anisotropy. We then develop a system for approximating these mechanical responses with yarn-level cloth simulation. To do so, we introduce an efficient pipeline for converting between fabric-level data and yarn-level simulation, including a novel swatch-level approximation for speeding up computation, and some small-but-necessary extensions to yarn-level models used in computer graphics. The dataset used for this paper can be found at http://mslab.es/projects/YarnLevelFabrics.	https://dl.acm.org/doi/abs/10.1145/3528223.3530167	Georg Sperl, Rosa M. Sánchez-Banderas, Manwen Li, Chris Wojtan, Miguel A. Otaduy
Eternals: Tackling The Large Scale Water Whirlpools	In Eternals third act we were faced with the challenge of creating extremely large water whirlpools that would spread for kilometres wide and deep. It was imperative that we approached this in an optimal way that would allow artistic control, reasonable turnaround time whilst maintaining a plausible and realistic look. This article focuses in the methodology used and how we took advantage of our in-house package Loki to achieve the results.	https://dl.acm.org/doi/abs/10.1145/3532836.3536267	Gerardo Aguilera, Kevin Blom
Exploring the use of mobile AR to aid decision-making on-the-go	Extended reality adaptation is gaining traction across a broad spectrum of disciplines. Mobile phones and markerless augmented reality (AR) are fascinating methods for retrieving information because they enable content to be rendered in the user's immediate environment while they are on the move. This method of rendering virtual content within a physical environment will be accessible to a large audience that, for the most part, does not have access to high-end equipment such as head-mounted displays. Along with a large audience, augmented reality on mobile devices can display dynamic content in real time, even in outdoor settings, providing users with information about physical artifacts that is both immediate and contextually relevant. The purpose of this paper is to discuss the development of a mobile application that will assist users in making on-the-spot decisions based on online content displayed nearby. Along with developing an application that assists users in locating nearby restaurants, a strong emphasis is placed on architecture, user interface, and experience design, as the application will be used in environments with limited focus (e.g., traffic noise or occlusion with real objects). The study concludes with a discussion of the application's user experience and future research directions in this field. With the rapid evolution of mobile devices, it is critical to understand some of the challenges and design constraints inherent in developing better user experiences for applications that support on-the-go decision-making.	https://dl.acm.org/doi/abs/10.1145/3532723.3535468	W. Kyle Soeltz, Jeff Hickey, Zona Kostic, Anna Lin, Kartik Swamy
EyeNeRF: a hybrid representation for photorealistic synthesis, animation and relighting of human eyes	A unique challenge in creating high-quality animatable and relightable 3D avatars of real people is modeling human eyes, particularly in conjunction with the surrounding periocular face region. The challenge of synthesizing eyes is multifold as it requires 1) appropriate representations for the various components of the eye and the periocular region for coherent viewpoint synthesis, capable of representing diffuse, refractive and highly reflective surfaces, 2) disentangling skin and eye appearance from environmental illumination such that it may be rendered under novel lighting conditions, and 3) capturing eyeball motion and the deformation of the surrounding skin to enable re-gazing. These challenges have traditionally necessitated the use of expensive and cumbersome capture setups to obtain high-quality results, and even then, modeling of the full eye region holistically has remained elusive. We present a novel geometry and appearance representation that enables high-fidelity capture and photorealistic using only a sparse set of lights and cameras. Our hybrid representation combines an explicit parametric surface model for the eyeball surface with implicit deformable volumetric representations for the periocular region and the interior of the eye. This novel hybrid model has been designed specifically to address the various parts of that exceptionally challenging facial area - the explicit eyeball surface allows modeling refraction and high frequency specular reflection at the cornea, whereas the implicit representation is well suited to model lower frequency skin reflection via spherical harmonics and can represent non-surface structures such as hair (i.e. eyebrows) or highly diffuse volumetric bodies (i.e. sclera), both of which are a challenge for explicit surface models. Tightly integrating the two representations in a joint framework allows controlled photoreal image synthesis and joint optimization of both the geometry parameters of the eyeball and the implicit neural network in continuous 3D space. We show that for high-resolution close-ups of the human eye, our model can synthesize high-fidelity animated gaze from novel views under unseen illumination conditions, allowing to generate visually rich eye imagery.	https://dl.acm.org/doi/abs/10.1145/3528223.3530130	Gengyan Li, Abhimitra Meka, Franziska Mueller, Marcel C. Buehler, Otmar Hilliges, Thabo Beeler
FUTURE.STAGE by Evil Eye Pictures & Patrick Osborne	In this paper, we describe a distributed collaborative system to present a real-time live computer animated show featuring more than one character and more than one interactive show element that are individually and remotely controlled each from different physical locations.	https://dl.acm.org/doi/abs/10.1145/3532833.3538685	Dan Rosen, Patrick Osborne, Yovel Schwartz, Alastair Macleod, Justin Schubert, Brian Smith, Ben Peck, James Ritts, Andrew Angulo
Face Extrusion Quad Meshes	We propose a 3D object construction methodology built on face-loop modeling operations. Our Face Extrusion Quad (FEQ) meshes, have a well designed face-loop structure similar to artist crafted 3D models. Furthermore, we define a construction graph which encodes a sequence of primitive extrude/collapse and bridge/separate operations that operate on admissible face-loops. We show that FEQs are imbued with a meaningful face-loop induced shape skeleton, part segmentation, plausible construction history, and possess the many advantages of extrusion-based 3D modeling. Our evaluation is threefold: we show a gallery of challenging 3D models transformed to FEQs with compelling face-loop structure; we showcase the potential of an inherent construction graph, using FEQ-based cut-paste and inverse modeling applications; and we demonstrate the impact of various algorithmic and parameter related choices for FEQ modeling and application.	https://dl.acm.org/doi/abs/10.1145/3528233.3530754	Karran Pandey, J. Andreas Bærentzen, Karan Singh
Face deblurring using dual camera fusion on mobile phones	Motion blur of fast-moving subjects is a longstanding problem in photography and very common on mobile phones due to limited light collection efficiency, particularly in low-light conditions. While we have witnessed great progress in image deblurring in recent years, most methods require significant computational power and have limitations in processing high-resolution photos with severe local motions. To this end, we develop a novel face deblurring system based on the dual camera fusion technique for mobile phones. The system detects subject motion to dynamically enable a reference camera, e.g., ultrawide angle camera commonly available on recent premium phones, and captures an auxiliary photo with faster shutter settings. While the main shot is low noise but blurry (Figure 1(a)), the reference shot is sharp but noisy (Figure 1(b)). We learn ML models to align and fuse these two shots and output a clear photo without motion blur (Figure 1(c)). Our algorithm runs efficiently on Google Pixel 6, which takes 463 ms overhead per shot. Our experiments demonstrate the advantage and robustness of our system against alternative single-image, multi-frame, face-specific, and video deblurring algorithms as well as commercial products. To the best of our knowledge, our work is the first mobile solution for face motion deblurring that works reliably and robustly over thousands of images in diverse motion and lighting conditions.	https://dl.acm.org/doi/abs/10.1145/3528223.3530131	Wei-Sheng Lai, Yichang Shih, Lun-Cheng Chu, Xiaotong Wu, Sung-Fang Tsai, Michael Krainin, Deqing Sun, Chia-Kai Liang
Facial hair tracking for high fidelity performance capture	Facial hair is a largely overlooked topic in facial performance capture. Most production pipelines in the entertainment industry do not have a way to automatically capture facial hair or track the skin underneath it. Thus, actors are asked to shave clean before face capture, which is very often undesirable. Capturing the geometry of individual facial hairs is very challenging, and their presence makes it harder to capture the deforming shape of the underlying skin surface. Some attempts have already been made at automating this task, but only for static faces with relatively sparse 3D hair reconstructions. In particular, current methods lack the temporal correspondence needed when capturing a sequence of video frames depicting facial performance. The problem of robustly tracking the skin underneath also remains unaddressed. In this paper, we propose the first multiview reconstruction pipeline that tracks both the dense 3D facial hair, as well as the underlying 3D skin for entire performances. Our method operates with standard setups for face photogrammetry, without requiring dense camera arrays. For a given capture subject, our algorithm first reconstructs a dense, high-quality neutral 3D facial hairstyle by registering sparser hair reconstructions over multiple frames that depict a neutral face under quasi-rigid motion. This custom-built, reference facial hairstyle is then tracked throughout a variety of changing facial expressions in a captured performance, and the result is used to constrain the tracking of the 3D skin surface underneath. We demonstrate the proposed capture pipeline on a variety of different facial hairstyles and lengths, ranging from sparse and short to dense full-beards.	https://dl.acm.org/doi/abs/10.1145/3528223.3530116	Sebastian Winberg, Gaspard Zoss, Prashanth Chandran, Paulo Gotardo, Derek Bradley
Fast evaluation of smooth distance constraints on co-dimensional geometry	We present a new method for computing a smooth minimum distance function based on the LogSumExp function for point clouds, edge meshes, triangle meshes, and combinations of all three. We derive blending weights and a modified Barnes-Hut acceleration approach that ensure our method approximates the true distance, and is conservative (points outside the zero isosurface are guaranteed to be outside the surface) and efficient to evaluate for all the above data types. This, in combination with its ability to smooth sparsely sampled and noisy data, like point clouds, shortens the gap between data acquisition and simulation, and thereby enables new applications such as direct, co-dimensional rigid body simulation using unprocessed lidar data.	https://dl.acm.org/doi/abs/10.1145/3528223.3530093	Abhishek Madan, David I. W. Levin
Filament based plasma	Simulation of stellar atmospheres, such as that of our own sun, is a common task in CGI for scientific visualization, movies and games. A fibrous volumetric texture is a visually dominant feature of the solar corona---the plasma that extends from the solar surface into space. These coronal fibers can be modeled as magnetic filaments whose shape is governed by the magnetohydrostatic equation. The magnetic filaments provide a Lagrangian curve representation and their initial configuration can be prescribed by an artist or generated from magnetic flux given as a scalar texture on the sun's surface. Subsequently, the shape of the filaments is determined based on a variational formulation. The output is a visual rendering of the whole sun. We demonstrate the fidelity of our method by comparing the resulting renderings with actual images of our sun's corona.	https://dl.acm.org/doi/abs/10.1145/3528223.3530102	Marcel Padilla, Oliver Gross, Felix Knöppel, Albert Chern, Ulrich Pinkall, Peter Schröder
FirstPersonScience: An Open Source Tool for Studying FPS Esports Aiming	First-person shooters (FPS) games are dominant in the competitive gaming and esports community. However, relatively few tools are available for experimenters interested in studying mechanics of these games in a controlled, repeatable environment. While other researchers have made progress with one-off applications as well as custom content and mods for existing games, we are not aware of a general purpose application for empirically studying a broad set of user interactions in the FPS context. For the past few years our team has developed, maintained, and deployed First Person Science (FPSci), a tool for controlled user studies in FPS gaming. FPSci experimenters configure their desired base environment, as well as conditions and user preferences using a simplified JSON-esque set of input configurations, and results are stored in an SQLite database. By allowing finer grained parametric control of the environment together with frame-wise logging of player state and performance metrics, we achieve a level of granularity of control not offered by other solutions. FPSci is available as an open source project 1 under a CC BY-NC-SA 4.0 license.	https://dl.acm.org/doi/abs/10.1145/3532836.3536233	Ben Boudaoud, Josef Spjut, Joohwan Kim
Fracture-aware Tessellation of Subdivision Surfaces	We introduce a new tessellation algorithm for production rendering of fractured subdivision surfaces that produces higher quality results with less distortion than previous approaches. Our tessellator is provided with a fractured subdivision control mesh along with the corresponding unfractured mesh. We use the unfractured mesh to evaluate limit positions of the fractured mesh vertices before tessellation, and we apply displacements at this time. During tessellation, we iteratively split edges, evaluating displaced limit positions at added points. Additionally, we measure deformations that were performed during the fracture process and apply these during tessellation to allow effects like animated crack spreading. Our approach produces seamless results with no distortion of the unfractured surface, and enables a more efficient fracture pipeline.	https://dl.acm.org/doi/abs/10.1145/3532836.3536262	Brent Burley, Francisco Rodriguez
Free2CAD: parsing freehand drawings into CAD commands	CAD modeling, despite being the industry-standard, remains restricted to usage by skilled practitioners due to two key barriers. First, the user must be able to mentally parse a final shape into a valid sequence of supported CAD commands; and second, the user must be sufficiently conversant with CAD software packages to be able to execute the corresponding CAD commands. As a step towards addressing both these challenges, we present Free2CAD wherein the user can simply sketch the final shape and our system parses the input strokes into a sequence of commands expressed in a simplified CAD language. When executed, these commands reproduce the sketched object. Technically, we cast sketch-based CAD modeling as a sequence-to-sequence translation problem, for which we leverage the powerful Transformers neural network architecture. Given the sequence of pen strokes as input, we introduce the new task of grouping strokes that correspond to individual CAD operations. We combine stroke grouping with geometric fitting of the operation parameters, such that intermediate groups are geometrically corrected before being reused, as context, for subsequent steps in the sequence inference. Although trained on synthetically-generated data, we demonstrate that Free2CAD generalizes to sketches created from real-world CAD models as well as to sketches drawn by novice users.	https://dl.acm.org/doi/abs/10.1145/3528223.3530133	Changjian Li, Hao Pan, Adrien Bousseau, Niloy J. Mitra
From Procedural Panda-monium to Fast Vectorized Execution using PCF Crowd Primitives	"In animation and VFX, crowds are too often considered an ""edge case"", to be handled by specialized pipeline outside the main workflows. Requirements of scale and traditional reliance on history based simulation have been obstacles to properly building crowd systems into the core functionality of digital content creation software. Pixar's crowds team has worked to reverse this trend, developing a fast vectorized crowd system directly within the execution engine of our proprietary animation software, Presto. Dubbed Pcf, for Presto Crowds Framework, this system uses aggregate models, called crowd primitives, to provide artists directly manipulable crowds while maintaining proceduralism for mass edits. Like traditional models, they contain rigs (a graph of operators) which run parallelized through Presto's execution engine [Watt et al. 2014], but rather than posing points, they set joint angles and blendshape weights to pose entire crowds. The core operations of crowd artists: placement, casting, clip sequencing, transitions, look-ats, and curve following, are all well expressed as rigging operators (known as ""actions"" in Presto parlance) in Pcf. They provide interactive control of entire crowds in context using the same animation tool as our layout artists, animators, simulation TDs, etc. The first film to use Pcf, Turning Red, reaped massive benefits by building a stadium's worth of characters in a fraction of the time of previous films' efforts. However, because Pcf is tightly integrated into Presto, the benefits extended beyond efficiency for the crowds team. By providing our layout department Pcf rigging controls, they were able to shoot inside the crowd and use procedural operators to clear room for the camera and maintain crowd density only where needed. Similarly, the principal animation team could animate main characters in context of the crowd they were acting in, providing the proper context which all too often is absent in crowd shots. Taken together Pcf, is a huge step forward in bringing crowds out of the margins and into the core of animation workflows at Pixar, demonstrating that fast vectorized crowds can be an integral part of digital content creation software."	https://dl.acm.org/doi/abs/10.1145/3532836.3536256	Paul Kanyuk, Aaron Lo, Venkateswaran Krishna, J.D. Northrup, Mark Hessler, Arnold Moon, Michael Lorenzen, Jonah Laird
Fruit Golf: An Asymmetrical Shared Space VR/Mobile Experience	While the global COVID-19 pandemic did not catalyze widespread adoption of virtual reality (VR) technologies across all industries as some had anticipated, studies like Hall et al. from 2022 have demonstrated that public valuation of VR remains strongly in gaming, entertainment, and socializing [Hall et al., 2022]. As we look towards a future in which indoor gatherings with friends and family are safe and encouraged once again, there is an opportunity to position VR gaming as a go-to add-on to social gatherings by emphasizing ease of access for players of all levels of experience, and designing gameplay that encourages engagement rather than isolation in shared space. Fruit Golf aims to use an asymmetric multiplayer format to offer an experience that spans collaborative and competitive experiences, and allows players to seamlessly interact with VR, mobile, and physical spaces in ways that most will have never seen before.	https://dl.acm.org/doi/abs/10.1145/3532834.3536214	Shane Nilsson, Tyler Nilsson, Liana Russell, Bruno Costa, Elizabeth Pieters, Jasmin Winter, Carlos Fernandez, Collin Myhre
Fuzzy Boundary (An Embroidery Installation)	This installation provides a hands-on attendee experience with procedural design workflows and programmable machine embroidery that connect various domains including biomimicry, visualization, and product design. Situated within contemporary studio craft practices of digitizing the handcrafts, this work recognizes time-based computation as machine agent for bio-logical pattern generation, and programmable machine embroidery as the realization of structural biomimicry. Critical making is a form of research and practice among contemporary craft artists, and such studio activity is a design methodology employing various material and technical explorations that are by nature complex interdisciplinary relations. Fuzzy Boundary (An Embroidery Installation) represents redefining educational relationships among 21st century industrial design practice, and visualization.	https://dl.acm.org/doi/abs/10.1145/3532725.3538709	Michael Gayk
GAN applied to Wave Function Collapse for procedural map generation	This paper describes the use of Generative Adversarial Network (GAN) applied to the Wave Function Collapse (WFC) algorithm for procedural content generation. The goal of this system is to enable level designers to generate coherent 3D worlds with brand new meshes generated by the GAN.	https://dl.acm.org/doi/abs/10.1145/3532719.3543198	Alain Lioret, Nicolas Ruche, Etienne Gibiat, Cédric Chopin
GANimator: neural motion synthesis from a single sequence	We present GANimator, a generative model that learns to synthesize novel motions from a single, short motion sequence. GANimator generates motions that resemble the core of the original motion, while simultaneously synthesizing novel and diverse movements. Existing data-driven techniques for motion synthesis require a large motion dataset which contains the desired and specific skeletal structure. By contrast, GANimator only requires training on a motion sequence, enabling novel motion synthesis for a variety of skeletal structures , bipeds, quadropeds, hexapeds, and more. Our framework contains a series of generative and adversarial neural networks, each responsible for generating motions in a specific frame rate. The framework progressively learns to synthesize motion from random noise, enabling hierarchical control over the generated motion content across varying levels of detail. We show a number of applications, including crowd simulation, key-frame editing, style transfer, and interactive control, which all learn from a single input sequence. Code and data for this paper are at https://peizhuoli.github.io/ganimator.	https://dl.acm.org/doi/abs/10.1145/3528223.3530157	Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, Olga Sorkine-Hornung
GWA: A Large High-Quality Acoustic Dataset for Audio Processing	We present the Geometric-Wave Acoustic (GWA) dataset, a large-scale audio dataset of about 2 million synthetic room impulse responses (IRs) and their corresponding detailed geometric and simulation configurations. Our dataset samples acoustic environments from over 6.8K high-quality diverse and professionally designed houses represented as semantically labeled 3D meshes. We also present a novel real-world acoustic materials assignment scheme based on semantic matching that uses a sentence transformer model. We compute high-quality impulse responses corresponding to accurate low-frequency and high-frequency wave effects by automatically calibrating geometric acoustic ray-tracing with a finite-difference time-domain wave solver. We demonstrate the higher accuracy of our IRs by comparing with recorded IRs from complex real-world environments. Moreover, we highlight the benefits of GWA on audio deep learning tasks such as automated speech recognition, speech enhancement, and speech separation. This dataset is the first data with accurate wave acoustic simulations in complex scenes. Codes and data are available at https://gamma.umd.edu/pro/sound/gwa.	https://dl.acm.org/doi/abs/10.1145/3528233.3530731	Zhenyu Tang, Rohith Aralikatti, Anton Jeran Ratnarajah, Dinesh Manocha
Generalized resampled importance sampling: foundations of ReSTIR	As scenes become ever more complex and real-time applications embrace ray tracing, path sampling algorithms that maximize quality at low sample counts become vital. Recent algorithms building on Talbot et al.'s [2005] resampled importance sampling (RIS) reuse paths spatiotemporally to render surprisingly complex light transport with a few samples per pixel. These reservoir-based spatiotemporal importance resamplers (ReSTIR) and their underlying RIS theory make various assumptions, including sample independence. But sample reuse , so ReSTIR-style iterative reuse loses most convergence guarantees that RIS theoretically provides. We introduce generalized resampled importance sampling (GRIS) to extend the theory, allowing RIS on correlated samples, with unknown PDFs and taken from varied domains. This solidifies the theoretical foundation, allowing us to derive variance bounds and convergence conditions in ReSTIR-based samplers. It also guides practical algorithm design and enables advanced path reuse between pixels via complex shift mappings. We show a path-traced resampler (ReSTIR PT) running interactively on complex scenes, capturing many-bounce diffuse and specular lighting while shading just one path per pixel. With our new theoretical foundation, we can also modify the algorithm to guarantee convergence for offline renderers.	https://dl.acm.org/doi/abs/10.1145/3528223.3530158	Daqi Lin, Markus Kettunen, Benedikt Bitterli, Jacopo Pantaleoni, Cem Yuksel, Chris Wyman
Generating 3D Human Texture from a Single Image with Sampling and Refinement	Generating the texture map for a 3D human mesh from a single image is challenging. To generate a plausible texture map, the invisible parts of the texture need to be synthesized with relevance to the visible part and the texture should semantically align to the UV space of the template mesh. To overcome such challenges, we propose a novel method that incorporates SamplerNet and RefineNet. SamplerNet predicts a sampling grid that enables sampling from the given visible texture information, and RefineNet refines the sampled texture to maintain spatial alignment.	https://dl.acm.org/doi/abs/10.1145/3532719.3543204	Sihun Cha, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh
Generating Diverse Indoor Furniture Arrangements	We present a method for generating arrangements of indoor furniture from human-designed furniture layout data. Our method creates arrangements that target specified diversity, such as the total price of all furniture in the room and the number of pieces placed. To generate realistic furniture arrangement, we train a generative adversarial network (GAN) on human-designed layouts. To target specific diversity in the arrangements, we optimize the latent space of the GAN via a quality diversity algorithm to generate a diverse arrangement collection. Experiments show our approach discovers a set of arrangements that are similar to human-designed layouts but varies in price and number of furniture pieces.	https://dl.acm.org/doi/abs/10.1145/3532719.3543244	Ya-Chuan Hsu, Matthew Fontaine, Sam Earle, Maria Edwards, Julian Togelius, Stefanos Nikolaidis
Generative GaitNet	Understanding the relation between anatomy and gait is key to successful predictive gait simulation. In this paper, we present Generative GaitNet, which is a novel network architecture based on deep reinforcement learning for controlling a comprehensive, full-body, musculoskeletal model with 304 Hill-type musculotendons. The Generative GaitNet is a pre-trained, integrated system of artificial neural networks learned in a 618-dimensional continuous domain of anatomy conditions (e.g., mass distribution, body proportion, bone deformity, and muscle deficits) and gait conditions (e.g., stride and cadence). The pre-trained GaitNet takes anatomy and gait conditions as input and generates a series of gait cycles appropriate to the conditions through physics-based simulation. We will demonstrate the efficacy and expressive power of Generative GaitNet to generate a variety of healthy and pathological human gaits in real-time physics-based simulation.	https://dl.acm.org/doi/abs/10.1145/3528233.3530717	Jungnam Park, Sehee Min, Phil Sik Chang, Jaedong Lee, Moon Seok Park, Jehee Lee
Glacier's lament	Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change.	https://dl.acm.org/doi/abs/10.1145/3532837.3534948	Jiabao Li, Cooper Galvin
Glacier's lament	Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change.	https://dl.acm.org/doi/abs/10.1145/3532837.3534948	Jiabao Li, Cooper Galvin
Glacier's lament	Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change.	https://dl.acm.org/doi/abs/10.1145/3532837.3534948	Jiabao Li, Cooper Galvin
Glacier's lament	Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change.	https://dl.acm.org/doi/abs/10.1145/3532837.3534948	Jiabao Li, Cooper Galvin
Glacier's lament	Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change.	https://dl.acm.org/doi/abs/10.1145/3532837.3534948	Jiabao Li, Cooper Galvin
Glacier's lament	Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change.	https://dl.acm.org/doi/abs/10.1145/3532837.3534948	Jiabao Li, Cooper Galvin
Glacier's lament	Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change.	https://dl.acm.org/doi/abs/10.1145/3532837.3534948	Jiabao Li, Cooper Galvin
Glacier's lament	Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change.	https://dl.acm.org/doi/abs/10.1145/3532837.3534948	Jiabao Li, Cooper Galvin
Glasshouse: A Dance for Virtual Reality: Glasshouse - A three part virtual reality experience for Oculus Quest 2	Glasshouse is a dance for virtual reality experience in three parts. A near future biosphere where plant and insect life thrive in an ecosystem of integrated biotechnology. Maintained by intuitive glasshouse keepers who farm water and light, biotech agriculture and drone insects work in synergy with ancient flora and heirloom edibles.	https://dl.acm.org/doi/abs/10.1145/3532834.3536200	Sarah Neville, Matthew Thomas, Alex DeGaris, Sue Hawksley, Daish Malani
Go Green: General Regularized Green’s Functions for Elasticity	The fundamental solutions (Green's functions) of linear elasticity for an infinite and isotropic media are ubiquitous in interactive graphics applications that cannot afford the computational costs of volumetric meshing and finite-element simulation. For instance, the recent work of de Goes and James [2017] leveraged these Green's functions to formulate sculpting tools capturing in real-time broad and physically-plausible deformations more intuitively and realistically than traditional editing brushes. In this paper, we extend this family of Green's functions by exploiting the anisotropic behavior of general linear elastic materials, where the relationship between stress and strain in the material depends on its orientation. While this more general framework prevents the existence of analytical expressions for its fundamental solutions, we show that a finite sum of spherical harmonics can be used to decompose a Green's function, which can be further factorized into directional, radial, and material-dependent terms. From such a decoupling, we show how to numerically derive sculpting brushes to generate anisotropic deformation and finely control their falloff profiles in real-time.	https://dl.acm.org/doi/abs/10.1145/3528233.3530726	Jiong Chen, Mathieu Desbrun
Graphic 2D-Inspired Characters in The Bad Guys	The world of The Bad Guys is an homage to illustration and our characters take inspiration from elements of hand-drawn 2D animation. In this talk we present various challenges of developing graphic characters with solutions that scale for feature production.	https://dl.acm.org/doi/abs/10.1145/3532836.3536244	Jeff Budsberg, Pablo Valle, JP Sans, Martin Costello, Nick Augello, Paolo de Guzman
Gravity Preloading for Maintaining Hair Shape Using the Simulator as a Closed-box Function	In animation, hair styles can often be modeled without the consideration of physics. One of the side effects of this workflow is that external forces such as gravity will deform the groom from the designed shape when simulated. We present a simple optimization algorithm that preloads the rest shape to compensate for the external forces to maintain the groom shape during simulation. The algorithm provides artistic control over how much force is compensated for at each vertex.	https://dl.acm.org/doi/abs/10.1145/3532836.3536238	Haixiang Liu
GravityPack: Exploring a Wearable Gravity Display for Immersive Interaction Using Liquid-based System	Previous works have done several kinds of haptic techniques for simulating the touching experience of the virtual object. However, the feedback on the object's weight has less been explored. This paper presents GravityPack, a wearable gravity display to simulate grabbing, holding, and releasing the virtual object in the virtual world using the liquid-based system consisting of pumps, pipes, valves, a water tank, and water packs. This system can provide a wide weight range from 110g to 1.8 kg in 40 seconds. Additionally, we design and investigate the visual feedback of weight transition for the delay time of liquid transfer to understand the feasibility of visualization by a user study. With the revealing of design consideration and implementation, the paper also shows the potential use of the liquid-based system and its possibility of the visualization technique to simulate the weight sensations.	https://dl.acm.org/doi/abs/10.1145/3532719.3543218	Yu-Yen Chen, Yi-Jie Lu, Ping-Hsuan Han
Green Screens, Green Pixels and Green Shooting	Sustainability and green producing are in high demand in all sectors of creative industries. Fortunately, this topic is received very well among film students providing an excellent opportunity for upcoming talent willing to apply new methods in creative processes. Virtualisation and Virtual Production in particular are predestined to play an essential role in fulfilling this demand. Factors that can be considered here are travel needs, lighting energy consumption, post-production complexity, energy sources and many more. The pandemic did propel these Virtual Production technologies to common practice, in particular large LED walls for In-Camera VFX (ICVFX). Some reports on the environmental impact of traditional film productions are available [albert 2020] estimating an average CO2 demand of 2840 tonnes for tentpole film productions. However, these tentpole productions did not consider VFX. To date, there is little to no knowledge on the sustainability of Virtual Production and how it compares to traditional offline VFX productions. We take a closer look at two comparable productions, one using traditional offline rendering and post-production, the other using an LED wall and ICVFX. Energy requirements, creative opportunities and scalability are subjects of investigation and further discussion. This abstract is a summary of a self published report on Virtual Production and its opportunities for sustainable film productions 1.	https://dl.acm.org/doi/abs/10.1145/3532836.3536235	Volker Helzle, Simon Spielmann, Jonas Trottnow
Grid-free Monte Carlo for PDEs with spatially varying coefficients	Partial differential equations (PDEs) with spatially varying coefficients arise throughout science and engineering, modeling rich heterogeneous material behavior. Yet conventional PDE solvers struggle with the immense complexity found in nature, since they must first discretize the problem---leading to spatial aliasing, and global meshing/sampling that is costly and error-prone. We describe a method that approximates neither the domain geometry, the problem data, nor the solution space, providing the exact solution (in expectation) even for problems with extremely detailed geometry and intricate coefficients. Our main contribution is to extend the algorithm from constant- to variable-coefficient problems, by drawing on techniques from volumetric rendering. In particular, an approach inspired by yields unbiased Monte Carlo estimators for a large class of 2nd order elliptic PDEs, which share many attractive features with Monte Carlo rendering: no meshing, trivial parallelism, and the ability to evaluate the solution at any point without solving a global system of equations.	https://dl.acm.org/doi/abs/10.1145/3528223.3530134	Rohan Sawhney, Dario Seyb, Wojciech Jarosz, Keenan Crane
Guided bubbles and wet foam for realistic whitewater simulation	"We present a method for enhancing fluid simulations with realistic bubble and foam detail. We treat bubbles as discrete air particles, two-way coupled with a sparse volumetric Euler flow, as first suggested in [Stomakhin et al. 2020]. We elaborate further on their scheme and introduce a bubble inertia correction term for improved convergence. We also show how one can add bubbles to an already existing fluid simulation using our novel guiding technique, which performs local re-simulation of fluid to achieve more interesting bubble dynamics through coupling. As bubbles reach the surface, they are converted into foam and simulated separately. Our foam is discretized with smoothed particle hydrodynamics (SPH), and we replace forces normal to the fluid surface with a fluid surface manifold advection constraint to achieve more robust and stable results. The SPH forces are derived through proper constitutive modeling of an incompressible viscous liquid, and we explain why this choice is appropriate for ""wet"" types of foam. This allows us to produce believable dynamics from close-up scenarios to large oceans, with just a few parameters that work intuitively across a variety of scales. Additionally, we present relevant research on air entrainment metrics and bubble distributions that have been used in this work."	https://dl.acm.org/doi/abs/10.1145/3528223.3530059	Joel Wretborn, Sean Flynn, Alexey Stomakhin
HDR Lighting Dilation for Dynamic Range Reduction on Virtual Production Stages	We present a technique to reduce the dynamic range of an HDRI lighting environment map in an efficient, energy-preserving manner by spreading out the light of concentrated light sources. This allows us to display a reasonable approximation of the illumination of an HDRI map in a lighting reproduction system with limited dynamic range such as virtual production LED Stage. The technique identifies regions of the HDRI map above a given pixel threshold, dilates these regions until the average pixel value within each is below the threshold, and finally replaces each dilated region's pixels with the region's average pixel value. The new HDRI map contains the same energy as the original, spreads the light as little as possible, and avoids chromatic fringing.	https://dl.acm.org/doi/abs/10.1145/3532719.3543243	Paul Debevec, Chloe LeGendre
HDR VR	The human visual system can resolve luminances from near zero to over a million candelas per meter squared (nits), and is able to simultaneously resolve over 4 orders of magnitude without adaptation [Kunkel and Reinhard 2010]. While most traditional displays only replicate a fraction of this range, high-dynamic-range (HDR) displays aim to support ranges closer to perceptual limits [Reinhard et al. 2010] and have achieved widespread use in cinemas and home theaters, but the perceptual impact of HDR remain largely unexplored in the context of Virtual Reality (VR) displays, which are typically limited to peak luminance values below 200 nits [Mehrfard et al. 2019]. To address this, we present an HDR VR demonstrator with a display system comprised entirely of off-the-shelf parts, capable of peak luminances over 16,000 nits. We achieve this without reducing the field of view (FOV) or simultaneous contrast relative to commercially-available VR headsets, while maintaining support for binocular and motion parallax depth cues. Consequently, our prototype has the potential to achieve a higher degree of perceptual realism than existing direct-view devices like HDR televisions and other high-luminance prototypes.	https://dl.acm.org/doi/abs/10.1145/3532721.3535566	Nathan Matsuda, Yang Zhao, Alex Chapiro, Clinton Smith, Douglas Lanman
Hair Emoting with Style Guides in Turning Red	For Pixar's feature film Turning Red, the grooming and simulation teams faced the challenge of handling characters with millions of fur and hair curves, which often needed to behave differently in each shot reflecting the characters' emotional states. This work describes new tools developed to assist artists in managing and sculpting these large amounts of fur and hair. In particular, we present a novel surface-aware technique for curve deformation that interpolates hair sculpts at varying levels of detail, accompanied by a customized user interface for interactively browsing hair layers.	https://dl.acm.org/doi/abs/10.1145/3532836.3536253	Brandon Montell, Fernando de Goes, Jacob Brooks
Hands-on Exploration of Hybrid 4D Printing Design Space	4D printing is an emerging field of fabrication garnering interest from the HCI community in its potential applications. In this work we present a hands-on exploration of the hybrid design space for 4D printing which makers can utilize to create their own artifacts by designing parts to control their bend, supply different heat triggers, and integrate craft techniques based on the intended use.	https://dl.acm.org/doi/abs/10.1145/3532725.3535587	Himani Deshpande, Courtney Starrett, Jinsil Hwaryoung Seo, Clement Zheng, Jeeeun Kim
High dynamic range and super-resolution from raw image bursts	Photographs captured by smartphones and mid-range cameras have limited spatial resolution and dynamic range, with noisy response in underexposed regions and color artefacts in saturated areas. This paper introduces the first approach (to the best of our knowledge) to the reconstruction of highresolution, high-dynamic range color images from raw photographic bursts captured by a handheld camera with exposure bracketing. This method uses a physically-accurate model of image formation to combine an iterative optimization algorithm for solving the corresponding inverse problem with a learned image representation for robust alignment and a learned natural image prior. The proposed algorithm is fast, with low memory requirements compared to state-of-the-art learning-based approaches to image restoration, and features that are learned end to end from synthetic yet realistic data. Extensive experiments demonstrate its excellent performance with super-resolution factors of up to ×4 on real photographs taken in the wild with hand-held cameras, and high robustness to low-light conditions, noise, camera shake, and moderate object motion.	https://dl.acm.org/doi/abs/10.1145/3528223.3530180	Bruno Lecouat, Thomas Eboli, Jean Ponce, Julien Mairal
High-Low Tech Ombro-Cinéma	Barrier-grid animation is a primitive animation technique that utilizes an interlaced image with a transparent striped overlay. Ombro-Cinéma, a toy developed in 1921, utilizes this technique effectively with a scroll of graphics. By rotating an attached hand-crank, it presents a scrolling animated story, the narrative of which is often tightly associated with its mechanical structure. This paper describes High-Low Tech Ombro-Cinéma, a prototype device that extends Ombro-Cinéma using an embedded system. Conductive adhesive tape strips are affixed to the back side of a scroll and used to trigger various sonic events to enhance its narrativity. Such an investigation into the reproduction of an obsolete toy with digital technology may provide an interesting opportunity to review the narrative of an obsolete device and to reconsider the effects tangible interfaces have on narratives.	https://dl.acm.org/doi/abs/10.1145/3532719.3543225	Wakasa Noguchi, Hiroki Nishino
High-fidelity facial reconstruction from a single photo using photo-realistic rendering	We propose a fully automated method for realistic 3D face reconstruction from a single frontal photo that produces a high-resolution head mesh and a diffuse map. The photo is input to a convolutional neural network that estimates the weights of a morphable model to produce an initial head shape that is further adjusted through landmark-guided deformation. Two key features of the method are: 1) the network is exclusively trained on synthetic photos that are photo-realistic enough to learn real shape predictive features, making it unnecessary to train with real facial photos and corresponding 3D scans; 2) the landmarking statistical errors are incorporated in the reconstruction for optimal accuracy. While the method is based on a limited amount of real data, we show that it robustly and quickly performs plausible face reconstructions from real photos.	https://dl.acm.org/doi/abs/10.1145/3532836.3536273	Mariana Dias, Alexis Roche, Margarida Fernandes, Verónica Orvalho
Holographic Glasses for Virtual Reality	We present Holographic Glasses, a holographic near-eye display system with an eyeglasses-like form factor for virtual reality. Holographic Glasses are composed of a pupil-replicating waveguide, a spatial light modulator, and a geometric phase lens to create holographic images in a lightweight and thin form factor. The proposed design can deliver full-color 3D holographic images using an optical stack of 2.5 mm thickness. A novel pupil-high-order gradient descent algorithm is presented for the correct phase calculation with the user's varying pupil size. We implement benchtop and wearable prototypes for testing. Our binocular wearable prototype supports 3D focus cues and provides a diagonal field of view of 22.8° with a 2.3 mm static eye box and additional capabilities of dynamic eye box with beam steering, while weighing only 60 g excluding the driving board.	https://dl.acm.org/doi/abs/10.1145/3528233.3530739	Jonghyun Kim, Manu Gopakumar, Suyeon Choi, Yifan Peng, Ward Lopes, Gordon Wetzstein
Holographic Sign Language Interpreters	We describe the implementation of a prototype system of 3D holographic sign language interpreters. The signing avatars, observed through wearable Mixed Reality (MR) smartglasses (e.g., Microsoft HoloLens), translate speech to Signed Exact English (SEE) in real-time. Such a system can be used by deaf and hard of hearing students in the classroom or other contexts to remove current accessibility barriers.	https://dl.acm.org/doi/abs/10.1145/3532724.3535593	Fu-Chia Yang, Christos Mousas, Nicoletta Adamo
How Ron’s Gone Wrong Went Right With Procedurally Generated Vector Graphics	In the feature animation film Ron's Gone Wrong there are many robot characters designed to have their surface working like a display. The acting of those characters relied mostly on moving graphic elements represented on their surface: animated faces, costumes and props. All graphic elements had to be art-directed, requiring the animation department to be able to control them interactively. We propose a procedural workflow to produce animated textures from a rig, apply filters and project them onto the character's surface to mimic a screen-like effect. The workflow proved to be flexible and scalable to address many characters in production, with several levels of complexity and a lot of creative input.	https://dl.acm.org/doi/abs/10.1145/3532836.3536230	Toba Siebzener, Jesus R. Nieto, Ryan Chan, Paul Baaske
Hummingbird: A Collaborative Live Theater and Virtual Reality Adventure	Hummingbird: is a modern, innovative performance merging live theater and interactive virtual reality by bringing a group of active participants into a shared space for a live performance. The performance premiered as part of Chicago's Tony Award-winning Goodman Theatre New Stages Festival showcasing innovative and ground-breaking theater works in December 2021. This project bridges art, science and live theater through a collaborative research effort between computer science and design faculty and students at the University of Illinois Chicago (UIC) Electronic Visualization Laboratory (EVL) and Chicago theater directors, actors, videographers and producers. Hummingbird's story celebrates courage and coming of age through the eyes of a gutsy teen who must outsmart her mother's narcissistic boss and survive dangerous new technology in a live, immersive adventure. Hummingbird extends traditional live theater and makes virtual reality art accessible to a broader audience, demonstrating how virtual reality can transform theatrical storytelling.	https://dl.acm.org/doi/abs/10.1145/3532834.3536213	Daria Tsoupikova, Jo Cattell, Andrew Johnson, Lance Long, Arthur Nishimoto, Sai Priya Jyothula
HyperJumping in Virtual Vancouver: Combating Motion Sickness by Merging Teleporting and Continuous VR Locomotion in an Embodied Hands-Free VR Flying Paradigm	Motion sickness, unintuitive navigation, and limited agency are critical issues in VR/XR impeding wide-spread adoption and enjoyable user experiences. To tackle these challenges, we present HyperJump, a novel VR interface merging advantages of continuous locomotion and teleportation/dashing into one seamless, hands-free, and easily learnable user interface supporting both flying and ground-based navigation across multiple scales.	https://dl.acm.org/doi/abs/10.1145/3532834.3536211	Bernhard E. Riecke, David Clement, Ashu Adhikari, Denise Quesnel, Daniel Zielasko, Markus von der Heyde
ImLoveNet: Misaligned Image-supported Registration Network for Low-overlap Point Cloud Pairs	Low-overlap regions between paired point clouds make the captured features very low-confidence, leading cutting edge models to point cloud registration with poor quality. Beyond the traditional wisdom, we raise an intriguing question: Is it possible to exploit an intermediate yet misaligned image between two low-overlap point clouds to enhance the performance of cutting-edge registration models? To answer it, we propose a misaligned image supported registration network for low-overlap point cloud pairs, dubbed ImLoveNet. ImLoveNet first learns triple deep features across different modalities and then exports these features to a two-stage classifier, for progressively obtaining the high-confidence overlap region between the two point clouds. Therefore, soft correspondences are well established on the predicted overlap region, resulting in accurate rigid transformations for registration. ImLoveNet is simple to implement yet effective, since 1) the misaligned image provides clearer overlap information for the two low-overlap point clouds to better locate overlap parts; 2) it contains certain geometry knowledge to extract better deep features; and 3) it does not require the extrinsic parameters of the imaging device with respect to the reference frame of the 3D point cloud. Extensive qualitative and quantitative evaluations on different kinds of benchmarks demonstrate the effectiveness and superiority of our ImLoveNet over state-of-the-art approaches.	https://dl.acm.org/doi/abs/10.1145/3528233.3530744	Honghua Chen, Zeyong Wei, Yabin Xu, Mingqiang Wei, Jun Wang
Image features influence reaction time: a learned probabilistic perceptual model for saccade latency	"We aim to ask and answer an essential question "" do we react observing a displayed visual target?"" To this end, we present psychophysical studies that characterize the remarkable disconnect between human saccadic behaviors and spatial visual acuity. Building on the results of our studies, we develop a perceptual model to predict temporal gaze behavior, particularly saccadic latency, as a function of the statistics of a displayed image. Specifically, we implement a neurologically-inspired probabilistic model that mimics the accumulation of confidence that leads to a perceptual decision. We validate our model with a series of objective measurements and user studies using an eye-tracked VR display. The results demonstrate that our model prediction is in statistical alignment with real-world human behavior. Further, we establish that many sub-threshold image modifications commonly introduced in graphics pipelines may significantly alter human reaction timing, even if the differences are visually undetectable. Finally, we show that our model can serve as a metric to predict and alter reaction latency of users in interactive computer graphics applications, thus may improve gaze-contingent rendering, design of virtual experiences, and player performance in e-sports. We illustrate this with two examples: estimating competition fairness in a video game with two different team colors, and tuning display viewing distance to minimize player reaction time."	https://dl.acm.org/doi/abs/10.1145/3528223.3530055	Budmonde Duinkharjav, Praneeth Chakravarthula, Rachel Brown, Anjul Patney, Qi Sun
ImageFlowing-Enhance Emotional Expression by Reproducing the Vital Signs of the Photographer	ImageFlowing is a 'living' photograph that reproduces the biometric signs of the photographer. Viewers can feel how the photographer felt through photographer's breathing, heartbeats and skin temperature. We extend a two-dimensional picture into a multi-modal experience, aiming at creating a tighter emotional link between the viewer and the photographer.	https://dl.acm.org/doi/abs/10.1145/3532721.3535565	Qianqian Mu, George Chernyshov, Ziyue Wang, Danny Hynds, Dingding Zheng, Kouta Minamizawa, Dunya Chen, Atsuro Ueki, Masa Inakage, Kai Kunze
Imagraph	"is a medium that arbitrates between two primordial ""attitudes"": projecting the images and closing the senses. Participants lie on their backs in a box-shaped bed and shut their eyes. Two pieces of LED display board are suspended above their heads, with optical fibers from each pixel extending to the surface of their eyelids. The video prepared in advance is unilaterally played after the spectral compensation against the ""blood-red"" unique to their own flesh. There, the eyelid becomes the medium for the very object it is trying to block off. The intended colors and their placements are sent, but this in no way implies the triumph of projection over closing. The ritualistic posture forms a distinctive video place filled with the ""tone of the unconscious."" Just as eye closure deprives the participant of the rejection capability, projection as the arrayed commands also loses its authority. Participants unintentionally derive movements and colors that are not actually presented to them and blend the internally sprung image into the video irreversibly. Both projecting and closing are dysfunctional, and a singular relationship is established between the ""projector"" and the ""projectee."" What is the freedom each pole has, and where does the ""image"" inhabit?"	https://dl.acm.org/doi/abs/10.1145/3532837.3534947	Goki Muramoto
Imagraph	"is a medium that arbitrates between two primordial ""attitudes"": projecting the images and closing the senses. Participants lie on their backs in a box-shaped bed and shut their eyes. Two pieces of LED display board are suspended above their heads, with optical fibers from each pixel extending to the surface of their eyelids. The video prepared in advance is unilaterally played after the spectral compensation against the ""blood-red"" unique to their own flesh. There, the eyelid becomes the medium for the very object it is trying to block off. The intended colors and their placements are sent, but this in no way implies the triumph of projection over closing. The ritualistic posture forms a distinctive video place filled with the ""tone of the unconscious."" Just as eye closure deprives the participant of the rejection capability, projection as the arrayed commands also loses its authority. Participants unintentionally derive movements and colors that are not actually presented to them and blend the internally sprung image into the video irreversibly. Both projecting and closing are dysfunctional, and a singular relationship is established between the ""projector"" and the ""projectee."" What is the freedom each pole has, and where does the ""image"" inhabit?"	https://dl.acm.org/doi/abs/10.1145/3532837.3534947	Goki Muramoto
Imagraph	"is a medium that arbitrates between two primordial ""attitudes"": projecting the images and closing the senses. Participants lie on their backs in a box-shaped bed and shut their eyes. Two pieces of LED display board are suspended above their heads, with optical fibers from each pixel extending to the surface of their eyelids. The video prepared in advance is unilaterally played after the spectral compensation against the ""blood-red"" unique to their own flesh. There, the eyelid becomes the medium for the very object it is trying to block off. The intended colors and their placements are sent, but this in no way implies the triumph of projection over closing. The ritualistic posture forms a distinctive video place filled with the ""tone of the unconscious."" Just as eye closure deprives the participant of the rejection capability, projection as the arrayed commands also loses its authority. Participants unintentionally derive movements and colors that are not actually presented to them and blend the internally sprung image into the video irreversibly. Both projecting and closing are dysfunctional, and a singular relationship is established between the ""projector"" and the ""projectee."" What is the freedom each pole has, and where does the ""image"" inhabit?"	https://dl.acm.org/doi/abs/10.1145/3532837.3534947	Goki Muramoto
Imagraph	"is a medium that arbitrates between two primordial ""attitudes"": projecting the images and closing the senses. Participants lie on their backs in a box-shaped bed and shut their eyes. Two pieces of LED display board are suspended above their heads, with optical fibers from each pixel extending to the surface of their eyelids. The video prepared in advance is unilaterally played after the spectral compensation against the ""blood-red"" unique to their own flesh. There, the eyelid becomes the medium for the very object it is trying to block off. The intended colors and their placements are sent, but this in no way implies the triumph of projection over closing. The ritualistic posture forms a distinctive video place filled with the ""tone of the unconscious."" Just as eye closure deprives the participant of the rejection capability, projection as the arrayed commands also loses its authority. Participants unintentionally derive movements and colors that are not actually presented to them and blend the internally sprung image into the video irreversibly. Both projecting and closing are dysfunctional, and a singular relationship is established between the ""projector"" and the ""projectee."" What is the freedom each pole has, and where does the ""image"" inhabit?"	https://dl.acm.org/doi/abs/10.1145/3532837.3534947	Goki Muramoto
Imagraph	"is a medium that arbitrates between two primordial ""attitudes"": projecting the images and closing the senses. Participants lie on their backs in a box-shaped bed and shut their eyes. Two pieces of LED display board are suspended above their heads, with optical fibers from each pixel extending to the surface of their eyelids. The video prepared in advance is unilaterally played after the spectral compensation against the ""blood-red"" unique to their own flesh. There, the eyelid becomes the medium for the very object it is trying to block off. The intended colors and their placements are sent, but this in no way implies the triumph of projection over closing. The ritualistic posture forms a distinctive video place filled with the ""tone of the unconscious."" Just as eye closure deprives the participant of the rejection capability, projection as the arrayed commands also loses its authority. Participants unintentionally derive movements and colors that are not actually presented to them and blend the internally sprung image into the video irreversibly. Both projecting and closing are dysfunctional, and a singular relationship is established between the ""projector"" and the ""projectee."" What is the freedom each pole has, and where does the ""image"" inhabit?"	https://dl.acm.org/doi/abs/10.1145/3532837.3534947	Goki Muramoto
Imagraph	"is a medium that arbitrates between two primordial ""attitudes"": projecting the images and closing the senses. Participants lie on their backs in a box-shaped bed and shut their eyes. Two pieces of LED display board are suspended above their heads, with optical fibers from each pixel extending to the surface of their eyelids. The video prepared in advance is unilaterally played after the spectral compensation against the ""blood-red"" unique to their own flesh. There, the eyelid becomes the medium for the very object it is trying to block off. The intended colors and their placements are sent, but this in no way implies the triumph of projection over closing. The ritualistic posture forms a distinctive video place filled with the ""tone of the unconscious."" Just as eye closure deprives the participant of the rejection capability, projection as the arrayed commands also loses its authority. Participants unintentionally derive movements and colors that are not actually presented to them and blend the internally sprung image into the video irreversibly. Both projecting and closing are dysfunctional, and a singular relationship is established between the ""projector"" and the ""projectee."" What is the freedom each pole has, and where does the ""image"" inhabit?"	https://dl.acm.org/doi/abs/10.1145/3532837.3534947	Goki Muramoto
Imagraph	"is a medium that arbitrates between two primordial ""attitudes"": projecting the images and closing the senses. Participants lie on their backs in a box-shaped bed and shut their eyes. Two pieces of LED display board are suspended above their heads, with optical fibers from each pixel extending to the surface of their eyelids. The video prepared in advance is unilaterally played after the spectral compensation against the ""blood-red"" unique to their own flesh. There, the eyelid becomes the medium for the very object it is trying to block off. The intended colors and their placements are sent, but this in no way implies the triumph of projection over closing. The ritualistic posture forms a distinctive video place filled with the ""tone of the unconscious."" Just as eye closure deprives the participant of the rejection capability, projection as the arrayed commands also loses its authority. Participants unintentionally derive movements and colors that are not actually presented to them and blend the internally sprung image into the video irreversibly. Both projecting and closing are dysfunctional, and a singular relationship is established between the ""projector"" and the ""projectee."" What is the freedom each pole has, and where does the ""image"" inhabit?"	https://dl.acm.org/doi/abs/10.1145/3532837.3534947	Goki Muramoto
Imagraph	"is a medium that arbitrates between two primordial ""attitudes"": projecting the images and closing the senses. Participants lie on their backs in a box-shaped bed and shut their eyes. Two pieces of LED display board are suspended above their heads, with optical fibers from each pixel extending to the surface of their eyelids. The video prepared in advance is unilaterally played after the spectral compensation against the ""blood-red"" unique to their own flesh. There, the eyelid becomes the medium for the very object it is trying to block off. The intended colors and their placements are sent, but this in no way implies the triumph of projection over closing. The ritualistic posture forms a distinctive video place filled with the ""tone of the unconscious."" Just as eye closure deprives the participant of the rejection capability, projection as the arrayed commands also loses its authority. Participants unintentionally derive movements and colors that are not actually presented to them and blend the internally sprung image into the video irreversibly. Both projecting and closing are dysfunctional, and a singular relationship is established between the ""projector"" and the ""projectee."" What is the freedom each pole has, and where does the ""image"" inhabit?"	https://dl.acm.org/doi/abs/10.1145/3532837.3534947	Goki Muramoto
Immersive Real World through Deep Billboards	"An aspirational goal for virtual reality (VR) is to bring in a rich diversity of real world objects losslessly. Existing VR applications often convert objects into explicit 3D models with meshes or point clouds, which allow fast interactive rendering but also severely limit its quality and the types of supported objects, fundamentally upper-bounding the ""realism"" of VR. Inspired by the classic ""billboards"" technique in gaming, we develop Deep Billboards that model 3D objects implicitly using neural networks, where only 2D image is rendered at a time based on the user's viewing direction. Our system, connecting a commercial VR headset with a server running neural rendering, allows real-time high-resolution simulation of detailed rigid objects, hairy objects, actuated dynamic objects and more in an interactive VR world, drastically narrowing the existing real-to-simulation (real2sim) gap. Additionally, we augment Deep Billboards with physical interaction capability, adapting classic billboards from screen-based games to immersive VR. At our pavilion, the visitors can use our off-the-shelf setup for quickly capturing their favorite objects, and within minutes, experience them in an immersive and interactive VR world – with minimal loss of reality. Our project page: https://sites.google.com/view/deepbillboards/"	https://dl.acm.org/doi/abs/10.1145/3532834.3536210	Naruya Kondo, So Kuroki, Ryosuke Hyakuta, Yutaka Matsuo, Shixiang Shane Gu, Yoichi Ochiai
Immersive-Labeler: Immersive Annotation of Large-Scale 3D Point Clouds in Virtual Reality	We present Immersive-Labeler, an environment for the annotation of large-scale 3D point cloud scenes of urban environments. Our concept is based on the full immersion of the user in a VR-based environment that represents the 3D point cloud scene while offering adapted visual aids and intuitive interaction and navigation modalities. Through a user-centric design, we aim to improve the annotation experience and thus reduce its costs. For the preliminary evaluation of our environment, we conduct a user study (N=20) to quantify the effect of higher levels of immersion in combination with the visual aids we implemented on the annotation process. Our findings reveal that higher levels of immersion combined with object-based visual aids lead to a faster and more engaging annotation process.	https://dl.acm.org/doi/abs/10.1145/3532719.3543249	Achref Doula, Tobias Güdelhöfer, Andrii Mativiienko, Max Mühlhäuser, Alejandro Sanchez Guinea
Implicit neural representation for physics-driven actuated soft bodies	Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.	https://dl.acm.org/doi/abs/10.1145/3528223.3530156	Lingchen Yang, Byungsoo Kim, Gaspard Zoss, Baran Gözcü, Markus Gross, Barbara Solenthaler
Inclusive Character Creator: A Showcase of Inclusive Design Principles for 3D Character Creators	"Inclusive Character Creator is a speculative design research project that seeks to address some of the long-standing issues of sexism, racism, ableism, and sizeism prevalent in most 3D character creators in interactive media. This project focuses on stylized and expressive features rather than hyperrealism. It seeks to redefine what it means to start a character from a ""default body,"" a definition that usually results in creating a biased system that relies on media norms. A version 1.0 has been built, encapsulating fundamental principles resulting from research."	https://dl.acm.org/doi/abs/10.1145/3532719.3543201	Michelle Ma, Marientina Gotsis, Bonnie Harris-Lowe, Matthew Coopilton
Instant Neural Radiance Fields	We extend our instant NeRF implementation [Müller et al. 2022] to allow training from an incremental stream of images and camera poses, provided by a realtime Simultaneous Localization And Mapping (SLAM) system. Camera poses are refined end-to-end by back-propagating the gradients from NeRF training. Reconstruction quality is further improved by compensating for various camera properties, such as rolling shutter, non-linear lens distortion, and variable exposure typical of digital cameras. Static scenes can be scanned, the NeRF model trained, and the reconstruction verified in an interactive fashion, in under a minute.	https://dl.acm.org/doi/abs/10.1145/3532833.3538678	Thomas Müller, Alex Evans, Christoph Schied, Marco Foco, András Bódis-Szomorú, Isaac Deutsch, Michael Shelley, Alexander Keller
Instant neural graphics primitives with a multiresolution hash encoding	Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.	https://dl.acm.org/doi/abs/10.1145/3528223.3530127	Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller
Interactive Editing of Monocular Depth	Recent advances in computer vision have made 3D structure-aware editing of still photographs a reality. Such computational photography applications use a depth map that is automatically generated by monocular depth estimation methods to represent the scene structure. In this work, we present a lightweight, web-based interactive depth editing and visualization tool that adapts low-level conventional image editing operations for geometric manipulation to enable artistic control in the 3D photography workflow. Our tool provides real-time feedback on the geometry through a 3D scene visualization to make the depth map editing process more intuitive for artists. Our web-based tool is open-source1 and platform-independent to support wider adoption of 3D photography techniques in everyday digital photography.	https://dl.acm.org/doi/abs/10.1145/3532719.3543235	Obumneme Stanley Dukor, S. Mahdi H. Miangoleh, Mahesh Kumar Krishna Reddy, Long Mai, Yağız Aksoy
Interactive augmented reality storytelling guided by scene semantics	We present a novel interactive augmented reality (AR) storytelling approach guided by indoor scene semantics. Our approach automatically populates virtual contents in real-world environments to deliver AR stories, which match both the story plots and scene semantics. During the storytelling process, a player can participate as a character in the story. Meanwhile, the behaviors of the virtual characters and the placement of the virtual items adapt to the player's actions. An input raw story is represented as a sequence of events, which contain high-level descriptions of the characters' states, and is converted into a graph representation with automatically supplemented low-level spatial details. Our hierarchical story sampling approach samples realistic character behaviors that fit the story contexts through optimizations; and an animator, which estimates and prioritizes the player's actions, animates the virtual characters to tell the story in AR. Through experiments and a user study, we validated the effectiveness of our approach for AR storytelling in different environments.	https://dl.acm.org/doi/abs/10.1145/3528223.3530061	Changyang Li, Wanwan Li, Haikun Huang, Lap-Fai Yu
Intro to Creating Real-time Shaders	Real-time shaders are incredibly powerful in game design. This hands-on session will introduce beginners to the basics of creating shaders for games and other real-time applications using a node-based shader editor. It will include a brief overview of the subject, step-by-step tutorials for creating three simple shaders (Distortion, Flipbook Animation, and Environmental Blend), as well as explanations of the math principles used.  The session will be ideal for those wanting to get a jump start in learning shader creation. Though this course will be taught in Unity, the processes are very similar and can easily be translated to work inside of Unreal or other game engines with a node-based interface.	https://dl.acm.org/doi/abs/10.1145/3532725.3538517	Ben Cloward
Investigating Experimental Design+Theater+Coding Methodology to Teach Virtual Reality Design: A Case Study: Investigating Experimental Design+Theater+Coding Methodology	"This talk presents a case study of how two Chicago-area faculty based at two different academic institutions collaboratively investigated the use of an original, experimental, and cross-disciplinary ""Design+Theater+Coding"" methodology for the teaching and guiding of University of Illinois Chicago (UIC) undergraduate design students in the creation of thought-provoking and empathy-engaging interactive Virtual Reality team projects regarding global issues, all within the context of a single-semester creative coding course in Fall 2021. This experiment explored and leveraged the relationship between theater, creative coding, and VR technology and considered how the relationship might enhance design studies."	https://dl.acm.org/doi/abs/10.1145/3532724.3535595	Daria Tsoupikova, Jeff Nyhoff
Iterative poisson surface reconstruction (iPSR) for unoriented points	Poisson surface reconstruction (PSR) remains a popular technique for reconstructing watertight surfaces from 3D point samples thanks to its efficiency, simplicity, and robustness. Yet, the existing PSR method and subsequent variants work only for oriented points. This paper intends to validate that an improved PSR, called iPSR, can completely eliminate the requirement of point normals and proceed in an iterative manner. In each iteration, iPSR takes as input point samples with normals directly computed from the surface obtained in the preceding iteration, and then generates a new surface with better quality. Extensive quantitative evaluation confirms that the new iPSR algorithm converges in 5--30 iterations even with randomly initialized normals. If initialized with a simple visibility based heuristic, iPSR can further reduce the number of iterations. We conduct comprehensive comparisons with PSR and other powerful implicit-function based methods. Finally, we confirm iPSR's effectiveness and scalability on the AIM@SHAPE dataset and challenging (indoor and outdoor) scenes. Code and data for this paper are at https://github.com/houfei0801/ipsr.	https://dl.acm.org/doi/abs/10.1145/3528223.3530096	Fei Hou, Chiyu Wang, Wencheng Wang, Hong Qin, Chen Qian, Ying He
Joint neural phase retrieval and compression for energy- and computation-efficient holography on the edge	Recent deep learning approaches have shown remarkable promise to enable high fidelity holographic displays. However, lightweight wearable display devices cannot afford the computation demand and energy consumption for hologram generation due to the limited onboard compute capability and battery life. On the other hand, if the computation is conducted entirely remotely on a cloud server, transmitting lossless hologram data is not only challenging but also result in prohibitively high latency and storage. In this work, by distributing the computation and optimizing the transmission, we propose the first framework that jointly generates and compresses high-quality phase-only holograms. Specifically, our framework asymmetrically separates the hologram generation process into high-compute remote encoding (on the server), and low-compute decoding (on the edge) stages. Our encoding enables light weight latent space data, thus faster and efficient transmission to the edge device. With our framework, we observed a reduction of 76% computation and consequently 83% in energy cost on edge devices, compared to the existing hologram generation methods. Our framework is robust to transmission and decoding errors, and approach high image fidelity for as low as 2 bits-per-pixel, and further reduced average bit-rates and decoding time for holographic videos.	https://dl.acm.org/doi/abs/10.1145/3528223.3530070	Yujie Wang, Praneeth Chakravarthula, Qi Sun, Baoquan Chen
Journal of My Journey: Seamless Interaction in Virtuality and Reality with Digital Fabrication and Sensory Feedback	Whether in virtuality or reality, the decisions or actions pursue the goal we acquire in the virtual or real world. Besides, the decision or action that the users make in virtuality or reality only affects one physical or virtual environment. Previous works have revealed the concept of substitutional reality that utilizes the physical environment to enhance the immersive experience. However, there was no trace that the event in virtuality has parallel happened in the physical space. We present Journal of My Journey, an extended reality system with digital fabrication and sensory feedback for seamless interaction in virtuality and reality. The user's behavior in the virtuality does affect reality. To show the ideal, we developed an immersive game to explore the possibilities of bringing sensory feedback from the real world to help with puzzle-solving in the game and exporting the output of the decisions made by the players perform in the virtual world into reality by using digital fabrication.	https://dl.acm.org/doi/abs/10.1145/3532834.3536205	Miguel Ying Jie Then, Ching Lui, Yvone Tsai Chen, Zin Yin Lim, Ping Hsuan Han
Learning From Documents in the Wild to Improve Document Unwarping	Document image unwarping is important for document digitization and analysis. The state-of-the-art approach relies on purely synthetic data to train deep networks for unwarping. As a result, the trained networks have generalization limitations when testing on real-world images, often yielding unsatisfying results. In this work, we propose to improve document unwarping performance by incorporating real-world images in training. We collected Document-in-the-Wild (DIW) dataset contains 5000 captured document images with large diversities in content, shape, and capturing environment. We annotate the boundaries of all DIW images and use them for weakly supervised learning. We propose a novel network architecture, PaperEdge, to train with a hybrid of synthetic and real document images. Additionally, we identify and analyze the flaws of popular evaluation metrics, e.g., MS-SSIM and Local Distortion (LD), for document unwarping and propose a more robust and reliable error metric called Aligned Distortion (AD). Training with a combination of synthetic and real-world document images, we demonstrate state-of-the-art performance on popular benchmarks with comprehensive quantitative evaluations and ablation studies. Code and data are available at https://github.com/cvlab-stonybrook/PaperEdge.	https://dl.acm.org/doi/abs/10.1145/3528233.3530756	Ke Ma, Sagnik Das, Zhixin Shu, Dimitris Samaras
Learning Smooth Neural Functions via Lipschitz Regularization	Neural implicit fields have recently emerged as a useful representation for 3D shapes. These fields are commonly represented as neural networks which map latent descriptors and 3D coordinates to implicit function values. The latent descriptor of a neural field acts as a deformation handle for the 3D shape it represents. Thus, smoothness with respect to this descriptor is paramount for performing shape-editing operations. In this work, we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the field's Lipschitz constant. Compared with prior Lipschitz regularized networks, ours is computationally fast, can be implemented in four lines of code, and requires minimal hyperparameter tuning for geometric applications. We demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from 3D point clouds, showing both qualitative and quantitative improvements over existing state-of-the-art and non-regularized baselines.	https://dl.acm.org/doi/abs/10.1145/3528233.3530713	Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, Or Litany
Learning Soccer Juggling Skills with Layer-wise Mixture-of-Experts	Learning physics-based character controllers that can successfully integrate diverse motor skills using a single policy remains a challenging problem. We present a system to learn control policies for multiple soccer juggling skills, based on deep reinforcement learning. We introduce a task-description framework for these skills which facilitates the specification of individual soccer juggling tasks and the transitions between them. Desired motions can be authored using interpolation of crude reference poses or based on motion capture data. We show that a layer-wise mixture-of-experts architecture offers significant benefits. During training, transitions are chosen with the help of an adaptive random walk, in support of efficient learning. We demonstrate foot, head, knee, and chest juggles, foot stalls, the challenging around-the-world trick, as well as robust transitions. Our work provides a significant step towards realizing physics-based characters capable of the precision-based motor skills of human athletes. Code is available at https://github.com/ZhaomingXie/soccer_juggle_release.	https://dl.acm.org/doi/abs/10.1145/3528233.3530735	Zhaoming Xie, Sebastian Starke, Hung Yu Ling, Michiel van de Panne
Learning high-DOF reaching-and-grasping via dynamic representation of gripper-object interaction	We approach the problem of high-DOF reaching-and-grasping via learning joint planning of grasp and motion with deep reinforcement learning. To resolve the sample efficiency issue in learning the high-dimensional and complex control of dexterous grasping, we propose an effective representation of grasping state characterizing the spatial interaction between the gripper and the target object. To represent gripper-object interaction, we adopt Interaction Bisector Surface (IBS) which is the Voronoi diagram between two close by 3D geometric objects and has been successfully applied in characterizing spatial relations between 3D objects. We found that IBS is surprisingly effective as a state representation since it well informs the finegrained control of each finger with spatial relation against the target object. This novel grasp representation, together with several technical contributions including a fast IBS approximation, a novel vector-based reward and an effective training strategy, facilitate learning a strong control model of high-DOF grasping with good sample efficiency, dynamic adaptability, and cross-category generality. Experiments show that it generates high-quality dexterous grasp for complex shapes with smooth grasping motions. Code and data for this paper are at https://github.com/qijinshe/IBS-Grasping.	https://dl.acm.org/doi/abs/10.1145/3528223.3530091	Qijin She, Ruizhen Hu, Juzhan Xu, Min Liu, Kai Xu, Hui Huang
Learning to Brachiate via Simplified Model Imitation	Brachiation is the primary form of locomotion for gibbons and siamangs, in which these primates swing from tree limb to tree limb using only their arms. It is challenging to control because of the limited control authority, the required advance planning, and the precision of the required grasps. We present a novel approach to this problem using reinforcement learning, and as demonstrated on a finger-less 14-link planar model that learns to brachiate across challenging handhold sequences. Key to our method is the use of a simplified model, a point mass with a virtual arm, for which we first learn a policy that can brachiate across handhold sequences with a prescribed order. This facilitates the learning of the policy for the full model, for which it provides guidance by providing an overall center-of-mass trajectory to imitate, as well as for the timing of the holds. Lastly, the simplified model can also readily be used for planning suitable sequences of handholds in a given environment. Our results demonstrate brachiation motions with a variety of durations for the flight and hold phases, as well as emergent extra back-and-forth swings when this proves useful. The system is evaluated with a variety of ablations. The method enables future work towards more general 3D brachiation, as well as using simplified model imitation in other settings. For videos, supplementary material and code, visit: https://brachiation-rl.github.io/brachiation.	https://dl.acm.org/doi/abs/10.1145/3528233.3530728	Daniele Reda, Hung Yu Ling, Michiel van de Panne
Learning to Get Up	Getting up from an arbitrary fallen state is a basic human skill. Existing methods for learning this skill often generate highly dynamic and erratic get-up motions, which do not resemble human get-up strategies, or are based on tracking recorded human get-up motions. In this paper, we present a staged approach using reinforcement learning, without recourse to motion capture data. The method first takes advantage of a strong character model, which facilitates the discovery of solution modes. A second stage then learns to adapt the control policy to work with progressively weaker versions of the character. Finally, a third stage learns control policies that can reproduce the weaker get-up motions at much slower speeds. We show that across multiple runs, the method can discover a diverse variety of get-up strategies, and execute them at a variety of speeds. The results usually produce policies that use a final stand-up strategy that is common to the recovery motions seen from all initial states. However, we also find policies for which different strategies are seen for prone and supine initial fallen states. The learned get-up control strategies often have significant static stability, i.e., they can be paused at a variety of points during the get-up motion. We further test our method on novel constrained scenarios, such as having a leg and an arm in a cast.	https://dl.acm.org/doi/abs/10.1145/3528233.3530697	Tianxin Tao, Matthew Wilson, Ruiyu Gou, Michiel van de Panne
Learning to use chopsticks in diverse gripping styles	Learning dexterous manipulation skills is a long-standing challenge in computer graphics and robotics, especially when the task involves complex and delicate interactions between the hands, tools and objects. In this paper, we focus on chopsticks-based object relocation tasks, which are common yet demanding. The key to successful chopsticks skills is steady gripping of the sticks that also supports delicate maneuvers. We automatically discover physically valid chopsticks holding poses by Bayesian Optimization (BO) and Deep Reinforcement Learning (DRL), which works for multiple gripping styles and hand morphologies without the need of example data. Given as input the discovered gripping poses and desired objects to be moved, we build physics-based hand controllers to accomplish relocation tasks in two stages. First, kinematic trajectories are synthesized for the chopsticks and hand in a motion planning stage. The key components of our motion planner include a grasping model to select suitable chopsticks configurations for grasping the object, and a trajectory optimization module to generate collision-free chopsticks trajectories. Then we train physics-based hand controllers through DRL again to track the desired kinematic trajectories produced by the motion planner. We demonstrate the capabilities of our framework by relocating objects of various shapes and sizes, in diverse gripping styles and holding positions for multiple hand morphologies. Our system achieves faster learning speed and better control robustness, when compared to vanilla systems that attempt to learn chopstick-based skills without a gripping pose optimization module and/or without a kinematic motion planner. Our code and models are available at this link.	https://dl.acm.org/doi/abs/10.1145/3528223.3530057	Zeshi Yang, Kangkang Yin, Libin Liu
LeviPrint: Contactless Fabrication using Full Acoustic Trapping of Elongated Parts.	LeviPrint is a system for assembling objects in a contactless manner using acoustic levitation. We explore a set of optimum acoustic fields that enables full trapping in position and orientation of elongated objects such as sticks. We then evaluate the capabilities of different ultrasonic levitators to dynamically manipulate these elongated objects. The combination of novel optimization algorithms and levitators enable the manipulation of sticks, beads and droplets to fabricate complex objects. A system prototype composed of a robot arm and a levitator is tested for different fabrication processes. We highlight the reduction of cross-contamination and the capability of building on top of objects from different angles as well as inside closed spaces. We hope that this technique inspires novel fabrication techniques and that reaches fields such as microfabrication of electromechanical components or even in-vivo additive manufacturing.	https://dl.acm.org/doi/abs/10.1145/3528233.3530752	Iñigo Ezcurdia, Rafael Morales, Marco A. B. Andrade, Asier Marzo
Light & Shadow AR App: A photo lighting workshop in Augmented Reality	The Light and Shadow App is an immersive photography studio lighting workshop in Augmented Reality. The App allows photo enthusiasts to virtually step into a full-size photo lighting studio and visualize the placement of studio lights for portrait photography in three dimensions. With the Light and Shadow App, users can use smartphones or iPads, and Augmented Reality, to move around the photo studio scenes and study the position of lights from different points of view. The App was originally developed for university level photography students at Kwantlen Polytechnic University during the covid-19 restrictions of 2020 and 2021 when in-person learning was not possible. The goal of the app is to recreate the learning environment of the photo studio and allow students to visualize the effect of lighting on the portrait in an interactive and immersive format.	https://dl.acm.org/doi/abs/10.1145/3532723.3535469	Paulo Majano
Lightyear Look Development - Materials and Beyond	Lightyear presented interesting look development challenges. A widely familiar character, Buzz, needed to remain recognizable, but as a human being instead of a toy. Similarly, the environments in the film needed to buttress that look with cohesion and authenticity. Additionally, Buzz and the other characters needed stylized hair in order to fit in with the look of the film. All of this needed to work seamlessly together to create a cohesive look for Lightyear. In light of this, Lightyear consolidated the look teams from sets, characters and grooming all into one group to make a unified world on the screen. Come and learn about the journey we took to get there!	https://dl.acm.org/doi/abs/10.1145/3532836.3536274	Thomas Jordan, Ben Beech, Ben Porter, Ethan Dean, Colin Thompson
Living with Smell Dysfunction: A Multi-sensory VR Experience	"""Living with Smell Dysfunction"" is a multi-sensory short film that introduces scents in Virtual Reality (VR) Experience. Through this first-person immersive film in VR, the participant will face daily adventure, confusion and danger as a patient with smell dysfunction. Olfactory disorders simulated in this film are anosmia (absence of smell), hyposmia (diminished sensitivity of smell), and dysosmia (distortion of normal smell). [Schiffman 2007] Olfactory dysfunction and disability are tend to be overlooked and invisible to vast population. This novel immersive experience could bring more discussion and attention to the treatment and daily lives of smell dysfunctional patients."	https://dl.acm.org/doi/abs/10.1145/3532834.3536219	Yuting Wang, Ziqing Li
Living worlds: city regreening	Journey through the cosmos to discover how a deeper understanding of Earth can help us spot other living worlds, light years away.	https://dl.acm.org/doi/abs/10.1145/3512752.3528367	Mike Schmitt, Ken Ackerman, Cheryl Vanderbilt, Ryan Wyatt
Local Scale Adaptation for Augmenting Hand Shape Models	The accuracy of hand pose and shape recovery algorithms depends on how closely the geometric hand model resembles the user's hand. Most existing methods rely on learned shape space, e.g. MANO; but this shape model fails to generalize to unseen hand shapes with large deviations from the training set. We introduce a new hand shape model, aMANO, that augments MANO by introducing local scale adaptation that enables modeling substantially different hand sizes. We use both MANO and aMANO for calibrating the shape to new users from a stream of depth images and observe the improvement of aMANO over MANO. We believe that our new hand shape model is a significant step in improving the robustness and accuracy of existing hand tracking solutions.	https://dl.acm.org/doi/abs/10.1145/3532719.3543246	Pratik Kalshetti, Parag Chaudhuri
Local anatomically-constrained facial performance retargeting	Generating realistic facial animation for CG characters and digital doubles is one of the hardest tasks in animation. A typical production workflow involves capturing the performance of a real actor using mo-cap technology, and transferring the captured motion to the target digital character. This process, known as , has been used for over a decade, and typically relies on either large blendshape rigs that are expensive to create, or direct deformation transfer algorithms that operate on individual geometric elements and are prone to artifacts. We present a new method for high-fidelity offline facial performance retargeting that is neither expensive nor artifact-prone. Our two step method first transfers local expression details to the target, and is followed by a global face surface prediction that uses anatomical constraints in order to stay in the feasible shape space of the target character. Our method also offers artists with familiar blendshape controls to perform fine adjustments to the retargeted animation. As such, our method is ideally suited for the complex task of human-to-human 3D facial performance retargeting, where the quality bar is extremely high in order to avoid the uncanny valley, while also being applicable for more common human-to-creature settings. We demonstrate the superior performance of our method over traditional deformation transfer algorithms, while achieving a quality comparable to current blendshape-based techniques used in production while requiring significantly fewer input shapes at setup time. A detailed user study corroborates the realistic and artifact free animations generated by our method in comparison to existing techniques.	https://dl.acm.org/doi/abs/10.1145/3528223.3530114	Prashanth Chandran, Loïc Ciccone, Markus Gross, Derek Bradley
Loki: a unified multiphysics simulation framework for production	We introduce Loki, a new framework for robust simulation of fluid, rigid, and deformable objects with non-compromising fidelity on any single element, and capabilities for coupling and representation transitions across multiple elements. Loki adapts multiple best-in-class solvers into a unified framework driven by a declarative state machine where users declare 'what' is simulated but not 'when,' so an automatic scheduling system takes care of mixing any combination of objects. This leads to intuitive setups for coupled simulations such as hair in the wind or objects transitioning from one representation to another, for example bulk water FLIP particles to SPH spray particles to volumetric mist. We also provide a consistent treatment for components used in several domains, such as unified collision and attachment constraints across 1D, 2D, 3D deforming and rigid objects. Distribution over MPI, custom linear equation solvers, and aggressive application of sparse techniques keep performance within production requirements. We demonstrate a variety of solvers within the framework and their interactions, including FLIPstyle liquids, spatially adaptive volumetric fluids, SPH, MPM, and mesh-based solids, including but not limited to discrete elastic rods, elastons, and FEM with state-of-the-art constitutive models. Our framework has proven powerful and intuitive enough for voluntary artist adoption and has delivered creature and FX simulations for multiple major movie productions in the preceding four years.	https://dl.acm.org/doi/abs/10.1145/3528223.3530058	Steve Lesser, Alexey Stomakhin, Gilles Daviet, Joel Wretborn, John Edholm, Noh-Hoon Lee, Eston Schweickart, Xiao Zhai, Sean Flynn, Andrew Moffat
Low-poly Mesh Generation for Building Models	As a common practice, game modelers manually craft low-poly meshes for given 3D building models in order to achieve the ideal balance between the small element count and the visual similarity. This can take hours and involve tedious trial and error. We propose a novel and simple algorithm to automate this process by converting high-poly 3D building models into both simple and visually preserving low-poly meshes. Our algorithm has three stages: First, a watertight, self-collision-free visual hull is generated via Boolean intersecting 3D extrusions of input's silhouettes; We then carve out notable but redundant structures from the visual hull via Boolean subtracting 3D primitives derived from parts of the input; Finally, we generate a progressively simplified low-poly mesh sequence from the carved mesh and extract the Pareto front for users to select the desired output. Each stage of our approach is guided by visual metrics, aiming to preserve the visual similarity to the input. We have tested our method on a dataset containing 100 building models with different styles, most of which are used in popular digital games. We highlight the superior robustness and quality by comparing with state-of-the-art competing techniques. Executable program for this paper is at lowpoly-modeling.github.io.	https://dl.acm.org/doi/abs/10.1145/3528233.3530716	Xifeng Gao, Kui Wu, Zherong Pan
Madrid Noir	We have created a 45 minute Virtual Reality Narrative with moments of interaction to engage the viewer. The narrative concerns a woman named Lola, and follows her memories of a summer spent with her estranged uncle in Madrid in 1930. Madrid Noir aims to explore the possibilities for narrative storytelling using the medium of Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532834.3536217	James A. Castillo, Antoine Cayrol, Luke Gibbard
Making Encanto with USD: Rebuilding a Production Pipeline Working from Home	"In 2017, Walt Disney Animation Studios began a transition towards Universal Scene Description (USD) as its primary data interchange format [Pixar 2016]. The design and roll out encompassed production on Walt Disney Animation Studios' ""Raya and the Last Dragon"" and ""Encanto"". In addition to the general challenge of revamping a production pipeline, the studio was fully remote for a majority of the deployment. Even though pipeline changes were extensively tested during development, exercising fundamental pipeline changes in production invariably uncovered many issues. As such, the undertaking required a significant amount of communication and coordination among software developers, technical directors, artists and show leadership."	https://dl.acm.org/doi/abs/10.1145/3532836.3536236	Tad Miller, Harmony Li, Neelima Karanam, Nadim Sinno, Todd Scopio
Making a Guinea Pig Monster in The Bad Guys	In the film The Bad Guys, the villain uses a mind control device to manipulate an army of guinea pigs which in themselves evolve through various stages of scale and coalescence, ultimately forming a large monster. In order to create congruity between these different scales of motion we created targeted solutions and worked cross departmentally in an effort to maintain artistic control throughout the process.	https://dl.acm.org/doi/abs/10.1145/3532836.3536234	Steve Avoujageli, John Kosnik, Carl Hooper, Stephen Wood, Max Bruce, Spencer Knapp
MatBuilder: mastering sampling uniformity over projections	Many applications ranging from quasi-Monte Carlo integration over optimal control to neural networks benefit from high-dimensional, highly uniform samples. In the case of computer graphics, and more particularly in rendering, despite the need for uniformity, several sub-problems expose a low-dimensional structure. In this context, mastering sampling uniformity over projections while preserving high-dimensional uniformity has been intrinsically challenging. This difficulty may explain the relatively small number of mathematical constructions for such samplers. We propose a novel approach by showing that uniformity constraints can be expressed as an integer linear program that results in a sampler with the desired properties. As it turns out, complex constraints are easy to describe by means of stratification and sequence properties of digital nets. Formalized using generator matrix determinants, our new MatBuilder software solves the set of constraints by iterating the linear integer program solver in a greedy fashion to compute a problem-specific set of generator matrices that can be used as a drop-in replacement in the popular digital net samplers. The samplers created by MatBuilder achieve the uniformity of classic low discrepancy sequences. More importantly, we demonstrate the benefit of the unprecedented versatility of our constraint approach with respect to low-dimensional problem structure for several applications.	https://dl.acm.org/doi/abs/10.1145/3528223.3530063	Loïs Paulin, Nicolas Bonneel, David Coeurjolly, Jean-Claude Iehl, Alexander Keller, Victor Ostromoukhov
MatFormer: a generative model for procedural materials	Procedural material graphs are a compact, parameteric, and resolution-independent representation that are a popular choice for material authoring. However, designing procedural materials requires significant expertise and publicly accessible libraries contain only a few thousand such graphs. We present MatFormer, a generative model that can produce a diverse set of high-quality procedural materials with complex spatial patterns and appearance. While procedural materials can be modeled as directed (operation) graphs, they contain arbitrary numbers of heterogeneous nodes with unstructured, often long-range node connections, and functional constraints on node parameters and connections. MatFormer addresses these challenges with a multi-stage transformer-based model that sequentially generates nodes, node parameters, and edges, while ensuring the semantic validity of the graph. In addition to generation, MatFormer can be used for the auto-completion and exploration of partial material graphs. We qualitatively and quantitatively demonstrate that our method outperforms alternative approaches, in both generated graph and material quality.	https://dl.acm.org/doi/abs/10.1145/3528223.3530173	Paul Guerrero, Miloš Hašan, Kalyan Sunkavalli, Radomír Měch, Tamy Boubekeur, Niloy J. Mitra
Mentorship Matters	Opening the industry to historically underrepresented artists is vital to the growth and development of the world of VFX. In an effort to support this, a pilot program was conducted at Santa Monica College to integrate a mentorship experience into a community college composition course. Existing research tells us that students are more successful when they are mentored by an industry professional. However, this mentorship often looks like an internship. Due to location, time, and money, this is unattainable for many student populations. In its place, stakeholders from across the VFX community partnered with the school to integrate mentorship into scheduled course times. While not a perfect solution given time constraints and time zone limitations, this did open the door for students to access priceless guidance from established industry professionals. From our initial meeting to the program exit surveys, we will share our journey to embed industry experiences into an advanced composition course. Additionally, we will address the bumps along the way and where we see room for improvement. Is an integrated mentorship element the next step in secondary and postsecondary programs? What are the implications of the program for best practices in VFX education? How do we hope to see this program grow from the pilot? What methods of evaluation could be used to determine the impact of the program?	https://dl.acm.org/doi/abs/10.1145/3532724.3535590	Kirstyn Salehi, Ann Marie Leahy, Devin Uzan
Meta Avatar Robot Cafe: Linking Physical and Virtual Cybernetic Avatars to Provide Physical Augmentation for People with Disabilities	Meta avatar robot cafe is a cafe that fuses cyberspace and physical space to create new encounters with people. We create a place where people with disabilities who have difficulty going out can freely switch between their physical bodies and virtual bodies, and communicate their presence and warmth to each other.	https://dl.acm.org/doi/abs/10.1145/3532721.3546117	Yoichi Yamazaki, Tsukuto Yamada, Hiroki Nomura, Nobuaki Hosoda, Ryoma Kawamura, Kazuaki Takeuchi, Hiroaki Kato, Ryuma Niiyama, Kentaro Yoshifuji
Meta Flowers: An Analogy of Life in the XR Era	"""Meta Flowers"" is a multiparticipant installation artwork with Cross Reality (XR) . Wearing a glove-type tactile device consisting of linear resonant actuators (LRA) and a microcontroller with Wi-Fi unit, and HoloLens 2, participants can experience XR through the act of arranging virtual flowers (VFs) which have 'shadow,' 'rigid-body,' and 'sound (see Figure 1 (a)).' The VF blooms at the location of the VIVE Tracker, which is installed at the tip of a metal rod inserted into a vase on the table (see Figure 1 (b) (c)). Real shadows of the vases, sticks, and artificial flowers, as well as the virtual shadows of the VFs, are projected onto the table by the light and images projected from the projector installed on the celling, creating a fusion that is not unnatural. Participants can move the VFs and arrange them in vases on the table. When participants touch the VF, petals of the VF fall and participants get tactile sensations through the haptics glove (see Figure 1 (d)). VFs play sounds while they are in bloom, and their pitch change depending on the position of them. In the other words, the sound and appearance of VFs will change depending on the relationship among participants and VFs. In addition, when real water is poured by the sensor-equipped jug into a vase in which a metal rod without petals is put, the VF blooms again (see Figure 1 (e) (f))."	https://dl.acm.org/doi/abs/10.1145/3532834.3536199	Ken Sonobe, Masaya Furukawa, Ayaka Yamanaka, Hidefumi Ohmura, Takuro Shibayama, Ryu Nakagawa
MetaPo: A Robotic Meta Portal for Interspace Communication	We introduce MetaPo, a mobile robot with spheric display, 360° media I/O and robotic hands for creating a unified model of interspace communication. MetaPo works as a portal between pairs of physical-physical, cyber-cyber and cyber-physical spaces to provide 1) panoramic communication for multiple remote users, and 2) immersive interspace migration with mobility functionality. The paper overviews our concept and first prototype of MetaPo with its hardware and software implementation.	https://dl.acm.org/doi/abs/10.1145/3532719.3543255	Takuro Yonezawa, Nozomi Hayashida, Kenta Urano, Johannes Przybilla, Yutaro Kyono, Nobuo Kawaguchi
Mixed Reality Collaboration for Complementary Working Styles	Our project combines immersive VR, multitouch AR, real-time volumetric capture, motion capture, robotically-actuated tangible interfaces at multiple scales, and live coding, in service of a human-centric way of collaborating. Participants bring their unique talents and preferences to collaboratively tackle complex problems in a shared mixed reality world.	https://dl.acm.org/doi/abs/10.1145/3532834.3536216	Keru Wang, Zhu Wang, Karl Rosenberg, Zhenyi He, Dong Woo Yoo, Un Joo Christopher, Ken Perlin
Mixed integer neural inverse design	In computational design and fabrication, neural networks are becoming important surrogates for bulky forward simulations. A long-standing, intertwined question is that of inverse design: how to compute a design that satisfies a desired target performance? Here, we show that the piecewise linear property, very common in everyday neural networks, allows for an inverse design formulation based on mixed-integer linear programming. Our mixed-integer inverse design uncovers globally optimal or near optimal solutions in a principled manner. Furthermore, our method significantly facilitates emerging, but challenging, combinatorial inverse design tasks, such as material selection. For problems where finding the optimal solution is intractable, we develop an efficient yet near-optimal hybrid approach. Eventually, our method is able to find solutions provably robust to possible fabrication perturbations among multiple designs with similar performances. Our code and data are available at https://gitlab.mpi-klsb.mpg.de/nansari/mixed-integer-neural-inverse-design.	https://dl.acm.org/doi/abs/10.1145/3528223.3530083	Navid Ansari, Hans-Peter Seidel, Vahid Babaei
MoRF: Morphable Radiance Fields for Multiview Neural Head Modeling	Recent research work has developed powerful generative models (e.g., StyleGAN2) that can synthesize complete human head images with impressive photorealism, enabling applications such as photorealistically editing real photographs. While these models can be trained on large collections of unposed images, their lack of explicit 3D knowledge makes it difficult to achieve even basic control over 3D viewpoint without unintentionally altering identity. On the other hand, recent Neural Radiance Field (NeRF) methods have already achieved multiview-consistent, photorealistic renderings but they are so far limited to a single facial identity. In this paper, we propose a new Morphable Radiance Field (MoRF) method that extends a NeRF into a generative neural model that can realistically synthesize multiview-consistent images of complete human heads, with variable and controllable identity. MoRF allows for morphing between particular identities, synthesizing arbitrary new identities, or quickly generating a NeRF from few images of a new subject, all while providing realistic and consistent rendering under novel viewpoints. We train MoRF in a supervised fashion by leveraging a high-quality database of multiview portrait images of several people, captured in studio with polarization-based separation of diffuse and specular reflection. Here, we demonstrate how MoRF is a strong new step forwards towards generative NeRFs for 3D neural head modeling.	https://dl.acm.org/doi/abs/10.1145/3528233.3530753	Daoye Wang, Prashanth Chandran, Gaspard Zoss, Derek Bradley, Paulo Gotardo
Modeling Animated Jumbo Floral Display on Disney’s ”Encanto”	"Walt Disney Animation Studios' ""Encanto"" called for many dazzling musical numbers with special effects. Isabela's song ""What Else Can I Do?"" was one of the most technically challenging. In this sequence, Isabela finds freedom from the pressure and constraints of society's expectations, and lets her true self shine. Throughout this musical number, her environment represents her emotional transformation through the colorful, expressive, and hypnotizing flower patterns that appear on the walls of her bedroom. This effect required hundreds of thousands of flowers to change colors and move in kaleidoscopic patterns. This talk discusses the technical challenges encountered in bringing Isabela's room to life and the subsequent artistic and technical solutions."	https://dl.acm.org/doi/abs/10.1145/3532836.3536254	Andrew Finley, Jesse Erickson, Peter De Mund, Ying Liu
Modeling and rendering non-euclidean spaces approximated with concatenated polytopes	A non-Euclidean space is characterized as a manifold with a specific structure that violates Euclid's postulates. This paper proposes to approximate a manifold with polytopes. Based on the scene designer's specification, the polytopes are automatically concatenated and embedded in a higher-dimensional Euclidean space. Then, the scene is navigated and rendered via novel methods tailored to concatenated polytopes. The proof-of-concept implementation and experiments with it show that the proposed methods bring the virtual-world users unusual and fascinating experiences, which cannot be provided in Euclidean-space applications.	https://dl.acm.org/doi/abs/10.1145/3528223.3530186	Seung-Wook Kim, Jaehyung Doh, Junghyun Han
Modular Scene Filtering via the Pixar Hydra 2.0 Architecture	Pixar's Hydra project began as an abstract scene interface to an OpenGL-based renderer intended for interactive viewports across multiple applications. As the project added integrations with other renderers (including path tracers), it became clear that it needed a richer scene interface to convey a broader set of renderer features, as well as a more structured and modular way to resolve the scene into rendering primitives. We'll discuss the Hydra 2.0 architecture that was developed to address this problem, with examples from two scene filtering cases:	https://dl.acm.org/doi/abs/10.1145/3532836.3536280	Tom Cauchois, Steve LaVietes
Morphogenesis I: coast, shrub, forest	"Soil is the foundation for healthy ecosystems and plays an important role in sustaining life. Soil serves as the baseline for life and provides a variety of ecosystems and societal functions. Despite sustaining the world's demands of food, fibers, and fuels, regulating Earth's air and water quality, and storing greenhouse gases, the value of soil, commonly termed ""dirt"", is often dismissed. This human disconnect from this important source of life, especially in the middle of our climate crisis, needs to be addressed as a vital sustainability practice. Soil health is largely dependent on a diverse thriving microorganism composition. This unseen world of microorganism biodiversity (diversity of life) can be detected, enumerated, and characterized through advancement in molecular methods such as the use of Environmental DNA (eDNA). This tool takes advantage of DNA found in the environment, from organisms in the form of feces, skin cells, pollen, whole cells (microbes) that can then be collected from soil, sediment, water, and even air! We can use the eDNA from the sample to obtain information on the organisms found in soil from microbes, plants, and vertebrates. From the Greek morphê - shape, and genesis - creation, literally means ""the generation of form."" Morphogenesis is the process that controls the spatial organization and distribution of cells during the development of an organism. An example of this mechanism includes the transformation of a caterpillar to a butterfly, which demonstrates the ability of an organism's cells to self-organize a new body shape. However, morphogenesis also represents coordinated behavior to build and develop the shape, position, and interconnection of a structure, for example, the replicated pattern of termite mounds. Morphogenesis is a transcending concept that captivates the human imagination. Morphogenesis captures the essence of this project, by expressing how soil can be considered a complex living organism in combination with the computational process of neural cellular automata."	https://dl.acm.org/doi/abs/10.1145/3532837.3534949	Hye Min Cho, Maru Garcia, Maura Palacios Mejia
Morphogenesis I: coast, shrub, forest	"Soil is the foundation for healthy ecosystems and plays an important role in sustaining life. Soil serves as the baseline for life and provides a variety of ecosystems and societal functions. Despite sustaining the world's demands of food, fibers, and fuels, regulating Earth's air and water quality, and storing greenhouse gases, the value of soil, commonly termed ""dirt"", is often dismissed. This human disconnect from this important source of life, especially in the middle of our climate crisis, needs to be addressed as a vital sustainability practice. Soil health is largely dependent on a diverse thriving microorganism composition. This unseen world of microorganism biodiversity (diversity of life) can be detected, enumerated, and characterized through advancement in molecular methods such as the use of Environmental DNA (eDNA). This tool takes advantage of DNA found in the environment, from organisms in the form of feces, skin cells, pollen, whole cells (microbes) that can then be collected from soil, sediment, water, and even air! We can use the eDNA from the sample to obtain information on the organisms found in soil from microbes, plants, and vertebrates. From the Greek morphê - shape, and genesis - creation, literally means ""the generation of form."" Morphogenesis is the process that controls the spatial organization and distribution of cells during the development of an organism. An example of this mechanism includes the transformation of a caterpillar to a butterfly, which demonstrates the ability of an organism's cells to self-organize a new body shape. However, morphogenesis also represents coordinated behavior to build and develop the shape, position, and interconnection of a structure, for example, the replicated pattern of termite mounds. Morphogenesis is a transcending concept that captivates the human imagination. Morphogenesis captures the essence of this project, by expressing how soil can be considered a complex living organism in combination with the computational process of neural cellular automata."	https://dl.acm.org/doi/abs/10.1145/3532837.3534949	Hye Min Cho, Maru Garcia, Maura Palacios Mejia
Morphogenesis I: coast, shrub, forest	"Soil is the foundation for healthy ecosystems and plays an important role in sustaining life. Soil serves as the baseline for life and provides a variety of ecosystems and societal functions. Despite sustaining the world's demands of food, fibers, and fuels, regulating Earth's air and water quality, and storing greenhouse gases, the value of soil, commonly termed ""dirt"", is often dismissed. This human disconnect from this important source of life, especially in the middle of our climate crisis, needs to be addressed as a vital sustainability practice. Soil health is largely dependent on a diverse thriving microorganism composition. This unseen world of microorganism biodiversity (diversity of life) can be detected, enumerated, and characterized through advancement in molecular methods such as the use of Environmental DNA (eDNA). This tool takes advantage of DNA found in the environment, from organisms in the form of feces, skin cells, pollen, whole cells (microbes) that can then be collected from soil, sediment, water, and even air! We can use the eDNA from the sample to obtain information on the organisms found in soil from microbes, plants, and vertebrates. From the Greek morphê - shape, and genesis - creation, literally means ""the generation of form."" Morphogenesis is the process that controls the spatial organization and distribution of cells during the development of an organism. An example of this mechanism includes the transformation of a caterpillar to a butterfly, which demonstrates the ability of an organism's cells to self-organize a new body shape. However, morphogenesis also represents coordinated behavior to build and develop the shape, position, and interconnection of a structure, for example, the replicated pattern of termite mounds. Morphogenesis is a transcending concept that captivates the human imagination. Morphogenesis captures the essence of this project, by expressing how soil can be considered a complex living organism in combination with the computational process of neural cellular automata."	https://dl.acm.org/doi/abs/10.1145/3532837.3534949	Hye Min Cho, Maru Garcia, Maura Palacios Mejia
Morphogenesis I: coast, shrub, forest	"Soil is the foundation for healthy ecosystems and plays an important role in sustaining life. Soil serves as the baseline for life and provides a variety of ecosystems and societal functions. Despite sustaining the world's demands of food, fibers, and fuels, regulating Earth's air and water quality, and storing greenhouse gases, the value of soil, commonly termed ""dirt"", is often dismissed. This human disconnect from this important source of life, especially in the middle of our climate crisis, needs to be addressed as a vital sustainability practice. Soil health is largely dependent on a diverse thriving microorganism composition. This unseen world of microorganism biodiversity (diversity of life) can be detected, enumerated, and characterized through advancement in molecular methods such as the use of Environmental DNA (eDNA). This tool takes advantage of DNA found in the environment, from organisms in the form of feces, skin cells, pollen, whole cells (microbes) that can then be collected from soil, sediment, water, and even air! We can use the eDNA from the sample to obtain information on the organisms found in soil from microbes, plants, and vertebrates. From the Greek morphê - shape, and genesis - creation, literally means ""the generation of form."" Morphogenesis is the process that controls the spatial organization and distribution of cells during the development of an organism. An example of this mechanism includes the transformation of a caterpillar to a butterfly, which demonstrates the ability of an organism's cells to self-organize a new body shape. However, morphogenesis also represents coordinated behavior to build and develop the shape, position, and interconnection of a structure, for example, the replicated pattern of termite mounds. Morphogenesis is a transcending concept that captivates the human imagination. Morphogenesis captures the essence of this project, by expressing how soil can be considered a complex living organism in combination with the computational process of neural cellular automata."	https://dl.acm.org/doi/abs/10.1145/3532837.3534949	Hye Min Cho, Maru Garcia, Maura Palacios Mejia
Morphogenesis I: coast, shrub, forest	"Soil is the foundation for healthy ecosystems and plays an important role in sustaining life. Soil serves as the baseline for life and provides a variety of ecosystems and societal functions. Despite sustaining the world's demands of food, fibers, and fuels, regulating Earth's air and water quality, and storing greenhouse gases, the value of soil, commonly termed ""dirt"", is often dismissed. This human disconnect from this important source of life, especially in the middle of our climate crisis, needs to be addressed as a vital sustainability practice. Soil health is largely dependent on a diverse thriving microorganism composition. This unseen world of microorganism biodiversity (diversity of life) can be detected, enumerated, and characterized through advancement in molecular methods such as the use of Environmental DNA (eDNA). This tool takes advantage of DNA found in the environment, from organisms in the form of feces, skin cells, pollen, whole cells (microbes) that can then be collected from soil, sediment, water, and even air! We can use the eDNA from the sample to obtain information on the organisms found in soil from microbes, plants, and vertebrates. From the Greek morphê - shape, and genesis - creation, literally means ""the generation of form."" Morphogenesis is the process that controls the spatial organization and distribution of cells during the development of an organism. An example of this mechanism includes the transformation of a caterpillar to a butterfly, which demonstrates the ability of an organism's cells to self-organize a new body shape. However, morphogenesis also represents coordinated behavior to build and develop the shape, position, and interconnection of a structure, for example, the replicated pattern of termite mounds. Morphogenesis is a transcending concept that captivates the human imagination. Morphogenesis captures the essence of this project, by expressing how soil can be considered a complex living organism in combination with the computational process of neural cellular automata."	https://dl.acm.org/doi/abs/10.1145/3532837.3534949	Hye Min Cho, Maru Garcia, Maura Palacios Mejia
Morphogenesis I: coast, shrub, forest	"Soil is the foundation for healthy ecosystems and plays an important role in sustaining life. Soil serves as the baseline for life and provides a variety of ecosystems and societal functions. Despite sustaining the world's demands of food, fibers, and fuels, regulating Earth's air and water quality, and storing greenhouse gases, the value of soil, commonly termed ""dirt"", is often dismissed. This human disconnect from this important source of life, especially in the middle of our climate crisis, needs to be addressed as a vital sustainability practice. Soil health is largely dependent on a diverse thriving microorganism composition. This unseen world of microorganism biodiversity (diversity of life) can be detected, enumerated, and characterized through advancement in molecular methods such as the use of Environmental DNA (eDNA). This tool takes advantage of DNA found in the environment, from organisms in the form of feces, skin cells, pollen, whole cells (microbes) that can then be collected from soil, sediment, water, and even air! We can use the eDNA from the sample to obtain information on the organisms found in soil from microbes, plants, and vertebrates. From the Greek morphê - shape, and genesis - creation, literally means ""the generation of form."" Morphogenesis is the process that controls the spatial organization and distribution of cells during the development of an organism. An example of this mechanism includes the transformation of a caterpillar to a butterfly, which demonstrates the ability of an organism's cells to self-organize a new body shape. However, morphogenesis also represents coordinated behavior to build and develop the shape, position, and interconnection of a structure, for example, the replicated pattern of termite mounds. Morphogenesis is a transcending concept that captivates the human imagination. Morphogenesis captures the essence of this project, by expressing how soil can be considered a complex living organism in combination with the computational process of neural cellular automata."	https://dl.acm.org/doi/abs/10.1145/3532837.3534949	Hye Min Cho, Maru Garcia, Maura Palacios Mejia
Morphogenesis I: coast, shrub, forest	"Soil is the foundation for healthy ecosystems and plays an important role in sustaining life. Soil serves as the baseline for life and provides a variety of ecosystems and societal functions. Despite sustaining the world's demands of food, fibers, and fuels, regulating Earth's air and water quality, and storing greenhouse gases, the value of soil, commonly termed ""dirt"", is often dismissed. This human disconnect from this important source of life, especially in the middle of our climate crisis, needs to be addressed as a vital sustainability practice. Soil health is largely dependent on a diverse thriving microorganism composition. This unseen world of microorganism biodiversity (diversity of life) can be detected, enumerated, and characterized through advancement in molecular methods such as the use of Environmental DNA (eDNA). This tool takes advantage of DNA found in the environment, from organisms in the form of feces, skin cells, pollen, whole cells (microbes) that can then be collected from soil, sediment, water, and even air! We can use the eDNA from the sample to obtain information on the organisms found in soil from microbes, plants, and vertebrates. From the Greek morphê - shape, and genesis - creation, literally means ""the generation of form."" Morphogenesis is the process that controls the spatial organization and distribution of cells during the development of an organism. An example of this mechanism includes the transformation of a caterpillar to a butterfly, which demonstrates the ability of an organism's cells to self-organize a new body shape. However, morphogenesis also represents coordinated behavior to build and develop the shape, position, and interconnection of a structure, for example, the replicated pattern of termite mounds. Morphogenesis is a transcending concept that captivates the human imagination. Morphogenesis captures the essence of this project, by expressing how soil can be considered a complex living organism in combination with the computational process of neural cellular automata."	https://dl.acm.org/doi/abs/10.1145/3532837.3534949	Hye Min Cho, Maru Garcia, Maura Palacios Mejia
Morphogenesis I: coast, shrub, forest	"Soil is the foundation for healthy ecosystems and plays an important role in sustaining life. Soil serves as the baseline for life and provides a variety of ecosystems and societal functions. Despite sustaining the world's demands of food, fibers, and fuels, regulating Earth's air and water quality, and storing greenhouse gases, the value of soil, commonly termed ""dirt"", is often dismissed. This human disconnect from this important source of life, especially in the middle of our climate crisis, needs to be addressed as a vital sustainability practice. Soil health is largely dependent on a diverse thriving microorganism composition. This unseen world of microorganism biodiversity (diversity of life) can be detected, enumerated, and characterized through advancement in molecular methods such as the use of Environmental DNA (eDNA). This tool takes advantage of DNA found in the environment, from organisms in the form of feces, skin cells, pollen, whole cells (microbes) that can then be collected from soil, sediment, water, and even air! We can use the eDNA from the sample to obtain information on the organisms found in soil from microbes, plants, and vertebrates. From the Greek morphê - shape, and genesis - creation, literally means ""the generation of form."" Morphogenesis is the process that controls the spatial organization and distribution of cells during the development of an organism. An example of this mechanism includes the transformation of a caterpillar to a butterfly, which demonstrates the ability of an organism's cells to self-organize a new body shape. However, morphogenesis also represents coordinated behavior to build and develop the shape, position, and interconnection of a structure, for example, the replicated pattern of termite mounds. Morphogenesis is a transcending concept that captivates the human imagination. Morphogenesis captures the essence of this project, by expressing how soil can be considered a complex living organism in combination with the computational process of neural cellular automata."	https://dl.acm.org/doi/abs/10.1145/3532837.3534949	Hye Min Cho, Maru Garcia, Maura Palacios Mejia
Moving level-of-detail surfaces	We present a simple, fast, and smooth scheme to approximate Algebraic Point Set Surfaces using non-compact kernels, which is particularly suited for filtering and reconstructing point sets presenting large missing parts. Our key idea is to consider a moving level-of-detail of the input point set which is adaptive w.r.t. to the evaluation location, just such as the samples weights are output sensitive in the traditional moving least squares scheme. We also introduce an adaptive progressive octree refinement scheme, driven by the resulting implicit surface, to properly capture the modeled geometry even far away from the input samples. Similarly to typical compactly-supported approximations, our operator runs in logarithmic time while defining high quality surfaces even on challenging inputs for which only global optimizations achieve reasonable results. We demonstrate our technique on a variety of point sets featuring geometric noise as well as large holes.	https://dl.acm.org/doi/abs/10.1145/3528223.3530151	Corentin Mercier, Thibault Lescoat, Pierre Roussillon, Tamy Boubekeur, Jean-Marc Thiery
My data body	In , the magnetic resonance (MR) scanned body of the artist Marilène Oliver floats prone within a `cloud' of her textual Facebook data. Into the semi-transparent, virtual body are multiple other data corpuses downloaded from social media platforms plotted into cross sections of the body. In the horizontal plane, Mac terminal data is plotted into bone, Google data into muscle and Facebook data into fat. In the vertical plane are plotted data usage agreements. Passwords and logins flow back and forth through veins and arteries, whilst retinal images, dental scans and 3D meshes of organs and bones are suspended within the quantified and datafied body [Lupton 2016; Van Dijck 2014]. There is a continuous stream of text particles that flow through and around the data body that one can swat away or nestle into.	https://dl.acm.org/doi/abs/10.1145/3532837.3534953	Marilène Oliver, Scott Smallwood, Stephan Moore, J. R. Carpenter
My data body	In , the magnetic resonance (MR) scanned body of the artist Marilène Oliver floats prone within a `cloud' of her textual Facebook data. Into the semi-transparent, virtual body are multiple other data corpuses downloaded from social media platforms plotted into cross sections of the body. In the horizontal plane, Mac terminal data is plotted into bone, Google data into muscle and Facebook data into fat. In the vertical plane are plotted data usage agreements. Passwords and logins flow back and forth through veins and arteries, whilst retinal images, dental scans and 3D meshes of organs and bones are suspended within the quantified and datafied body [Lupton 2016; Van Dijck 2014]. There is a continuous stream of text particles that flow through and around the data body that one can swat away or nestle into.	https://dl.acm.org/doi/abs/10.1145/3532837.3534953	Marilène Oliver, Scott Smallwood, Stephan Moore, J. R. Carpenter
My data body	In , the magnetic resonance (MR) scanned body of the artist Marilène Oliver floats prone within a `cloud' of her textual Facebook data. Into the semi-transparent, virtual body are multiple other data corpuses downloaded from social media platforms plotted into cross sections of the body. In the horizontal plane, Mac terminal data is plotted into bone, Google data into muscle and Facebook data into fat. In the vertical plane are plotted data usage agreements. Passwords and logins flow back and forth through veins and arteries, whilst retinal images, dental scans and 3D meshes of organs and bones are suspended within the quantified and datafied body [Lupton 2016; Van Dijck 2014]. There is a continuous stream of text particles that flow through and around the data body that one can swat away or nestle into.	https://dl.acm.org/doi/abs/10.1145/3532837.3534953	Marilène Oliver, Scott Smallwood, Stephan Moore, J. R. Carpenter
My data body	In , the magnetic resonance (MR) scanned body of the artist Marilène Oliver floats prone within a `cloud' of her textual Facebook data. Into the semi-transparent, virtual body are multiple other data corpuses downloaded from social media platforms plotted into cross sections of the body. In the horizontal plane, Mac terminal data is plotted into bone, Google data into muscle and Facebook data into fat. In the vertical plane are plotted data usage agreements. Passwords and logins flow back and forth through veins and arteries, whilst retinal images, dental scans and 3D meshes of organs and bones are suspended within the quantified and datafied body [Lupton 2016; Van Dijck 2014]. There is a continuous stream of text particles that flow through and around the data body that one can swat away or nestle into.	https://dl.acm.org/doi/abs/10.1145/3532837.3534953	Marilène Oliver, Scott Smallwood, Stephan Moore, J. R. Carpenter
My data body	In , the magnetic resonance (MR) scanned body of the artist Marilène Oliver floats prone within a `cloud' of her textual Facebook data. Into the semi-transparent, virtual body are multiple other data corpuses downloaded from social media platforms plotted into cross sections of the body. In the horizontal plane, Mac terminal data is plotted into bone, Google data into muscle and Facebook data into fat. In the vertical plane are plotted data usage agreements. Passwords and logins flow back and forth through veins and arteries, whilst retinal images, dental scans and 3D meshes of organs and bones are suspended within the quantified and datafied body [Lupton 2016; Van Dijck 2014]. There is a continuous stream of text particles that flow through and around the data body that one can swat away or nestle into.	https://dl.acm.org/doi/abs/10.1145/3532837.3534953	Marilène Oliver, Scott Smallwood, Stephan Moore, J. R. Carpenter
My data body	In , the magnetic resonance (MR) scanned body of the artist Marilène Oliver floats prone within a `cloud' of her textual Facebook data. Into the semi-transparent, virtual body are multiple other data corpuses downloaded from social media platforms plotted into cross sections of the body. In the horizontal plane, Mac terminal data is plotted into bone, Google data into muscle and Facebook data into fat. In the vertical plane are plotted data usage agreements. Passwords and logins flow back and forth through veins and arteries, whilst retinal images, dental scans and 3D meshes of organs and bones are suspended within the quantified and datafied body [Lupton 2016; Van Dijck 2014]. There is a continuous stream of text particles that flow through and around the data body that one can swat away or nestle into.	https://dl.acm.org/doi/abs/10.1145/3532837.3534953	Marilène Oliver, Scott Smallwood, Stephan Moore, J. R. Carpenter
My data body	In , the magnetic resonance (MR) scanned body of the artist Marilène Oliver floats prone within a `cloud' of her textual Facebook data. Into the semi-transparent, virtual body are multiple other data corpuses downloaded from social media platforms plotted into cross sections of the body. In the horizontal plane, Mac terminal data is plotted into bone, Google data into muscle and Facebook data into fat. In the vertical plane are plotted data usage agreements. Passwords and logins flow back and forth through veins and arteries, whilst retinal images, dental scans and 3D meshes of organs and bones are suspended within the quantified and datafied body [Lupton 2016; Van Dijck 2014]. There is a continuous stream of text particles that flow through and around the data body that one can swat away or nestle into.	https://dl.acm.org/doi/abs/10.1145/3532837.3534953	Marilène Oliver, Scott Smallwood, Stephan Moore, J. R. Carpenter
My data body	In , the magnetic resonance (MR) scanned body of the artist Marilène Oliver floats prone within a `cloud' of her textual Facebook data. Into the semi-transparent, virtual body are multiple other data corpuses downloaded from social media platforms plotted into cross sections of the body. In the horizontal plane, Mac terminal data is plotted into bone, Google data into muscle and Facebook data into fat. In the vertical plane are plotted data usage agreements. Passwords and logins flow back and forth through veins and arteries, whilst retinal images, dental scans and 3D meshes of organs and bones are suspended within the quantified and datafied body [Lupton 2016; Van Dijck 2014]. There is a continuous stream of text particles that flow through and around the data body that one can swat away or nestle into.	https://dl.acm.org/doi/abs/10.1145/3532837.3534953	Marilène Oliver, Scott Smallwood, Stephan Moore, J. R. Carpenter
Möbius Convolutions for Spherical CNNs	Möbius transformations play an important role in both geometry and spherical image processing – they are the group of conformal automorphisms of 2D surfaces and the spherical equivalent of homographies. Here we present a novel, Möbius-equivariant spherical convolution operator which we call Möbius convolution; with it, we develop the foundations for Möbius-equivariant spherical CNNs. Our approach is based on the following observation: to achieve equivariance, we only need to consider the lower-dimensional subgroup which transforms the positions of points as seen in the frames of their neighbors. To efficiently compute Möbius convolutions at scale we derive an approximation of the action of the transformations on spherical filters, allowing us to compute our convolutions in the spectral domain with the fast Spherical Harmonic Transform. The resulting framework is flexible and descriptive, and we demonstrate its utility by achieving promising results in both shape classification and image segmentation tasks.	https://dl.acm.org/doi/abs/10.1145/3528233.3530724	Thomas W. Mitchel, Noam Aigerman, Vladimir G. Kim, Michael Kazhdan
NIMBLE: a non-rigid hand model with bones and muscles	Emerging Metaverse applications demand reliable, accurate, and photorealistic reproductions of human hands to perform sophisticated operations as if in the physical world. While real human hand represents one of the most intricate coordination between bones, muscle, tendon, and skin, state-of-the-art techniques unanimously focus on modeling only the skeleton of the hand. In this paper, we present NIMBLE, a novel parametric hand model that includes the missing key components, bringing 3D hand model to a new level of realism. We first annotate muscles, bones and skins on the recent Magnetic Resonance Imaging hand (MRI-Hand) dataset [Li et al. 2021] and then register a volumetric template hand onto individual poses and subjects within the dataset. NIMBLE consists of 20 bones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin mesh. Via iterative shape registration and parameter learning, it further produces shape blend shapes, pose blend shapes, and a joint regressor. We demonstrate applying NIMBLE to modeling, rendering, and visual inference tasks. By enforcing the inner bones and muscles to match anatomic and kinematic rules, NIMBLE can animate 3D hands to new poses at unprecedented realism. To model the appearance of skin, we further construct a photometric HandStage to acquire high-quality textures and normal maps to model wrinkles and palm print. Finally, NIMBLE also benefits learning-based hand pose and shape estimation by either synthesizing rich data or acting directly as a differentiable layer in the inference network.	https://dl.acm.org/doi/abs/10.1145/3528223.3530079	Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang, Nianyi Li, Yuexin Ma, Yuyao Zhang, Lan Xu, Jingyi Yu
Nachtalb: A multisensory Neurofeedback VR-Interface	Nachtalb is an immersive interface that enables brain-to-brain interaction using multisensory feedback. With the help of the g.tec Unicorn Hybrid Black brain-computer-interface (BCI), brain-activity-data is measured and translated visually with the Oculus Quest 2, tactilely with the bHaptics TactSuit and auditorily with 3D Sound. This intends to create a feedback loop that turns brain activity from data-input into sensory output which directly influences the brain activity data-input again.	https://dl.acm.org/doi/abs/10.1145/3532834.3536207	Paul Morat, Aaron Schwerdtfeger, Frank Heidmann
NeAT: neural adaptive tomography	In this paper, we present Neural Adaptive Tomography (NeAT), the first adaptive, hierarchical neural rendering pipeline for tomography. Through a combination of neural features with an adaptive explicit representation, we achieve reconstruction times far superior to existing neural inverse rendering methods. The adaptive explicit representation improves efficiency by facilitating empty space culling and concentrating samples in complex regions, while the neural features act as a neural regularizer for the 3D reconstruction. The NeAT framework is designed specifically for the tomographic setting, which consists only of semi-transparent volumetric scenes instead of opaque objects. In this setting, NeAT outperforms the quality of existing optimization-based tomography solvers while being substantially faster. https://github.com/darglein/NeAT	https://dl.acm.org/doi/abs/10.1145/3528223.3530121	Darius Rückert, Yuanhao Wang, Rui Li, Ramzi Idoughi, Wolfgang Heidrich
NeROIC: neural rendering of objects from online image collections	We present a novel method to acquire object representations from online image collections, capturing high-quality geometry and material properties of arbitrary objects from photographs with varying cameras, illumination, and backgrounds. This enables various object-centric rendering applications such as novel-view synthesis, relighting, and harmonized background composition from challenging in-the-wild input. Using a multi-stage approach extending neural radiance fields, we first infer the surface geometry and refine the coarsely estimated initial camera parameters, while leveraging coarse foreground object masks to improve the training efficiency and geometry quality. We also introduce a robust normal estimation technique which eliminates the effect of geometric noise while retaining crucial details. Lastly, we extract surface material properties and ambient illumination, represented in spherical harmonics with extensions that handle transient elements, sharp shadows. The union of these components results in a highly modular and efficient object acquisition framework. Extensive evaluations and comparisons demonstrate the advantages of our approach in capturing high-quality geometry and appearance properties useful for rendering applications.	https://dl.acm.org/doi/abs/10.1145/3528223.3530177	Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, Sergey Tulyakov
Near-Gaze Visualisations of Empathic Communication Cues in Mixed Reality Collaboration	In this poster, we present a live 360° panoramic-video based empathic Mixed Reality (MR) collaboration system that shares various Near-Gaze non-verbal communication cues including gaze, hand pointing, gesturing, and heart rate visualisations in real-time. The preliminary results indicate that the interface with the partner's communication cues visualised close to the gaze point allows users to focus without dividing attention to the collaborator's physical body movements yet still effectively communicate. Shared gaze visualisations coupled with deictic languages are primarily used to affirm joint attention and mutual understanding, while hand pointing and gesturing are used as secondary. Our approach provides a new way to help enable effective remote collaboration through varied empathic communication visualisations and modalities which covers different task properties and spatial setups.	https://dl.acm.org/doi/abs/10.1145/3532719.3543213	Allison Jing, Kunal Gupta, Jeremy McDade, Gun Lee, Mark Billinghurst
Neural Layered BRDFs	"Bidirectional reflectance distribution functions (BRDFs) are pervasively used in computer graphics to produce realistic physically-based appearance. Many common materials in the real world have more than one layer, like wood, skin, car paint, and many decorative materials. However, precise simulation of layered material optics is non-trivial. The most accurate approaches rely on Monte Carlo random walks to simulate the light transport within the layers, leading to high variance and cost. Other approaches are efficient, but less accurate. In this paper, we propose to perform layering in the neural space, by compressing BRDFs into latent codes via a proposed representation neural network, and performing a learned layering operation on these latent vectors via a layering network. Our BRDF evaluation is noise-free and computationally efficient, compared to the state-of-the-art approach; it is also a first step towards a ""neural algebra"" of operations on BRDFs in a latent space."	https://dl.acm.org/doi/abs/10.1145/3528233.3530732	Jiahui Fan, Beibei Wang, Milos Hasan, Jian Yang, Ling-Qi Yan
Neural Shadow Mapping	We present a neural extension of basic shadow mapping for fast, high quality hard and soft shadows. We compare favorably to fast pre-filtering shadow mapping, all while producing visual results on par with ray traced hard and soft shadows. We show that combining memory bandwidth-aware architecture specialization and careful temporal-window training leads to a fast, compact and easy-to-train neural shadowing method. Our technique is memory bandwidth conscious, eliminates the need for post-process temporal anti-aliasing or denoising, and supports scenes with dynamic view, emitters and geometry while remaining robust to unseen objects.	https://dl.acm.org/doi/abs/10.1145/3528233.3530700	Sayantan Datta, Derek Nowrouzezahrai, Christoph Schied, Zhao Dong
Neural dual contouring	We introduce (NDC), a new data-driven approach to mesh reconstruction based on dual contouring (DC). Like traditional DC, it produces exactly one vertex per grid cell and one quad for each grid edge intersection, a natural and efficient structure for reproducing sharp features. However, rather than computing vertex locations and edge crossings with hand-crafted functions that depend directly on difficult-to-obtain surface gradients, NDC uses a neural network to predict them. As a result, NDC can be trained to produce meshes from signed or unsigned distance fields, binary voxel grids, or point clouds (with or without normals); and it can produce open surfaces in cases where the input represents a sheet or partial surface. During experiments with five prominent datasets, we find that NDC, when trained on one of the datasets, generalizes well to the others. Furthermore, NDC provides better surface reconstruction accuracy, feature preservation, output complexity, triangle quality, and inference time in comparison to previous learned (e.g., neural marching cubes, convolutional occupancy networks) and traditional (e.g., Poisson) methods. Code and data are available at https://github.com/czq142857/NDC.	https://dl.acm.org/doi/abs/10.1145/3528223.3530108	Zhiqin Chen, Andrea Tagliasacchi, Thomas Funkhouser, Hao Zhang
Neural jacobian fields: learning intrinsic mappings of arbitrary meshes	This paper introduces a framework designed to accurately predict piecewise linear mappings of arbitrary meshes via a neural network, enabling training and evaluating over heterogeneous collections of meshes that do not share a triangulation, as well as producing highly detail-preserving maps whose accuracy exceeds current state of the art. The framework is based on reducing the neural aspect to a prediction of a matrix for a single given point, conditioned on a global shape descriptor. The field of matrices is then projected onto the tangent bundle of the given mesh, and used as candidate jacobians for the predicted map. The map is computed by a standard Poisson solve, implemented as a differentiable layer with cached pre-factorization for efficient training. This construction is agnostic to the triangulation of the input, thereby enabling applications on datasets with varying triangulations. At the same time, by operating in the intrinsic gradient domain of each individual mesh, it allows the framework to predict highly-accurate mappings. We validate these properties by conducting experiments over a broad range of scenarios, from semantic ones such as morphing, registration, and deformation transfer, to optimization-based ones, such as emulating elastic deformations and contact correction, as well as being the first work, to our knowledge, to tackle the task of learning to compute UV parameterizations of arbitrary meshes. The results exhibit the high accuracy of the method as well as its versatility, as it is readily applied to the above scenarios without any changes to the framework.	https://dl.acm.org/doi/abs/10.1145/3528223.3530141	Noam Aigerman, Kunal Gupta, Vladimir G. Kim, Siddhartha Chaudhuri, Jun Saito, Thibault Groueix
Neural rendering in a room: amodal 3D understanding and free-viewpoint rendering for the closed scene composed of pre-captured objects	We, as human beings, can understand and picture a familiar scene from arbitrary viewpoints given a single image, whereas this is still a grand challenge for computers. We hereby present a novel solution to mimic such human perception capability based on a new paradigm of amodal 3D scene understanding with neural rendering for a closed scene. Specifically, we first learn the prior knowledge of the objects in a closed scene via an offline stage, which facilitates an online stage to understand the room with unseen furniture arrangement. During the online stage, given a panoramic image of the scene in different layouts, we utilize a holistic neural-rendering-based optimization framework to efficiently estimate the correct 3D scene layout and deliver realistic free-viewpoint rendering. In order to handle the domain gap between the offline and online stage, our method exploits compositional neural rendering techniques for data augmentation in the offline training. The experiments on both synthetic and real datasets demonstrate that our two-stage design achieves robust 3D scene understanding and outperforms competing methods by a large margin, and we also show that our realistic free-viewpoint rendering enables various applications, including scene touring and editing. Code and data are available on the project webpage: https://zju3dv.github.io/nr_in_a_room/.	https://dl.acm.org/doi/abs/10.1145/3528223.3530163	Bangbang Yang, Yinda Zhang, Yijin Li, Zhaopeng Cui, Sean Fanello, Hujun Bao, Guofeng Zhang
NeuralPassthrough: Learned Real-Time View Synthesis for VR	Virtual reality (VR) headsets provide an immersive, stereoscopic visual experience, but at the cost of blocking users from directly observing their physical environment. Passthrough techniques are intended to address this limitation by leveraging outward-facing cameras to reconstruct the images that would otherwise be seen by the user without the headset. This is inherently a real-time view synthesis challenge, since passthrough cameras cannot be physically co-located with the user's eyes. Existing passthrough techniques suffer from distracting reconstruction artifacts, largely due to the lack of accurate depth information (especially for near-field and disoccluded objects), and also exhibit limited image quality (e.g., being low resolution and monochromatic). In this paper, we propose the first learned passthrough method and assess its performance using a custom VR headset that contains a stereo pair of RGB cameras. Through both simulations and experiments, we demonstrate that our learned passthrough method delivers superior image quality compared to state-of-the-art methods, while meeting strict VR requirements for real-time, perspective-correct stereoscopic view synthesis over a wide field of view for desktop-connected headsets.	https://dl.acm.org/doi/abs/10.1145/3528233.3530701	Lei Xiao, Salah Nouri, Joel Hegland, Alberto Garcia Garcia, Douglas Lanman
NeuralSound: learning-based modal sound synthesis with acoustic transfer	We present a novel learning-based modal sound synthesis approach that includes a mixed vibration solver for modal analysis and a radiation network for acoustic transfer. Our mixed vibration solver consists of a 3D sparse convolution network and a Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) module for iterative optimization. Moreover, we highlight the correlation between a standard numerical vibration solver and our network architecture. Our radiation network predicts the Far-Field Acoustic Transfer maps (FFAT Maps) from the surface vibration of the object. The overall running time of our learning-based approach for most new objects is less than one second on a RTX 3080 Ti GPU while maintaining a high sound quality close to the ground truth solved by standard numerical methods. We also evaluate the numerical and perceptual accuracy of our approach on different objects with various shapes and materials.	https://dl.acm.org/doi/abs/10.1145/3528223.3530184	Xutong Jin, Sheng Li, Guoping Wang, Dinesh Manocha
NeuralTailor: reconstructing sewing pattern structures from 3D point clouds of garments	The fields of SocialVR, performance capture, and virtual try-on are often faced with a need to faithfully reproduce real garments in the virtual world. One critical task is the disentanglement of the intrinsic garment shape from deformations due to fabric properties, physical forces, and contact with the body. We propose to use a garment sewing pattern, a realistic and compact garment descriptor, to facilitate the intrinsic garment shape estimation. Another major challenge is a high diversity of shapes and designs in the domain. The most common approach for Deep Learning on 3D garments is to build specialized models for individual garments or garment types. We argue that building a unified model for various garment designs has the benefit of generalization to novel garment types, hence covering a larger design domain than individual models would. We introduce NeuralTailor, a novel architecture based on point-level attention for set regression with variable cardinality, and apply it to the task of reconstructing 2D garment sewing patterns from the 3D point cloud garment models. Our experiments show that NeuralTailor successfully reconstructs sewing patterns and generalizes to garment types with pattern topologies unseen during training.	https://dl.acm.org/doi/abs/10.1145/3528223.3530179	Maria Korosteleva, Sung-Hee Lee
Neural 3D Reconstruction in the Wild	We are witnessing an explosion of neural implicit representations in computer vision and graphics. Their applicability has recently expanded beyond tasks such as shape generation and image-based rendering to the fundamental problem of image-based 3D reconstruction. However, existing methods typically assume constrained 3D environments with constant illumination captured by a small set of roughly uniformly distributed cameras. We introduce a new method that enables efficient and accurate surface reconstruction from Internet photo collections in the presence of varying illumination. To achieve this, we propose a hybrid voxel- and surface-guided sampling technique that allows for more efficient ray sampling around surfaces and leads to significant improvements in reconstruction quality. Further, we present a new benchmark and protocol for evaluating reconstruction performance on such in-the-wild scenes. We perform extensive experiments, demonstrating that our approach surpasses both classical and neural reconstruction methods on a wide variety of metrics. Code and data will be made available at https://zju3dv.github.io/neuralrecon-w.	https://dl.acm.org/doi/abs/10.1145/3528233.3530718	Jiaming Sun, Xi Chen, Qianqian Wang, Zhengqi Li, Hadar Averbuch-Elor, Xiaowei Zhou, Noah Snavely
New types of smooth subdivision algorithms	We suggest a new type of subdivision schemes based on matrix dilation for generating smooth surfaces. At each iteration, the number of the nodes in the mesh is doubled and the direction of their weighted averaging changes. The scheme has a low complexity because of a small number of coefficients (four, five or six). Using the recent techniques related to the notion of joint spectral characteristics of matrices, we find the smoothness of generated surfaces which in some cases is surprisingly better than for classical schemes.	https://dl.acm.org/doi/abs/10.1145/3532719.3543261	Tatyana Zaitseva
Node Graph Optimization Using Differentiable Proxies	Graph-based procedural materials are ubiquitous in content production industries. Procedural models allow the creation of photo-realistic materials with parametric control for flexible editing of appearance. However, designing a specific material is a time-consuming process in terms of building a model and fine-tuning parameters. Previous work [Hu et al. 2022; Shi et al. 2020] introduced material graph optimization frameworks for matching target material samples. However, these previous methods were limited to optimizing differentiable functions in the graphs. In this paper, we propose a fully differentiable framework which enables end-to-end gradient-based optimization of material graphs, even if some functions of the graph are non-differentiable. We leverage the Differentiable Proxy, a differentiable approximator of a non-differentiable black-box function. We use our framework to match structure and appearance of an output material to a target material, through a multi-stage differentiable optimization. Differentiable Proxies offer a more general optimization solution to material appearance matching than previous work.	https://dl.acm.org/doi/abs/10.1145/3528233.3530733	Yiwei Hu, Paul Guerrero, Milos Hasan, Holly Rushmeier, Valentin Deschaintre
Noise-based enhancement for foveated rendering	Human visual sensitivity to spatial details declines towards the periphery. Novel image synthesis techniques, so-called foveated rendering, exploit this observation and reduce the spatial resolution of synthesized images for the periphery, avoiding the synthesis of high-spatial-frequency details that are costly to generate but not perceived by a viewer. However, contemporary techniques do not make a clear distinction between the range of spatial frequencies that must be reproduced and those that can be omitted. For a given eccentricity, there is a range of frequencies that are detectable but not resolvable. While the accurate reproduction of these frequencies is not required, an observer can detect their absence if completely omitted. We use this observation to improve the performance of existing foveated rendering techniques. We demonstrate that this specific range of frequencies can be efficiently replaced with procedural noise whose parameters are carefully tuned to image content and human perception. Consequently, these frequencies do not have to be synthesized during rendering, allowing more aggressive foveation, and they can be replaced by noise generated in a less expensive post-processing step, leading to improved performance of the rendering system. Our main contribution is a perceptually-inspired technique for deriving the parameters of the noise required for the enhancement and its calibration. The method operates on rendering output and runs at rates exceeding 200 FPS at 4K resolution, making it suitable for integration with real-time foveated rendering systems for VR and AR devices. We validate our results and compare them to the existing contrast enhancement technique in user experiments.	https://dl.acm.org/doi/abs/10.1145/3528223.3530101	Taimoor Tariq, Cara Tursun, Piotr Didyk
Non-Line-of-Sight Transient Rendering	Transient imaging methods often analyze time-resolved light transport for applications such as range imaging, reflectance estimation and, especially, non-line-of-sight (NLOS) imaging, which targets the reconstruction of hidden geometry using measurements of indirect diffuse reflections emitted by a laser. Transient rendering is a key tool for developing such new applications. In this work, we introduce a set of simple, yet effective subpath sampling techniques targeting transient light transport simulation in occluded scenes. We analyze the usual capture setups of NLOS scenes, where the light and camera indirectly aim at hidden geometry through a secondary surface. We leverage that configuration to reduce the integration path space. We implement our techniques in our modified version of Mitsuba 2, adapted for transient light transport, allowing us to support parallelization, polarization, and differentiable rendering.	https://dl.acm.org/doi/abs/10.1145/3532719.3543223	Diego Royo, Jorge Garcia, Pablo Luesia-Lahoz, Julio Marco, Diego Gutiérrez, Adolfo Muñoz, Adrián Jarabo
Novel View Synthesis of Human Interactions from Sparse Multi-view Videos	This paper presents a novel system for generating free-viewpoint videos of multiple human performers from very sparse RGB cameras. The system reconstructs a layered neural representation of the dynamic multi-person scene from multi-view videos with each layer representing a moving instance or static background. Unlike previous work that requires instance segmentation as input, a novel approach is proposed to decompose the multi-person scene into layers and reconstruct neural representations for each layer in a weakly-supervised manner, yielding both high-quality novel view rendering and accurate instance masks. Camera synchronization error is also addressed in the proposed approach. The experiments demonstrate the better view synthesis quality of the proposed system compared to previous ones and the capability of producing an editable free-viewpoint video of a real soccer game using several asynchronous GoPro cameras. The dataset and code are available at https://github.com/zju3dv/EasyMocap .	https://dl.acm.org/doi/abs/10.1145/3528233.3530704	Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, Hujun Bao
On Fairness in Face Albedo Estimation	Digital avatars will be crucial components for immersive telecommunication, gaming, and the coming metaverse. Unfortunately, current methods for estimating the facial appearance (albedo) are biased to estimate light skin tones. This talk raises awareness of the problem with an analysis of (1) dataset biases and (2) the light/albedo ambiguity. We show how these problems can be ameliorated by recent advances, improving fairness in albedo estimation.	https://dl.acm.org/doi/abs/10.1145/3532836.3536281	Haiwen Feng, Timo Bolkart, Joachim Tesch, Michael Black, Victoria Abrevaya
On the Influence of Dynamic Illumination in the Perception of Translucency	Translucent materials are ubiquitous in the real world, from organic materials such as food or human skin, to synthetic materials like plastic or rubber. While multiple models for translucent materials exist, understanding how we perceive translucent appearance, and how it is affected by illumination and geometry, remains an open problem. In this work, we analyze how well human observers estimate the density of translucent objects for static and dynamic illumination scenarios. Interestingly, our results suggest that dynamic illumination may not be critical to assess the nature of translucent materials.	https://dl.acm.org/doi/abs/10.1145/3532719.3543234	Dario Lanza, Juan Raúl Padrón-Griffe, Adrian Jarabo, Belen Masia
Onward!: Creative Careers in Animation, Computer Graphics, and Interactive Techniques	Industry panelists share perspectives and insights for students and educators who are considering careers in computer graphics and interactive techniques. Creative industries have transformed as a result of the global pandemic. Transformed workplace cultures and new technologies make room for alternative and dehabituated career paths, presenting a variety of opportunities and unforeseen challenges. Individual representatives discuss the general and specific state of affairs within their own industries, and provide insight into changing employment paradigms. Discussion includes advice for educators to help prepare students for changing workplace paradigms, as well as the preparation, training, and personal attributes students need to enter related career fields. Panelists consider what qualities make for desirable entry-level applicants in their respective fields, and elaborate upon changes in the transition from school to work resulting from the global pandemic. Represented industry segments include animation, big-data, interactive design, and game media. Questions considered include how pedagogy can help prepare and empower students for successful creative careers; what entry-level applicants should have (and should not have) on resumes, portfolios, and demo reels; and what can students do on their own to proactively acquire requisite credentials. Discussion will expose fresh outlooks on the futures of creative fields in computer graphics and interactive techniques.	https://dl.acm.org/doi/abs/10.1145/3532724.3535612	Johannes DeYoung
Optimizing vision and visuals: lectures on cameras, displays and perception	The evolution of the internet is underway, where immersive virtual 3D environments (commonly known as ) will replace flat 2D interfaces. Crucial ingredients in this transformation are next-generation displays and cameras representing genuinely 3D visuals while meeting the human visual system's perceptual requirements. This course will provide a fast-paced introduction to optimization methods for next-generation interfaces geared towards immersive virtual 3D environments. Firstly, we will introduce lensless cameras for high dimensional compressive sensing (e.g., single exposure capture to a video or one-shot 3D). Our audience will learn to process images from a lensless camera at the end. Secondly, we introduce holographic displays as a potential candidate for next-generation displays. By the end of this course, you will learn to create your 3D images that can be viewed using a standard holographic display. Lastly, we will introduce perceptual guidance that could be an integral part of the optimization routines of displays and cameras. Our audience will gather experience in integrating perception to display and camera optimizations. This course targets a wide range of audiences, from domain experts to newcomers. To do so, examples from this course will be based on our in-house toolkit to be replicable for future use. The course material will provide example codes and a broad survey with crucial information on cameras, displays and perception.	https://dl.acm.org/doi/abs/10.1145/3532720.3535650	Koray Kavakli, David Robert Walton, Nick Antipa, Rafał Mantiuk, Douglas Lanman, Kaan Akşit
Orogenesis	Before dawn, an odd character born from the sky crashes down in a desert. Awoken and weak, he's going to struggle to get back to the highest point in the sky, while facing a rising earth. However in his frantic ascension, something is happening under his steps...	https://dl.acm.org/doi/abs/10.1145/3512752.3517792	Axel Vendrely, Damien Barthas, Lise Delacroix, Emma Gaillien, Pierre Legargeant, Roland Van Hollebeke
Palette: Image-to-Image Diffusion Models	This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code.	https://dl.acm.org/doi/abs/10.1145/3528233.3530757	Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, Mohammad Norouzi
Penetration-free projective dynamics on the GPU	We present a GPU algorithm for deformable simulation. Our method offers good computational efficiency and penetration-free guarantee at the same time, which are not common with existing techniques. The main idea is an algorithmic integration of projective dynamics (PD) and incremental potential contact (IPC). PD is a position-based simulation framework, favored for its robust convergence and convenient implementation. We show that PD can be employed to handle the variational optimization with the interior point method e.g., IPC. While conceptually straightforward, this requires a dedicated rework over the collision resolution and the iteration modality to avoid incorrect collision projection with improved numerical convergence. IPC exploits a barrier-based formulation, which yields an infinitely large penalty when the constraint is on the verge of being violated. This mechanism guarantees intersection-free trajectories of deformable bodies during the simulation, as long as they are apart at the rest configuration. On the downside, IPC brings a large amount of nonlinearity to the system, making PD slower to converge. To mitigate this issue, we propose a novel GPU algorithm named A-Jacobi for faster linear solve at the global step of PD. A-Jacobi is based on Jacobi iteration, but it better harvests the computation capacity on modern GPUs by lumping several Jacobi steps into a single iteration. In addition, we also re-design the CCD root finding procedure by using a new minimum-gradient Newton algorithm. Those saved time budgets allow more iterations to accommodate stiff IPC barriers so that the result is both realistic and collision-free. Putting together, our algorithm simulates complicated models of both solids and shells on the GPU at an interactive rate or even in real time.	https://dl.acm.org/doi/abs/10.1145/3528223.3530069	Lei Lan, Guanqun Ma, Yin Yang, Changxi Zheng, Minchen Li, Chenfanfu Jiang
Perception of letter glyph parameters for InfoTypography	The advent of variable font technologies---where typographic parameters such as weight, x-height and slant are easily adjusted across a range---enables encoding ordinal, interval or ratio data into text that is still readable. This is potentially valuable to represent additional information in text labels in visualizations (e.g., font weight can indicate city size in a geographical visualization) or in text itself (e.g., the intended reading speed of a sentence can be encoded with the font width). However, we do not know how different parameters, which are complex variations of shape, are perceived by the human visual system. Without this information it is difficult to select appropriate parameters and mapping functions that maximize perception of differences within the parameter range. We provide an empirical characterization of seven typographical parameters of Latin fonts in terms of absolute perception and just noticeable differences (JNDs) to help visualization designers to choose typographic parameters for visualizations that contain text, as well as support typographers and type designers when selecting which levels of these parameters to implement to achieve differentiability between normal text, emphasized text and different headings.	https://dl.acm.org/doi/abs/10.1145/3528223.3530111	Johannes Lang, Miguel A. Nacenta
Perceptual Requirements for Eye-Tracked Distortion Correction in VR	We present a virtual reality display system simulator that accurately reproduces gaze-contingent distortions created by any viewing optic. The simulator hardware supports rapid prototyping by presenting stereoscopic distortions on a high-speed television paired with shutter glasses, eliminating the need to fabricate physical optics. We further introduce light field portals as an efficient and general-purpose representation for VR optics, enabling real-time emulation using our simulator. This platform is used to conduct the first user study of perceptual requirements for eye-tracked optical distortion correction. Because our hardware platform facilitates consistent head and eye movements, it enables direct comparison of these requirements across observers, optical designs, and scene content. We conclude by introducing a simple binocular distortion metric, built using light field portals, which agrees with key trends identified in the user study and lays a foundation for the design of perceptually-based distortion metrics and correction schemes.	https://dl.acm.org/doi/abs/10.1145/3528233.3530699	Phillip Guan, Olivier Mercier, Michael Shvartsman, Douglas Lanman
Photo-to-shape material transfer for diverse structures	We introduce a method for assigning photorealistic relightable materials to 3D shapes in an automatic manner. Our method takes as input a photo exemplar of a real object and a 3D object with segmentation, and uses the exemplar to guide the assignment of materials to the parts of the shape, so that the appearance of the resulting shape is as similar as possible to the exemplar. To accomplish this goal, our method combines an with a The image translation network translates the color from the exemplar to a projection of the 3D shape and the part segmentation from the projection to the exemplar. Then, the material prediction network assigns materials from a collection of realistic materials to the projected parts, based on the translated images and perceptual similarity of the materials. One key idea of our method is to use the translation network to establish a correspondence between the exemplar and shape projection, which allows us to transfer materials between objects with Another key idea of our method is to use the two pairs of (color, segmentation) images provided by the image translation to guide the material assignment, which enables us to ensure the in the assignment. We demonstrate that our method allows us to assign materials to shapes so that their appearances better resemble the input exemplars, improving the quality of the results over the state-of-the-art method, and allowing us to automatically create thousands of shapes with high-quality photorealistic materials. Code and data for this paper are available at https://github.com/XiangyuSu611/TMT.	https://dl.acm.org/doi/abs/10.1145/3528223.3530088	Ruizhen Hu, Xiangyu Su, Xiangkai Chen, Oliver Van Kaick, Hui Huang
Physics informed neural fields for smoke reconstruction with sparse data	High-fidelity reconstruction of dynamic fluids from sparse multiview RGB videos remains a formidable challenge, due to the complexity of the underlying physics as well as the severe occlusion and complex lighting in the captured data. Existing solutions either assume knowledge of obstacles and lighting, or only focus on simple fluid scenes without obstacles or complex lighting, and thus are unsuitable for real-world scenes with unknown lighting conditions or arbitrary obstacles. We present the first method to reconstruct dynamic fluid phenomena by leveraging the governing physics (ie, Navier -Stokes equations) in an end-to-end optimization from a mere set of sparse video frames without taking lighting conditions, geometry information, or boundary conditions as input. Our method provides a continuous spatio-temporal scene representation using neural networks as the ansatz of density and velocity solution functions for fluids as well as the radiance field for static objects. With a hybrid architecture that separates static and dynamic contents apart, fluid interactions with static obstacles are reconstructed for the first time without additional geometry input or human labeling. By augmenting time-varying neural radiance fields with physics-informed deep learning, our method benefits from the supervision of images and physical priors. Our progressively growing model with regularization further disentangles the density-color ambiguity in the radiance field, which allows for a more robust optimization from the given input of sparse views. A pretrained density-to-velocity fluid model is leveraged in addition as the data prior to avoid suboptimal velocity solutions which underestimate vorticity but trivially fulfill physical equations. Our method exhibits high-quality results with relaxed constraints and strong flexibility on a representative set of synthetic and real flow captures. Code and sample tests are at https://people.mpi-inf.mpg.de/~mchu/projects/PI-NeRF/.	https://dl.acm.org/doi/abs/10.1145/3528223.3530169	Mengyu Chu, Lingjie Liu, Quan Zheng, Erik Franz, Hans-Peter Seidel, Christian Theobalt, Rhaleb Zayer
Physics-based Character Control Using conditional GAIL	The goal of our research is to control a physics-based character that learns several dynamic motor skills using conditional Generative adversarial imitation learning(GAIL). We present a network-based learning algorithm that learns various motor skills and changing motions between the motor skills from disparate motion clips. The overall framework for our controller is composed of a control policy which generates a character's behavior, and a discriminator which induces the policy to produce proper motions from a user's commands. The discriminator and the policy take outputs from each other as input and improve each performance through an adversarial training process. Using this system, when a user commands a specific motion to the character, the character can design a motion plan to perform the motion from the current pose. We demonstrated the effectiveness of our approach through examples with an interactive character that learns various dynamics motor skills and follows a user command in the physics simulation.	https://dl.acm.org/doi/abs/10.1145/3532719.3543224	Yongwoo Lee, Jehee Lee
Physics-based character controllers using conditional VAEs	High-quality motion capture datasets are now publicly available, and researchers have used them to create kinematics-based controllers that can generate plausible and diverse human motions without conditioning on specific goals (i.e., a task-agnostic generative model). In this paper, we present an algorithm to build such controllers for physically simulated characters having many degrees of freedom. Our physics-based controllers are learned by using conditional VAEs, which can perform a variety of behaviors that are similar to motions in the training dataset. The controllers are robust enough to generate more than a few minutes of motion without conditioning on specific goals and to allow many complex downstream tasks to be solved efficiently. To show the effectiveness of our method, we demonstrate controllers learned from several different motion capture databases and use them to solve a number of downstream tasks that are challenging to learn controllers that generate natural-looking motions from scratch. We also perform ablation studies to demonstrate the importance of the elements of the algorithm. Code and data for this paper are available at: https://github.com/facebookresearch/PhysicsVAE	https://dl.acm.org/doi/abs/10.1145/3528223.3530067	Jungdam Won, Deepak Gopinath, Jessica Hodgins
Piecewise-smooth surface fitting onto unstructured 3D sketches	We propose a method to transform unstructured 3D sketches into piecewise smooth surfaces that preserve sketched geometric features. Immersive 3D drawing and sketch-based 3D modeling applications increasingly produce imperfect and unstructured collections of 3D strokes as design output. These 3D sketches are readily perceived as piecewise smooth surfaces by viewers, but are poorly handled by existing 3D surface techniques tailored to well-connected curve networks or sparse point sets. Our algorithm is aligned with human tendency to imagine the strokes as a small set of simple smooth surfaces joined along stroke boundaries. Starting with an initial proxy surface, we iteratively segment the surface into smooth patches joined sharply along some strokes, and optimize these patches to fit surrounding strokes. Our evaluation is fourfold: we demonstrate the impact of various algorithmic parameters, we evaluate our method on synthetic sketches with known ground truth surfaces, we compare to prior art, and we show compelling results on more than 50 designs from a diverse set of 3D sketch sources.	https://dl.acm.org/doi/abs/10.1145/3528223.3530100	Emilie Yu, Rahul Arora, J. Andreas Bærentzen, Karan Singh, Adrien Bousseau
Position-free multiple-bounce computations for smith microfacet BSDFs	Bidirectional Scattering Distribution Functions (BSDFs) encode how a material reflects or transmits the incoming light. The most commonly used model is the microfacet BSDF. It computes the material response from the microgeometry of the surface assuming a single bounce on specular microfacets. The original model ignores multiple bounces on the microgeometry, resulting in an energy loss, especially for rough materials. In this paper, we present a new method to compute the multiple bounces inside the microgeometry, eliminating this energy loss. Our method relies on a position-free formulation of multiple bounces inside the microgeometry. We use an explicit mathematical definition of the path space that describes single and multiple bounces in a uniform way. We then study the behavior of light on the different vertices and segments in the path space, leading to a reciprocal multiple-bounce description of BSDFs. Furthermore, we present practical, unbiased Monte Carlo estimators to compute multiple scattering. Our method is less noisy than existing algorithms for computing multiple scattering. It is almost noise-free with a very-low sampling rate, from 2 to 4 samples per pixel (spp).	https://dl.acm.org/doi/abs/10.1145/3528223.3530112	Beibei Wang, Wenhua Jin, Jiahui Fan, Jian Yang, Nicolas Holzschuch, Ling-Qi Yan
Powering up Rig Deformation: Shot Sculpting on DC League of Super-Pets	We present the shot sculpting toolset used on DC League of Super-Pets to enhance rig deformation in order to meet creative goals in animation shots. This toolset repurposes rigging tools, primarily Animal Logic's proprietary deformation system Bond [Baillet et al. 2020], to give animators intuitive and flexible ways to expand upon a rig's deformation stack and push the rig past its intended range of motion. Shot sculpting workflows have been fully integrated into our pipeline and designed for minimal playback speed loss, providing an overall seamless experience for artists.	https://dl.acm.org/doi/abs/10.1145/3532836.3536247	Valerie Bernard, Miguel Gao, Daniel Springall, David Ward
Practical Multiple-Scattering Sheen Using Linearly Transformed Cosines	We introduce a new volumetric sheen BRDF that approximates scattering observed in surfaces covered with normally-oriented fibers. Our previous sheen model was motivated by measured cloth reflectance, but lacked significant backward scattering. The model presented here allows a more realistic cloth appearance and can also approximate a dusty appearance. Our sheen model is implemented using a linearly transformed cosine (LTC) lobe fitted to a volumetric scattering layer. We detail the fitting process, and present and discuss our results.	https://dl.acm.org/doi/abs/10.1145/3532836.3536240	Tizian Zeltner, Brent Burley, Matt Jen-Yuan Chiang
Practical aspects of spectral data in digital content production	Compared to path tracing, spectral rendering is still often considered to be a niche application used mainly to produce optical wave effects like dispersion or diffraction. And while over the last years more and more people started exploring the potential of spectral image synthesis, it is still widely assumed to be only of importance in high-quality offline applications associated with long render times and high visual fidelity. While it is certainly true that describing light interactions in a spectral way is a necessity for predictive rendering, its true potential goes far beyond that. Used correctly, not only will it guarantee colour fidelity, but it will also simplify workflows for all sorts of applications. Wētā Digital's renderer Manuka showed that there is a place for a spectral renderer in a production environment and how workflows can be simplified if the whole pipeline adapts. Picking up from the course last year, we want to continue the discussion we started as we firmly believe that spectral data is the future in content production. The authors feel enthusiastic about more people being aware of the advantages that spectral rendering and spectral workflows bring and share the knowledge we gained over many years. The novel workflows emerged during the adaptation of spectral techniques at a number of large companies are introduced to a wide audience including technical directors, artists and researchers. However, while last year's course concentrated primarily on the algorithmic sides of spectral image synthesis, this year we want to focus on the practical aspects. We will draw examples from virtual production, digital humans over spectral noise reduction to image grading, therefore showing the usage of spectral data enhancing each and every single part of the image pipeline.	https://dl.acm.org/doi/abs/10.1145/3532720.3535632	Andrea Weidlich, Chloe LeGendre, Carlos Aliaga, Christophe Hery, Jean-Marie Aubry, Jiří Vorba, Daniele Siragusano, Richard Kirk
Practical course on computing derivatives in code	Derivatives occur frequently in computer graphics and arise in many different contexts. Gradients and often Hessians of objective functions are required for efficient optimization. Gradients of potential energy are used to compute forces. Constitutive models are frequently formulated from an energy density, which must be differentiated to compute stress. Hessians of potential energy or energy density are needed for implicit integration. As the methods used in computer graphics become more accurate and sophisticated, the complexity of the functions that must be differentiated also increases. The purpose of this course is to show that it is practical to compute derivatives even for functions that may seem impossibly complex. This course provides practical strategies and techniques for planning, computing, testing, debugging, and optimizing routines for computing first and second derivatives of real-world routines. This course will also introduce and explore auto differentiation, which encompasses a variety of techniques for obtaining derivatives automatically. Applications to machine learning and differentiable simulation are also considered. The goal of this course is not to introduce the concept of derivatives, how to use them, or even how to calculate them per se. This is not intended to be a calculus course; we will assume that our audience is familiar with multivariable calculus. Instead, the emphasis is on implementing derivatives of complicated computational procedures in computer programs and actually getting them to work.	https://dl.acm.org/doi/abs/10.1145/3532720.3535643	Craig Schroeder
Practical level-of-detail aggregation of fur appearance	Fur appearance rendering is crucial for the realism of computer generated imagery, but is also a challenge in computer graphics for many years. Much effort has been made to accurately simulate the multiple-scattered light transport among fur fibers, but the computation cost is still very high, since the number of fur fibers is usually extremely large. In this paper, we aim at reducing the number of fur fibers while preserving realistic fur appearance. We present an aggregated fur appearance model, using one thick cylinder to accurately describe the aggregated optical behavior of a bunch of fur fibers, including the multiple scattering of light among them. Then, to acquire the parameters of our aggregated model, we use a lightweight neural network to map individual fur fiber's optical properties to those in our aggregated model. Finally, we come up with a practical heuristic that guides the simplification process of fur dynamically at different bounces of the light, leading to a practical level-of-detail rendering scheme. Our method achieves nearly the same results as the ground truth, but performs 3.8×-13.5× faster.	https://dl.acm.org/doi/abs/10.1145/3528223.3530105	Junqiu Zhu, Sizhe Zhao, Lu Wang, Yanning Xu, Ling-Qi Yan
Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks	We present a learning algorithm that uses bone-driven motion networks to predict the deformation of loose-fitting garment meshes at interactive rates. Given a garment, we generate a simulation database and extract virtual bones from simulated mesh sequences using skin decomposition. At runtime, we separately compute low- and high-frequency deformations in a sequential manner. The low-frequency deformations are predicted by transferring body motions to virtual bones' motions, and the high-frequency deformations are estimated leveraging the global information of virtual bones' motions and local information extracted from low-frequency meshes. In addition, our method can estimate garment deformations caused by variations of the simulation parameters (e.g., fabric's bending stiffness) using an RBF kernel ensembling trained networks for different sets of simulation parameters. Through extensive comparisons, we show that our method outperforms state-of-the-art methods in terms of prediction accuracy of mesh deformations by about 20% in RMSE and 10% in Hausdorff distance and STED. The code and data are available at https://github.com/non-void/VirtualBones.	https://dl.acm.org/doi/abs/10.1145/3528233.3530709	Xiaoyu Pan, Jiaming Mai, Xinwei Jiang, Dongxue Tang, Jingxiang Li, Tianjia Shao, Kun Zhou, Xiaogang Jin, Dinesh Manocha
Presenting Architectural Research in VR	In recent years, new standards for sustainable development and innovations in technologies require us to rethink the forms and functions of architecture that meet emerging needs for future cities. Anticipated developments in digital communication, autonomous vehicles, smart cities, and new energy harvesting technologies are expected to change our lifestyles, leading us to reconsider how we design and organize spatial conditions in our built environment. Such changes will challenge the roles of designers and how they represent and promote their design solutions using new media, including virtual reality (VR) and digital design technologies. In this submission, I present a design assignment that challenges students to create speculative building designs by applying anticipated technologies that can make our built environment more sustainable. The process in this assignment can be applied to other design fields, including product design and urban design. Many design disciplines can benefit by exploring educational approaches that help integrate new emerging technologies for sustainable product developments and incorporating new digital means for representations for such visions.	https://dl.acm.org/doi/abs/10.1145/3532724.3535602	Taro Narahara
Procedural Modeling with Blender's Geometry Nodes: A workshop on taking advantage of the Geometry Nodes feature in Blender for procedural modeling	Since about a year ago, the 3D program Blender has shipped with a set of procedural/parametric modeling tools called Geometry Nodes. While still fairly new and in development, the system has already unlocked several new ways for users to express their creativity and solve practical problems. It allows for the creation of new geometry from scratch, modification of existing geometry, and distribution of instances using a node graph. This presentation will cover those three major use cases, what's possible to create with the system, and demonstrate how to work with the tools by showing how to create a simple model generator.	https://dl.acm.org/doi/abs/10.1145/3532725.3538516	Jason van Gumster, Jonathan Lampel
Procedural texturing of solid wood with knots	We present a procedural framework for modeling the annual ring pattern of solid wood with knots. Although wood texturing is a well-studied topic, there have been few previous attempts at modeling knots inside the wood texture. Our method takes the skeletal structure of a tree log as input and produces a three-dimensional scalar field representing the time of added growth, which defines the volumetric annual ring pattern. First, separate fields are computed around each strand of the skeleton, i.e., the stem and each knot. The strands are then merged into a single field using smooth minimums. We further suggest techniques for controlling the smooth minimum to adjust the balance of smoothness and reproduce the distortion effects observed around dead knots. Our method is implemented as a shader program running on a GPU with computation times of approximately 0.5 s per image and an input data size of 600 KB. We present rendered images of solid wood from pine and spruce as well as plywood and cross-laminated timber (CLT). Our results were evaluated by wood experts, who confirmed the plausibility of the rendered annual ring patterns. Link to code: https://github.com/marialarsson/procedural_knots.	https://dl.acm.org/doi/abs/10.1145/3528223.3530081	Maria Larsson, Takashi Ijiri, Hironori Yoshida, Johannes A. J. Huber, Magnus Fredriksson, Olof Broman, Takeo Igarashi
ProjecString: Turning an Everyday String Curtain into an Interactive Projection Display	We present ProjecString, a touch-sensitive string curtain projection display that encourages novel interactions via touching, grasping, and seeing and walking through the display. We embed capacitive-sensing conductive chains into an everyday string curtain, turning it into both a space divider and an interactive display. This novel take on transforming an everyday object into an interactive projection surface with a unique translucent property creates novel interactions that are both immersive and isolating.	https://dl.acm.org/doi/abs/10.1145/3532719.3543203	Wooje Chang, Yeeun Shin, Yeon Soo Kim, Woohun Lee
Project Hubble: Multi-User XR Collaboration Tool	The work introduces Project Hubble, Unity's internal multi-user XR collaboration tool. Hubble provides a delightful interface to navigate and manipulate freeform and guided experiences. Tools and technology are provided on top for voice chat, avatars, annotations, and sharing.	https://dl.acm.org/doi/abs/10.1145/3532834.3536215	Brandon Kruysman, Andrew P. Maneri, Yinghua Yang, Kyle Vaidyanathan, Donnavon Webb, Matt Schoen
Pseudo-3D Scene Modeling for Virtual Reality Using Stylized Novel View Synthesis	Stylized Novel View Synthesis is an emerging technique that combines style transfer and view synthesis. However, none of the existing works explore their applications in Virtual Reality (VR). This work devises a novel application for stylized novel view synthesis. We propose to replace actual 3D scene models or 360 images with stylized stereoscopic images for the areas outside the major play area but are still visible to the user. User study results reveal that users can feel 3D sense and tell them from plane texture. Codes and other materials are available at: kuan-wei-tseng.github.io/ArtNV	https://dl.acm.org/doi/abs/10.1145/3532719.3543232	Kuan-Wei Tseng, Jing-Yuan Huang, Yang-Shen Chen, Chu-Song Chen, Yi-Ping Hung
QuickPose: Real-time Multi-view Multi-person Pose Estimation in Crowded Scenes	This work proposes a real-time algorithm for reconstructing 3D human poses in crowded scenes from multiple calibrated views. The key challenge of this problem is to efficiently match 2D observations across multiple views. Previous methods perform multi-view matching either at the full-body level, which is sensitive to 2D pose estimation error, or at the part level, which ignores 2D constraints between different types of body parts in the same view. Instead, our approach reasons about all plausible skeleton proposals during multi-view matching, where each skeleton may consist of an arbitrary number of parts instead of being a whole body or a single part. To this end, we formulate the multi-view matching problem as mode seeking in the space of skeleton proposals and develop an efficient algorithm named QuickPose to solve the problem, which enables real-time motion capture in crowded scenes. Experiments show that the proposed algorithm achieves the state-of-the-art performance in terms of both speed and accuracy on public datasets.	https://dl.acm.org/doi/abs/10.1145/3528233.3530746	Zhize Zhou, Qing Shuai, Yize Wang, Qi Fang, Xiaopeng Ji, Fashuai Li, Hujun Bao, Xiaowei Zhou
R2E2: low-latency path tracing of terabyte-scale scenes using thousands of cloud CPUs	"In this paper we explore the viability of path tracing massive scenes using a ""supercomputer"" constructed on-the-fly from thousands of small, serverless cloud computing nodes. We present R2E2 (Really Elastic Ray Engine) a scene decomposition-based parallel renderer that rapidly acquires thousands of cloud CPU cores, loads scene geometry from a pre-built scene BVH into the aggregate memory of these nodes in parallel, and performs full path traced global illumination using an inter-node messaging service designed for communicating ray data. To balance ray tracing work across many nodes, R2E2 adopts a service-oriented design that statically replicates geometry and texture data from frequently traversed scene regions onto multiple nodes based on estimates of load, and dynamically assigns ray tracing work to lightly loaded nodes holding the required data. We port pbrt's ray-scene intersection components to the R2E2 architecture, and demonstrate that scenes with up to a terabyte of geometry and texture data (where as little as 1/250th of the scene can fit on any one node) can be path traced at 4K resolution, in tens of seconds using thousands of tiny serverless nodes on the AWS Lambda platform."	https://dl.acm.org/doi/abs/10.1145/3528223.3530171	Sadjad Fouladi, Brennan Shacklett, Fait Poms, Arjun Arora, Alex Ozdemir, Deepti Raghavan, Pat Hanrahan, Kayvon Fatahalian, Keith Winstein
RAY	"In 1922, Man Ray, a significant contributor to Dada and Surrealism movements, invented the term ""Rayograph"" to describe his cameraless darkroom created photo images. He revealed the dreamlike characteristics of daily objects by placing them in photosensitized paper and exposing them to light. His Rayographs are situated in-between visual representation and abstraction, pointing to a new cameraless photographic method. Man Ray wrote in 1921 that he wanted to ""make my photography automatic---to use my camera as I would a typewriter."""	https://dl.acm.org/doi/abs/10.1145/3532837.3534951	Weidi Zhang
RAY	"In 1922, Man Ray, a significant contributor to Dada and Surrealism movements, invented the term ""Rayograph"" to describe his cameraless darkroom created photo images. He revealed the dreamlike characteristics of daily objects by placing them in photosensitized paper and exposing them to light. His Rayographs are situated in-between visual representation and abstraction, pointing to a new cameraless photographic method. Man Ray wrote in 1921 that he wanted to ""make my photography automatic---to use my camera as I would a typewriter."""	https://dl.acm.org/doi/abs/10.1145/3532837.3534951	Weidi Zhang
RAY	"In 1922, Man Ray, a significant contributor to Dada and Surrealism movements, invented the term ""Rayograph"" to describe his cameraless darkroom created photo images. He revealed the dreamlike characteristics of daily objects by placing them in photosensitized paper and exposing them to light. His Rayographs are situated in-between visual representation and abstraction, pointing to a new cameraless photographic method. Man Ray wrote in 1921 that he wanted to ""make my photography automatic---to use my camera as I would a typewriter."""	https://dl.acm.org/doi/abs/10.1145/3532837.3534951	Weidi Zhang
RAY	"In 1922, Man Ray, a significant contributor to Dada and Surrealism movements, invented the term ""Rayograph"" to describe his cameraless darkroom created photo images. He revealed the dreamlike characteristics of daily objects by placing them in photosensitized paper and exposing them to light. His Rayographs are situated in-between visual representation and abstraction, pointing to a new cameraless photographic method. Man Ray wrote in 1921 that he wanted to ""make my photography automatic---to use my camera as I would a typewriter."""	https://dl.acm.org/doi/abs/10.1145/3532837.3534951	Weidi Zhang
RAY	"In 1922, Man Ray, a significant contributor to Dada and Surrealism movements, invented the term ""Rayograph"" to describe his cameraless darkroom created photo images. He revealed the dreamlike characteristics of daily objects by placing them in photosensitized paper and exposing them to light. His Rayographs are situated in-between visual representation and abstraction, pointing to a new cameraless photographic method. Man Ray wrote in 1921 that he wanted to ""make my photography automatic---to use my camera as I would a typewriter."""	https://dl.acm.org/doi/abs/10.1145/3532837.3534951	Weidi Zhang
RAY	"In 1922, Man Ray, a significant contributor to Dada and Surrealism movements, invented the term ""Rayograph"" to describe his cameraless darkroom created photo images. He revealed the dreamlike characteristics of daily objects by placing them in photosensitized paper and exposing them to light. His Rayographs are situated in-between visual representation and abstraction, pointing to a new cameraless photographic method. Man Ray wrote in 1921 that he wanted to ""make my photography automatic---to use my camera as I would a typewriter."""	https://dl.acm.org/doi/abs/10.1145/3532837.3534951	Weidi Zhang
RAY	"In 1922, Man Ray, a significant contributor to Dada and Surrealism movements, invented the term ""Rayograph"" to describe his cameraless darkroom created photo images. He revealed the dreamlike characteristics of daily objects by placing them in photosensitized paper and exposing them to light. His Rayographs are situated in-between visual representation and abstraction, pointing to a new cameraless photographic method. Man Ray wrote in 1921 that he wanted to ""make my photography automatic---to use my camera as I would a typewriter."""	https://dl.acm.org/doi/abs/10.1145/3532837.3534951	Weidi Zhang
RAY	"In 1922, Man Ray, a significant contributor to Dada and Surrealism movements, invented the term ""Rayograph"" to describe his cameraless darkroom created photo images. He revealed the dreamlike characteristics of daily objects by placing them in photosensitized paper and exposing them to light. His Rayographs are situated in-between visual representation and abstraction, pointing to a new cameraless photographic method. Man Ray wrote in 1921 that he wanted to ""make my photography automatic---to use my camera as I would a typewriter."""	https://dl.acm.org/doi/abs/10.1145/3532837.3534951	Weidi Zhang
Radiant soma	The human body has historically been a constant source of fascination in the arts and sciences. With the impact of the COVID-19 pandemic, the debate over the virtues and disadvantages of physical versus virtual bodies has increased dramatically. One of the most difficult attributes of the human body to translate into other forms is the essence of human movement and, by extension, energy. emphasizes the ephemerality of human movement by visualizing motion capture data with light. The installation of lasers and phosphorescent objects transforms choreography that the original performer can no longer perform into a constant stream of lively spirits.	https://dl.acm.org/doi/abs/10.1145/3532837.3534950	Jayson Haebich, Eugenia S. Kim, Alvaro Cassinelli
Radiant soma	The human body has historically been a constant source of fascination in the arts and sciences. With the impact of the COVID-19 pandemic, the debate over the virtues and disadvantages of physical versus virtual bodies has increased dramatically. One of the most difficult attributes of the human body to translate into other forms is the essence of human movement and, by extension, energy. emphasizes the ephemerality of human movement by visualizing motion capture data with light. The installation of lasers and phosphorescent objects transforms choreography that the original performer can no longer perform into a constant stream of lively spirits.	https://dl.acm.org/doi/abs/10.1145/3532837.3534950	Jayson Haebich, Eugenia S. Kim, Alvaro Cassinelli
Radiant soma	The human body has historically been a constant source of fascination in the arts and sciences. With the impact of the COVID-19 pandemic, the debate over the virtues and disadvantages of physical versus virtual bodies has increased dramatically. One of the most difficult attributes of the human body to translate into other forms is the essence of human movement and, by extension, energy. emphasizes the ephemerality of human movement by visualizing motion capture data with light. The installation of lasers and phosphorescent objects transforms choreography that the original performer can no longer perform into a constant stream of lively spirits.	https://dl.acm.org/doi/abs/10.1145/3532837.3534950	Jayson Haebich, Eugenia S. Kim, Alvaro Cassinelli
Radiant soma	The human body has historically been a constant source of fascination in the arts and sciences. With the impact of the COVID-19 pandemic, the debate over the virtues and disadvantages of physical versus virtual bodies has increased dramatically. One of the most difficult attributes of the human body to translate into other forms is the essence of human movement and, by extension, energy. emphasizes the ephemerality of human movement by visualizing motion capture data with light. The installation of lasers and phosphorescent objects transforms choreography that the original performer can no longer perform into a constant stream of lively spirits.	https://dl.acm.org/doi/abs/10.1145/3532837.3534950	Jayson Haebich, Eugenia S. Kim, Alvaro Cassinelli
Radiant soma	The human body has historically been a constant source of fascination in the arts and sciences. With the impact of the COVID-19 pandemic, the debate over the virtues and disadvantages of physical versus virtual bodies has increased dramatically. One of the most difficult attributes of the human body to translate into other forms is the essence of human movement and, by extension, energy. emphasizes the ephemerality of human movement by visualizing motion capture data with light. The installation of lasers and phosphorescent objects transforms choreography that the original performer can no longer perform into a constant stream of lively spirits.	https://dl.acm.org/doi/abs/10.1145/3532837.3534950	Jayson Haebich, Eugenia S. Kim, Alvaro Cassinelli
Radiant soma	The human body has historically been a constant source of fascination in the arts and sciences. With the impact of the COVID-19 pandemic, the debate over the virtues and disadvantages of physical versus virtual bodies has increased dramatically. One of the most difficult attributes of the human body to translate into other forms is the essence of human movement and, by extension, energy. emphasizes the ephemerality of human movement by visualizing motion capture data with light. The installation of lasers and phosphorescent objects transforms choreography that the original performer can no longer perform into a constant stream of lively spirits.	https://dl.acm.org/doi/abs/10.1145/3532837.3534950	Jayson Haebich, Eugenia S. Kim, Alvaro Cassinelli
Radiant soma	The human body has historically been a constant source of fascination in the arts and sciences. With the impact of the COVID-19 pandemic, the debate over the virtues and disadvantages of physical versus virtual bodies has increased dramatically. One of the most difficult attributes of the human body to translate into other forms is the essence of human movement and, by extension, energy. emphasizes the ephemerality of human movement by visualizing motion capture data with light. The installation of lasers and phosphorescent objects transforms choreography that the original performer can no longer perform into a constant stream of lively spirits.	https://dl.acm.org/doi/abs/10.1145/3532837.3534950	Jayson Haebich, Eugenia S. Kim, Alvaro Cassinelli
Radiant soma	The human body has historically been a constant source of fascination in the arts and sciences. With the impact of the COVID-19 pandemic, the debate over the virtues and disadvantages of physical versus virtual bodies has increased dramatically. One of the most difficult attributes of the human body to translate into other forms is the essence of human movement and, by extension, energy. emphasizes the ephemerality of human movement by visualizing motion capture data with light. The installation of lasers and phosphorescent objects transforms choreography that the original performer can no longer perform into a constant stream of lively spirits.	https://dl.acm.org/doi/abs/10.1145/3532837.3534950	Jayson Haebich, Eugenia S. Kim, Alvaro Cassinelli
Random Walks for Adversarial Meshes	A polygonal mesh is the most-commonly used representation of surfaces in computer graphics. Therefore, it is not surprising that a number of mesh classification networks have recently been proposed. However, while adversarial attacks are wildly researched in 2D, the field of adversarial meshes is under explored. This paper proposes a novel, unified, and general adversarial attack, which leads to misclassification of several state-of-the-art mesh classification neural networks. Our attack approach is black-box, i.e. it has access only to the network's predictions, but not to the network's full architecture or gradients. The key idea is to train a network to imitate a given classification network. This is done by utilizing random walks along the mesh surface, which gather geometric information. These walks provide insight onto the regions of the mesh that are important for the correct prediction of the given classification network. These mesh regions are then modified more than other regions in order to attack the network in a manner that is barely visible to the naked eye.	https://dl.acm.org/doi/abs/10.1145/3528233.3530710	Amir Belder, Gal Yefet, Ran Ben-Itzhak, Ayellet Tal
Rapid Design of Articulated Objects: An Interactive Showcase	Designing articulated objects is challenging because, unlike with static objects, it requires complex decisions regarding the form, parts, rig, poses, and motion. We showcase a novel 3D sketching system for authoring concepts of articulated objects during the early stages of design, when such decisions are made. Designers can easily learn and use our system to produce compelling concepts rapidly, demonstrating that 3D sketching can bridge the gap between 2D sketching and 3D modeling, and be extended to designing articulated objects in films, animations, games, and products.	https://dl.acm.org/doi/abs/10.1145/3532721.3545165	Joon Hyub Lee, Hanbit Kim, Seok-Hyung Bae
Rapid design of articulated objects	Designing articulated objects is challenging because, unlike with static objects, it requires complex decisions to be made regarding the form, parts, rig, poses, and motion. We present a novel 3D sketching system for rapidly authoring concepts of articulated objects for the early stages of design, when designers make such decisions. Compared to existing CAD software, which focuses on slowly but elaborately producing models consisting of precise surfaces and volumes, our system focuses on quickly but roughly producing models consisting of key curves through a small set of coherent pen and multi-touch gestures. We found that professional designers could easily learn and use our system and author compelling concepts in a short time, showing that 3D sketching can be extended to designing articulated objects and is generally applicable in film, animation, game, and product design.	https://dl.acm.org/doi/abs/10.1145/3528223.3530092	Joon Hyub Lee, Hanbit Kim, Seok-Hyung Bae
ReLU Fields: The Little Non-linearity That Could	In many recent works, multi-layer perceptions (MLPs) have been shown to be suitable for modeling complex spatially-varying functions including images and 3D scenes. Although the MLPs are able to represent complex scenes with unprecedented quality and memory footprint, this expressive power of the MLPs, however, comes at the cost of long training and inference times. On the other hand, bilinear/trilinear interpolation on regular grid-based representations can give fast training and inference times, but cannot match the quality of MLPs without requiring significant additional memory. Hence, in this work, we investigate what is the smallest change to grid-based representations that allows for retaining the high fidelity result of MLPs while enabling fast reconstruction and rendering times. We introduce a surprisingly simple change that achieves this task – simply allowing a fixed non-linearity (ReLU) on interpolated grid values. When combined with coarse-to-fine optimization, we show that such an approach becomes competitive with the state-of-the-art. We report results on radiance fields, and occupancy fields, and compare against multiple existing alternatives. Code and data for the paper are available at https://geometry.cs.ucl.ac.uk/projects/2022/relu_fields.	https://dl.acm.org/doi/abs/10.1145/3528233.3530707	Animesh Karnewar, Tobias Ritschel, Oliver Wang, Niloy Mitra
ReQTable: Square tabletop display that provides dual-sided mid-air images to each of four users	"We propose ""ReQTable"", an optical system displaying dual-sided mid-air images to each of four users with less stray light. Dual Slit Mirror Arrays (dual SMAs) can produce a mid-air image that multiple people can view simultaneously without wearing special equipment. Dual SMAs are rectangular and can theoretically display mid-air images in the four surrounding directions. However, they also produce unwanted light, called stray light, which is especially noticeable when mid-air images are displayed in some directions. It is difficult for users to identify stray lights, and they cannot concentrate on viewing only the mid-air images. To improve a user experience, it is necessary to suppress stray light. ReQTable displays mid-air images in four directions with stray light reduction."	https://dl.acm.org/doi/abs/10.1145/3532721.3535563	Mizuki Takenawa, Tomoyo Kikuchi, Yuchi Yahagi, Shogo Fukushima, Takeshi Naemura
Real-time controllable motion transition for characters	Real-time in-between motion generation is universally required in games and highly desirable in existing animation pipelines. Its core challenge lies in the need to satisfy three critical conditions simultaneously: and , which renders any methods that need offline computation (or post-processing) or cannot incorporate (often unpredictable) user control undesirable. To this end, we propose a new real-time transition method to address the aforementioned challenges. Our approach consists of two key components: motion manifold and conditional transitioning. The former learns the important low-level motion features and their dynamics; while the latter synthesizes transitions conditioned on a target frame and the desired transition duration. We first learn a motion manifold that explicitly models the intrinsic transition stochasticity in human motions via a multi-modal mapping mechanism. Then, during generation, we design a transition model which is essentially a sampling strategy to sample from the learned manifold, based on the target frame and the aimed transition duration. We validate our method on different datasets in tasks where no post-processing or offline computation is allowed. Through exhaustive evaluation and comparison, we show that our method is able to generate motions measured under multiple metrics. Our method is also under various target frames (with extreme cases).	https://dl.acm.org/doi/abs/10.1145/3528223.3530090	Xiangjun Tang, He Wang, Bo Hu, Xu Gong, Ruifan Yi, Qilong Kou, Xiaogang Jin
Real-time lens distortion algorithm on embedded GPU systems	The lens distortion is essential for displaying VR contents on a head-mounted display (HMD) with a distorted display surface. We propose a novel lens distortion algorithm on an embedded GPU system. To minimize the memory access overhead, we propose a compressed form of a lookup table. We also utilize the integrated memory architecture of the edge GPU system (e.g., NVIDIA's Jetson devices) to reduce data communication overhead between host and device. As a result, our method shows up to 1.72-times higher performance than prior lookup table-based lens distortion approaches while it consumes up to 28.93% less power. Finally, our algorithm achieved real-time performance for high-resolution images on edge GPU systems (e.g., 94 FPS for 8K image on Jetson NX). These results demonstrate the benefits of our approach from the perspectives of both performance and energy.	https://dl.acm.org/doi/abs/10.1145/3532719.3543241	Young-Woo Kim, Duksu Kim
Reconstructing Translucent Objects using Differentiable Rendering	Inverse rendering is a powerful approach to modeling objects from photographs, and we extend previous techniques to handle translucent materials that exhibit subsurface scattering. Representing translucency using a heterogeneous bidirectional scattering-surface reflectance distribution function (BSSRDF), we extend the framework of path-space differentiable rendering to accommodate both surface and subsurface reflection. This introduces new types of paths requiring new methods for sampling moving discontinuities in material space that arise from visibility and moving geometry. We use this differentiable rendering method in an end-to-end approach that jointly recovers heterogeneous translucent materials (represented by a BSSRDF) and detailed geometry of an object (represented by a mesh) from a sparse set of measured 2D images in a coarse-to-fine framework incorporating Laplacian preconditioning for the geometry. To efficiently optimize our models in the presence of the Monte Carlo noise introduced by the BSSRDF integral, we introduce a dual-buffer method for evaluating the L2 image loss. This efficiently avoids potential bias in gradient estimation due to the correlation of estimates for image pixels and their derivatives and enables correct convergence of the optimizer even when using low sample counts in the renderer. We validate our derivatives by comparing against finite differences and demonstrate the effectiveness of our technique by comparing inverse-rendering performance with previous methods. We show superior reconstruction quality on a set of synthetic and real-world translucent objects as compared to previous methods that model only surface reflection.	https://dl.acm.org/doi/abs/10.1145/3528233.3530714	Xi Deng, Fujun Luan, Bruce Walter, Kavita Bala, Steve Marschner
Regression-based Monte Carlo integration	Monte Carlo integration is typically interpreted as an estimator of the expected value using stochastic samples. There exists an alternative interpretation in calculus where Monte Carlo integration can be seen as estimating a function---from the stochastic evaluations of the integrand---that integrates to the original integral. The integral mean value theorem states that this function should be the mean (or expectation) of the integrand. Since both interpretations result in the same estimator, little attention has been devoted to the calculus-oriented interpretation. We show that the calculus-oriented interpretation actually implies the possibility of using a more function than a one to construct a more efficient estimator for Monte Carlo integration. We build a new estimator based on this interpretation and relate our estimator to control variates with least-squares regression on the stochastic samples of the integrand. Unlike prior work, our resulting estimator is better than or equal to the conventional Monte Carlo estimator. To demonstrate the strength of our approach, we introduce a practical estimator that can act as a simple drop-in replacement for conventional Monte Carlo integration. We experimentally validate our framework on various light transport integrals. The code is available at https://github.com/iribis/regressionmc.	https://dl.acm.org/doi/abs/10.1145/3528223.3530095	Corentin Salaün, Adrien Gruson, Binh-Son Hua, Toshiya Hachisuka, Gurprit Singh
Rendering Iridescent Rock Dove Neck Feathers	Bird feathers exhibit fascinating reflectance, which is governed by fiber-like structures. Unlike hair and fur, the feather geometric structures follow intricate hierarchical patterns that span many orders of magnitude in scale. At the smallest scales, fiber elements have strongly non-cylindrical cross-sections and are often complemented by regular nanostructures, causing rich structural color. Therefore, past attempts to render feathers using fiber- or texture-based appearance models missed characteristic aspects of the visual appearance. We introduce a new feather modeling and rendering framework, which abstracts the microscopic geometry and reflectance into a microfacet-like BSDF. The R, TRT and T lobes, also known from hair and fur, here account for specular reflection off the cortex, diffuse reflection off the medulla, and transmission due to barbule spacing, respectively. Our BSDF, which does not require precomputation or storage, can be efficiently importance-sampled and readily integrated into rendering pipelines that represent feather geometry down to the barb level. We verify our approach using a BSDF-capturing setup for small biological structures, as well as against calibrated photographs of rock dove neck feathers.	https://dl.acm.org/doi/abs/10.1145/3528233.3530749	Weizhen Huang, Sebastian Merzbach, Clara Callenberg, Doekele Stavenga, Matthias Hullin
Rendering Neural Materials on Curved Surfaces	Neural material reflectance representations address some limitations of traditional analytic BRDFs with parameter textures; they can theoretically represent any material data, whether a complex synthetic microgeometry with displacements, shadows and inter-reflections, or real measured reflectance. However, they still approximate the material on an infinite plane, which prevents them from correctly handling silhouette and parallax effects for viewing directions close to grazing. The goal of this paper is to design a neural material representation capable of correctly handling such silhouette effects. We extend the neural network query to take surface curvature information as input, while the query output is extended to return a transparency value in addition to reflectance. We train the new neural representation on synthetic data that contains queries spanning a variety of surface curvatures. We show an ability to accurately represent complex silhouette behavior that would traditionally require more expensive and less flexible techniques, such as on-the-fly geometry displacement or ray marching.	https://dl.acm.org/doi/abs/10.1145/3528233.3530721	Alexandr Kuznetsov, Xuezheng Wang, Krishna Mullia, Fujun Luan, Zexiang Xu, Milos Hasan, Ravi Ramamoorthi
Rendering of Scratched Transparent Materials using Precomputed SV-BSDFs	Recently, many methods have been proposed to realistically render various materials. The realism of the synthetic images can be improved by rendering small-scale details on the surfaces of 3D objects. We focus on the efficient rendering of the scratches on the transparent objects. Although a fast rendering method using precomputed 2D BRDFs for a scratched material has been proposed, the method is limited to the opaque materials such as metals. We extend this method to the transparent objects. On the surface of the transparent object, rays are split into specular reflections and refractions. We therefore precompute Bidirectional Scattering Distribution Functions (BSDFs). We use a 2D ray tracer as in the previous method to accelerate the precomputation. We show several examples to demonstrate the effectiveness of our method.	https://dl.acm.org/doi/abs/10.1145/3532719.3543207	Shintaro Fuji, Syuhei Sato, Kei Iwasaki, Yoshinori Dobashi, Shangce Gao, Zheng Tang
Revamping the Cloth Tailoring Pipeline at Pixar	This work presents the most recent updates to the cloth tailoring pipeline at Pixar. We start by reviewing the evolution of cloth authoring tools used at Pixar from 2001 to the present day. Motivated by previous approaches, we introduce a structured workflow for cloth tailoring that manages multiple mesh versions concurrently. In our implementation, artists interact primarily with a low-resolution quad-dominant mesh, which defines the garment look as well as setups for rigging and simulation. Our system then converts this coarse input model into a triangulated mesh for simulation and a quadrangulated subdivision surface for rendering. To this end, we developed a new remeshing tool that outputs surface triangulations with adaptive resolution and conforming to edge constraints. We also devised procedural routines to generate render meshes by applying fold-over thickness, refining the mesh, and inserting seams. In addition, we introduced a suite of algorithms for transferring input attributes onto the derived meshes, including UV shells, face colors, crease edges, and vertex weights. Our revamped pipeline was deployed on Pixar's feature films Turning Red and Lightyear, producing hundreds of high-quality garment meshes.	https://dl.acm.org/doi/abs/10.1145/3532836.3536252	Christine Waggoner, Fernando de Goes
Rewriting geometric rules of a GAN	"Deep generative models make visual content creation more accessible to novice users by automating the synthesis of diverse, realistic content based on a collected dataset. However, the current machine learning approaches miss a key element of the creative process - the ability to synthesize things that go far beyond the data distribution and everyday experience. To begin to address this issue, we enable a user to ""warp"" a given model by editing just a handful of original model outputs with desired geometric changes. Our method applies a low-rank update to a single model layer to reconstruct edited examples. Furthermore, to combat overfitting, we propose a latent space augmentation method based on style-mixing. Our method allows a user to create a model that synthesizes endless objects with defined geometric changes, enabling the creation of a new generative model without the burden of curating a large-scale dataset. We also demonstrate that edited models can be composed to achieve aggregated effects, and we present an interactive interface to enable users to create new models through composition. Empirical measurements on multiple test cases suggest the advantage of our method against recent GAN fine-tuning methods. Finally, we showcase several applications using the edited models, including latent space interpolation and image editing."	https://dl.acm.org/doi/abs/10.1145/3528223.3530065	Sheng-Yu Wang, David Bau, Jun-Yan Zhu
Robust computation of implicit surface networks for piecewise linear functions	Implicit surface networks, such as arrangements of implicit surfaces and materials interfaces, are used for modeling piecewise smooth or partitioned shapes. However, accurate and numerically robust algorithms for discretizing either structure on a grid are still lacking. We present a unified approach for computing both types of surface networks for piecewise linear functions defined on a tetrahedral grid. Both algorithms are guaranteed to produce a correct combinatorial structure for any number of functions. Our main contribution is an exact and efficient method for partitioning a tetrahedron using the level sets of linear functions defined by barycentric interpolation. To further improve performance, we designed look-up tables to speed up processing of tetrahedra involving few functions and introduced an efficient algorithm for identifying nested 3D regions.	https://dl.acm.org/doi/abs/10.1145/3528223.3530176	Xingyi Du, Qingnan Zhou, Nathan Carr, Tao Ju
Rosita's Cocina: Fun and Engaging Virtual Cooking and Spanish Education: Fun and Engaging Virtual Cooking and Spanish Education	When removed from their home immigrants often find it hard to maintain their culture, one of the ways they often hang on to it is through food. Rosita's Cocina is designed to help kids learn about their family's history, language, and traditional food. Rosita's Cocina is an app designed to teach kids to cook in a digital environment while their parents or grandparents cook the real dish. The digital twins of the ingredients and tools will allow the child to experiment with cooking techniques traditional to Mexican cooking in a safe way, while using familiar Mexican recipes like flan or chiles rellenos in order to teach new words and enforce and master old vocab. As they learn more about their traditional cuisine and the history behind their favorite dishes, they will also practice their Spanish to be better able to communicate with their elders who may have a language barrier. Rosita's Cocina also helps engage children in the process of cooking using tools they are familiar with and excited to use, integrating technology into meaningful cultural experiences. The simple recipe tool allows elders to record the dishes they remember from their childhoods and pass it down to the younger generations in a fun and engaging way for the elder and the child. Rosita's Cocina was designed in AdobeXD as a prototype and visualization for the ultimate goals of the application.	https://dl.acm.org/doi/abs/10.1145/3532723.3535466	Charles Reitcheck, Estela Antunez
SNeRF: stylized neural implicit representations for 3D scenes	This paper presents a stylized novel view synthesis method. Applying state-of-the-art stylization methods to novel views frame by frame often causes jittering artifacts due to the lack of cross-view consistency. Therefore, this paper investigates 3D scene stylization that provides a strong inductive bias for consistent novel view synthesis. Specifically, we adopt the emerging neural radiance fields (NeRF) as our choice of 3D scene representation for their capability to render high-quality novel views for a variety of scenes. However, as rendering a novel view from a NeRF requires a large number of samples, training a stylized NeRF requires a large amount of GPU memory that goes beyond an off-the-shelf GPU capacity. We introduce a new training method to address this problem by alternating the NeRF and stylization optimization steps. Such a method enables us to make full use of our hardware memory capacity to both generate images at higher resolution and adopt more expressive image style transfer methods. Our experiments show that our method produces stylized NeRFs for a wide range of content, including indoor, outdoor and dynamic scenes, and synthesizes high-quality novel views with cross-view consistency.	https://dl.acm.org/doi/abs/10.1145/3528223.3530107	Thu Nguyen-Phuoc, Feng Liu, Lei Xiao
SPAGHETTI: editing implicit shapes through part aware generation	Neural implicit fields are quickly emerging as an attractive representation for learning based techniques. However, adopting them for 3D shape modeling and editing is challenging. We introduce a method for Editing Implicit Shapes Through Part Aware GeneraTion, permuted in short as SPAGHETTI. Our architecture allows for manipulation of implicit shapes by means of transforming, interpolating and combining shape segments together, without requiring explicit part supervision. SPAGHETTI disentangles shape part representation into extrinsic and intrinsic geometric information. This characteristic enables a generative framework with part-level control. The modeling capabilities of SPAGHETTI are demonstrated using an interactive graphical interface, where users can directly edit neural implicit shapes. Our code, editing user interface demo and pre-trained models are available at github.com/amirhertz/spaghetti.	https://dl.acm.org/doi/abs/10.1145/3528223.3530084	Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or
SPCBPT: subspace-based probabilistic connections for bidirectional path tracing	Bidirectional path tracing (BDPT) can be accelerated by selecting appropriate light sub-paths for connection. However, existing algorithms need to perform frequent distribution reconstruction and have expensive overhead. We present a novel approach, SPCBPT, for probabilistic connections that constructs the light selection distribution in sub-path space. Our approach bins the sub-paths into multiple subspaces and keeps the sub-paths in the same subspace of low discrepancy, wherein the light sub-paths can be selected by a subspace-based two-stage sampling method, i.e., first sampling the light subspace and then resampling the light sub-paths within this subspace. The subspace-based distribution is free of reconstruction and provides efficient light selection at a very low cost. We also propose a method that considers the Multiple Importance Sampling (MIS) term in the light selection and thus obtain an MIS-aware distribution that can minimize the upper bound of variance of the combined estimator. Prior methods typically omit this MIS weights term. We evaluate our algorithm using various benchmarks, and the results show that our approach has superior performance and can significantly reduce the noise compared with the state-of-the-art method.	https://dl.acm.org/doi/abs/10.1145/3528223.3530183	Fujia Su, Sheng Li, Guoping Wang
SPHERE: a Novel Approach to 3d and Active Sound Localization: A naturalistic approach to spatial hearing studies	In everyday life, localizing a sound source entails more than the sole extraction of auditory cues to define its three-dimensions: spatial hearing also takes into account the available visual information (e.g. cues to sound position) and resolves perceptual ambiguities through active listening behavior (e.g. exploring the auditory environment with head movements). We introduce a novel approach to sound localization in 3D named SPHERE which exploits a commercially available Virtual Reality Head-mounted display system with real- time kinematic tracking to combine all of these elements (controlled positioning of a real sound source, recording of participants' responses in 3D, controlled visual stimulations and active listening behavior). SPHERE allows accurate sampling of 3D spatial hearing abilities, and allows detection and quantification of the contribution of active listening.	https://dl.acm.org/doi/abs/10.1145/3532719.3543206	Valerie Gaveau, Aurélie Coudert, Romeo Salemme, Eric Koun, Clément Desoche, Eric Truy, Alessandro Farnè, Francesco Pavani
Samurai frog golf	A retired frog samurai wants nothing more than to be left alone and spend his remaining years in peace on the golf course. But when he unwillingly becomes the protector of a baby turtle he must draw his club for one more round.	https://dl.acm.org/doi/abs/10.1145/3512752.3527270	Brent Forrest, Mayumi Tachikawa
Sancho : The Cursed Conquistador In Jungle Cruise	In Jungle Cruise, Sancho is one of the cursed conquistadors whose partially-decomposed body is composed of honeycombs that the honeybees ride on and has honey dripping off all the time. His topology will change throughout the sequence and need to interact with surrounding environment, which creates unique visual and technical challenges. To achieve this we established a procedural workflow from modeling to rendering that is both efficient and art-directable.	https://dl.acm.org/doi/abs/10.1145/3532836.3536229	Pei-Zhi Huang, Adam Lee, Fanny Chan, Michael Wang
Scabo: XR Smart Cardboard Toy Kit	We provide a new eco-friendly solution to the toy industry, which includes both hardware and software integration: a cardboard robot kit and an AR platform. This solution offers users a D-I-Y experience of making a legged robot from scratch as well as interacting with the AR scenarios. We utilize both AI and XR technologies to digitalize the real environment and then enhance users' experiences.	https://dl.acm.org/doi/abs/10.1145/3532725.3535586	Bao Huy Nguyen, Minh Phuong Hoang, An Khang Vo, Minh Tuan Dam, Quoc Phu Huynh
Scalable neural indoor scene rendering	We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m . Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.	https://dl.acm.org/doi/abs/10.1145/3528223.3530153	Xiuchao Wu, Jiamin Xu, Zihan Zhu, Hujun Bao, Qixing Huang, James Tompkin, Weiwei Xu
Seeing through obstructions with diffractive cloaking	Unwanted camera obstruction can severely degrade captured images, including both scene occluders near the camera and partial occlusions of the camera cover glass. Such occlusions can cause catastrophic failures for various scene understanding tasks such as semantic segmentation, object detection, and depth estimation. Existing camera arrays capture multiple redundant views of a scene to see around thin occlusions. Such multi-camera systems effectively form a large synthetic aperture, which can suppress nearby occluders with a large defocus blur, but significantly increase the overall form factor of the imaging setup. In this work, we propose a imaging approach that obstructions by emulating a large array. Instead of relying on different camera views, we learn a diffractive optical element (DOE) that performs depth-dependent optical encoding, scattering nearby occlusions while allowing paraxial wavefronts to be focused. We computationally reconstruct unobstructed images from these superposed measurements with a neural network that is trained jointly with the optical layer of the proposed imaging system. We assess the proposed method in simulation and with an experimental prototype, validating that the proposed computational camera is capable of recovering occluded scene information in the presence of severe camera obstruction.	https://dl.acm.org/doi/abs/10.1145/3528223.3530185	Zheng Shi, Yuval Bahat, Seung-Hwan Baek, Qiang Fu, Hadi Amata, Xiao Li, Praneeth Chakravarthula, Wolfgang Heidrich, Felix Heide
Self-Conditioned GANs for Image Editing	Generative Adversarial Networks (GANs) are susceptible to bias, learned from either the unbalanced data, or through mode collapse. The networks focus on the core of the data distribution, leaving the tails — or the edges of the distribution — behind. We argue that this bias is responsible not only for fairness concerns, but that it plays a key role in the collapse of latent-traversal editing methods when deviating away from the distribution's core. Building on this observation, we outline a method for mitigating generative bias through a self-conditioning process, where distances in the latent-space of a pre-trained generator are used to provide initial labels for the data. By fine-tuning the generator on a re-sampled distribution drawn from these self-labeled data, we force the generator to better contend with rare semantic attributes and enable more realistic generation of these properties. We compare our models to a wide range of latent editing methods, and show that by alleviating the bias they achieve finer semantic control and better identity preservation through a wider range of transformations. Our code and models will be available at https://github.com/yzliu567/sc-gan	https://dl.acm.org/doi/abs/10.1145/3528233.3530698	Yunzhe Liu, Rinon Gal, Amit H. Bermano, Baoquan Chen, Daniel Cohen-Or
Self-Distilled StyleGAN: Towards Generation from Internet Photos	"StyleGAN is known to produce high-fidelity images, while also offering unprecedented semantic editing. However, these fascinating abilities have been demonstrated only on a limited set of datasets, which are usually structurally aligned and well curated. In this paper, we show how StyleGAN can be adapted to work on raw uncurated images collected from the Internet. Such image collections impose two main challenges to StyleGAN: they contain many outlier images, and are characterized by a multi-modal distribution. Training StyleGAN on such raw image collections results in degraded image synthesis quality. To meet these challenges, we proposed a StyleGAN-based self-distillation approach, which consists of two main components: (i) A generative-based self-filtering of the dataset to eliminate outlier images, in order to generate an adequate training set, and (ii) Perceptual clustering of the generated images to detect the inherent data modalities, which are then employed to improve StyleGAN's ""truncation trick"" in the image synthesis process. The presented technique enables the generation of high-quality images, while minimizing the loss in diversity of the data. Through qualitative and quantitative evaluation, we demonstrate the power of our approach to new challenging and diverse domains collected from the Internet. New datasets and pre-trained models are provided in our project website https://self-distilled-stylegan.github.io/."	https://dl.acm.org/doi/abs/10.1145/3528233.3530708	Ron Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar Mosseri, Tali Dekel, Daniel Cohen-Or, Michal Irani
Self-Intersection-Aware Deformation Transfer for Garment Simulation Meshes	Deformation Transfer is a well-known technique which copies deformations between two meshes to re-use animation. One drawback of existing methods is that they can introduce self-intersections on the resulting mesh, even when there is no self-intersections on the source mesh. We propose a novel deformation transfer technique that suppresses self-intersections on the resulting target mesh, called Self-Intersection-aware Deformation Transfer (SIDT). We demonstrate that SIDT helps to produce simulation friendly mesh for garment simulation.	https://dl.acm.org/doi/abs/10.1145/3532836.3536232	Akinobu Maejima, Akihiro Ozawa, Ken Anjyo, Tatsuo Yotsukura, Takehiro Tagawa
Self-Supervised Post-Correction for Monte Carlo Denoising	Using a network trained by a large dataset is becoming popular for denoising Monte Carlo rendering. Such a denoising approach based on supervised learning is currently considered the best approach in terms of quality. Nevertheless, this approach may fail when the image to be rendered (i.e., the test data) has very different characteristics than the images included in the training dataset. A pre-trained network may not properly denoise such an image since it is unseen data from a supervised learning perspective. To address this fundamental issue, we introduce a post-processing network that improves the performance of supervised learning denoisers. The key idea behind our approach is to train this post-processing network with self-supervised learning. In contrast to supervised learning, our self-supervised model does not need a reference image in its training process. We can thus use a noisy test image and self-correct the model on the fly to improve denoising performance. Our main contribution is a self-supervised loss that can guide the post-correction network to optimize its parameters without relying on the reference. Our work is the first to apply this self-supervised learning concept in denoising Monte Carlo rendered estimates. We demonstrate that our post-correction framework can boost supervised denoising via our self-supervised optimization. Our implementation is available at https://github.com/CGLab-GIST/self-supervised-post-corr.	https://dl.acm.org/doi/abs/10.1145/3528233.3530730	Jonghee Back, Binh-Son Hua, Toshiya Hachisuka, Bochang Moon
Semantically supervised appearance decomposition for virtual staging from a single panorama	We describe a novel approach to decompose a single panorama of an empty indoor environment into four appearance components: specular, direct sunlight, diffuse and diffuse ambient without direct sunlight. Our system is weakly supervised by automatically generated semantic maps (with floor, wall, ceiling, lamp, window and door labels) that have shown success on perspective views and are trained for panoramas using transfer learning without any further annotations. A GAN-based approach supervised by coarse information obtained from the semantic map extracts specular reflection and direct sunlight regions on the floor and walls. These lighting effects are removed via a similar GAN-based approach and a semantic-aware inpainting step. The appearance decomposition enables multiple applications including sun direction estimation, virtual furniture insertion, floor material replacement, and sun direction change, providing an effective tool for virtual home staging. We demonstrate the effectiveness of our approach on a large and recently released dataset of panoramas of empty homes.	https://dl.acm.org/doi/abs/10.1145/3528223.3530148	Tiancheng Zhi, Bowei Chen, Ivaylo Boyadzhiev, Sing Bing Kang, Martial Hebert, Srinivasa G. Narasimhan
Sense of Embodiment Inducement for People with Reduced Lower-body Mobility and Sensations with Partial-Visuomotor Stimulation	To induce the Sense of Embodiment (SoE) on the virtual 3D avatar during a Virtual Reality (VR) walking scenario, VR interfaces have employed the visuotactile or visuomotor approaches. However, people with reduced lower-body mobility and sensation (PRLMS) who are incapable of feeling or moving their legs would find this task extremely challenging. Here, we propose an upper-body motion tracking-based partial-visuomotor technique to induce SoE and positive feedback for PRLMS patients. We design partial-visuomotor stimulation consisting of two distinctive inputs (Button Control & Upper Motion tracking) and outputs (wheelchair motion & Gait Motion). The preliminary user study was conducted to explore subjective preference with qualitative feedback. From the qualitative study result, we observed the positive response on the partial-visuomotor regarding SoE in the asynchronous VR experience for PRLMS.	https://dl.acm.org/doi/abs/10.1145/3532721.3535568	HYUCKJIN JANG, TAEHEI KIM, SEOYOUNG OH, JEONGMI LEE, SUNGHEE LEE, SANG HO YOON
Severance: intro title sequence credits (Apple TV+)	An animated intro that illustrates the struggle of this main character navigating life split between professional and personal dilemmas.	https://dl.acm.org/doi/abs/10.1145/3512752.3528417	Oliver Latta
ShaderTransformer: Predicting Shader Quality via One-shot Embedding for Fast Simplification	Given specific scene configurations and target functions, automatic shader simplification searches for the best simplified shader variant from an optimization space with many candidates. Although various speedup methods have been proposed, there is still a costly render-and-evaluate process to obtain variant's performance and quality, especially when the scene changes. In this paper, we present a deep learning-based framework for predicting a shader's simplification space, where the shader's variants can be embedded into a metric space all at once for efficient quality evaluation. The novel framework allows the one-shot embedding of a space rather than a single instance. In addition, the simplification errors can be interpreted by mutual attention between shader fragments, presenting an informative focus-aware simplification framework that can assist experts in optimizing the codes. The results show that the new framework achieves significant speedup compared with existing search approaches. The focus-aware simplification framework reveals a new possibility of interpreting shaders for various applications.	https://dl.acm.org/doi/abs/10.1145/3528233.3530722	Yuchi Huo, Shi Li, Yazhen Yuan, Xu Chen, Rui Wang, Wenting Zheng, Hai Lin, Hujun Bao
Shape dithering for 3D printing	We present an efficient, purely geometric, algorithmic, and parameter free approach to improve surface quality and accuracy in voxel-controlled 3D printing by counteracting quantization artifacts. Such artifacts arise due to the discrete voxel sampling of the continuous shape used to control the 3D printer, and are characterized by low-frequency geometric patterns on surfaces of any orientation. They are visually disturbing, particularly on small prints or smooth surfaces, and adversely affect the fatigue behavior of printed parts. We use implicit shape dithering, displacing the part's signed distance field with a high-frequent signal whose amplitude is adapted to the (anisotropic) print resolution. We expand the reverse generalized Fourier slice theorem by shear transforms, which we leverage to optimize a 3D blue-noise mask to generate the anisotropic dither signal. As a point process it is efficient and does not adversely affect 3D halftoning. We evaluate our approach for efficiency, geometric accuracy and show its advantages over the state of the art.	https://dl.acm.org/doi/abs/10.1145/3528223.3530129	Mostafa Morsy Abdelkader Morsy, Alan Brunton, Philipp Urban
Shoot360: Normal View Video Creation from City Panorama Footage	We present Shoot360, a system that efficiently generates multi-shot normal view videos with desired content presentation and various cinematic styles, given a collection of 360 video recordings on different environments. The core of our system is a three-step decision process: 1) It firstly semantically analyzes the contents of interest from each panorama environment based on shot units, and produces a guidance that specifies the semantic focus and movement type of its output shot according to the user specification on content presentation and cinematic styles. 2) Based on the obtained guidance, it generates video candidates for each shot with shot-level control parameters for view projections following the filming rules. 3) The system further aggregates the projected normal view shots with the imposed local and global constraints, which incorporates the external knowledge learned from exemplar videos and professional filming rules. Extensive experiments verify the effectiveness of our system design, and we conclude with promising extensions for applying it to more generalized scenarios.	https://dl.acm.org/doi/abs/10.1145/3528233.3530702	Anyi Rao, Linning Xu, Dahua Lin
Simulation and optimization of magnetoelastic thin shells	Magnetoelastic thin shells exhibit great potential in realizing versatile functionalities through a broad range of combination of material stiffness, remnant magnetization intensity, and external magnetic stimuli. In this paper, we propose a novel computational method for forward simulation and inverse design of magnetoelastic thin shells. Our system consists of two key components of forward simulation and backward optimization. On the simulation side, we have developed a new continuum mechanics model based on the Kirchhoff-Love thin-shell model to characterize the behaviors of a megnetolelastic thin shell under external magnetic stimuli. Based on this model, we proposed an implicit numerical simulator facilitated by the magnetic energy Hessian to treat the elastic and magnetic stresses within a unified framework, which is versatile to incorporation with other thin shell models. On the optimization side, we have devised a new differentiable simulation framework equipped with an efficient adjoint formula to accommodate various PDE-constraint, inverse design problems of magnetoelastic thin-shell structures, in both static and dynamic settings. It also encompasses applications of magnetoelastic soft robots, functional Origami, artworks, and meta-material designs. We demonstrate the efficacy of our framework by designing and simulating a broad array of magnetoelastic thin-shell objects that manifest complicated interactions between magnetic fields, materials, and control policies.	https://dl.acm.org/doi/abs/10.1145/3528223.3530142	Xuwen Chen, Xingyu Ni, Bo Zhu, Bin Wang, Baoquan Chen
Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images	This paper deals with the challenging task of synthesizing novel views for in-the-wild photographs. Existing methods have shown promising results leveraging monocular depth estimation and color inpainting with layered depth representations. However, these methods still have limited capability to handle scenes with complex 3D geometry. We propose a new method based on the multiplane image (MPI) representation. To accommodate diverse scene layouts in the wild and tackle the difficulty in producing high-dimensional MPI contents, we design a network structure that consists of two novel modules, one for plane depth adjustment and another for depth-aware color prediction. The former adjusts the initial plane positions using the RGBD context feature and an attention mechanism. Given adjusted depth values, the latter predicts the color and density for each plane separately with proper inter-plane interactions achieved via a feature masking strategy. To train our method, we construct large-scale stereo training data using only unconstrained single-view image collections by a simple yet effective warp-back strategy. The experiments on both synthetic and real datasets demonstrate that our trained model works remarkably well and achieves state-of-the-art results.	https://dl.acm.org/doi/abs/10.1145/3528233.3530755	Yuxuan Han, Ruicheng Wang, Jiaolong Yang
Sketch2Pose: estimating a 3D character pose from a bitmap sketch	Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist's intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.	https://dl.acm.org/doi/abs/10.1145/3528223.3530106	Kirill Brodt, Mikhail Bessmeltsev
Solaris Speed Run: Transition to Solaris in An Hour	This hands-on class breaks down the steps that a small 30 person studio took to transition to SideFX Solaris. Solaris is a suite of look development, layout and lighting tools that empower artists to create USD based scene graphs that go from asset creation to final render. Solaris integrates with USD's HYDRA Imaging Framework for access to a wide range of renderers such as the new SideFX Karma,* Pixar RenderMan. Autodesk Arnold, Maxon Redshift, AMD ProRender and more. In this class we will simplifying Solaris, the pipeline, and USD concepts. We will discuss practical tips & tricks to guide how our small studio transitioned to Solaris as our daily driver. We will help you look at how you think about Solaris, what to build; and the practical steps getting a Solaris project done from start to finish.	https://dl.acm.org/doi/abs/10.1145/3532725.3545311	Andreas Weidman
Sound Scope Phone: Focusing Parts by Natural Movement	This paper describes Sound Scope Phone, an application that enables you to emphasize the part you want to listen to in a song consisting of multiple parts by head direction or hand gestures. The previously proposed interface required special headphones equipped with a digital compass and distance sensor to detect the direction of the head and distance between the head and a hand, respectively. Sound Scope Phone integrates face tracking information on the basis of images from the front camera of a commercially available smartphone with information from the built-in acceleration/gyro sensor to detect the head direction. The built application is published on the Apple App store under the name SoundScopePhone.	https://dl.acm.org/doi/abs/10.1145/3532723.3535465	Masatoshi Hamanaka
Space Rangers with Cornrows: Methods for Modeling Braids and Curls in Pixar’s Groom Pipeline	This presentation is a debrief of the processes and methods added to Pixar's groom pipeline to create the hairstyles of Lightyear characters Alisha and Izzy Hawthorne. The processes include novel ways of generating braids, curls, braid partitioning hairs (edge hairs), and graphic shapes populated with hair.	https://dl.acm.org/doi/abs/10.1145/3532836.3536277	Sofya Ogunseitan
Sparse ellipsometry: portable acquisition of polarimetric SVBRDF and shape with unstructured flash photography	Ellipsometry techniques allow to measure polarization information of materials, requiring precise rotations of optical components with different configurations of lights and sensors. This results in cumbersome capture devices, carefully calibrated in lab conditions, and in very long acquisition times, usually in the order of a few days per object. Recent techniques allow to capture polarimetric spatially-varying reflectance information, but limited to a single view, or to cover all view directions, but limited to spherical objects made of a single homogeneous material. We present , a portable polarimetric acquisition method that captures both polarimetric SVBRDF and 3D shape simultaneously. Our handheld device consists of off-the-shelf, fixed optical components. Instead of days, the total acquisition time varies between twenty and thirty minutes per object. We develop a complete polarimetric SVBRDF model that includes diffuse and specular components, as well as single scattering, and devise a novel polarimetric inverse rendering algorithm with data augmentation of specular reflection samples via generative modeling. Our results show a strong agreement with a recent ground-truth dataset of captured polarimetric BRDFs of real-world objects.	https://dl.acm.org/doi/abs/10.1145/3528223.3530075	Inseung Hwang, Daniel S. Jeon, Adolfo Muñoz, Diego Gutierrez, Xin Tong, Min H. Kim
Spatial Storytelling and Scientific Data Visualization	With misinformation spreading as quickly as the coronavirus itself, we saw an urgent need to illustrate complicated scientific concepts in a clear way to help readers assess risk and empower them to stay safe. We're going to dive into reporting/design/production and visualization of three pieces that were published recently.	https://dl.acm.org/doi/abs/10.1145/3532836.3536276	Karthik Patanjali, Yuliya Parshina-Kottas, Noah Pisner, Nick Bartzokas
Spelunking the deep: guaranteed queries on general neural implicit surfaces via range analysis	Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.	https://dl.acm.org/doi/abs/10.1145/3528223.3530155	Nicholas Sharp, Alec Jacobson
Stability-Aware Simplification of Curve Networks	Designing curve networks for fabrication requires simultaneous consideration of structural stability, cost effectiveness, and visual appeal—complex, interrelated objectives that make manual design a difficult and tedious task. We present a novel method for fabrication-aware simplification of curve networks, algorithmically selecting a stable subset of given 3D curves. While traditionally stability is measured as magnitude of deformation induced by a set of pre-defined loads, predicting applied forces for common day objects can be challenging. Instead, we directly optimize for minimal deformation under the worst-case load. Our technical contribution is a novel formulation of 3D curve network simplification for worst-case stability, leading to a mixed-integer semi-definite programming problem (MI-SDP). We show that while solving MI-SDP directly is infeasible, a physical insight suggests an efficient greedy approximation algorithm. We demonstrate the potential of our approach on a variety of curve network designs and validate its effectiveness compared to simpler alternatives using numerical experiments.	https://dl.acm.org/doi/abs/10.1145/3528233.3530711	William Neveu, Ivan Puhachov, Bernhard Thomaszewski, Mikhail Bessmeltsev
Star-Stuff: a way for the universe to know itself	"Inspired by Carl Sagan, Star-Stuff: a way for the universe to know itself is an immersive experience created to remind immersants of their fundamental connection to humanity and the universe. This hybrid VR artwork brings two people together in a surreal experience that can be shared with a remote stranger or a co-present friend. Their bodies are transformed into constellations surrounded by a myriad of orbiting stars whose lifetimes unfold before their eyes. By reframing the body in a shared aesthetic this unique experience encourages immersants to see themselves and others in a common light, as ""star-stuff"" brought to life, free of superficial characteristics that divide us."	https://dl.acm.org/doi/abs/10.1145/3532834.3536198	John Desnoyers-Stewart
Stereoscopic Transparent Display Visible with Naked Eye	In this study, we propose a stereoscopic transparent display that can be viewed with the naked eye. Existing methods for generating realistic high-quality stereoscopic images require wearable devices and the presence of a display, which degrades the sense of presence. Our method increases the sense of presence by making the stereoscopic images blend into the surrounding environment.	https://dl.acm.org/doi/abs/10.1145/3532719.3543205	Mari Shiina, Naoki Hashimoto
Stroke Transfer: Example-based Synthesis of Animatable Stroke Styles	We present stroke transfer, an example-based synthesis method of brushstrokes for animated scenes under changes in viewpoint, lighting conditions, and object shapes. We introduce stroke field for guiding the generation of strokes, consisting of spatially varying attributes of strokes, namely, their orientations, lengths, widths, and colors. Strokes are synthesized as the integral curves of the stroke field. In essence, we separate elements that constitute the artistic stroke into style-specific transferable elements and instance-intrinsic ones. To generate the stroke field, we first compute a set of vector fields that reflect the instance-intrinsic elements and then combine them using style-specific weight functions learned from exemplars, with the weights computed in a proxy feature space shared among a variety of objects. The rendered animation using our method captures time-varying viewpoint, lighting conditions, and object shapes, as well as the artistic style given in the form of exemplars.	https://dl.acm.org/doi/abs/10.1145/3528233.3530703	Hideki Todo, Kunihiko Kobayashi, Jin Katsuragi, Haruna Shimotahira, Shizuo Kaji, Yonghao Yue
StyleGAN-NADA: CLIP-guided domain adaptation of image generators	"Can a generative model be trained to produce images from a specific domain, guided only by a text prompt, without seeing any image? In other words: can an image generator be trained ""blindly""? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or infeasible to reach with existing methods. We conduct an extensive set of experiments across a wide range of domains. These demonstrate the effectiveness of our approach, and show that our models preserve the latent-space structure that makes generative models appealing for downstream tasks. Code and videos available at: stylegan-nada.github.io/"	https://dl.acm.org/doi/abs/10.1145/3528223.3530164	Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or
StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets	Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN's performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of 10242 at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object classes. Code, models, and supplementary videos can be found at https://sites.google.com/view/stylegan-xl/ .	https://dl.acm.org/doi/abs/10.1145/3528233.3530738	Axel Sauer, Katja Schwarz, Andreas Geiger
Symmetry-driven 3D Reconstruction from Concept Sketches	Concept sketches, ubiquitously used in industrial design, are inherently imprecise yet highly effective at communicating 3D shape to human observers. We present a new symmetry-driven algorithm for recovering designer-intended 3D geometry from concept sketches. We observe that most concept sketches of human-made shapes are structured around locally symmetric building blocks, defined by triplets of orthogonal symmetry planes. We identify potential building blocks using a combination of 2D symmetries and drawing order. We reconstruct each such building block by leveraging a combination of perceptual cues and observations about designer drawing choices. We cast this reconstruction as an integer programming problem where we seek to identify, among the large set of candidate symmetry correspondences formed by approximate pen strokes, the subset that results in the most symmetric and well-connected shape. We demonstrate the robustness of our approach by reconstructing 82 sketches, which exhibit significant over-sketching, inaccurate perspective, partial symmetry, and other imperfections. In a comparative study, participants judged our results as superior to the state-of-the-art by a ratio of 2:1.	https://dl.acm.org/doi/abs/10.1145/3528233.3530723	Felix Hähnlein, Yulia Gryaditskaya, Alla Sheffer, Adrien Bousseau
Sympathetic wear	In situations where people must maintain physical distance from one another and rely on communication through digital screens, we sometimes feel a sense of absence and loneliness. is artwork that supplements communication through digital displays and considers the person on the other side of the network. When we are sad or in pain, the action of having our backs rubbed can provide comfort. Adopting the back as our theme, brings gentle healing to people's minds and bodies by creating a soft tactile sensation on the back that is invisible on screen.	https://dl.acm.org/doi/abs/10.1145/3532837.3534955	Junichi Kanebako, Naoya Watabe, Miki Yamamura, Haruki Nakamura, Keisuke Shuto, Hiroko Uchiyama
Sympathetic wear	In situations where people must maintain physical distance from one another and rely on communication through digital screens, we sometimes feel a sense of absence and loneliness. is artwork that supplements communication through digital displays and considers the person on the other side of the network. When we are sad or in pain, the action of having our backs rubbed can provide comfort. Adopting the back as our theme, brings gentle healing to people's minds and bodies by creating a soft tactile sensation on the back that is invisible on screen.	https://dl.acm.org/doi/abs/10.1145/3532837.3534955	Junichi Kanebako, Naoya Watabe, Miki Yamamura, Haruki Nakamura, Keisuke Shuto, Hiroko Uchiyama
Sympathetic wear	In situations where people must maintain physical distance from one another and rely on communication through digital screens, we sometimes feel a sense of absence and loneliness. is artwork that supplements communication through digital displays and considers the person on the other side of the network. When we are sad or in pain, the action of having our backs rubbed can provide comfort. Adopting the back as our theme, brings gentle healing to people's minds and bodies by creating a soft tactile sensation on the back that is invisible on screen.	https://dl.acm.org/doi/abs/10.1145/3532837.3534955	Junichi Kanebako, Naoya Watabe, Miki Yamamura, Haruki Nakamura, Keisuke Shuto, Hiroko Uchiyama
Sympathetic wear	In situations where people must maintain physical distance from one another and rely on communication through digital screens, we sometimes feel a sense of absence and loneliness. is artwork that supplements communication through digital displays and considers the person on the other side of the network. When we are sad or in pain, the action of having our backs rubbed can provide comfort. Adopting the back as our theme, brings gentle healing to people's minds and bodies by creating a soft tactile sensation on the back that is invisible on screen.	https://dl.acm.org/doi/abs/10.1145/3532837.3534955	Junichi Kanebako, Naoya Watabe, Miki Yamamura, Haruki Nakamura, Keisuke Shuto, Hiroko Uchiyama
Sympathetic wear	In situations where people must maintain physical distance from one another and rely on communication through digital screens, we sometimes feel a sense of absence and loneliness. is artwork that supplements communication through digital displays and considers the person on the other side of the network. When we are sad or in pain, the action of having our backs rubbed can provide comfort. Adopting the back as our theme, brings gentle healing to people's minds and bodies by creating a soft tactile sensation on the back that is invisible on screen.	https://dl.acm.org/doi/abs/10.1145/3532837.3534955	Junichi Kanebako, Naoya Watabe, Miki Yamamura, Haruki Nakamura, Keisuke Shuto, Hiroko Uchiyama
Sympathetic wear	In situations where people must maintain physical distance from one another and rely on communication through digital screens, we sometimes feel a sense of absence and loneliness. is artwork that supplements communication through digital displays and considers the person on the other side of the network. When we are sad or in pain, the action of having our backs rubbed can provide comfort. Adopting the back as our theme, brings gentle healing to people's minds and bodies by creating a soft tactile sensation on the back that is invisible on screen.	https://dl.acm.org/doi/abs/10.1145/3532837.3534955	Junichi Kanebako, Naoya Watabe, Miki Yamamura, Haruki Nakamura, Keisuke Shuto, Hiroko Uchiyama
Sympathetic wear	In situations where people must maintain physical distance from one another and rely on communication through digital screens, we sometimes feel a sense of absence and loneliness. is artwork that supplements communication through digital displays and considers the person on the other side of the network. When we are sad or in pain, the action of having our backs rubbed can provide comfort. Adopting the back as our theme, brings gentle healing to people's minds and bodies by creating a soft tactile sensation on the back that is invisible on screen.	https://dl.acm.org/doi/abs/10.1145/3532837.3534955	Junichi Kanebako, Naoya Watabe, Miki Yamamura, Haruki Nakamura, Keisuke Shuto, Hiroko Uchiyama
Sympathetic wear	In situations where people must maintain physical distance from one another and rely on communication through digital screens, we sometimes feel a sense of absence and loneliness. is artwork that supplements communication through digital displays and considers the person on the other side of the network. When we are sad or in pain, the action of having our backs rubbed can provide comfort. Adopting the back as our theme, brings gentle healing to people's minds and bodies by creating a soft tactile sensation on the back that is invisible on screen.	https://dl.acm.org/doi/abs/10.1145/3532837.3534955	Junichi Kanebako, Naoya Watabe, Miki Yamamura, Haruki Nakamura, Keisuke Shuto, Hiroko Uchiyama
Synchronized Hand Difference Visualization for Piano Learning	When learning a dexterous skill such as playing the piano, people commonly watch videos of a teacher. However, this conventional way has some downsides such as limited information to be retrieved and less intuitive instructions. We propose a virtual training system by visualizing differences between hands to provide intuitive feedback for skill acquisition. After synchronizing the data, two visual cues are proposed including a hand-overlay manner and a two-keyboards visualization. A pilot study confirm the superiority of the proposed methods over conventional video-viewing.	https://dl.acm.org/doi/abs/10.1145/3532719.3543196	Ruofan Liu, Erwin Wu, Chen-Chieh Liao, Hayato Nishioka, Shinichi Furuya, Hideki Koike
Tell me a story	Every evening, Solen tells a fantastic story in which she's the main character to her little sister Luna. However, Luna's far from guessing the secret hidden between the lines...	https://dl.acm.org/doi/abs/10.1145/3512752.3519925	Sara Arzalier, Albane Brenci, Lucie Daniel, Sacha Duru, Anaëlle Gil, Alice Jacob, Laura Munos, Inès Tchirikhtchian
Text2Human: text-driven controllable human image generation	Generating high-quality and diverse human images is an important yet challenging task in vision and graphics. However, existing generative models often fall short under the high diversity of clothing shapes and textures. Furthermore, the generation process is even desired to be intuitively controllable for layman users. In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation. We synthesize full-body human images starting from a given human pose with two dedicated steps. 1) With some texts describing the shapes of clothes, the given human pose is first translated to a human parsing map. 2) The final human image is then generated by providing the system with more attributes about the textures of clothes. Specifically, to model the diversity of clothing textures, we build a hierarchical texture-aware codebook that stores multi-scale neural representations for each type of texture. The codebook at the coarse level includes the structural representations of textures, while the codebook at the fine level focuses on the details of textures. To make use of the learned hierarchical codebook to synthesize desired images, a diffusion-based transformer sampler with mixture of experts is firstly employed to sample indices from the coarsest level of the codebook, which then is used to predict the indices of the codebook at finer levels. The predicted indices at different levels are translated to human images by the decoder learned accompanied with hierarchical codebooks. The use of mixture-of-experts allows for the generated image conditioned on the fine-grained text input. The prediction for finer level indices refines the quality of clothing textures. Extensive quantitative and qualitative evaluations demonstrate that our proposed Text2Human framework can generate more diverse and realistic human images compared to state-of-the-art methods. Our project page is https://yumingj.github.io/projects/Text2Human.html. Code and pretrained models are available at https://github.com/yumingj/Text2Human.	https://dl.acm.org/doi/abs/10.1145/3528223.3530104	Yuming Jiang, Shuai Yang, Haonan Qju, Wayne Wu, Chen Change Loy, Ziwei Liu
The Accursed Share of Non-fungibles: What NFTs and Blockchains Suggest for Arts Practice, Pedagogy, and the General Economy	As described by Georges Bataille in his 1949 essays on General Economy, the Accursed Share refers to the excessive and non-recuperable portions of an economy that must be spent lavishly or luxuriously, or otherwise be destined to outrageous, destructive, and catastrophic expressions — most often resulting in war, or other ruinous and destructive acts [Baitaille and Hurley 1988]. How might we might consider Bataille's idea as expressed through the recent boom in Non-Fungible Tokens (NFTs) and contemporary Blockchain technology? Between 2020 and 2021, the global trading volume of NFTs increased ten-fold [Nadini et al. 2021], with a reported global market of 22 billion USD in 2021 [Milmo 2021]. Researchers Matthieu Nadini (et al.) note that digital art sales accounted for roughly 10 percent of all NFT transactions in 2021 [Nadini et al. 2021]. The volume and frequency of the booming NFT market cannot be ignored, but what does it suggest for the future of art practice and pedagogy, and how does it relate to the excesses suggested in Bataille's notion of general economy? A panel of artists and NFT pioneers demystify NFTs, discussing the virtues and vices of such radical shifts in art-commerce. Panelists expose the process of making and selling NFTs, the ecological and pedagogical impacts of blockchain technologies, and speculate on future outlooks for digital art practice. Discussion provides critical reflection on the philosophical and cultural implications that accompany shifts into evermore virtualized art-experiences. Questions considered include what import does Bataille's seminal text bear in relation to contemporary digital art practice; what are the tangible cultural and ecological effects of digital art practice; and what insights does Bataille's notion of general economy hold for the experience of contemporary art and future art practice. Discussion will expose fresh outlooks on possible futures of art practice and pedagogy, as related to the fields of computer graphics and interactive techniques.	https://dl.acm.org/doi/abs/10.1145/3532724.3535616	Johannes DeYoung
The Art of Cloth simulation on The Boss Baby : Family Business	DreamWorks Animation's The Boss Baby : Family Business has a strong design language. Clothing is an essential part of characterization we needed to maintain strong graphic silhouettes on garments. The characters love changing clothes pretty frequently. Additional Challenges included dream sequences, animated scaling and downsizing of characters, with an extensive catalog and large crowds wearing multiple layered winter clothes. This talk illustrates various techniques used to tackle those and the use of animation-driven physics based garment simulation to emphasize the art of dressing an animated feature.	https://dl.acm.org/doi/abs/10.1145/3532836.3536251	Mathias Rodriguez, Pradipto Sengupta
The Hyper Drumhead: A Musical Instrument For The Audio/Visual Manipulation Of Sound Waves	The Hyper Drumhead is a novel digital musical instrument that allows for the visualization and the manipulation of sound waves. At its core, a GPU-accelerated physical model computes in real-time the propagation of sound waves in two dimensions, allowing for the audio/visual simulation of massive domains. Every time the musician touches the surface of the instrument, an excitation signal is injected into the simulation domain, triggering wave propagation in all directions. By drawing boundaries in the domain and modifying the acoustic parameters of the simulated medium, the musician may generate and modulate reflections and resonances, effectively shaping the timbre of the resulting sound. In this short paper, we describe the main components of the instrument, including the control interface and the underlying numerical simulation.	https://dl.acm.org/doi/abs/10.1145/3532833.3538686	Victor Zappi, Mike Frengel, Andrew Stewart Allen, Sidney Fels
The Seine's tears	"17 october 1961, ""Algerian workers"" get down the streets to manifest against the mandatory curfew imposed by the Police prefecture."	https://dl.acm.org/doi/abs/10.1145/3512752.3517798	Yanis Belaid, Eliott Benard, Nicolas Mayeur, Etienne Moulin, Hadrien Pinot, Lisa Vicente, Philippine Singer, Alice Letailleur
The VFX of dune: part 1	DNEG was thrilled to reunite with director Denis Villeneuve to help deliver this remarkable science fiction epic. Led by Production VFX Supervisor Paul Lambert, alongside DNEG VFX Supervisors Tristan Myles and Brian Connor, our team contributed to 28 sequences and nearly 1,200 VFX shots of the film's 1,700 total. Collaborating closely with the special effects (SFX) team on-set, DNEG's visual effects crew ensured that all post-production work remained as believable as possible. Each VFX element was designed to heighten the photorealism of Villeneuve's immersive vision. Massive and meticulously detailed FX simulations, sweeping environments, full-CG vehicles and creatures all helped to bring Dune's dystopian atmosphere to life on-screen.	https://dl.acm.org/doi/abs/10.1145/3512752.3527785	Dneg Vfx
The bad guys show and tell reel	Breakdowns of techniques used to create the graphic style of The Bad Guys.	https://dl.acm.org/doi/abs/10.1145/3512752.3528136	Matt Baer, Andrew Pearce
The end of war	Filmed with hundreds of frame-by-frame sculpted CG models, this Anti-war short film is about the nature of War, inspired by Kollwitz's art and her life.	https://dl.acm.org/doi/abs/10.1145/3512752.3527994	Lei Chen
The outer worlds 2: official reveal trailer	A game trailer that is a tongue-in-check spoof of a game trailer.	https://dl.acm.org/doi/abs/10.1145/3512752.3528037	Nils Lagergren
The power particle-in-cell method	This paper introduces a new weighting scheme for particle-grid transfers that generates hybrid Lagrangian/Eulerian fluid simulations with uniform particle distributions and precise volume control. At its core, our approach reformulates the construction of Power Particles [de Goes et al. 2015] by computing volume-constrained density kernels. We employ these optimized kernels as particle domains within the Generalized Interpolation Material Point method (GIMP) in order to incorporate Power Particles into the Particle-In-Cell framework, hence the name the method. We address the construction of volume-constrained density kernels as a regularized optimal transportation problem and describe an iterative solver based on localized Gaussian convolutions that leads to a significant performance speedup compared to [de Goes et al. 2015]. We also present novel extensions for handling free surfaces and solid obstacles that bypass the need for cell clipping and ghost particles. We demonstrate the advantages of our transfer weights by improving hybrid schemes for fluid simulation such as the Fluid Implicit Particle (FLIP) method and the Affine Particle-In-Cell (APIC) method with volume preservation and robustness to varying particle-per-cell ratio, while retaining low numerical dissipation, conserving linear and angular momenta, and avoiding particle reseeding or post-process relaxations.	https://dl.acm.org/doi/abs/10.1145/3528223.3530066	Ziyin Qu, Minchen Li, Fernando De Goes, Chenfanfu Jiang
The soloists	In a small village ruled by ridiculous laws, three singing sisters and their dog rehearse for the Annual Autumn Festival. But an unexpected event will disrupt their plans.	https://dl.acm.org/doi/abs/10.1145/3512752.3528426	Mehrnaz Abdollahinia, Feben Elias Woldehawariat, Razahk Issaka, Celeste Jamneck, Yi Liu
There and Back Again: Graphics Education with a View Towards the Future	Teaching computer graphics has always been a juggling act between teaching fundamental techniques and expressing those techniques through graphics APIs. This talk surveys different teaching options over time, including our techniques expressed in our widely-used textbook. We consider the relevance of teaching graphics APIs given their evolution, and present a proposal for evolving teaching graphics.	https://dl.acm.org/doi/abs/10.1145/3532724.3535594	Dave Shreiner, Edward Angel
ThermoBlinds: Non-Contact, Highly Responsive Thermal Feedback for Thermal Interaction	We present ThermoBlinds, a non-contact, highly responsive thermal feedback device for thermal interactions. It provides responsive feedback by rapidly adjusting infrared irradiance with a shutter mechanism. The feedback can be applied according to visual media contents to reproduce thermal experiences. By providing thermal feedback in accordance with the user's gaze, it is possible to create a visual media experience with a high resolution thermal sensation. Additionally, it supports remote communication by providing thermal feedback based on the other person's gaze.	https://dl.acm.org/doi/abs/10.1145/3532721.3535569	Sosuke Ichihashi, Arata Horie, Masaharu Hirose, Zendai Kashino, Shigeo Yoshida, Sohei Wakisaka, Masahiko Inami
Thin films in production with extended anisotropic kernels	Particles offer a convenient way of modeling fluid simulations, but ultimately the goal in photorealistic VFX is to extract a surface representing the fluid which looks true-to-life. A characteristic of dynamic fluid simulations is thin fluid sheets which are challenging to extract a surface from. Bumpy artifacts are frequently exposed in these features. Anisotropic smoothing kernels [Yu and Turk 2013] are a method to combat this, but they also present their own usability challenges. Similar ideas have been used before in the film industry [Museth et al. 2007]. This paper extends the anisotropic kernels technique to provide an automatic scaling correction feature so artists may work in normalized terms. We also use a metaball implicit function over smoothed-particle hydrodynamics (SPH) kernels to achieve smooth surfaces. With these extensions, artists can quickly create high quality surfaces with fewer particles.	https://dl.acm.org/doi/abs/10.1145/3532836.3536264	Paul Kilgo, Jamie Pilgrim, Parag Havaldar, Ashraf Ghoniem
Three stage drawing transfer: collaborative drawing between a generative adversarial network, co-robotic arm, and five-year-old child	Three Stage Drawing Transfer is an experimental human-robot performance and emerging media arts research project exploring drawing, cognition, and new possibilities for creative, embodied interactions between humans and machines. The work creates a visual-mental-physical circuit between a Generative Adversarial Network (GAN), a co-robotic arm, and a five-year-old child. Building on traditions from experimental performance and collaborative drawing, it extends these with the emergent capabilities of generative neural networks and robotic automation. The live child-robot interaction occurs within the space of a shared sheet of paper, where they together develop a collaborative drawing guided by the GAN's understanding and the child's imagination, resulting in a drawing as artifact and video as documentation.	https://dl.acm.org/doi/abs/10.1145/3532837.3534954	Robert Twomey
Three stage drawing transfer: collaborative drawing between a generative adversarial network, co-robotic arm, and five-year-old child	Three Stage Drawing Transfer is an experimental human-robot performance and emerging media arts research project exploring drawing, cognition, and new possibilities for creative, embodied interactions between humans and machines. The work creates a visual-mental-physical circuit between a Generative Adversarial Network (GAN), a co-robotic arm, and a five-year-old child. Building on traditions from experimental performance and collaborative drawing, it extends these with the emergent capabilities of generative neural networks and robotic automation. The live child-robot interaction occurs within the space of a shared sheet of paper, where they together develop a collaborative drawing guided by the GAN's understanding and the child's imagination, resulting in a drawing as artifact and video as documentation.	https://dl.acm.org/doi/abs/10.1145/3532837.3534954	Robert Twomey
Three stage drawing transfer: collaborative drawing between a generative adversarial network, co-robotic arm, and five-year-old child	Three Stage Drawing Transfer is an experimental human-robot performance and emerging media arts research project exploring drawing, cognition, and new possibilities for creative, embodied interactions between humans and machines. The work creates a visual-mental-physical circuit between a Generative Adversarial Network (GAN), a co-robotic arm, and a five-year-old child. Building on traditions from experimental performance and collaborative drawing, it extends these with the emergent capabilities of generative neural networks and robotic automation. The live child-robot interaction occurs within the space of a shared sheet of paper, where they together develop a collaborative drawing guided by the GAN's understanding and the child's imagination, resulting in a drawing as artifact and video as documentation.	https://dl.acm.org/doi/abs/10.1145/3532837.3534954	Robert Twomey
Three stage drawing transfer: collaborative drawing between a generative adversarial network, co-robotic arm, and five-year-old child	Three Stage Drawing Transfer is an experimental human-robot performance and emerging media arts research project exploring drawing, cognition, and new possibilities for creative, embodied interactions between humans and machines. The work creates a visual-mental-physical circuit between a Generative Adversarial Network (GAN), a co-robotic arm, and a five-year-old child. Building on traditions from experimental performance and collaborative drawing, it extends these with the emergent capabilities of generative neural networks and robotic automation. The live child-robot interaction occurs within the space of a shared sheet of paper, where they together develop a collaborative drawing guided by the GAN's understanding and the child's imagination, resulting in a drawing as artifact and video as documentation.	https://dl.acm.org/doi/abs/10.1145/3532837.3534954	Robert Twomey
Three stage drawing transfer: collaborative drawing between a generative adversarial network, co-robotic arm, and five-year-old child	Three Stage Drawing Transfer is an experimental human-robot performance and emerging media arts research project exploring drawing, cognition, and new possibilities for creative, embodied interactions between humans and machines. The work creates a visual-mental-physical circuit between a Generative Adversarial Network (GAN), a co-robotic arm, and a five-year-old child. Building on traditions from experimental performance and collaborative drawing, it extends these with the emergent capabilities of generative neural networks and robotic automation. The live child-robot interaction occurs within the space of a shared sheet of paper, where they together develop a collaborative drawing guided by the GAN's understanding and the child's imagination, resulting in a drawing as artifact and video as documentation.	https://dl.acm.org/doi/abs/10.1145/3532837.3534954	Robert Twomey
Three stage drawing transfer: collaborative drawing between a generative adversarial network, co-robotic arm, and five-year-old child	Three Stage Drawing Transfer is an experimental human-robot performance and emerging media arts research project exploring drawing, cognition, and new possibilities for creative, embodied interactions between humans and machines. The work creates a visual-mental-physical circuit between a Generative Adversarial Network (GAN), a co-robotic arm, and a five-year-old child. Building on traditions from experimental performance and collaborative drawing, it extends these with the emergent capabilities of generative neural networks and robotic automation. The live child-robot interaction occurs within the space of a shared sheet of paper, where they together develop a collaborative drawing guided by the GAN's understanding and the child's imagination, resulting in a drawing as artifact and video as documentation.	https://dl.acm.org/doi/abs/10.1145/3532837.3534954	Robert Twomey
Three stage drawing transfer: collaborative drawing between a generative adversarial network, co-robotic arm, and five-year-old child	Three Stage Drawing Transfer is an experimental human-robot performance and emerging media arts research project exploring drawing, cognition, and new possibilities for creative, embodied interactions between humans and machines. The work creates a visual-mental-physical circuit between a Generative Adversarial Network (GAN), a co-robotic arm, and a five-year-old child. Building on traditions from experimental performance and collaborative drawing, it extends these with the emergent capabilities of generative neural networks and robotic automation. The live child-robot interaction occurs within the space of a shared sheet of paper, where they together develop a collaborative drawing guided by the GAN's understanding and the child's imagination, resulting in a drawing as artifact and video as documentation.	https://dl.acm.org/doi/abs/10.1145/3532837.3534954	Robert Twomey
Three stage drawing transfer: collaborative drawing between a generative adversarial network, co-robotic arm, and five-year-old child	Three Stage Drawing Transfer is an experimental human-robot performance and emerging media arts research project exploring drawing, cognition, and new possibilities for creative, embodied interactions between humans and machines. The work creates a visual-mental-physical circuit between a Generative Adversarial Network (GAN), a co-robotic arm, and a five-year-old child. Building on traditions from experimental performance and collaborative drawing, it extends these with the emergent capabilities of generative neural networks and robotic automation. The live child-robot interaction occurs within the space of a shared sheet of paper, where they together develop a collaborative drawing guided by the GAN's understanding and the child's imagination, resulting in a drawing as artifact and video as documentation.	https://dl.acm.org/doi/abs/10.1145/3532837.3534954	Robert Twomey
Time-multiplexed Neural Holography: A Flexible Framework for Holographic Near-eye Displays with Fast Heavily-quantized Spatial Light Modulators	Holographic near-eye displays offer unprecedented capabilities for virtual and augmented reality systems, including perceptually important focus cues. Although artificial intelligence–driven algorithms for computer-generated holography (CGH) have recently made much progress in improving the image quality and synthesis efficiency of holograms, these algorithms are not directly applicable to emerging phase-only spatial light modulators (SLM) that are extremely fast but offer phase control with very limited precision. The speed of these SLMs offers time multiplexing capabilities, essentially enabling partially-coherent holographic display modes. Here we report advances in camera-calibrated wave propagation models for these types of holographic near-eye displays and we develop a CGH framework that robustly optimizes the heavily quantized phase patterns of fast SLMs. Our framework is flexible in supporting runtime supervision with different types of content, including 2D and 2.5D RGBD images, 3D focal stacks, and 4D light fields. Using our framework, we demonstrate state-of-the-art results for all of these scenarios in simulation and experiment.	https://dl.acm.org/doi/abs/10.1145/3528233.3530734	Suyeon Choi, Manu Gopakumar, Yifan Peng, Jonghyun Kim, Matthew O'Toole, Gordon Wetzstein
TopoCut: fast and robust planar cutting of arbitrary domains	"Given a complex three-dimensional domain delimited by a closed and non-degenerate input triangle mesh without any self-intersection, a common geometry processing task consists in cutting up the domain into cells through a set of planar cuts, creating a ""cut-cell mesh"", i.e., a volumetric decomposition of the domain amenable to visualization (e.g., exploded views), animation (e.g., virtual surgery), or simulation (finite volume computations). A large number of methods have proposed either or solutions, sometimes restricting the cuts to form a regular or adaptive grid for simplicity; yet, none can guarantee properties, severely limiting their usefulness in practice. At the core of the difficulty is the determination of topological relationships among large numbers of vertices, edges, faces and cells in order to assemble a proper cut-cell mesh: while exact geometric computations provide a robust solution to this issue, their high computational cost has prompted a number of faster solutions based on, e.g., local floating-point angle sorting to significantly accelerate the process --- but losing robustness in doing so. In this paper, we introduce a new approach to planar cutting of 3D domains that substitutes topological inference for numerical ordering through a novel mesh data structure, and revert to exact numerical evaluations only in the few rare cases where it is strictly necessary. We show that our novel concept of exploits the inherent structure of cut-cell mesh generation to save computational time while still guaranteeing exactness for, and robustness to, arbitrary cuts and surface geometry. We demonstrate the superiority of our approach over state-of-the-art methods on almost 10,000 meshes with a wide range of geometric and topological complexity. We also provide an open source implementation."	https://dl.acm.org/doi/abs/10.1145/3528223.3530149	Xianzhong Fang, Mathieu Desbrun, Hujun Bao, Jin Huang
Towards practical physical-optics rendering	"Physical light transport (PLT) algorithms can represent the wave nature of light globally in a scene, and are consistent with Maxwell's theory of electromagnetism. As such, they are able to reproduce the wave-interference and diffraction effects of real physical optics. However, the recent works that have proposed PLT are too expensive to apply to real-world scenes with complex geometry and materials. To address this problem, we propose a novel framework for physical light transport based on several key ideas that actually makes PLT practical for complex scenes. First, we restrict the spatial coherence shape of light to an anisotropic Gaussian and justify this restriction with general arguments based on entropy. This restriction serves to simplify the rest of the derivations, without practical loss of generality. To describe partially-coherent light, we present new rendering primitives that generalize the radiometric radiance and irradiance, and are based on the well-known Stokes parameters. We are able to represent light of arbitrary spectral content and states of polarization, and with any coherence volume and anisotropy. We also present the wave BSDF to accurately render diffractions and wave-interference effects. Furthermore, we present an approach to importance sample this wave BSDF to facilitate bi-directional path tracing, which has been previously impossible. We show good agreement with state-of-the-art methods, but unlike them we are able to render complex scenes where all the materials are new, coherence-aware physical optics materials, and with performance approaching that of ""classical"" rendering methods."	https://dl.acm.org/doi/abs/10.1145/3528223.3530119	Shlomi Steinberg, Pradeep Sen, Ling-Qi Yan
Tracking Character Diversity in the Animation Pipeline	As we explore a broad range of characters and stories in our films, it has become increasingly valuable to view breakdowns of our character pools and selections by demographic: to build and use our assets efficiently, reinforce storytelling and world building choices, and ensure consistent decision-making across the pipeline. With the Character Linker App within Traction (Traction is Pixar's asset and shot-tracking tool), production is able to see a live breakdown of the character pool as assets are built, and sequence/shot composition, as they are populated–with the ability to visualize by a range of categories, including gender, ethnicity, body-type, and age, among others. Each film can define and populate these categories specific to their story, set breakdown goals to measure progress against, and iterate on crowd asset selections to ensure each character is utilized to the fullest.	https://dl.acm.org/doi/abs/10.1145/3532836.3536257	Paul Kanyuk, Mara MacMahon, Emily Wilson, Peter Nye, Gordon Cameron, Jessica Heidt, Joshua Minor
Training a Deep Remastering Model	The success of video streaming platforms has pushed studios to make available TV shows from legacy catalog, and there is an increased demand for remastering this content. Ideally, film reels are re-scanned with modern devices directly into high quality digital format. However this is not always possible as parts of the original film reels can be damaged or missing, and the content is then available in its entirety only in the broadcast version, typically NTSC. In this work, we present a deep learning solution to bring the NTSC version to the new scan quality levels, which would be otherwise impossible with existing tools.	https://dl.acm.org/doi/abs/10.1145/3532836.3536228	Abdelaziz Djelouah, Andrew J. Wahlquist, Sally Hattori, Christopher Schroers
Trios: Stylistic Rendering of 3D Photos	3D photography has emerged as a medium that provides an immersive dimension to 2D photos. We present Trios, an interactive mobile app that combines the vividness of image-based artistic rendering with 3D photos by implementing an end-to-end pipeline for their generation and stylization. Trios uses Apple's accelerated image-processing APIs and dedicated Neural Engine for depth-generation and learning-based artistic rendering. The pipeline runs at interactive frame rates and outputs a compact video, which can easily be shared. Thus, it serves as a unique interactive tool for digital artists interested in creating immersive artistic content.	https://dl.acm.org/doi/abs/10.1145/3532723.3535467	Ulrike Bath, Sumit Shekhar, Hendrik Tjabben, Amir Semmo, Sebastian Pasewaldt, Jürgen Döllner, Matthias Trapp
True seams: modeling seams in digital garments	Seams play a fundamental role in the way a garment looks, fits, feels and behaves. Seams can have very different shapes and mechanical properties depending on how fabric is overlapped, folded and stitched together, with garment designers often choosing specific seam and stitch type combinations depending on the appearance and behavior they want for the garment. Yet, virtually all 3D CAD tools for fashion and visual effects ignore most of the visual and mechanical complexity of seams, and just treat them as joint edges, their simplest possible form, drastically limiting the fidelity of digital garments. In this paper, we present a method that models seams following their true, real-life construction. Each seam brings together and overlaps the fabric pieces to be sewn, folds the fabric according to the type of seam, and stitches the resulting assembly following the type of stitch. To avoid dealing with the complexities of folding in 3D space, we cast the problem into a sequence of simpler 2D problems where we can easily shape the seam and produce a result free of self-intersections, before lifting the folded geometry back to 3D space. We run a series of constrained optimizations to enforce spatial properties in these 2D settings, allowing us to treat asymmetric seams, gatherings and overlapping construction orders. Using a variety of common seams and stitches, we show how our approach substantially improves the visual appearance of full garments, for a better and more predictive digital replica.	https://dl.acm.org/doi/abs/10.1145/3528223.3530128	Alejandro Rodríguez, Gabriel Cirio
Umbrella meshes: elastic mechanisms for freeform shape deployment	We present a computational inverse design framework for a new class of volumetric deployable structures that have compact rest states and deploy into bending-active 3D target surfaces. consist of elastic beams, rigid plates, and hinge joints that can be directly printed or assembled in a zero-energy fabrication state. During deployment, as the elastic beams of varying heights rotate from vertical to horizontal configurations, the entire structure transforms from a compact block into a target curved surface. Umbrella Meshes encode both intrinsic and extrinsic curvature of the target surface and in principle are free from the area expansion ratio bounds of past auxetic material systems. We build a reduced physics-based simulation framework to accurately and efficiently model the complex interaction between the elastically deforming components. To determine the mesh topology and optimal shape parameters for approximating a given target surface, we propose an inverse design optimization algorithm initialized with conformal flattening. Our algorithm minimizes the structure's strain energy in its deployed state and optimizes actuation forces so that the final deployed structure is in stable equilibrium close to the desired surface with few or no external constraints. We validate our approach by fabricating a series of physical models at various scales using different manufacturing techniques.	https://dl.acm.org/doi/abs/10.1145/3528223.3530089	Yingying Ren, Uday Kusupati, Julian Panetta, Florin Isvoranu, Davide Pellis, Tian Chen, Mark Pauly
Unbiased and consistent rendering using biased estimators	We introduce a general framework for transforming biased estimators into unbiased and consistent estimators for the same quantity. We show how several existing unbiased and consistent estimation strategies in rendering are special cases of this framework, and are part of a broader debiasing principle. We provide a recipe for constructing estimators using our generalized framework and demonstrate its applicability by developing novel unbiased forms of transmittance estimation, photon mapping, and finite differences.	https://dl.acm.org/doi/abs/10.1145/3528223.3530160	Zackary Misso, Benedikt Bitterli, Iliyan Georgiev, Wojciech Jarosz
Unbiased inverse volume rendering with differential trackers	"Volumetric representations are popular in inverse rendering because they have a simple parameterization, are smoothly varying, and transparently handle topology changes. However, incorporating the full volumetric transport of light is costly and challenging, often leading practitioners to implement simplified models, such as purely emissive and absorbing volumes with ""baked"" lighting. One such challenge is the efficient estimation of the gradients of the volume's appearance with respect to its scattering and absorption parameters. We show that the straightforward approach---differentiating a volumetric free-flight sampler---can lead to biased and high-variance gradients, hindering optimization. Instead, we propose using a new sampling strategy: , which is unbiased, yields low-variance gradients, and runs in linear time. Differential ratio tracking combines ratio tracking and reservoir sampling to estimate gradients by sampling distances proportional to the unweighted transmittance rather than the usual extinction-weighted transmittance. In addition, we observe local minima when optimizing scattering parameters to reproduce dense volumes or surfaces. We show that these local minima can be overcome by bootstrapping the optimization from nonphysical emissive volumes that are easily optimized."	https://dl.acm.org/doi/abs/10.1145/3528223.3530073	Merlin Nimier-David, Thomas Müller, Alexander Keller, Wenzel Jakob
Under the medal	The chest medals represent not only the prestigious and righteous decorations but also the heavy guilt. A veteran chased the stealing medals and started recalling the life with getting these medals. It reveals the ambiguous righteous under the medals.	https://dl.acm.org/doi/abs/10.1145/3512752.3520543	Ying-Ying Liu
Unified many-worlds browsing of arbitrary physics-based animations	"Manually tuning physics-based animation parameters to explore a simulation outcome space or achieve desired motion outcomes can be notoriously tedious. This problem has motivated many sophisticated and specialized optimization-based methods for fine-grained (keyframe) control, each of which are typically limited to specific animation phenomena, usually complicated, and, unfortunately, not widely used. In this paper, we propose (UMWB), a practical method for sample-level control and exploration of physics-based animations. Our approach supports browsing of large simulation ensembles of arbitrary animation phenomena by using a unified volumetric WORLDPACK representation based on spatiotemporally compressed voxel data associated with geometric occupancy and other low-fidelity animation state. Beyond memory reduction, the WORLDPACK representation also enables unified query support for interactive browsing: it provides fast evaluation of approximate spatiotemporal queries, such as occupancy tests that find ensemble samples (""worlds"") where material is either IN or NOT IN a user-specified spacetime region. WORLDPACKS also support real-time hardware-accelerated voxel rendering by exploiting the spatially hierarchical and temporal RLE raster data structure. Our UMWB implementation supports interactive browsing (and offline refinement) of ensembles containing thousands of simulation samples, and fast spatiotemporal queries and ranking. We show UMWB results using a wide variety of physics-based animation phenomena---not just JELL-O ."	https://dl.acm.org/doi/abs/10.1145/3528223.3530082	Purvi Goel, Doug L. James
Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape Collections	3D models of manufactured objects are important for populating virtual worlds and for synthetic data generation for vision and robotics. To be most useful, such objects should be articulated: their parts should move when interacted with. While articulated object datasets exist, creating them is labor-intensive. Learning-based prediction of part motions can help, but all existing methods require annotated training data. In this paper, we present an unsupervised approach for discovering articulated motions in a part-segmented 3D shape collection. Our approach is based on a concept we call category closure: any valid articulation of an object's parts should keep the object in the same semantic category (e.g. a chair stays a chair). We operationalize this concept with an algorithm that optimizes a shape's part motion parameters such that it can transform into other shapes in the collection. We evaluate our approach by using it to re-discover part motions from the PartNet-Mobility dataset. For almost all shape categories, our method's predicted motion parameters have low error with respect to ground truth annotations, outperforming two supervised motion prediction methods.	https://dl.acm.org/doi/abs/10.1145/3528233.3530742	Xianghao Xu, Yifan Ruan, Srinath Sridhar, Daniel Ritchie
Using STS to Bridge Long Histories of Blackness, Specularity, and Rendering	"Science and Technology Studies (STS) is an academic interdisicpline that uses sociological and historical methods to study the interrelations of society and technoscience. This paper uses an STS approach to examine the historical feedback loops between ""rendering"" the shine and specularity of Black skin–across painting, video, and photography–and how computer graphics programmers and artists should question some of the fundamental assumptions of their rendering workflows to both create more equitable representation of human form, and also to understand how computational renderings influence the real world they represent."	https://dl.acm.org/doi/abs/10.1145/3532836.3536279	James Malazita
Using a storyline to increase engagement in a course review	We present an approach to the end of course review session that increases student participation and helps them to identify how the topics that have been covered fit into a broader context. We provide examples of this approach being used for visual computing courses, including the refinements made based on the experience of running them. The approach uses a single strong narrative to connect a series of problems that allow students to apply what they have learnt and, in the process, realise how much they now know and how it fits together. It also serves to remind them what it was that initially attracted them to the topic. We also introduce an element of false jeopardy by pitting groups against each other in a friendly way that further increases the students' engagement in the session.	https://dl.acm.org/doi/abs/10.1145/3532724.3535600	Ken Cameron
VEMPIC: particle-in-polyhedron fluid simulation for intricate solid boundaries	"The comprehensive visual modeling of fluid motion has historically been a challenging task, due in no small part to the difficulties inherent in geometries that are non-manifold, open, or thin. Modern geometric cut-cell mesh generators have been shown to produce, both robustly and quickly, workable volumetric elements in the presence of these problematic geometries, and the resulting volumetric representation would seem to offer an ideal infrastructure with which to perform fluid simulations. However, cut-cell mesh elements are general polyhedra that often contain holes and are non-convex; it is therefore difficult to construct the explicit function spaces required to employ standard functional discretizations, such as the Finite Element Method. The Virtual Element Method (VEM) has recently emerged as a functional discretization that successfully operates with complex polyhedral elements through a weak formulation of its function spaces. We present a novel cut-cell fluid simulation framework that exactly represents boundary geometry during the simulation. Our approach enables, for the first time, detailed fluid simulation with ""in-the-wild"" obstacles, including ones that contain non-manifold parts, self-intersections, and extremely thin features. Our key technical contribution is the generalization of the Particle-In-Cell fluid simulation methodology to arbitrary polyhedra using VEM. Coupled with a robust cut-cell generation scheme, this produces a fluid simulation algorithm that can operate on previously infeasible geometries without requiring any additional mesh modification or repair."	https://dl.acm.org/doi/abs/10.1145/3528223.3530138	Michael Tao, Christopher Batty, Mirela Ben-Chen, Eugene Fiume, David I. W. Levin
VRm: A Virtual Reality Tool for Anatomical Study	VRm is the prototype for a Virtual Reality (VR) software tool for medical professionals to study anatomy. The tool is designed to improve learning outcomes for complex anatomical subject matter that is difficult to teach using traditional methods. VRm was developed at the Rochester Institute of Technology by faculty and students. In addition to developing the tool, the goal of the project was to explore pedagogical approaches emphasizing cross-disciplinary collaboration, production-based learning, and to utilize industry-standard software development methodologies in the context of a class.	https://dl.acm.org/doi/abs/10.1145/3532724.3535592	Jesse O'Brien, Craig Foster
Variable Bitrate Neural Fields	Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; Müller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 × and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.	https://dl.acm.org/doi/abs/10.1145/3528233.3530727	Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Müller, Morgan McGuire, Alec Jacobson, Sanja Fidler
Variational quadratic shape functions for polygons and polyhedra	Solving partial differential equations (PDEs) on geometric domains is an important component of computer graphics, geometry processing, and many other fields. Typically, the given discrete mesh the geometric representation and should not be altered for simulation purposes. Hence, accurately solving PDEs on general meshes is a central goal and has been considered for various differential operators over the last years. While it is known that using higher-order basis functions on simplicial meshes can substantially improve accuracy and convergence, extending these benefits to general surface or volume tessellations in an efficient fashion remains an open problem. Our work proposes variationally optimized piecewise quadratic shape functions for polygons and polyhedra, which generalize quadratic 2 elements, exactly reproduce them on simplices, and inherit their beneficial numerical properties. To mitigate the associated cost of increased computation time, particularly for volumetric meshes, we introduce a custom two-level multigrid solver which significantly improves computational performance.	https://dl.acm.org/doi/abs/10.1145/3528223.3530137	Astrid Bunge, Philipp Herholz, Olga Sorkine-Hornung, Mario Botsch, Michael Kazhdan
VastWaste	Humans once perceived oceans as boundless, and thus impossible to pollute---until we created the Great Pacific Garbage Patch. The same pattern is now repeating in outer space. VastWaste is a data-driven, projection art installation that illuminates the parallels and interplay between marine pollution and space debris. It can also be experienced in Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532837.3534952	Ozge Samanci, Jennifer Hutson, Jonathan David, Wren Gottschalk, Jack Burkhardt, Rachel Kantor, Stephan Moore, Liza Salvi
VastWaste	Humans once perceived oceans as boundless, and thus impossible to pollute---until we created the Great Pacific Garbage Patch. The same pattern is now repeating in outer space. VastWaste is a data-driven, projection art installation that illuminates the parallels and interplay between marine pollution and space debris. It can also be experienced in Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532837.3534952	Ozge Samanci, Jennifer Hutson, Jonathan David, Wren Gottschalk, Jack Burkhardt, Rachel Kantor, Stephan Moore, Liza Salvi
VastWaste	Humans once perceived oceans as boundless, and thus impossible to pollute---until we created the Great Pacific Garbage Patch. The same pattern is now repeating in outer space. VastWaste is a data-driven, projection art installation that illuminates the parallels and interplay between marine pollution and space debris. It can also be experienced in Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532837.3534952	Ozge Samanci, Jennifer Hutson, Jonathan David, Wren Gottschalk, Jack Burkhardt, Rachel Kantor, Stephan Moore, Liza Salvi
VastWaste	Humans once perceived oceans as boundless, and thus impossible to pollute---until we created the Great Pacific Garbage Patch. The same pattern is now repeating in outer space. VastWaste is a data-driven, projection art installation that illuminates the parallels and interplay between marine pollution and space debris. It can also be experienced in Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532837.3534952	Ozge Samanci, Jennifer Hutson, Jonathan David, Wren Gottschalk, Jack Burkhardt, Rachel Kantor, Stephan Moore, Liza Salvi
VastWaste	Humans once perceived oceans as boundless, and thus impossible to pollute---until we created the Great Pacific Garbage Patch. The same pattern is now repeating in outer space. VastWaste is a data-driven, projection art installation that illuminates the parallels and interplay between marine pollution and space debris. It can also be experienced in Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532837.3534952	Ozge Samanci, Jennifer Hutson, Jonathan David, Wren Gottschalk, Jack Burkhardt, Rachel Kantor, Stephan Moore, Liza Salvi
VastWaste	Humans once perceived oceans as boundless, and thus impossible to pollute---until we created the Great Pacific Garbage Patch. The same pattern is now repeating in outer space. VastWaste is a data-driven, projection art installation that illuminates the parallels and interplay between marine pollution and space debris. It can also be experienced in Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532837.3534952	Ozge Samanci, Jennifer Hutson, Jonathan David, Wren Gottschalk, Jack Burkhardt, Rachel Kantor, Stephan Moore, Liza Salvi
VastWaste	Humans once perceived oceans as boundless, and thus impossible to pollute---until we created the Great Pacific Garbage Patch. The same pattern is now repeating in outer space. VastWaste is a data-driven, projection art installation that illuminates the parallels and interplay between marine pollution and space debris. It can also be experienced in Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532837.3534952	Ozge Samanci, Jennifer Hutson, Jonathan David, Wren Gottschalk, Jack Burkhardt, Rachel Kantor, Stephan Moore, Liza Salvi
VastWaste	Humans once perceived oceans as boundless, and thus impossible to pollute---until we created the Great Pacific Garbage Patch. The same pattern is now repeating in outer space. VastWaste is a data-driven, projection art installation that illuminates the parallels and interplay between marine pollution and space debris. It can also be experienced in Virtual Reality.	https://dl.acm.org/doi/abs/10.1145/3532837.3534952	Ozge Samanci, Jennifer Hutson, Jonathan David, Wren Gottschalk, Jack Burkhardt, Rachel Kantor, Stephan Moore, Liza Salvi
Virtual Nature as a Digital Twin Botanically Correct 3D AR and VR Optimized Low-polygon and Photogrammetry High-polygon Plant Models: A short overview of construction methods	Virtual nature construction methods are covered in two processes, first with low-polygon 3D plant models ideal for augmented reality (AR) and virtual reality (VR) and the second with high-polygon 3D plant models using Unreal Engine 5 and Reality Capture. Critical for scientific and information accuracy is the iterative review process with the domain expert.	https://dl.acm.org/doi/abs/10.1145/3532724.3535599	Maria C. R. Harrington, Chris Jones, Crissy Peters
Virtual Production in Action: A Creative Implementation of Expanded Cinematography and Narratives	"Virtual Production fulfills George Lucas's early dream of having an engulfing ""space-opera in the sky"" (1). Epic Games' focus on realistic interactive 3D game environments using Unreal Engine, revolutionized the field of film-making, by replacing rear film projections with large format, curved, high resolution, immersive LED video screens, allowing backdrops to adapt in real time to the narrative needs of each scene by tracking the movement of the camera. Cinematographers and Art Directors are adapting to the challenges of virtual and real lighting and props, recruiting animators and new media developers who create, usually in very little time, virtual and real props, and metahuman actors and characters, enhancing the production value, optimizing and reducing costs in unparalleled ways. This poster presents the results of the first Virtual Production class offered by the Film Animation and New Media Department at the University of Tampa. In a very short time span, students working in interdisciplinary teams have seen the possibilities of these new technologies for science fiction, fantasy and experimental films that otherwise would have been impossible to create with very limited student budgets."	https://dl.acm.org/doi/abs/10.1145/3532719.3543231	Gregg William Perkins, Santiago Echeverry
Visual Effects Pedagogy:: Diversity, Equity, and Inclusion as Visible and Invisible Attributes	Due to our proximity to industry pathways, VFX curriculums are good at mapping visible graduate attributes to core skills. Visible attributes are skills that can be measured via portfolio work and are reflected on student transcripts. Examples of such attributes may be building digital humans, creating physically accurate shaders, and designing story worlds. However, in order for the discipline of VFX to reflect our dynamic global culture and ensure equitable workplaces, we must also find ways to map graduate attributes to the values that drive technical and cultural diversity. Such attributes are harder to measure and can be understood as invisible attributes.	https://dl.acm.org/doi/abs/10.1145/3532724.3535598	Raqi Syed, Areito Echevarria, Edgar Mallari, Mehau Tikaua-Williams, Soto Solis, Mathew Cross, Zichen Jie
Visualizing the Production Process of “Encanto” with the Command Center	"Walt Disney Animation Studios presents the Command Center, a web application for communicating and monitoring high dimensional production metrics at the studio. Developed and utilized during the production of ""Encanto"", the Command Center provides near-real time insights into render performance metrics, department staffing, completion data, and film production progression statistics, all integrated into a singular novel film interface. The source data is collated into multiple ""buckets"" of aggregation, allowing observation at high, medium, and low levels of granularity. Since its inauguration on ""Encanto"", the Command Center has grown in adoption among several new productions and has shifted our studio's direction and perception of collecting and monitoring our production metrics."	https://dl.acm.org/doi/abs/10.1145/3532836.3536242	Justin Tennant, Mitch Counsell, Far Jangtrakool, Salina Ortega, Rajesh Sharma, Tad Miller, Scott Kersavage
VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting	We propose VoLux-GAN, a generative framework to synthesize 3D-aware faces with convincing relighting. Our main contribution is a volumetric HDRI relighting method that can efficiently accumulate albedo, diffuse and specular lighting contributions along each 3D ray for any desired HDR environmental map. Additionally, we show the importance of supervising the image decomposition process using multiple discriminators. In particular, we propose a data augmentation technique that leverages recent advances in single image portrait relighting to enforce consistent geometry, albedo, diffuse and specular components. Multiple experiments and comparisons with other generative frameworks show how our model is a step forward towards photorealistic relightable 3D generative models. Code and pre-trained models are available at: https://github.com/google/volux-gan.	https://dl.acm.org/doi/abs/10.1145/3528233.3530751	Feitong Tan, Sean Fanello, Abhimitra Meka, Sergio Orts-Escolano, Danhang Tang, Rohit Pandey, Jonathan Taylor, Ping Tan, Yinda Zhang
Volume parametrization quantization for hexahedral meshing	Developments in the field of parametrization-based quad mesh generation on surfaces have been impactful over the past decade. In this context, an important advance has been the replacement of error-prone rounding in the generation of integer-grid maps, by robust quantization methods. In parallel, parametrization-based hex mesh generation for volumes has been advanced. In this volumetric context, however, the state-of-the-art still relies on fragile rounding, not rarely producing defective meshes, especially when targeting a coarse mesh resolution. We present a method to robustly quantize volume parametrizations, i.e., to determine guaranteed valid choices of integers for 3D integer-grid maps. Inspired by the 2D case, we base our construction on a non-conforming cell decomposition of the volume, a 3D analogue of a T-mesh. In particular, we leverage the motorcycle complex, a recent generalization of the motorcycle graph, for this purpose. Integer values are expressed in a differential manner on the edges of this complex, enabling the efficient formulation of the conditions required to strictly prevent forcing the map into degeneration. Applying our method in the context of hexahedral meshing, we demonstrate that hexahedral meshes can be generated with significantly improved flexibility.	https://dl.acm.org/doi/abs/10.1145/3528223.3530123	Hendrik Brückler, David Bommes, Marcel Campen
WIZ: DreamWorks GPU-Accelerated Interactive Hair/Fur Deformation and Visualization Toolset	WIZ is a hair/fur visualization toolset at DreamWorks that uses the power of the GPU for both interactive deformation and display in the 3-D viewport. WIZ closely approximates the final render hair motion on the GPU based on skin and guide curve motion, and provides the artist with various options for render hair deformation. It can use the character's hair textures and closely approximate the final look of the hair. Hundreds of thousands or millions of curves in motion can be visualized close to real-time which can help the artist evaluate hair/fur motion iteratively in a shot without having to do a render or having to use much slower existing viewport displays, thus providing significant time and resource savings.	https://dl.acm.org/doi/abs/10.1145/3532836.3536245	Arunachalam Somasundaram, Kelly Shay
Walking a Turtle	Walking a Turtle is a virtual reality experience where players go on a walk led by a tortoise. Part game, part quantified-self wellness tracker, Walking is a farcical tool for resisting the attention economy.	https://dl.acm.org/doi/abs/10.1145/3532834.3536218	Jeremy Rotsztain, Alice Rotsztain
WallPlan: synthesizing floorplans by learning to generate wall graphs	Floorplan generation has drawn widespread interest in the community. Recent learning-based methods for generating realistic floorplans have made significant progress while a complex heuristic post-processing is still necessary to obtain desired results. In this paper, we propose a novel wall-oriented method, called , for automatically and efficiently generating plausible floorplans from various design constraints. We pioneer the representation of the floorplan as a wall graph with room labels and consider the floorplan generation as a graph generation. Given the boundary as input, we first initialize the boundary with windows predicted by WinNet. Then a graph generation network GraphNet and semantics prediction network LabelNet are coupled to generate the wall graph progressively by imitating graph traversal. can be applied for practical architectural designs, especially the wall-based constraints. We conduct ablation experiments, qualitative evaluations, quantitative comparisons, and perceptual studies to evaluate our method's feasibility, efficacy, and versatility. Intensive experiments demonstrate our method requires no post-processing, producing higher quality floorplans than state-of-the-art techniques.	https://dl.acm.org/doi/abs/10.1145/3528223.3530135	Jiahui Sun, Wenming Wu, Ligang Liu, Wenjie Min, Gaofeng Zhang, Liping Zheng
Waving Blanket: Dynamic Liquid Distribution for Multiple Tactile Feedback using Rewirable Piping System	"Perceiving multiple tactile sensations in virtual reality(VR) is one of the keys to enabling a compelling, immersive experience. The haptic experience consists of different receptors on the human body. Although several haptic technologies can produce different tactile stimulation to achieve the experience, the hybrid haptic system needs to combine each technique, requiring a complex configuration. Therefore, our goal is to provide several stimulations in one technique to reduce the effort to integrate haptic devices. This paper presents Waving Blanket, a dynamic liquid distribution system for multiple tactile feedback, utilizing a water pump and air valve to transmit and allocate the liquid in the pipe. We designed a virtual natural scene and developed a relaxation application called ""Water Forest"" with our haptic system to show the possibility by combining visual-auditory feedback. Additionally, a rewirable piping system is adopted to explore the mechanism of simulating vibration, pressure, weight, and weight-shifting feedback."	https://dl.acm.org/doi/abs/10.1145/3532721.3535562	Ping-Hsuan Han, Yu-Yen Chen, Wu-Ting Pan, Hui-Wen Hsu, Jin-Rong Jiang, Wen-Jun Wu
Wet	A lady's vaporous dream in a land of soft and peachy flesh where her affection for her masseur transpires.	https://dl.acm.org/doi/abs/10.1145/3512752.3528202	Marianne Bergeonneau, Lauriane Montpert, Mélina Mandon, Cloé Peyrebrune, Elvira Taussac
Which cross fields can be quadrangulated?: global parameterization from prescribed holonomy signatures	We describe a method for the generation of seamless surface parametrizations with guaranteed local injectivity and full control over holonomy. Previous methods guarantee only one of the two. Local injectivity is required to enable these parametrizations' use in applications such as surface quadrangulation and spline construction. Holonomy control is crucial to enable guidance or prescription of the parametrization's isocurves based on directional information, in particular from cross-fields or feature curves, and more generally to constrain the parametrization topologically. To this end we investigate the relation between cross-field topology and seamless parametrization topology. Leveraging previous results on locally injective parametrization and combining them with insights on this relation in terms of holonomy, we propose an algorithm that meets these requirements. A key component relies on the insight that arbitrary surface cut graphs, as required for global parametrization, can be homeomorphically modified to assume almost any set of turning numbers with respect to a given target cross-field.	https://dl.acm.org/doi/abs/10.1145/3528223.3530187	Hanxiao Shen, Leyi Zhu, Ryan Capouellez, Daniele Panozzo, Marcel Campen, Denis Zorin
Wizart DCC Platform: extensible USD-based toolset	As an indie animation studio for Secret Magic Control Agency (SMCA), we moved to Pixar's Universal Scene Description (USD) as the backbone of our pipeline. To make this possible, we introduced our in-house application framework, Wizart DCC Platform as a USD-native toolset. We used its extensibility to successfully implement new scene assembly, shading, hair grooming, and lighting workflows.	https://dl.acm.org/doi/abs/10.1145/3532836.3536259	Alexander Kalyuzhnyy, Konstantin Giliarovsky, Mikhail Kutsov, Vlad Tagintsev, Maksim Tokarev
WonderScope: Practical Near-surface AR Device for Museum Exhibits	Mobile augmented reality (AR) applications have become essential tools for delivering additional information to museum visitors. However, interacting through a mobile screen can potentially distract visitors from the exhibits. We propose WonderScope which is a peripheral system for mobile devices that enables practical near-surface AR interaction. Using a single small RFID tag on the exhibit as the origin, WonderScope can detect the position and orientation of the device on the surface of the exhibit. It performs on various surfaces of different materials based on the result of data fusion from two types of displacement sensors and an accelerometer of an inertial measurement unit (IMU). The mobile application utilizes the data for spatial registration of digital content on the exhibit's surface, which make the users feel like seeing-through or magnifying the surface of exhibits.	https://dl.acm.org/doi/abs/10.1145/3532721.3535564	HyeonBeom Yi, Yeeun Shin, Sehee Lee, Eunhye Youn, Auejin Ham, Geehyuk Lee, Woohun Lee
Yallah!	Beirut, 1982. As Nicolas prepares to flee his hometown, torn apart by an endless civil war, he crosses the path of Naji, a reckless teenager determined to go to the Swimming Pool. Trying to protect the young man, Nicolas finds himself pulled into a surreal race against war, all for the mere freedom of going swimming.	https://dl.acm.org/doi/abs/10.1145/3512752.3517761	Nayla Nassar, Edouard Pitula, Renaud de Saint Albin, Cécile Adant, Anaïs Sassatelli, Candice Behague
Zoon	Small shimmering animals are in heat. A two-legged forest dweller encounters the lustful group. He and his companions snack on the little creatures and soon a feast begins.	https://dl.acm.org/doi/abs/10.1145/3512752.3527867	Jonatan Schwenk
stelaCSF: a unified model of contrast sensitivity as the function of spatio-temporal frequency, eccentricity, luminance and area	A contrast sensitivity function, or CSF, is a cornerstone of many visual models. It explains whether a contrast pattern is visible to the human eye. The existing CSFs typically account for a subset of relevant dimensions describing a stimulus, limiting the use of such functions to either static or foveal content but not both. In this paper, we propose a unified CSF, stelaCSF, which accounts for all major dimensions of the stimulus: spatial and temporal frequency, eccentricity, luminance, and area. To model the 5-dimensional space of contrast sensitivity, we combined data from 11 papers, each of which studied a subset of this space. While previously proposed CSFs were fitted to a single dataset, stelaCSF can predict the data from all these studies using the same set of parameters. The predictions are accurate in the entire domain, including low frequencies. In addition, stelaCSF relies on psychophysical models and experimental evidence to explain the major interactions between the 5 dimensions of the CSF. We demonstrate the utility of our new CSF in a flicker detection metric and in foveated rendering.	https://dl.acm.org/doi/abs/10.1145/3528223.3530115	Rafał K. Mantiuk, Maliha Ashraf, Alexandre Chapiro
“Comandante’’: Braving the Waves With Near Real-Time Virtual Production Workflows	"The naval feature film ""Comandante"" is to be shot on a water stage within a waterproof but very low resolution LED volume. We developed our Near Real-Time (NRT) workflow to immediately improve the quality of a shot, seamlessly bridging In-Camera VFX (ICVFX) and post-production, allowing VFX to start on-set as soon as the director says 'cut'. The NRT workflow brings together key advancements in lens calibration, machine learning and real-time rendering to deliver higher quality composites of what was just shot to the filmmakers, in a matter of minutes."	https://dl.acm.org/doi/abs/10.1145/3532836.3536272	Dan Ring, Kevin Tod Haug, Pierpaulo Verga, David Stump, Niall Redmond, Dan Caffrey, Peter Canning
“Get Real” Defining, Designing and Executing Virtual Production Curriculum at SCAD	"Higher educational programs in Savannah College of Art and Design's (SCAD) School of Digital Media and Entertainment Arts have taken the leap to create a curriculum around virtual production including its adoption of the 40'x20' LED volume set on SCAD's new 10,000 square feet backlot for the Savannah Film Studios and aggressively including real-time training for its students. In this article, we summarize a new way to define, design and execute virtual production curriculum through lessons learned in its flagship production, ""Get Real""."	https://dl.acm.org/doi/abs/10.1145/3532724.3538504	Danyl Bartlett, SuAnne Fu
“OpenVPCal”: An Open Source In-Camera Visual Effects Calibration Framework	This presentation introduces the OpenVPCal toolset, an open source in-camera visual effects calibration framework. The toolset includes reference patch generation, creation of calibration color transforms, and custom 1D roll off lookup tables (LUTs) to control content brightness while attempting to maintain a linear light output from the LED panels. The resulting transforms can be expressed via an OpenColorIO Config [OCIO Contributors 2022] file, making tracking and transport easier for productions to manage and reproduce. We hope that by making this workflow open source and accessible, it will increase dialogue between practitioners in the space, leading to new novel improvements for creating more amazing content worldwide.	https://dl.acm.org/doi/abs/10.1145/3532836.3536265	Carol Payne, Francesco Luigi Giardiello
”Encanto” - Let’s Talk About Bruno’s Visions	"In Walt Disney Animation Studios' ""Encanto"", Mirabel discovers the remnants of her Uncle Bruno's mysterious visions of the future. Developing the look and lighting for the emerald shards required close collaboration between our Visual Development, Look Development, Lighting, and Technology departments to create a holographic effect. With an innovative new teleporting holographic shader, we were able to bring a unique and unusual effect to the screen."	https://dl.acm.org/doi/abs/10.1145/3532836.3536269	Corey Butler, Brent Burley, Wei-Feng Wayne Huang, Yining Karl Li, Benjamin Min Huang
