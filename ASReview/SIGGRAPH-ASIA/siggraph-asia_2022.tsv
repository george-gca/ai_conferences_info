title	abstract	url	authors
3QNet: 3D Point Cloud Geometry Quantization Compression Network	Since the development of 3D applications, the point cloud, as a spatial description easily acquired by sensors, has been widely used in multiple areas such as SLAM and 3D reconstruction. Point Cloud Compression (PCC) has also attracted more attention as a primary step before point cloud transferring and saving, where the geometry compression is an important component of PCC to compress the points geometrical structures. However, existing non-learning-based geometry compression methods are often limited by manually pre-defined compression rules. Though learning-based compression methods can significantly improve the algorithm performances by learning compression rules from data, they still have some defects. Voxel-based compression networks introduce precision errors due to the voxelized operations, while point-based methods may have relatively weak robustness and are mainly designed for sparse point clouds. In this work, we propose a novel learning-based point cloud compression framework named 3D Point Cloud Geometry Quantiation Compression Network (3QNet), which overcomes the robustness limitation of existing point-based methods and can handle dense points. By learning a codebook including common structural features from simple and sparse shapes, 3QNet can efficiently deal with multiple kinds of point clouds. According to experiments on object models, indoor scenes, and outdoor scans, 3QNet can achieve better compression performances than many representative methods.	https://dl.acm.org/doi/abs/10.1145/3550454.3555481	Tianxin Huang, Jiangning Zhang, Jun Chen, Zhonggan Ding, Ying Tai, Zhenyu Zhang, Chengjie Wang, Yong Liu
A Biologically Inspired Hair Aging Model	Hair rendering has been a focal point of attention in computer graphics for the last couple of decades. However, there have been few contributions to the modeling and rendering of the natural hair aging phenomenon. We present a new technique that simulates the process of hair graying and hair thinning on digital models due to aging. Given a 3D human head model with hair, we first compute a segmentation of the head using K-means since hair aging occurs at different rates in distinct head parts. Hair graying is simulated according to recent biological knowledge on aging factors for hairs, and hair thinning decreases hair diameters linearly with time. Our system is biologically inspired, supports facial hair, both genders and many ethnicities, and is compatible with different lengths of hair strands. Our real-time results resemble real-life hair aging, accomplished by simulating the stochastic nature of the process and the gradual decrease of melanin.	https://dl.acm.org/doi/abs/10.1145/3550454.3555444	Arthur E. Balbão, Marcelo Walter
A High Frame Rate Affordable Nystagmus Detection Method with Smartphones Used in Outpatient Clinic	In this work, we present a high frame rate affordable nystagmus detection aid for outpatient clinic use. Using a Frenzel goggles and the high-speed video recording feature of a smartphone, high-frame rate eye videos of patients are obtained. Then, OpenCV morphological operation and blob detection library were modified to manually adjust the parameters according to the video to capture and track the eye center movements. Next, we correct the error caused by hand shaking during photography and predict the eyeball movement during blinking. Finally, with rule-based detecting method, nystagmus was detected and classified based on the modified tracking data. In our experimental, we collaborated with a professional ENT doctor to obtain multiple high-frame rate eye videos of patients. Experimental results show that this procedure can provide preliminary diagnosis results for physicians to refer to, but the final diagnosis needs to be made by a professional ophthalmologist. Plus, this method has been used by the ENT doctor to record the result simultaneously.	https://dl.acm.org/doi/abs/10.1145/3550082.3564164	Ting-Hua Yang, Hsien-Yuan Hsieh, Tse-Han Lin, Wei-Zen Sun, Ming Ouhyoung
A Hybrid Boundary Element and Boundary Integral Equation Method for Accurate Diffusion Curves	In theory, diffusion curves promise complex color gradations for infinite-resolution vector graphics. In practice, existing realizations suffer from poor scaling, discretization artifacts, or insufficient support for rich boundary conditions. In this paper, we utilize the boundary integral equation method to accurately and efficiently solve the underlying partial differential equation.	https://dl.acm.org/doi/abs/10.1145/3550340.3564233	Seungbae Bang, Kirill Serkh, Oded Stein, Alec Jacobson
A Monte Carlo Method for Fluid Simulation	We present a novel Monte Carlo-based fluid simulation approach capable of pointwise and stochastic estimation of fluid motion. Drawing on the Feynman-Kac representation of the vorticity transport equation, we propose a recursive Monte Carlo estimator of the Biot-Savart law and extend it with a stream function formulation that allows us to treat free-slip boundary conditions using a Walk-on-Spheres algorithm. Inspired by the Monte Carlo literature in rendering, we design and compare variance reduction schemes suited to a fluid simulation context for the first time, show its applicability to complex boundary settings, and detail a simple and practical implementation with temporal grid caching. We validate the correctness of our approach via quantitative and qualitative evaluations - across a range of settings and domain geometries - and thoroughly explore its parameters' design space. Finally, we provide an in-depth discussion of several axes of future work building on this new numerical simulation modality.	https://dl.acm.org/doi/abs/10.1145/3550454.3555450	Damien Rioux-Lavoie, Ryusuke Sugimoto, Tümay Özdemir, Naoharu H. Shimada, Christopher Batty, Derek Nowrouzezahrai, Toshiya Hachisuka
A Neural Galerkin Solver for Accurate Surface Reconstruction	To reconstruct meshes from the widely-available 3D point cloud data, implicit shape representation is among the primary choices as an intermediate form due to its superior representation power and robustness in topological optimizations. Although different parameterizations of the implicit fields have been explored to model the underlying geometry, there is no explicit mechanism to ensure the fitting tightness of the surface to the input. We present in response, NeuralGalerkin, a neural Galerkin-method-based solver designed for reconstructing highly-accurate surfaces from the input point clouds. NeuralGalerkin internally discretizes the target implicit field as a linear combination of a set of spatially-varying basis functions inferred by an adaptive sparse convolution neural network. It then solves differentiably for a variational problem that incorporates both positional and normal constraints from the data in closed form within a single forward pass, highly respecting the raw input points. The reconstructed surface extracted from the implicit interpolants is hence very accurate and incorporates useful inductive biases benefiting from the training data. Extensive evaluations on various datasets demonstrate our method's promising reconstruction performance and scalability.	https://dl.acm.org/doi/abs/10.1145/3550454.3555457	Jiahui Huang, Hao-Xiang Chen, Shi-Min Hu
A Non-Associated MSCC Model for Simulating Structured and Destructured Clays	Soil structure describes the association and arrangement of the soil granules. Structured clays, such as the cracked dry ground, exhibit characteristics of solid-like materials, while destructured clays exhibit characteristics of granular materials. The structure strength weakens with the deviatoric plastic strain accumulation, which leads to clay degradation. However, the existing graphics methods for granular materials cannot simulate this unique mechanical behavior. We present a non-associated Modified Structured Cam Clay (MSCC) model for simulating structured and destructured clays. We track the deviatoric part of the plastic Hencky strain and design two degradation functions (corresponding to yield stress and mean effective stress) to control the size of the yield surface. Through a non-associated flow rule, our method generates visually plausible results while allowing volume preservation.	https://dl.acm.org/doi/abs/10.1145/3550082.3564174	Bohan Jing, Weiran Li, Qing Zhu
A Study on Sonification Method of Simulator-Based Ski Training for People with Visual Impairment	People with visual impairment (PVI) are eager to push their limits in extreme sports such as alpine skiing. However, training skiing is very difficult as it always requires assistance from an experienced guide. This paper explores sonification-based methods that enable PVI to train skiing using a simulator, which will allow them to train without a guide. Two types of sonification feedback for PVI are proposed and studied in our experiment. The results suggest that users without any visual information can also pass through over 80% of the poles compared to with visual information on average.	https://dl.acm.org/doi/abs/10.1145/3550082.3564172	Yusuke Miura, Masaki Kuribayashi, Erwin Wu, Hideki Koike, Shigeo Morishima
A novel solution to manufacturing multi-color medical preoperative models with transparent shells	Preoperative models manufactured by 3D printing technology are widely demanded in surgical procedures. Current printing methods such as Polyjet, color jet printing (CJP) and fused deposition molding (FDM) are either too expensive or of poor performance. In this paper, a novel solution is proposed. Preoperative models are first divided into internal part, such as blood vessel, and the main body. The former is printed in color and the latter is transparent. In particular, the internal part is printed using FDM. A housing template for the main body is generated by digital light processing (DLP) based on the preoperative model. After integrating the template and inner part, transparent elastomer precursor is injected and cured inside the template, leading to the formation of the transparent main body. For demonstration, a liver model is manufactured using the proposed scheme. Compared with the existing methods, our strategy could manufacture preoperative models at both high quality and affordable cost.	https://dl.acm.org/doi/abs/10.1145/3550082.3564192	Tianqin Yang, Mingli Xiang, Lidong Zhao, Zhi Zhao, Lifang Wu
AI driven Live Interactive Cinematic Experience: Aria Studios' Virtual Movie Pipeline	'The 'AI-Driven Interactive Cinematic Experience' is a content production system combines live action with real-time graphics and multiple artificial intelligence solutions to create a conversational interactive entertainment. Our target is to make interactive content that viewers can interact with the fictional characters to affect the story plot, in addition to AI-based story engine, audience intention and language analysis, Real-time visualization of virtual set space and virtual characters according to the prompts of the story engine. Viewer response collection unit that receives voice and text responses generates interactive scenarios of virtual content that affects branching story plots in real-time for a natural story interaction while making the viewers fully engaged as if they are talking to a movie. The act of 'Viewers change the plot of the story by communicating with the main character in the drama', suggests new type of media entertainment format.	https://dl.acm.org/doi/abs/10.1145/3550453.3570125	Chuck Chae, Patrick Song, Dahun Song
AIP: Adversarial Interaction Priors for Multi-Agent Physics-based Character Control	We address the problem of controlling and simulating interactions between multiple physics-based characters, using short unlabeled motion clips. We propose Adversarial Interaction Priors (AIP), a multi-agents generative adversarial imitation learning (MAGAIL) approach, which extends recent deep reinforcement learning (RL) works aiming at imitating single character example motions. The main contribution of this work is to extend the idea of motion imitation of a single character to interaction imitation between multiple characters. Our method uses a control policy for each character to imitate interactive behaviors provided by short example motion clips, and associates a discriminator for each character, which is trained on actor-specific interactive motion clips. The discriminator returns interaction rewards that measure the similarity between generated behaviors and demonstrated ones in the reference motion clips. The policies and discriminators are trained in a multi-agent adversarial reinforcement learning procedure, to improve the quality of the behaviors generated by each agent. The initial results show the effectiveness of our method on the interactive task of shadowboxing between two fighters.	https://dl.acm.org/doi/abs/10.1145/3550082.3564207	Mohamed Younes, Ewa Kijak, Simon Malinowski, Richard Kulpa, Franck Multon
ASAP: Auto-generating Storyboard and Previz	We present ASAP, a system that uses virtual humans to Automatically generate Storyboards And Pre-visualized scenes from movie scripts. In our ASAP system, virtual humans play the role of actors. To visualize the screenplay scene, our system understands the movie script, which is the text data, and then facilitates the automatic generation of the following virtual human's non-/verbal behavior: (1) co-speech gesture, (2) facial expression, and (3) body movements. First of all, co-speech gestures are created from dialogue paragraphs using a text-to-gesture model trained with 2D videos and 3D motion-captured data. Next, for the facial expressions, we interpret the actors' emotions in the parenthetical paragraphs and then adjust the virtual human's face animation to reflect emotions such as anger and sadness. For body movements, our system extract action entities from action paragraphs (e.g., subject, target, and action) and then combine sets of animations to make animation sequences (e.g., a man's act of sitting on a bed). As soon as possible, ASAP can reduce the amount of time, money, and labor-intensive work that needs to be done in the early stages of filmmaking.	https://dl.acm.org/doi/abs/10.1145/3550453.3570124	Jae-In Hwang, Ghazanfar Ali, Hanseob Kim, Jieun Kim, Bin Han, Hwangyoun Kim
Adaptive real-time interactive rendering of gigantic multi-resolution models	Real-time visualization of large-scale surface models is still a challenging problem. When using multiple levels of details (LOD), the main issues are popping between levels and/or cracks between level parts. We present a scheme (both mesh preprocessing and real-time rendering), which avoids both of these issues. Vertex interpolation between child and parent is used to achieve crack and popping free rendering. We implemented and tested our method on a modest laptop PC, and could render scanned models of multiples tens of million triangles at optimal visual quality and interactive frame rate.	https://dl.acm.org/doi/abs/10.1145/3550082.3564170	Rui Li
Adjusting Level of Abstraction for Stylized Image Composition	Recently, many image stylization techniques have been proposed to convert photographs into illustrations, paintings, cartoons, etc. Artists sometimes use these techniques to efficiently generate background images for their own illustrations of foreground objects, such as cartoon characters. However, this approach often produces unnatural results because the level of abstraction between the foreground and the background images are different, resulting in inconsistent atmosphere between them. In this paper, we address this problem and propose a method for adjusting the abstraction level of the stylized background images. The effectiveness of our method is demonstrated by several examples with different levels of abstraction.	https://dl.acm.org/doi/abs/10.1145/3550082.3564193	Ryoma Hashimoto, Yoshinori Dobashi
Affine-Transformed Ray Alignment for Fast Ray Traversal	In recent years, ray tracing has begun to be used for real-time rendering. However, the number of rays that can be emitted in real time is limited due to the high time complexity of ray tracing. In this paper, we propose affine-transformed ray alignment as a novel method of reducing time complexity. Using affine transformations, the proposed method aligns multiple rays to form a single ray while maintaining the intersection points obtained from the scene, thereby speeding up ray tracing without introducing errors. We also propose application of our method to the problems of primary ray traversal, stereo rendering, and multi-frame rendering and empirically prove that time complexity can be reduced by these simple implementations.	https://dl.acm.org/doi/abs/10.1145/3550340.3564228	Yuki Nishidate, Issei Fujishiro
Affordable Spectral Measurements of Translucent Materials	We present a spectral measurement approach for the bulk optical properties of translucent materials using only low-cost components. We focus on the translucent inks used in full-color 3D printing, and develop a technique with a high spectral resolution, which is important for accurate color reproduction. We enable this by developing a new acquisition technique for the three unknown material parameters, namely, the absorption and scattering coefficients, and its phase function anisotropy factor, that only requires three point measurements with a spectrometer. In essence, our technique is based on us finding a three-dimensional , computed using Monte Carlo rendering, that allows the conversion between the three observables and the material parameters. Our measurement setup works without laboratory equipment or expensive optical components. We validate our results on a 3D printed color checker with various ink combinations. Our work paves a path for more accurate appearance modeling and fabrication even for low-budget environments or affordable embedding into other devices.	https://dl.acm.org/doi/abs/10.1145/3550454.3555499	TomáŠ Iser, Tobias Rittig, Emilie Nogué, Thomas Klaus Nindel, Alexander Wilkie
AgileAvatar: Stylized 3D Avatar Creation via Cascaded Domain Bridging	Stylized 3D avatars have become increasingly prominent in our modern life. Creating these avatars manually usually involves laborious selection and adjustment of continuous and discrete parameters and is time-consuming for average users. Self-supervised approaches to automatically create 3D avatars from user selfies promise high quality with little annotation cost but fall short in application to stylized avatars due to a large style domain gap. We propose a novel self-supervised learning framework to create high-quality stylized 3D avatars with a mix of continuous and discrete parameters. Our cascaded domain bridging framework first leverages a modified portrait stylization approach to translate input selfies into stylized avatar renderings as the targets for desired 3D avatars. Next, we find the best parameters of the avatars to match the stylized avatar renderings through a differentiable imitator we train to mimic the avatar graphics engine. To ensure we can effectively optimize the discrete parameters, we adopt a cascaded relaxation-and-search pipeline. We use a human preference study to evaluate how well our method preserves user identity compared to previous work as well as manual creation. Our results achieve much higher preference scores than previous work and close to those of manual creation. We also provide an ablation study to justify the design choices in our pipeline.	https://dl.acm.org/doi/abs/10.1145/3550469.3555402	Shen Sang, Tiancheng Zhi, Guoxian Song, Minghao Liu, Chunpong Lai, Jing Liu, Xiang Wen, James Davis, Linjie Luo
An Implicit Parametric Morphable Dental Model	3D Morphable models of the human body capture variations among subjects and are useful in reconstruction and editing applications. Current dental models use an explicit mesh scene representation and model only the teeth, ignoring the gum. In this work, we present the first parametric 3D morphable dental model for both teeth and gum. Our model uses an implicit scene representation and is learned from rigidly aligned scans. It is based on a component-wise representation for each tooth and the gum, together with a learnable latent code for each of such components. It also learns a template shape thus enabling several applications such as segmentation, interpolation and tooth replacement. Our reconstruction quality is on par with the most advanced global implicit representations while enabling novel applications. The code will be available at https://github.com/cong-yi/DMM	https://dl.acm.org/doi/abs/10.1145/3550454.3555469	Congyi Zhang, Mohamed Elgharib, Gereon Fox, Min Gu, Christian Theobalt, Wenping Wang
Animatomy: an Animator-centric, Anatomically Inspired System for 3D Facial Modeling, Animation and Transfer	We present Animatomy, a novel anatomic+animator centric representation of the human face. Present FACS-based systems are plagued with problems of face muscle separation, coverage, opposition, and redundancy. We, therefore, propose a collection of muscle fiber curves as an anatomic basis, whose contraction and relaxation provide us with a fine-grained parameterization of human facial expression. We build an end-to-end modular deformation architecture using this representation that enables: automatic optimization of the parameters of a specific face from high-quality dynamic facial scans; face animation driven by performance capture, keyframes, or dynamic simulation; interactive and direct manipulation of facial expression; and animation transfer from an actor to a character. We validate our facial system by showing compelling animated results, applications, and a quantitative comparison of our facial reconstruction to ground truth performance capture. Our system is being intensively used by a large creative team on Avatar: The Way of Water. We report feedback from these users as qualitative evaluation of our system.	https://dl.acm.org/doi/abs/10.1145/3550469.3555398	Byungkuk Choi, Haekwang Eom, Benjamin Mouscadet, Stephen Cullingford, Kurt Ma, Stefanie Gassel, Suzi Kim, Andrew Moffat, Millicent Maier, Marco Revelant, Joe Letteri, Karan Singh
Anime-Like Motion Transfer with Optimal Viewpoints	In 3D character animation, the frame rate is often reduced to mimic hand-drawn animations, like anime (Japanese animation). However, anime with a low frame rate differs from real motions because it omits excessive movements and emphasizes speed by expressing motions with a small number of impressive poses. It is difficult to reproduce such motions only by downsampling mocap data. Thus, in this poster, we propose a method for converting mocap data into anime-like 2D motions by respecting production site techniques. The proposed method evaluates the characteristics of motion data using the time distributions of speeds and pose areas to select an appropriate sequence of viewpoints and extract effective poses for each viewpoint.	https://dl.acm.org/doi/abs/10.1145/3550082.3564212	Yui Koroku, Issei Fujishiro
Apart	The short film 'Apart' is a collaborative group project from the Master of Animation and Visualisation students at the University of Technology Sydney. The large scale project based learning is unusual for a university setting, and has been co-designed with the industry partner, one of Australia's leading 3D animation studios Animal Logic. Each year the students face creative and technical changes. With 'Apart' one of the more innovative approaches came when the students had to create a stop-frame style animation sequence using ever changing and intertwined sculptures. This is where the FX team under the supervision of the Visual Effects Lead Dylan Neill used Houdini to create a progressive FX sequence, rather than use expected animation techniques in Maya. The challenges such studio-style projects present to students develops their ability to become innovative problem-solvers when they enter industry.	https://dl.acm.org/doi/abs/10.1145/3550339.3557760	Alex Weight, Dylan Neill, Chris French
Apocalypse Dog	In a post-apocalyptic universe, Bob and his dog Pasha survive in a wasteland. They are hungry, thirsty and tired. When suddenly, they see a city in the distance!	https://dl.acm.org/doi/abs/10.1145/3550339.3556083	Aziliz Le Clainche, Camille Nasarre, Jing Qian, Juliette Barraux, Emma Plumey, Lucile Arnaud, Solène Cauchie, Philippe Meis
Artist-directed Modeling of Competitively Growing Corals	This paper presents a procedural modeling method for coral groups considering the territorial conflict between species. We developed a graphical interface to control the territorial battle by arranging locator points that represent available space to grow the coral branches. The coral skeletals are generated according to species-specific structural rules while competing for the locators between individuals.	https://dl.acm.org/doi/abs/10.1145/3550082.3564171	Takashi Horiuchi, Ziyuan Cao, Yuto Kominami, Wataru Umezawa, Yuhao Dou, Daichi Ando, Tomohiko Mukai
Assemble Them All: Physics-Based Planning for Generalizable Assembly by Disassembly	Assembly planning is the core of automating product assembly, maintenance, and recycling for modern industrial manufacturing. Despite its importance and long history of research, planning for mechanical assemblies when given the final assembled state remains a challenging problem. This is due to the complexity of dealing with arbitrary 3D shapes and the highly constrained motion required for real-world assemblies. In this work, we propose a novel method to efficiently plan physically plausible assembly motion and sequences for real-world assemblies. Our method leverages the assembly-by-disassembly principle and physics-based simulation to efficiently explore a reduced search space. To evaluate the generality of our method, we define a large-scale dataset consisting of thousands of physically valid industrial assemblies with a variety of assembly motions required. Our experiments on this new benchmark demonstrate we achieve a state-of-the-art success rate and the highest computational efficiency compared to other baseline algorithms. Our method also generalizes to rotational assemblies (e.g., screws and puzzles) and solves 80-part assemblies within several minutes.	https://dl.acm.org/doi/abs/10.1145/3550454.3555525	Yunsheng Tian, Jie Xu, Yichen Li, Jieliang Luo, Shinjiro Sueda, Hui Li, Karl D. D. Willis, Wojciech Matusik
Asynchronous Collaborative Autoscanning with Mode Switching for Multi-Robot Scene Reconstruction	When conducting autonomous scanning for the online reconstruction of unknown indoor environments, robots have to be competent at exploring scene structure and reconstructing objects with high quality. Our key observation is that different tasks demand specialized scanning properties of robots: rapid moving speed and far vision for global exploration and slow moving speed and narrow vision for local object reconstruction, which are referred as two different scanning modes: and , respectively. When requiring multiple robots to collaborate for efficient exploration and fine-grained reconstruction, the questions on when to generate and how to assign those tasks should be carefully answered. Therefore, we propose a novel asynchronous collaborative autoscanning method with mode switching, which generates two kinds of scanning tasks with associated scanning modes, i.e., exploration task with mode and reconstruction task with mode, and assign them to the robots to execute in an asynchronous collaborative manner to highly boost the scanning efficiency and reconstruction quality. The task assignment is optimized by solving a modified Multi-Depot Multiple Traveling Salesman Problem (MDMTSP). Moreover, to further enhance the collaboration and increase the efficiency, we propose a model that actives the task generation and assignment process immediately when any of the robots finish all its tasks with no need to wait for all other robots to complete the tasks assigned in the previous iteration. Extensive experiments have been conducted to show the importance of each key component of our method and the superiority over previous methods in scanning efficiency and reconstruction quality.	https://dl.acm.org/doi/abs/10.1145/3550454.3555483	Junfu Guo, Changhao Li, Xi Xia, Ruizhen Hu, Ligang Liu
Augmenting Everyday Objects into Personal Robotic Devices	Augmenting familiar physical objects has presented great potential in upgrading their functions by automation, granting aesthetics, and even changing access. The recent celebration of success in personal fabrication has brought novices where they can augment everyday objects, from automating routine tasks with mobilized smart devices to devising self-sustaining smart objects by harvesting energy from involved daily interactions, for example. While the overall process involves a line of steps of capturing specifications, design mechanisms and fabricating the parts, it remains challenging for non-experts as it demands domain expertise in robotics, design, programming, and even mechanical engineering. We introduce a series of augmented robots, smart domestic devices that are augmented from everyday objects, leveraging personal fabrication to assist daily essential interactions. Through user-demonstration of desired motions, 3D printed attachment mechanisms are auto-generated to build personal robotic devices that automate routine tasks and harvest energy.	https://dl.acm.org/doi/abs/10.1145/3550471.3564763	Abul Al Arabi, Jeeeun Kim
Automatic Deformation-based animation of 3D mesh	This work introduces a novel method for automating the object animation from a 3D mesh used in various applications such as AR/VR, gaming, etc. The 3D mesh could be a generic inflated object generated from an image or sketch. The method describes a medial axis based control point estimation to generate an animation path and perform deformation-based animations for a mesh. Our method eliminates user involvement or need for expertise in terms of rigging the mesh or region selection etc. Performance of our method shows meaningful, real-time animation of the 3D mesh. Experiments indicate that our method is more consistent and less error prone compared to existing works.	https://dl.acm.org/doi/abs/10.1145/3550082.3564184	Lakshmi Priya Muraleedharan, Vikas Kantha Gowda, Basavaraja Shanthappa Vandrotti
Bon Appetit!	Bon Appetit! is a computer-animated musical short film all about Gus, a flashy snail who longs to become the star of the show --- no matter where or what it takes. The goofy story starts when Gus accidentally arrives atop a box of produce in the kitchen of a high-end French Restaurant. Despite the potentially fatal implications, Gus breaks out into song, boldly declaring his dreams of becoming something greater in the style of classic Broadway musical numbers while dancing through surreal, food-filled set pieces of kitchenware and delectable ingredients. Will Gus's larger-than-life ambitions pan out? Or will his appetite for the spotlight cause his inevitable demise?	https://dl.acm.org/doi/abs/10.1145/3550339.3554493	David Liang Guo
BoolSurf: Boolean Operations on Surfaces	We port Boolean set operations between 2D shapes to surfaces of any genus, with any number of open boundaries. We combine shapes bounded by sets of freely intersecting loops, consisting of geodesic lines and cubic Bézier splines lying on a surface. We compute the arrangement of shapes directly on the surface and assign integer labels to the cells of such arrangement. Differently from the Euclidean case, some arrangements on a manifold may be inconsistent. We detect inconsistent arrangements and help the user to resolve them. Also, we extend to the manifold setting recent work on Boundary-Sampled Halfspaces, thus supporting operations more general than standard Booleans, which are well defined on inconsistent arrangements, too. Our implementation discretizes the input shapes into polylines at an arbitrary resolution, independent of the level of resolution of the underlying mesh. We resolve the arrangement inside each triangle of the mesh independently and combine the results to reconstruct both the boundaries and the interior of each cell in the arrangement. We reconstruct the control points of curves bounding cells, in order to free the result from discretization and provide an output in vector format. We support interactive usage, editing shapes consisting up to 100k line segments on meshes of up to 1M triangles.	https://dl.acm.org/doi/abs/10.1145/3550454.3555466	Marzia Riso, Giacomo Nazzaro, Enrico Puppo, Alec Jacobson, Qingnan Zhou, Fabio Pellacini
CAD2Sketch: Generating Concept Sketches from CAD Sequences	Concept sketches are ubiquitous in industrial design, as they allow designers to quickly depict imaginary 3D objects. To construct their sketches with accurate perspective, designers rely on longstanding drawing techniques, including the use of auxiliary construction lines to identify midpoints of perspective planes, to align points vertically and horizontally, and to project planar curves from one perspective plane to another. We present a method to synthesize such construction lines from CAD sequences. Importantly, our method balances the presence of construction lines with overall clutter, such that the resulting sketch is both well-constructed and readable, as professional designers are trained to do. In addition to generating sketches that are visually similar to real ones, we apply our method to synthesize a large quantity of paired sketches and normal maps, and show that the resulting dataset can be used to train a neural network to infer normals from concept sketches.	https://dl.acm.org/doi/abs/10.1145/3550454.3555488	Felix Hähnlein, Changjian Li, Niloy J. Mitra, Adrien Bousseau
CLIP-Mesh: Generating textured meshes from text using pretrained image-text models	We present a technique for zero-shot generation of a 3D model using only a target text prompt. Without any 3D supervision our method deforms the control shape of a limit subdivided surface along with its texture map and normal map to obtain a 3D asset that corresponds to the input text prompt and can be easily deployed into games or modeling applications. We rely only on a pre-trained CLIP model that compares the input text prompt with differentiably rendered images of our 3D model. While previous works have focused on stylization or required training of generative models we perform optimization on mesh parameters directly to generate shape, texture or both. To constrain the optimization to produce plausible meshes and textures we introduce a number of techniques using image augmentations and the use of a pretrained prior that generates CLIP image embeddings given a text embedding.	https://dl.acm.org/doi/abs/10.1145/3550469.3555392	Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, Tiberiu Popa
Capturing and Animation of Body and Clothing from Monocular Video	While recent work has shown progress on extracting clothed 3D human avatars from a single image, video, or a set of 3D scans, several limitations remain. Most methods use a holistic representation to jointly model the body and clothing, which means that the clothing and body cannot be separated for applications like virtual try-on. Other methods separately model the body and clothing, but they require training from a large set of 3D clothed human meshes obtained from 3D/4D scanners or physics simulations. Our insight is that the body and clothing have different modeling requirements. While the body is well represented by a mesh-based parametric 3D model, implicit representations and neural radiance fields are better suited to capturing the large variety in shape and appearance present in clothing. Building on this insight, we propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. The code and models are available at https://github.com/YadiraF/SCARF.	https://dl.acm.org/doi/abs/10.1145/3550469.3555423	Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J. Black, Timo Bolkart
Cat and Moth	Firstly, we meet Ditto, a fluffy plump white cat. Unlike most other cats, who seem to be able to sleep anywhere, Ditto is very particular. The universe seems out to get her as she tries to find the most comfortable spot. Until she falls upon the most wonderful warm yellow cushion she's ever seen. No spot has ever been so comfortable. But little does she know a fuzzy winged intruder, Monty, has his eye on it too. Monty is by no way bashful in disturbing Ditto's slumber. He is inherently attracted to warm bright spots, but you'd think he'd stay loyal to the all-seeing light in the sky...but that cushion is goddamn comfy! Monty will go to any lengths to knock Ditto from that comfy spot, but at what consequence?	https://dl.acm.org/doi/abs/10.1145/3550339.3556981	India Barnardo
Chandra X	"Chandra X is a series of artistic diagnoses on the state of Planetary, implemented as WebXR(2021) and 3D Performative Apparatus-Environment(2022). The series speculates on how the natural environment and biodiversity still functions as a large capital in the near future. Inspecting the multilayers of remote surveillance system powered by virtualreal networks and neo-colonialist interactions, the artwork reimagines the audience's performative engagement as a counterbalance to such system. In Chandra X, audiences are invited to perform as meta-authorities to Chandra(they/them), who are interplanetary network environment manager(s) living in a near future, authorized with a certain degree of access to bio-resources in the universe. Remotely accessing the planets in charge, Chandra perform a set of labor such as Wielding, Archiving, Supervising, Detecting ('WASD') the planetarian's production and use of resources, for which Chandra earn 'hyper-capital' from their meta-authorities. As meta-authorities to Chandra, the audience-performers are also remotely connected to the labor of Chandra(s), infinitely doubling their acts of Wielding, Archiving, Supervising, Detecting. The technical mise-en-scene of Chandra X is defined as ""3D Performative Apparatus-Environment"". This setting comprises 1) WebXR as a 3D virtual labor-scape and, 2) mobile-based AR as an intervention tool. These apparatuses together serve as audiovisual diegeses that are inter-connected via cross-device interactions, hence interoperate. In such a setting, audiences can connect and intervene in each other's experiences in real-time, across the heterogeneous realities in heterogeneous device network environments; give and receive interaction and feedback on their performative behaviors. In the 3D Performative Apparatus-Environment, Chandra X intends to constantly re-situate the audience-performers amid the indeterminate states of being colonized and/or colonizing; of being autonomous and/or subordinate to a predesigned labor and belief system; and as a contemporaneous being in AD 2022 who receives the SOS message and alerts sent by Chandra themselves."	https://dl.acm.org/doi/abs/10.1145/3550470.3558419	Inhwa Yeom, Seogsung Jang
Codeless Content Creator System: Anyone Can Make Their Own Mixed Reality Content Without Relying on Software Developer Tools	"Mixed Reality (MR) techniques begin to penetrate our daily life. Due to the features of MR techniques to seamlessly intermix a user's real environment with useful virtual contents in a natural manner, their potential applications range from information visualization and sharing and remote work collaboration to education and training in various fields. However, due to the steep learning curve in content creation and development, it is not trivial for novice users to create MR contents by themselves. In this work, we propose ""Codeless Content Creator System"", an MR system that helps novice users rapidly and conveniently create MR contents using MR devices rather than relying on complicated software development tools such as Unity. A simple user study found that our system is useful and convenient for the MR contents creation tasks."	https://dl.acm.org/doi/abs/10.1145/3550082.3564194	Suhyeon Kim, Dokyung Lee, Jaejun Park, Myungji Song, Younhyun Jung
Cohand VR: Towards A Shareable Immersive Experience via Wearable Gesture Interface between VR Audiences and External Audiences	Head-mounted displays (HMDs) increase immersion in virtual world, however which limits VR audiences' awareness of the external audiences that co-located in the same physical environment. In this paper, we superimposing the virtual world onto physical environment and propose a shareable VR experience between VR audiences and external audiences via a wearable interface, thereby enabling external audiences to observe and interact with the virtual objects by using hand gestures. Our system allows external audiences to explore the virtual scenario spatially and to generate asymmetric communication with VR audiences in three different modes. Research suggests that these design strategies could be adapted for future shareable VR experience.	https://dl.acm.org/doi/abs/10.1145/3550082.3564176	Yuetong Zhao, Shuo Yan, Xukun Shen
Cold Lunch	The lunch bell rings at an elementary school and a flurry of arms grasp at brown paper bags. One Korean-American girl is surprised to find a tupperware full of Japchae noodles in her backpack when all her friends brought sandwiches. Feeling out of place, she conceals her noodles, but her lunch refuses to hide, instead transforming into a dragon made of noodles. Immediately, it is excited to meet everyone, flying out of the classroom at the sound of laughter. The girl desperately chases it down the hall and barely manages to trap the dragon in her lunchbox. When the dragon stops struggling, the girl carefully opens her lunchbox and the dragon looks up at her, hurt by her rejection. She apologizes, and the dragon forgives her. It lifts a compartment in her lunchbox lid to reveal three sets of chopsticks, as the food was made to be shared. The girl's face lights up with hope, and the dragon returns to its place in the lunch-box, becoming regular Japchae noodles. With newfound courage, the girl shows her lunch to her friends. Her friends are excited to see this new meal, and in the end, they share food and eat lunch together.	https://dl.acm.org/doi/abs/10.1145/3550339.3554708	Insun Park, Jin Pei Lua
Color Animated Full-parallax High-definition Computer-generated Hologram	We realized color animation of a computer-generated hologram (CGH) while retaining the features of the conventional full-parallax high-definition CGH (FPHD-CGH) method such as large screen size and a wide viewing area. Since the FPHD-CGH is fabricated using a laser direct writing system, the only way to animate it is to switch structured illumination (SI), and even that method has the problem of overlapping images from different frames caused by light leakage. We propose a method of suppressing this light leakage while maintaining image quality by applying erosion to SI. We also developed the world's first two-frame color animation prototype, which is 18 cm square and has a 30-degree viewing angle horizontally and vertically. We did this by combining our method with the existing FPHD-CGH colorization method.	https://dl.acm.org/doi/abs/10.1145/3550082.3564162	Ryota Koiso, Keisuke Nonaka, Tatsuya Kobayashi, Kyoji Matsushima
Color-Perception-Guided Display Power Reduction for Virtual Reality	"Battery life is an increasingly urgent challenge for today's untethered VR and AR devices. However, the power efficiency of head-mounted displays is naturally at odds with growing computational requirements driven by better resolution, refresh rate, and dynamic ranges, all of which reduce the sustained usage time of untethered AR/VR devices. For instance, the Oculus Quest 2, under a fully-charged battery, can sustain only 2 to 3 hours of operation time. Prior display power reduction techniques mostly target smartphone displays. Directly applying smartphone display power reduction techniques, however, degrades the visual perception in AR/VR with noticeable artifacts. For instance, the ""power-saving mode"" on smartphones lowers the pixel luminance across the display and, as a result, presents an overall darkened visual perception to users if directly applied to VR content. Our key insight is that VR display power reduction must be cognizant of the gaze-contingent nature of high field-of-view VR displays. To that end, we present a gaze-contingent system that, without degrading luminance, minimizes the display power consumption while preserving high visual fidelity when users actively view immersive video sequences. This is enabled by constructing 1) a gaze-contingent color discrimination model through psychophysical studies, and 2) a display power model (with respect to pixel color) through real-device measurements. Critically, due to the careful design decisions made in constructing the two models, our algorithm is cast as a constrained optimization problem with a solution, which can be implemented as a real-time, image-space shader. We evaluate our system using a series of psychophysical studies and large-scale analyses on natural images. Experiment results show that our system reduces the display power by as much as 24% (14% on average) with little to no perceptual fidelity degradation."	https://dl.acm.org/doi/abs/10.1145/3550454.3555473	Budmonde Duinkharjav, Kenneth Chen, Abhishek Tyagi, Jiayi He, Yuhao Zhu, Qi Sun
Combining Augmented and Virtual Reality Experiences for Immersive Fire Drills	We propose an application that combines the benefits of augmented reality (AR) and virtual reality (VR) experiences for realistic and effective fire response training. Our analysis of recent literature reveals that there are different advantages to AR versus VR applications, and combining these experiences can generate new compound benefits in fire drills. In this study, a high-level training environment is automatically generated using three-dimensional object recognition technology, and the physical training tools are linked to the VR environment to improve immersion and training effectiveness. The combined advanced AR and VR experience demonstrates the potential use of this method in various industries.	https://dl.acm.org/doi/abs/10.1145/3550082.3564195	Ho-San Kang, Jong-Won Lee, Soo-Mi Choi
Combining GPU Tracing Methods within a Single Ray Query	A recent trend in real-time rendering is the utilization of the new hardware ray tracing capabilities. Often, usage of a distance field representation is proposed as an alternative when hardware ray tracing is deemed too costly, and the two are seen as competing approaches. In this work, we show that both approaches can work together effectively for a single ray query on modern hardware. We choose to use hardware ray tracing where precision is most important, while avoiding its heavy cost by using a distance field when possible. While a simple approach, in our experiments the resulting tracing algorithm overcomes the associated overhead and allows a user-defined middle ground between the performance of distance field traversal and the improved visual quality of hardware ray tracing.	https://dl.acm.org/doi/abs/10.1145/3550340.3564231	Pieterjan Bartels, Takahiro Harada
Compressing Geodesic Information for Fast Point-to-Point Geodesic Distance Queries	Geodesic distances between pairs of points on a 3D mesh surface are a crucial ingredient of many geometry processing tasks, but are notoriously difficult to compute efficiently on demand. We propose a novel method for the compact storage of geodesic distance information, which enables answering point-to-point geodesic distance queries very efficiently. For a triangle mesh with n vertices, if computing the geodesic distance to all vertices from a single source vertex costs O(f(n)) time, then we generate a database of size O(mnlog n) in time in a preprocessing stage, where m is a constant that depends on the geometric complexity of the surface. We achieve this by computing a nested bisection of the mesh surface using separator curves and storing compactly-described functions approximating the distances between each mesh vertex and a small relevant subset of these curves. Using this database, the geodesic distance between two mesh vertices can then be approximated well by solving a small number of simple univariate minimization problems in O(mlog n) worst case time and O(m) average case time. Our method provides an excellent tradeoff between the size of the database, query runtime, and accuracy of the result. It can be used to compress exact or approximate geodesic distances, e.g., those obtained by VTP (exact), fast DGG, fast marching, or the heat method (approximate) and is very efficient if f(n) = n, as for the fast DGG method.	https://dl.acm.org/doi/abs/10.1145/3550469.3555412	Craig Gotsman, Kai Hormann
Computational Alternative Photographic Process toward Sustainable Printing	We propose a computational alternative photographic process that integrates computer processing with the conventional printing method, particularly cyanotype. Cyanotype is a non-silver-halide process that has a lower environmental impact and is known for its availability to produce tri-color prints; however, the tri-color process is complex and time-consuming. To simplify this heavy printing process, our framework provides a user interface for image editing and optimization based on color simulation within a printable color gamut. We demonstrate image editing and tri-color cyanotype printing using our framework, indicating that it can reduce the number of user trials and errors.	https://dl.acm.org/doi/abs/10.1145/3550340.3564219	Chinatsu Ozawa, Kenta Yamamoto, Kazuya Izumi, Yoichi Ochiai
Computer Generated Hologram Optimization for Lens Aberration	We propose a lens aberration correction method for holographic displays via a light wave propagation simulation and optimization algorithm. Aberration correction is an important technology to obtain noise-less hologram images in holographic displays. We optimized phase holograms with an automatic differentiation technique and Adam optimizer, and aberration corrected images were achieved. Given an aberrated lens with a focal length of 20mm, the optimized holographic image has a PSNR value of 32.	https://dl.acm.org/doi/abs/10.1145/3550082.3564191	Kazushi Nakamura, Kenta Yamamoto, Yoichi Ochiai
Computing Medial Axis Transform with Feature Preservation via Restricted Power Diagram	We propose a novel framework for computing the medial axis transform of 3D shapes while preserving their via (RPD). Medial features, including such as the sharp edges and corners of the input mesh surface and such as the seams and junctions of medial axis, are important shape descriptors both topologically and geometrically. However, existing medial axis approximation methods fail to capture and preserve them due to the fundamentally under-sampling in the vicinity of medial features, and the difficulty to build their correct connections. In this paper we use the RPD of medial spheres and its affiliated structures to help solve these challenges. The dual structure of RPD provides the connectivity of medial spheres. The surfacic (RPC) of each medial sphere provides the tangential surface regions that these spheres have contact with. The connected components (CC) of surfacic RPC give us the classification of each sphere, to be on a medial sheet, a seam, or a junction. They allow us to detect insufficient sphere sampling around medial features and develop necessary conditions to preserve them. Using this RPD-based framework, we are able to construct high quality medial meshes with features preserved. Compared with existing sampling-based or voxel-based methods, our method is the first one that can preserve not only external features but also internal features of medial axes.	https://dl.acm.org/doi/abs/10.1145/3550454.3555465	Ningna Wang, Bin Wang, Wenping Wang, Xiaohu Guo
Constant Time Median Filter Using 2D Wavelet Matrix	The median filter is a simple yet powerful noise reduction technique that is extensively applied in image, signal, and speech processing. It can effectively remove impulsive noise while preserving the content of the image by taking the median of neighboring pixels; thus, it has various applications, such as restoration of a damaged image and facial beautification. The median filter is typically implemented in one of two major approaches: the histogram-based method, which requires (1) computation time per pixel when focusing on the kernel radius , and the sorting-based method, which requires approximately ( ) computation time per pixel but has a light constant factor. These are used differently depending on the kernel radius and the number of bits in the image. However, the computation time is still slow, particularly when the kernel radius is in the mid to large range. This paper introduces novel and efficient median filter with constant complexity (1) for kernel size using the wavelet matrix data structure, which has been applied to query-based searches on one-dimensional data. We extended the original wavelet matrix to two-dimensional data for application to computer graphics problems. The objective of this study was to achieve high-speed median filter computation in parallel computing environment with many threads (i.e., GPUs). Our implementation for the GPU is an order of magnitude faster than the histogram method for 8-bit images. Unlike traditional histogram methods, which suffer from significant computational overhead, the proposed method can handle images with high pixel depth (e.g., 16- and 32-bit high dynamic range images). When the kernel radius is greater than 12 for 8-bit images, the proposed method outperforms the other median filter computation methods.	https://dl.acm.org/doi/abs/10.1145/3550454.3555512	Yuji Moroto, Nobuyuki Umetani
Continuous deformation based panelization for design rationalization	Design rationalization is the process of simplifying a 3D shape to enable cost-efficient manufacturing. A common approach is to approximate the input shape by a collection of simple units, such as flat or spherical panels, that are easy to manufacture and simple to assemble. This panelization process typically involves a segmentation step, with each surface patch intended to be replaced by a single unit, followed by an approximation stage, where the final shapes and locations of the units are determined. While optimal panel parameters for given segments are readily determined, the discrete nature of segmentation—assigning surface elements to segments—prevents a continuous design optimization workflow. In this work, we propose a differentiable reformulation of panelization that enables its use in gradient-based design optimization. Our approach is to treat panelization as a smooth optimization problem, whose objective function encourages the surface to locally deform towards best-matching units. This formulation enables a fully-differentiable rationalization process with implicit segmentation in which panels emerge automatically. We integrate our rationalization process in a simple user interface allowing the designer to guide the optimization towards desired panelizations. We demonstrate the potential of our approach on a diverse set of complex shapes and different panel types.	https://dl.acm.org/doi/abs/10.1145/3550469.3555414	Elias Jadon, Bernhard Thomaszewski, Aleksandra Anna Apolinarska, Roi Poranne
ControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters	In this paper, we introduce ControlVAE, a novel model-based framework for learning generative motion control policies based on variational autoencoders (VAE). Our framework can learn a rich and flexible latent representation of skills and a skill-conditioned generative control policy from a diverse set of unorganized motion sequences, which enables the generation of realistic human behaviors by sampling in the latent space and allows high-level control policies to reuse the learned skills to accomplish a variety of downstream tasks. In the training of ControlVAE, we employ a learnable world model to realize direct supervision of the latent space and the control policy. This world model effectively captures the unknown dynamics of the simulation system, enabling efficient model-based learning of high-level downstream tasks. We also learn a state-conditional prior distribution in the VAE-based generative control policy, which generates a skill embedding that outperforms the non-conditional priors in downstream tasks. We demonstrate the effectiveness of ControlVAE using a diverse set of tasks, which allows realistic and interactive control of the simulated characters.	https://dl.acm.org/doi/abs/10.1145/3550454.3555434	Heyuan Yao, Zhenhua Song, Baoquan Chen, Libin Liu
Creating Intimacy and Connection Through Live Theater in VR	Looking at how we create an environment of intimacy and trust in an intimidating environment like VR. Using techniques from immersive theater, improv and stage, we are able to have audiences from around the world, engage in vulnerable discourse and experience emotional journeys and connection in a strictly virtual world.	https://dl.acm.org/doi/abs/10.1145/3550453.3586017	Whitton Anne Frank, Brendan Andolsek Bradley
Crush On	On a desolate street in a long-neglected city. The moment of silence was broken after a runaway train crashed into a derelict apartment. Numerous ladies rushed out of the train, filter through the apartment, and break into every room unscrupulously. They interrupt the men who are alone in each room, which made the men overwhelmed and losing the ability to express their thoughts. The different rooms in the apartment symbolize the different feelings of the man, and its shapes construct the profile of the man. Back to the scene of reality, the man is deeply smitten with the lady. P.S. At the moment I saw you, it is as though a train crash into my mind that I got a deranged mind.	https://dl.acm.org/doi/abs/10.1145/3550339.3554645	Shu-Yu Chu
Curl-Flow: Boundary-Respecting Pointwise Incompressible Velocity Interpolation for Grid-Based Fluids	We propose to augment standard grid-based fluid solvers with divergence-free velocity interpolation, thereby ensuring exact incompressibility down to the sub-cell level. Our method takes as input a discretely divergence-free velocity field generated by a staggered grid pressure projection, and first recovers a corresponding discrete vector potential. Instead of solving a costly Poisson problem for the potential, we develop a fast parallel sweeping strategy to find a candidate potential and apply a gauge transformation to enforce the Coulomb gauge condition and thereby make it numerically smooth. Interpolating this discrete potential generates a point-wise vector potential whose analytical curl is a pointwise incompressible velocity field. Our method further supports irregular solid geometry through the use of level set-based cut-cells and a novel Curl-Noise-inspired potential ramping procedure that simultaneously offers strictly non-penetrating velocities and incompressibility. Experimental comparisons demonstrate that the vector potential reconstruction procedure at the heart of our approach is consistently faster than prior such reconstruction schemes, especially those that solve vector Poisson problems. Moreover, in exchange for its modest extra cost, our overall framework produces significantly improved particle trajectories that closely respect irregular obstacles, do not suffer from spurious sources or sinks, and yield superior particle distributions over time.	https://dl.acm.org/doi/abs/10.1145/3550454.3555498	Jumyung Chang, Ruben Partono, Vinicius C. Azevedo, Christopher Batty
Declarative Specification for Unstructured Mesh Editing Algorithms	We introduce a novel approach to describe mesh generation, mesh adaptation, and geometric modeling algorithms relying on changing mesh connectivity using a high-level abstraction. The main motivation is to enable easy customization and development of these algorithms via a declarative specification consisting of a set of per-element invariants, operation scheduling, and attribute transfer for each editing operation. We demonstrate that widely used algorithms editing surfaces and volumes can be compactly expressed with our abstraction, and their implementation within our framework is simple, automatically parallelizable on shared-memory architectures, and with guaranteed satisfaction of the prescribed invariants. These algorithms are readable and easy to customize for specific use cases. We introduce a software library implementing this abstraction and providing automatic shared-memory parallelization.	https://dl.acm.org/doi/abs/10.1145/3550454.3555513	Zhongshi Jiang, Jiacheng Dai, Yixin Hu, Yunfan Zhou, Jeremie Dumas, Qingnan Zhou, Gurkirat Singh Bajwa, Denis Zorin, Daniele Panozzo, Teseo Schneider
Deep Adaptive Sampling and Reconstruction Using Analytic Distributions	We propose an adaptive sampling and reconstruction method for offline Monte Carlo rendering. Our method produces sampling maps constrained by a user-defined budget that minimize the expected future denoising error. Compared to other state-of-the-art methods, which produce the necessary training data on the fly by composing pre-rendered images, our method samples from analytic noise distributions instead. These distributions are compact and closely approximate the pixel value distributions stemming from Monte Carlo rendering. Our method can efficiently sample training data by leveraging only a few per-pixel statistics of the target distribution, which provides several benefits over the current state of the art. Most notably, our analytic distributions' modeling accuracy and sampling efficiency increase with sample count, essential for high-quality offline rendering. Although our distributions are approximate, our method supports joint end-to-end training of the sampling and denoising networks. Finally, we propose the addition of a global summary module to our architecture that accumulates valuable information from image regions outside of the network's receptive field. This information discourages sub-optimal decisions based on local information. Our evaluation against other state-of-the-art neural sampling methods demonstrates denoising quality and data efficiency improvements.	https://dl.acm.org/doi/abs/10.1145/3550454.3555515	Farnood Salehi, Marco Manzi, Gerhard Roethlin, Romann Weber, Christopher Schroers, Marios Papas
DeepJoin: Learning a Joint Occupancy, Signed Distance, and Normal Field Function for Shape Repair	We introduce DeepJoin, an automated approach to generate high-resolution repairs for fractured shapes using deep neural networks. Existing approaches to perform automated shape repair operate exclusively on symmetric objects, require a complete proxy shape, or predict restoration shapes using low-resolution voxels which are too coarse for physical repair. We generate a high-resolution restoration shape by inferring a corresponding complete shape and a break surface from an input fractured shape. We present a novel implicit shape representation for fractured shape repair that combines the occupancy function, signed distance function, and normal field. We demonstrate repairs using our approach for synthetically fractured objects from ShapeNet, 3D scans from the Google Scanned Objects dataset, objects in the style of ancient Greek pottery from the QP Cultural Heritage dataset, and real fractured objects. We outperform six baseline approaches in terms of chamfer distance and normal consistency. Unlike existing approaches and restorations generated using subtraction, DeepJoin restorations do not exhibit surface artifacts and join closely to the fractured region of the fractured shape. Our code is available at: https://github.com/Terascale-All-sensing-Research-Studio/DeepJoin.	https://dl.acm.org/doi/abs/10.1145/3550454.3555470	Nikolas Lamb, Sean Banerjee, Natasha Kholgade Banerjee
DeepMVSHair: Deep Hair Modeling from Sparse Views	We present DeepMVSHair, the first deep learning-based method for multi-view hair strand reconstruction. The key component of our pipeline is HairMVSNet, a differentiable neural architecture which represents a spatial hair structure as a continuous 3D hair growing direction field implicitly. Specifically, given a 3D query point, we decide its occupancy value and direction from observed 2D structure features. With the query point's pixel-aligned features from each input view, we utilize a view-aware transformer encoder to aggregate anisotropic structure features to an integrated representation, which is decoded to yield 3D occupancy and direction at the query point. HairMVSNet effectively gathers multi-view hair structure features and preserves high-frequency details based on this implicit representation. Guided by HairMVSNet, our hair-growing algorithm produces results faithful to input multi-view images. We propose a novel image-guided multi-view strand deformation algorithm to enrich modeling details further. Extensive experiments show that the results by our sparse-view method are comparable to those by state-of-the-art dense multi-view methods and significantly better than those by single-view and sparse-view methods. In addition, our method is an order of magnitude faster than previous multi-view hair modeling methods.	https://dl.acm.org/doi/abs/10.1145/3550469.3555385	Zhiyi Kuang, Yiyang Chen, Hongbo Fu, Kun Zhou, Youyi Zheng
Demonstrating Parallel Adaptation: How Switching between Two Virtual Bodies with Different Perspectives Enables Dual Motor Adaptation	We demonstrate a dual-motor adaptation over 2 virtual bodies with opposite visuomotor rotation (+ 15° and − 15°). A related study explored the effect of view perspective (1st person view perspective, 1pp, and 3rd person view perspective, 3pp) on a dual motor adaptation behavior. They showed that (1) in 1pp, the adaptation is explicit, while in 3pp, the adaptation is implicit, and (2) the implicit adaptation led to a better dual adaptation. Here we let the participants experiment with explicit and implicit dual motor adaptation by doing a reaching task in 1pp and then in 3pp. We present an ad-hoc motor adaptation score and discuss which adaptation behavior led to a better dual adaptation.	https://dl.acm.org/doi/abs/10.1145/3550472.3558416	Adrien Verhulst, Yasuko Namikawa, Shunichi Kasahara
Demonstration of PseudoJumpOn: Repetitive Step-up Jump in Virtual Reality	PseudoJumpOn is a novel locomotion technique that allows step-up jumping movements using actual jumping motions on a flat floor with a common virtual reality (VR) setup. In this demo, we introduce an installation utilizing PseudoJumpOn where the player repeatedly makes step-up jumps to climb up to higher platforms. The aim is to provide players with the experience of richer vertical locomotion in VR without causing them to feel unnaturally manipulated, even though they know there are no physical steps in reality. While jumping, the system applies two types of viewpoint manipulation (i.e., gain manipulation and peak shifting) based on the user's physical jumping height and the height of the jumping target. This calculation previously assumed that the height of the jumping target was known, but we improved this by predicting it using the user's facing direction when taking off.	https://dl.acm.org/doi/abs/10.1145/3550472.3558412	Kumpei Ogawa, Kazuyuki Fujita, Kazuki Takashima, Yoshifumi Kitamura
Demonstration of RedirectedDoors: Manipulating User's Orientation while Opening Doors in Virtual Reality	We present an installation demonstrating the applicability of RedirectedDoors, a redirection technique that occasionally manipulates the user's orientation during door-opening motions. In this demo, the player explores an indoor virtual environment containing doors while wearing a head-mounted display (HMD), and their orientation in reality is manipulated as a function of the door's opening angle. In addition, when the player opens the door by pushing or pulling the doorknob in virtual reality, the corresponding passive haptic feedback is provided by the self-actuated doorknob-type prop. When reaching the goal, they can see the manipulation results by comparing their virtual position with a real landmark position. Consequently, this demo both makes the player's experience more realistic and presents the virtual environment in a comparatively small physical space.	https://dl.acm.org/doi/abs/10.1145/3550472.3558405	Yukai Hoshikawa, Kazuyuki Fujita, Kazuki Takashima, Morten Fjeld, Yoshifumi Kitamura
Depth of Field Aware Differentiable Rendering	Cameras with a finite aperture diameter exhibit defocus for scene elements that are not at the focus distance, and have only a limited within which objects appear acceptably sharp. In this work we address the problem of applying inverse rendering techniques to input data that exhibits such defocus blurring. We present differentiable depth-of-field rendering techniques that are applicable to both rasterization-based methods using mesh representations, as well as ray-marching-based methods using either explicit [Yu et al. 2021] or implicit volumetric radiance fields [Mildenhall et al. 2020]. Our approach learns significantly sharper scene reconstructions on data containing blur due to depth of field, and recovers aperture and focus distance parameters that result in plausible forward-rendered images. We show applications to macro photography, where typical lens configurations result in a very narrow depth of field, and to multi-camera video capture, where maintaining sharp focus across a large capture volume for a moving subject is difficult.	https://dl.acm.org/doi/abs/10.1145/3550454.3555521	Stanislav Pidhorskyi, Timur Bagautdinov, Shugao Ma, Jason Saragih, Gabriel Schwartz, Yaser Sheikh, Tomas Simon
DifferSketching: How Differently Do People Sketch 3D Objects?	Multiple sketch datasets have been proposed to understand how people draw 3D objects. However, such datasets are often of small scale and cover a small set of objects or categories. In addition, these datasets contain freehand sketches mostly from expert users, making it difficult to compare the drawings by expert and novice users, while such comparisons are critical in informing more effective sketch-based interfaces for either user groups. These observations motivate us to analyze how differently people with and without adequate drawing skills sketch 3D objects. We invited 70 novice users and 38 expert users to sketch 136 3D objects, which were presented as 362 images rendered from multiple views. This leads to a new dataset of 3,620 freehand multi-view sketches, which are registered with their corresponding 3D objects under certain views. Our dataset is an order of magnitude larger than the existing datasets. We analyze the collected data at three levels, i.e., sketch-level, stroke-level, and pixel-level, under both spatial and temporal characteristics, and within and across groups of creators. We found that the drawings by professionals and novices show significant differences at stroke-level, both intrinsically and extrinsically. We demonstrate the usefulness of our dataset in two applications: (i) freehand-style sketch synthesis, and (ii) posing it as a potential benchmark for sketch-based 3D reconstruction. Our dataset and code are available at https://chufengxiao.github.io/DifferSketching/.	https://dl.acm.org/doi/abs/10.1145/3550454.3555493	Chufeng Xiao, Wanchao Su, Jing Liao, Zhouhui Lian, Yi-Zhe Song, Hongbo Fu
Differentiable Hybrid Traffic Simulation	We introduce a novel , which simulates traffic using a hybrid model of both macroscopic and microscopic models and can be directly This is the first differentiable traffic simulator for macroscopic and hybrid models that can compute gradients for traffic states across time steps and inhomogeneous lanes. To compute the gradient flow between two types of traffic models in a hybrid framework, we present a novel intermediate conversion component that bridges the lanes in a differentiable manner as well. We also show that we can use analytical gradients to accelerate the overall process and enhance scalability. Thanks to these gradients, our simulator can provide more efficient and scalable solutions for complex learning and control problems posed in traffic engineering than other existing algorithms. Refer to https://sites.google.com/umd.edu/diff-hybrid-traffic-sim for our project.	https://dl.acm.org/doi/abs/10.1145/3550454.3555492	Sanghyun Son, Yi-Ling Qiao, Jason Sewall, Ming C. Lin
Differentiable Point-Based Radiance Fields for Efficient View Synthesis	We propose a differentiable rendering algorithm for efficient novel view synthesis. By departing from volume-based representations in favor of a learned point representation, we improve on existing methods more than an order of magnitude in memory and runtime, both in training and inference. The method begins with a uniformly-sampled random point cloud and learns per-point position and view-dependent appearance, using a differentiable splat-based renderer to train the model to reproduce a set of input training images with the given pose. Our method is up to 300 × faster than NeRF in both training and inference, with only a marginal sacrifice in quality, while using less than 10 MB of memory for a static scene. For dynamic scenes, our method trains two orders of magnitude faster than STNeRF and renders at a near interactive rate, while maintaining high image quality and temporal coherence even without imposing any temporal-coherency regularizers.	https://dl.acm.org/doi/abs/10.1145/3550469.3555413	Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, Felix Heide
Differentiable Rendering Using RGBXY Derivatives and Optimal Transport	Traditional differentiable rendering approaches are usually hard to converge in inverse rendering optimizations, especially when initial and target object locations are not so close. Inspired by Lagrangian fluid simulation, we present a novel differentiable rendering method to address this problem. We associate each screen-space pixel with the visible 3D geometric point covered by the center of the pixel and compute derivatives on geometric points rather than on pixels. We refer to the associated geometric points as point proxies of pixels. For each point proxy, we compute its 5D RGBXY derivatives which measures how its 3D RGB color and 2D projected screen-space position change with respect to scene parameters. Furthermore, in order to capture global and long-range object motions, we utilize optimal transport based pixel matching to design a more sophisticated loss function. We have conducted experiments to evaluate the effectiveness of our proposed method on various inverse rendering applications and have demonstrated superior convergence behavior compared to state-of-the-art baselines.	https://dl.acm.org/doi/abs/10.1145/3550454.3555479	Jiankai Xing, Fujun Luan, Ling-Qi Yan, Xuejun Hu, Houde Qian, Kun Xu
Differentiable Rendering of Neural SDFs through Reparameterization	We present a method to automatically compute correct gradients with respect to geometric scene parameters in neural SDF renderers. Recent physically-based differentiable rendering techniques for meshes have used edge-sampling to handle discontinuities, particularly at object silhouettes, but SDFs do not have a simple parametric form amenable to sampling. Instead, our approach builds on area-sampling techniques and develops a continuous warping function for SDFs to account for these discontinuities. Our method leverages the distance to surface encoded in an SDF and uses quadrature on sphere tracer points to compute this warping function. We further show that this can be done by subsampling the points to make the method tractable for neural SDFs. Our differentiable renderer can be used to optimize neural shapes from multi-view images and produces comparable 3D reconstructions to recent SDF-based inverse rendering methods, without the need for 2D segmentation masks to guide the geometry optimization and no volumetric approximations to the geometry.	https://dl.acm.org/doi/abs/10.1145/3550469.3555397	Sai Praveen Bangaru, Michael Gharbi, Fujun Luan, Tzu-Mao Li, Kalyan Sunkavalli, Milos Hasan, Sai Bi, Zexiang Xu, Gilbert Bernstein, Fredo Durand
Differentiable Simulation of Inertial Musculotendons	We propose a simple and practical approach for incorporating the effects of muscle inertia, which has been ignored by previous musculoskeletal simulators in both graphics and biomechanics. We approximate the inertia of the muscle by assuming that muscle mass is distributed along the centerline of the muscle. We express the motion of the musculotendons in terms of the motion of the skeletal joints using a chain of Jacobians, so that at the top level, only the reduced degrees of freedom of the skeleton are used to completely drive both bones and musculotendons. Our approach can handle all commonly used musculotendon path types, including those with multiple path points and wrapping surfaces. For muscle paths involving wrapping surfaces, we use neural networks to model the Jacobians, trained using existing wrapping surface libraries, which allows us to effectively handle the Jacobian discontinuities that occur when musculotendon paths collide with wrapping surfaces. We demonstrate support for higher-order time integrators, complex joints, inverse dynamics, Hill-type muscle models, and differentiability. In the limit, as the muscle mass is reduced to zero, our approach gracefully degrades to traditional simulators without support for muscle inertia. Finally, it is possible to mix and match inertial and non-inertial musculotendons, depending on the application.	https://dl.acm.org/doi/abs/10.1145/3550454.3555490	Ying Wang, Jasper Verheul, Sang-Hoon Yeo, Nima Khademi Kalantari, Shinjiro Sueda
Digital Being: Thinking of the Stars	"Everything is going digital in our lives. I am looking for an invisible, formless creature born out of a gap of radical change. I call it, ""Digital Being"". I recently discovered a new version, NFT creature that has settled on the abandoned TV over the internet."	https://dl.acm.org/doi/abs/10.1145/3550470.3558420	Taezoo Park
Direct acquisition of volumetric scattering phase function using speckle correlations	In material acquisition we want to infer the internal properties of materials from the way they scatter light. In particular, we are interested in measuring the phase function of the material, governing the amount of energy scattered towards different directions. This phase function has been shown to carry a lot of information about the type and size of particles dispersed in the medium, and is therefore essential for its characterization. Previous approaches to this task have relied on computationally costly inverse rendering optimization. Alternatively, if the material can be made optically thin enough so that most light paths scatter only once, this optimization can be avoided and the phase function can be directly read from the profile of light scattering at different angles. However, in many realistic applications, it is not easy to slice or dilute the material so that it is thin enough for such a single scattering model to hold. In this work we suggest a simple closed-form approach for acquiring material parameters from thick samples, avoiding costly optimization. Our approach is based on imaging the material of interest under coherent laser light and capturing speckle patterns. We show that memory-effect correlations between speckle patterns produced under nearby illumination directions provide a gating mechanism, allowing us to measure the singly scattered component of the light, even when observing thick samples where most light is scattered multiple times. We have built an experimental prototype capable of measuring phase functions over a narrow angular cone. We test the accuracy of our approach using validation materials whose ground truth phase function is known; and we use it to capture a set of everyday materials.	https://dl.acm.org/doi/abs/10.1145/3550469.3555379	Marina Alterman, Evgeniia Saiko, Anat Levin
Directing Tangible Controllers with Computer Vision and Beholder	We present Beholder, a computer vision (CV) toolkit for building tangible controllers for interactive computer systems. Beholder facilitates designers to build physical inputs that are instrumented with CV markers. By observing the properties of these markers, a CV system can detect physical interactions that occur. Beholder provides a software editor that enables designers to map CV marker behavior to keyboard events; thus connecting the CV-driven tangible controllers to any software that responds to keyboard input. We propose three design scenarios for Beholder—controllers to support everyday work, alternative controllers for games, and transforming physical therapy equipment into controllers to monitor patient progress.	https://dl.acm.org/doi/abs/10.1145/3550471.3564764	Peter Gyory, Krithik Ranjan, Zhen Zhou Yong, Clement Zheng, Ellen Yi-Luen Do
Discretization-Agnostic Deep Self-Supervised 3D Surface Parameterization	We present a novel self-supervised framework for learning the discretization-agnostic surface parameterization of arbitrary 3D objects with both bounded and unbounded surfaces. Our framework leverages diffusion-enabled global-to-local shape context for each vertex first to partition the unbounded surface into multiple patches using the proposed self-supervised PatchNet and subsequently perform independent UV parameterization of these patches by learning forward and backward UV mapping for individual patches. Thus, our framework enables learning a discretization-agnostic parameterization at a lower resolution and then directly inferring the parameterization for a higher-resolution mesh without retraining. We evaluate our framework on multiple 3D objects from the publicly available SHREC [Lian et al. 2011] dataset and report superior/faster UV parameterization over conventional methods.	https://dl.acm.org/doi/abs/10.1145/3550340.3564235	Chandradeep Pokhariya, Shanthika Naik, Astitva Srivastava, Avinash Sharma
Disentangled Image Colorization via Global Anchors	Colorization is multimodal by nature and challenges existing frameworks to achieve colorful and structurally consistent results. Even the sophisticated autoregressive model struggles to maintain long-distance color consistency due to the fragility of sequential dependence. To overcome this challenge, we propose a novel colorization framework that disentangles color multimodality and structure consistency through global color anchors, so that both aspects could be learned effectively. Our key insight is that several carefully located anchors could approximately represent the color distribution of an image, and conditioned on the anchor colors, we can predict the image color in a deterministic manner by utilizing internal correlation. To this end, we construct a colorization model with dual branches, where the color modeler predicts the color distribution for anchor color representation, and the color generator predicts the pixel colors by referring the sampled anchor colors. Importantly, the anchors are located under two principles: color independence and global coverage, which is realized with clustering analysis on the deep color features. To simplify the computation, we creatively adopt soft superpixel segmentation to reduce the image primitives, which still nicely reserves the reversibility to pixel-wise representation. Extensive experiments show that our method achieves notable superiority over various mainstream frameworks in perceptual quality. Thanks to anchor-based color representation, our model has the flexibility to support diverse and controllable colorization as well.	https://dl.acm.org/doi/abs/10.1145/3550454.3555432	Menghan Xia, Wenbo Hu, Tien-Tsin Wong, Jue Wang
Display Size and Targeting Performance: Small Hurts, Large May Help	Which display size helps gamers win? Recommendations from the research and PC gaming communities are contradictory. We find that as display size grows, targeting performance improves. When size increases from 13′′ to 26′′, targeting time drops by over 3%. Further size increases from 26′′ through 39′′, 52′′ and 65′′, bring more modest improvements, with targeting time dropping a further 1%. While such improvements may not be meaningful for novice gamers, they are extremely important to skilled and competitive players. To produce these results, 30 gamers participated in a targeting task as we varied display size by placing a display at varying distances. We held field of view constant by varying viewport size, and resolution constant by rendering to a fixed-size off-screen buffer. This paper offers further experimental detail, and examines likely explanations for the effects of display size.	https://dl.acm.org/doi/abs/10.1145/3550469.3555396	Joohwan Kim, Arjun Madhusudan, Benjamin Watson, Ben Boudaoud, Roland Tarrazo, Josef Spjut
Dr.3D: Adapting 3D GANs to Artistic Drawings	While 3D GANs have recently demonstrated the high-quality synthesis of multi-view consistent images and 3D shapes, they are mainly restricted to photo-realistic human portraits. This paper aims to extend 3D GANs to a different, but meaningful visual form: artistic portrait drawings. However, extending existing 3D GANs to drawings is challenging due to the inevitable geometric ambiguity present in drawings. To tackle this, we present Dr.3D, a novel adaptation approach that adapts an existing 3D GAN to artistic drawings. Dr.3D is equipped with three novel components to handle the geometric ambiguity: a deformation-aware 3D synthesis network, an alternating adaptation of pose estimation and image synthesis, and geometric priors. Experiments show that our approach can successfully adapt 3D GANs to drawings and enable multi-view consistent semantic editing of drawings.	https://dl.acm.org/doi/abs/10.1145/3550469.3555422	Wonjoon Jin, Nuri Ryu, Geonung Kim, Seung-Hwan Baek, Sunghyun Cho
Dressing Avatars: Deep Photorealistic Appearance for Physically Simulated Clothing	Despite recent progress in developing animatable full-body avatars, realistic modeling of clothing - one of the core aspects of human self-expression - remains an open challenge. State-of-the-art physical simulation methods can generate realistically behaving clothing geometry at interactive rates. Modeling photorealistic appearance, however, usually requires physically-based rendering which is too expensive for interactive applications. On the other hand, data-driven deep appearance models are capable of efficiently producing realistic appearance, but struggle at synthesizing geometry of highly dynamic clothing and handling challenging body-clothing configurations. To this end, we introduce pose-driven avatars with explicit modeling of clothing that exhibit both photorealistic appearance learned from real-world data and realistic clothing dynamics. The key idea is to introduce a neural clothing appearance model that operates on top of explicit geometry: at training time we use high-fidelity tracking, whereas at animation time we rely on physically simulated geometry. Our core contribution is a physically-inspired appearance network, capable of generating photorealistic appearance with view-dependent and dynamic shadowing effects even for unseen body-clothing configurations. We conduct a thorough evaluation of our model and demonstrate diverse animation results on several subjects and different types of clothing. Unlike previous work on photorealistic full-body avatars, our approach can produce much richer dynamics and more realistic deformations even for many examples of loose clothing. We also demonstrate that our formulation naturally allows clothing to be used with avatars of different people while staying fully animatable, thus enabling, for the first time, photorealistic avatars with novel clothing.	https://dl.acm.org/doi/abs/10.1145/3550454.3555456	Donglai Xiang, Timur Bagautdinov, Tuur Stuyck, Fabian Prada, Javier Romero, Weipeng Xu, Shunsuke Saito, Jingfan Guo, Breannan Smith, Takaaki Shiratori, Yaser Sheikh, Jessica Hodgins, Chenglei Wu
DynaGAN: Dynamic Few-shot Adaptation of GANs to Multiple Domains	Few-shot domain adaptation to multiple domains aims to learn a complex image distribution across multiple domains from a few training images. A naïve solution here is to train a separate model for each domain using few-shot domain adaptation methods. Unfortunately, this approach mandates linearly-scaled computational resources both in memory and computation time and, more importantly, such separate models cannot exploit the shared knowledge between target domains. In this paper, we propose DynaGAN, a novel few-shot domain-adaptation method for multiple target domains. DynaGAN has an adaptation module, which is a hyper-network that dynamically adapts a pretrained GAN model into the multiple target domains. Hence, we can fully exploit the shared knowledge across target domains and avoid the linearly-scaled computational requirements. As it is still computationally challenging to adapt a large-size GAN model, we design our adaptation module to be lightweight using the rank-1 tensor decomposition. Lastly, we propose a contrastive-adaptation loss suitable for multi-domain few-shot adaptation. We validate the effectiveness of our method through extensive qualitative and quantitative evaluations.	https://dl.acm.org/doi/abs/10.1145/3550469.3555416	Seongtae Kim, Kyoungkook Kang, Geonung Kim, Seung-Hwan Baek, Sunghyun Cho
Dynamic Ocean Explorer: Interactive XR Visualisation of Massive Volumetric Data for Ocean Science	Ocean Explorer is a head-mounted virtual and augmented reality application designed to help researchers quickly visualise and analyse massive volumetric ocean datasets. Using a simple gestural interface, researchers can directly manipulate ocean volumes with their hands, scrub through time, and interrogate the spatial dimensions of ocean phenomena with free-axis clipping.	https://dl.acm.org/doi/abs/10.1145/3550453.3586014	Dominic Branchaud, Philip Grimmett, Nagida Helsby-Clark, Emma Krantz, Viveka Weiley
E.S.P.: Extra-Sensory Puck in Air Hockey using the Projection-Based Illusion	E.S.P. (Extra-Sensory Puck) provides a new experience by introducing optical illusions to air hockey. The perception of a solid puck randomly hit by a player is altered to show various physics-defying appearances and motions to the player's naked eye. Such altered perceptions are based on our high-speed projector-camera system generating stimulation patterns onto the moving puck. This paper presents two demonstrations. The first is the invisible puck, where the hit puck is camouflaged to disappear on the table. The second is the altered motion, where the direction and speed are altered.	https://dl.acm.org/doi/abs/10.1145/3550471.3558397	Kengo Sato, Hiroki Terashima, Shin'ya Nishida, Yoshihiro Watanabe
Edmond and Lucy	Meet Edmond and Lucy with their adventures in the forest!	https://dl.acm.org/doi/abs/10.1145/3550339.3556054	François Narboux, Hanna Mouchez, Carine Napiot
Effects of Font Type and Weight on Reading in VR	This study focused on font type and weight and we examined their effects on the readability of long texts in VR. In the experiment, we changed the font weight of three font types, Gothic, Mincho and Antigothic, to evaluate the readability of text with a length of 600 characters. Our results showed a tendency for Antigothic to be more readable and less fatiguing than Mincho and Gothic.	https://dl.acm.org/doi/abs/10.1145/3550082.3564182	Seina Kobayashi, Kei Kanari, Mie Sato
Efficient Differentiation of Pixel Reconstruction Filters for Path-Space Differentiable Rendering	Pixel reconstruction filters play an important role in physics-based rendering and have been thoroughly studied. In physics-based differentiable rendering, however, the proper treatment of pixel filters remains largely under-explored. We present a new technique to efficiently differentiate pixel reconstruction filters based on the path-space formulation. Specifically, we formulate the pixel integral that models discontinuities in pixel filters and introduce new antithetic sampling methods that support differentiable path sampling methods, such as adjoint particle tracing and bidirectional path tracing. We demonstrate both the need and efficacy of antithetic sampling when estimating this integral, and we evaluate its effectiveness across several differentiable- and inverse-rendering settings.	https://dl.acm.org/doi/abs/10.1145/3550454.3555500	Zihan Yu, Cheng Zhang, Derek Nowrouzezahrai, Zhao Dong, Shuang Zhao
Efficient Drone Exploration in Real Unknown Environments	We propose an autonomous drone exploration system (ADES) with a lightweight and low-latency saliency prediction model to explore unknown environments. Recent studies have applied saliency prediction to drone exploration. However, these studies are not sufficiently mature. The ADES system proposes a smaller and faster saliency prediction model and adopts a novel drone exploration approach based on visual-inertial odometry (VIO) to solve the practical problems encountered during exploration, i.e., exploring salient objects without colliding with them and not repeatedly exploring salient objects. The system not only has a performance comparable to that of the state-of-the-art multiple-discontinuous-image saliency prediction network (TA-MSNet) but also enables drones to explore unknown environments more efficiently.	https://dl.acm.org/doi/abs/10.1145/3550082.3564205	Ming-Ru Xie, Shing-Yun Jung, Kuan-Wen Chen
Efficient Light Probes for Real-Time Global Illumination	Reproducing physically-based global illumination (GI) effects has been a long-standing demand for many real-time graphical applications. In pursuit of this goal, many recent engines resort to some form of light probes baked in a precomputation stage. Unfortunately, the GI effects stemming from the precomputed probes are rather limited due to the constraints in the probe storage, representation or query. In this paper, we propose a new method for probe-based GI rendering which can generate a wide range of GI effects, including glossy reflection with multiple bounces, in complex scenes. The key contributions behind our work include a gradient-based search algorithm and a neural image reconstruction method. The search algorithm is designed to reproject the probes' contents to any query viewpoint, without introducing parallax errors, and converges fast to the optimal solution. The neural image reconstruction method, based on a dedicated neural network and several G-buffers, tries to recover high-quality images from low-quality inputs due to limited resolution or (potential) low sampling rate of the probes. This neural method makes the generation of light probes efficient. Moreover, a temporal reprojection strategy and a temporal loss are employed to improve temporal stability for animation sequences. The whole pipeline runs in realtime (>30 frames per second) even for high-resolution (1920×1080) outputs, thanks to the fast convergence rate of the gradient-based search algorithm and a light-weight design of the neural network. Extensive experiments on multiple complex scenes have been conducted to show the superiority of our method over the state-of-the-arts.	https://dl.acm.org/doi/abs/10.1145/3550454.3555452	Jie Guo, Zijing Zong, Yadong Song, Xihao Fu, Chengzhi Tao, Yanwen Guo, Ling-Qi Yan
Efficient Neural Radiance Fields for Interactive Free-viewpoint Video	This paper aims to tackle the challenge of efficiently producing interactive free-viewpoint videos. Some recent works equip neural radiance fields with image encoders, enabling them to generalize across scenes. When processing dynamic scenes, they can simply treat each video frame as an individual scene and perform novel view synthesis to generate free-viewpoint videos. However, their rendering process is slow and cannot support interactive applications. A major factor is that they sample lots of points in empty space when inferring radiance fields. We propose a novel scene representation, called ENeRF, for the fast creation of interactive free-viewpoint videos. Specifically, given multi-view images at one frame, we first build the cascade cost volume to predict the coarse geometry of the scene. The coarse geometry allows us to sample few points near the scene surface, thereby significantly improving the rendering speed. This process is fully differentiable, enabling us to jointly learn the depth prediction and radiance field networks from RGB images. Experiments on multiple benchmarks show that our approach exhibits competitive performance while being at least 60 times faster than previous generalizable radiance field methods.	https://dl.acm.org/doi/abs/10.1145/3550469.3555376	Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, Xiaowei Zhou
Efficient Neural Style Transfer for Volumetric Simulations	Artistically controlling fluids has always been a challenging task. Recently, volumetric Neural Style Transfer (NST) techniques have been used to artistically manipulate smoke simulation data with 2D images. In this work, we revisit previous volumetric NST techniques for smoke, proposing a suite of upgrades that enable stylizations that are significantly faster, simpler, more controllable and less prone to artifacts. Moreover, the energy minimization solved by previous methods is camera dependent. To avoid that, a computationally expensive iterative optimization performed for multiple views sampled around the original simulation is needed, which can take up to several minutes per frame. We propose a simple feed-forward neural network architecture that is able to infer view-independent stylizations that are three orders of the magnitude faster than its optimization-based counterpart.	https://dl.acm.org/doi/abs/10.1145/3550454.3555517	Joshua Aurand, Raphael Ortiz, Silvia Nauer, Vinicius C. Azevedo
ElastoMonolith: A Monolithic Optimization-Based Liquid Solver for Contact-Aware Elastic-Solid Coupling	Simultaneous coupling of diverse physical systems poses significant computational challenges in terms of speed, quality, and stability. Rather than treating all components with a single discretization methodology (e.g., smoothed particles, material point method, Eulerian grid, etc.) that is ill-suited to some components, our solver, , addresses three-way interactions among standard particle-in-cell-based viscous and inviscid fluids, Lagrangian mesh-based deformable bodies, and rigid bodies. While prior methods often treat some terms explicitly or in a decoupled fashion for efficiency, often at the cost of robustness or stability, we demonstrate the effectiveness of a strong coupling approach that expresses all of the relevant physics within one consistent and unified optimization problem, including fluid pressure and viscosity, elasticity of the deformables, frictional solid-solid contact, and solid-fluid interface conditions. We further develop a numerical solver to tackle this difficult optimization problem, incorporating projected Newton, an active set method, and a transformation of the inner linear system matrix to ensure symmetric positive definiteness. Our experimental evaluations show that our framework can achieve high quality coupling results that avoid artifacts such as volume loss, instability, sticky contacts, and spurious interpenetrations.	https://dl.acm.org/doi/abs/10.1145/3550454.3555474	Tetsuya Takahashi, Christopher Batty
Exact 3D Path Generation via 3D Cam-Linkage Mechanisms	is a fundamental problem of designing a mechanism to make a point move along a prescribed , driven by a single actuator. Existing mechanisms are insufficient to address this problem. Planar linkages and their combinations with gears and/or plate cams can only generate 2D paths while 1-DOF spatial linkages can only generate 3D paths with rather simple shapes. In this paper, we present a new 3D cam-linkage mechanism, consisting of two 3D cams and five links, for exactly generating a continuous 3D path. To design a 3D cam-linkage mechanism, we first model a 3-DOF 5-bar spatial linkage to exactly generate a prescribed 3D path and then reduce the spatial linkage's DOFs from 3 to 1 by composing the linkage with two 3D cam-follower mechanisms. Our computational approach optimizes the 3D cam-linkage mechanism's topology and geometry to minimize the mechanism's total weight while ensuring smooth, collision-free, and singularity-free motion. We show that our 3D cam-linkage mechanism is able to exactly generate a continuous 3D path with arbitrary shape and a finite number of points, evaluate the mechanism's kinematic performance with 3D printed prototypes, and demonstrate that the mechanism can be generalized for exact 3D motion generation.	https://dl.acm.org/doi/abs/10.1145/3550454.3555431	Yingjie Cheng, Peng Song, Yukun Lu, Wen Jie Jeremy Chew, Ligang Liu
FDNeRF: Few-shot Dynamic Neural Radiance Fields for Face Reconstruction and Expression Editing	We propose a Few-shot Dynamic Neural Radiance Field (FDNeRF), the first NeRF-based method capable of reconstruction and expression editing of 3D faces based on a small number of dynamic images. Unlike existing dynamic NeRFs that require dense images as input and can only be modeled for a single identity, our method enables face reconstruction across different persons with few-shot inputs. Compared to state-of-the-art few-shot NeRFs designed for modeling static scenes, the proposed FDNeRF accepts view-inconsistent dynamic inputs and supports arbitrary facial expression editing, i.e., producing faces with novel expressions beyond the input ones. To handle the inconsistencies between dynamic inputs, we introduce a well-designed conditional feature warping (CFW) module to perform expression conditioned warping in 2D feature space, which is also identity adaptive and 3D constrained. As a result, features of different expressions are transformed into the target ones. We then construct a radiance field based on these view-consistent features and use volumetric rendering to synthesize novel views of the modeled faces. Extensive experiments with quantitative and qualitative evaluation demonstrate that our method outperforms existing dynamic and few-shot NeRFs on both 3D face reconstruction and expression editing tasks. Code is available at https://fdnerf.github.io .	https://dl.acm.org/doi/abs/10.1145/3550469.3555404	Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao
Fast Dynamic Radiance Fields with Time-Aware Neural Voxels	Neural radiance fields (NeRF) have shown great success in modeling 3D scenes and synthesizing novel-view images. However, most previous NeRF methods take much time to optimize one single scene. Explicit data structures, e.g. voxel features, show great potential to accelerate the training process. However, voxel features face two big challenges to be applied to dynamic scenes, i.e. modeling temporal information and capturing different scales of point motions. We propose a radiance field framework by representing scenes with time-aware voxel features, named as TiNeuVox. A tiny coordinate deformation network is introduced to model coarse motion trajectories and temporal information is further enhanced in the radiance network. A multi-distance interpolation method is proposed and applied on voxel features to model both small and large motions. Our framework significantly accelerates the optimization of dynamic radiance fields while maintaining high rendering quality. Empirical evaluation is performed on both synthetic and real scenes. Our TiNeuVox completes training with only 8 minutes and 8-MB storage cost while showing similar or even better rendering performance than previous dynamic NeRF methods. Code is available at https://github.com/hustvl/TiNeuVox.	https://dl.acm.org/doi/abs/10.1145/3550469.3555383	Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, Qi Tian
Fast Editing of Singularities in Field-Aligned Stripe Patterns	Field-aligned parametrization is a method that maps a scalar function onto a surface, such that the gradient vector of the scalar function matches the input vector field. Using this idea, one can produce a stripe pattern that is convenient for various purposes such as remeshing, texture synthesis, and computational fabrication. In the final outcome, the positions of singularities (i.e., bifurcations of the stripe pattern) are essential for functionalities, manufacturability, or aesthetics. In this paper, we propose an algorithm to allow users to interactively edit the singularity positions of field-aligned stripe patterns. The algorithm computes a stripe pattern from a prescribed set of singularities, without generating any unwanted singularities. The solution of the algorithm is formulated as the global minima of a constrained quadratic optimization, whose computation speed is dominated by solving only two sparse linear systems. Furthermore, once the two matrices in the two linear systems are factorized, any update on singularity positions operates in linear time. We showcase several applications feasible with our fast yet simple algorithm.	https://dl.acm.org/doi/abs/10.1145/3550469.3555387	Yuta Noma, Nobuyuki Umetani, Yoshihiro Kawahara
Fast Octree Neighborhood Search for SPH Simulations	We present a new octree-based neighborhood search method for SPH simulation. A speedup of up to 1.9x is observed in comparison to state-of-the-art methods which rely on uniform grids. While our method focuses on maximizing performance in fixed-radius SPH simulations, we show that it can also be used in scenarios where the particle support radius is not constant thanks to the adaptive nature of the octree acceleration structure. Neighborhood search methods typically consist of an acceleration structure that prunes the space of possible particle neighbor pairs, followed by direct distance comparisons between the remaining particle pairs. Previous works have focused on minimizing the number of comparisons. However, in an effort to minimize the actual computation time, we find that distance comparisons exhibit very high throughput on modern CPUs. By permitting more comparisons than strictly necessary, the time spent on preparing and searching the acceleration structure can be reduced, yielding a net positive speedup. The choice of an octree acceleration structure, instead of the uniform grid typically used in fixed-radius methods, ensures balanced computational tasks. This benefits both parallelism and provides consistently high computational intensity for the distance comparisons. We present a detailed account of high-level considerations that, together with low-level decisions, enable high throughput for performance-critical parts of the algorithm. Finally, we demonstrate the high performance of our algorithm on a number of large-scale fixed-radius SPH benchmarks and show in experiments with a support radius ratio up to 3 that our method is also effective in multi-resolution SPH simulations.	https://dl.acm.org/doi/abs/10.1145/3550454.3555523	José Antonio Fernández-Fernández, Lukas Westhofen, Fabian Löschner, Stefan Rhys Jeske, Andreas Longva, Jan Bender
Fast Stabilization of Inducible Magnet Simulation	This paper presents a novel method for simulating inducible rigid magnets efficiently and stably. In the proposed method, inducible magnets are magnetized by a modified magnetization dynamics, so that the magnetic equilibrium can be obtained in a computationally efficient manner. Furthermore, our model of magnetic forces takes magnetization change into account to produce stable motions of inducible magnets. The experiments show that the proposed method enables a large-scale simulation involving a huge number of inducible magnets.	https://dl.acm.org/doi/abs/10.1145/3550469.3555410	Seung-wook Kim, JungHyun Han
Ferme les Yeux	A heavy secret and oblivious parents.	https://dl.acm.org/doi/abs/10.1145/3550339.3556383	Manon Bérardengo, Audrey Defonte, Léo Depoix, Denis Koessler, Clémentine Laurent, Pierre Guislain, Chloé Boursier, Carlos De Carvalho
Fiesta Blood	An avenging piñata fights back against birthday party piñata murder.	https://dl.acm.org/doi/abs/10.1145/3550339.3557003	Elena Fazio
FloRen: Real-time High-quality Human Performance Rendering via Appearance Flow Using Sparse RGB Cameras	We propose FloRen, a novel system for real-time, high-resolution free-view human synthesis. Our system runs at 15fps in 1K resolution with very sparse RGB cameras. In FloRen, a coarse-level implicit geometry is recovered at first as initialization, and then processed by a neural rendering framework based on appearance flow. Our appearance flow-based rendering framework consists of three steps, namely view-dependent depth refinement, appearance flow estimation and occlusion-aware color rendering. In this way, we resolve the view synthesis problem in the image plane, where 2D convolutional neural networks can be efficiently applied, contributing to high speed performance. For robust appearance flow estimation, we explicitly combine data-driven human prior knowledge with multiview geometric constraints. The accurate appearance flow enables precise color mapping from input view to novel view, which greatly facilitates high-resolution novel view generation. We demonstrate that our system achieves state-of-the-art performance and even outperforms many offline methods.	https://dl.acm.org/doi/abs/10.1145/3550469.3555409	Ruizhi Shao, Liliang Chen, Zerong Zheng, Hongwen Zhang, Yuxiang Zhang, Han Huang, Yandong Guo, Yebin Liu
Floagent: Interaction with Mid-Air Image via Hidden Sensors	This paper proposes Floagent as a human-computer interaction system that displays images in mid-air using infrared light reflected by a hot mirror. Floagent is an interaction system that allows users to focus on mid-air images without being aware of the sensors. By combining a hot mirror and a retroreflective transmissive optical element, Floagent conceals the camera from the user without affecting the mid-air image. We investigated the touch input interactions accuracy with mid-air images to evaluate the proposed system. The results show that the proposed system can effectively measure user input. Floagent enables an interaction design with a hidden sensor in which mid-air images appear to respond spontaneously to a wide variety of interaction events.	https://dl.acm.org/doi/abs/10.1145/3550471.3558398	Shohei Ando, Naoya Koizumi
Fluidic Topology Optimization with an Anisotropic Mixture Model	Fluidic devices are crucial components in many industrial applications involving fluid mechanics. Computational design of a high-performance fluidic system faces multifaceted challenges regarding its geometric representation and physical accuracy. We present a novel topology optimization method to design fluidic devices in a Stokes flow context. Our approach is featured by its capability in accommodating a broad spectrum of boundary conditions at the solid-fluid interface. Our key contribution is an anisotropic and differentiable constitutive model that unifies the representation of different phases and boundary conditions in a Stokes model, enabling a topology optimization method that can synthesize novel structures with accurate boundary conditions from a background grid discretization. We demonstrate the efficacy of our approach by conducting several fluidic system design tasks with over four million design parameters.	https://dl.acm.org/doi/abs/10.1145/3550454.3555429	Yifei Li, Tao Du, Sangeetha Grama Srinivasan, Kui Wu, Bo Zhu, Eftychios Sifakis, Wojciech Matusik
Force-Aware Interface via Electromyography for Natural VR/AR Interaction	While tremendous advances in visual and auditory realism have been made for virtual and augmented reality (VR/AR), introducing a plausible sense of physicality into the virtual world remains challenging. Closing the gap between real-world physicality and immersive virtual experience requires a closed interaction loop: applying user-exerted physical forces to the virtual environment and generating haptic sensations back to the users. However, existing VR/AR solutions either completely ignore the force inputs from the users or rely on obtrusive sensing devices that compromise user experience. By identifying users' muscle activation patterns while engaging in VR/AR, we design a learning-based neural interface for natural and intuitive force inputs. Specifically, we show that lightweight electromyography sensors, resting non-invasively on users' forearm skin, inform and establish a robust understanding of their complex hand activities. Fuelled by a neural-network-based model, our interface can decode finger-wise forces in real-time with 3.3% mean error, and generalize to new users with little calibration. Through an interactive psychophysical study, we show that human perception of virtual objects' physical properties, such as stiffness, can be significantly enhanced by our interface. We further demonstrate that our interface enables ubiquitous control via finger tapping. Ultimately, we envision our findings to push forward research towards more realistic physicality in future VR/AR.	https://dl.acm.org/doi/abs/10.1145/3550454.3555461	Yunxiang Zhang, Benjamin Liang, Boyuan Chen, Paul M. Torrens, S. Farokh Atashzar, Dahua Lin, Qi Sun
GENESIS	GENESIS embarks on an emotionally intense virtual reality journey to experience the dramatic milestones in the evolution of Earth and mankind. As a 360°, stereo 3D film, GENESIS has been pre rendered (no real time) and is usable as a three Degrees of Freedom (3DOF) experience. The project can be watched on all VR glasses that can play 3DOF, like 360° movies. Glasses such as Pico G2 or Oculus Go were developed specifically for 3DOF content. Of course, all glasses which are able to present 6DOF experiences can also play 3DOF content. The users can watch the content while standing or sitting.	https://dl.acm.org/doi/abs/10.1145/3550472.3558407	Maria Courtial, Joerg Courtial
Gaia	'Gaia' is an eco game that people can play on the web. The background of this game deals with climate change and pandemics experienced by the planet we live on. In this game, the players will view the enviromental information as they select meals, electricity, and medical treatment related to survival. This button means routine behavior in human survival, but there is a point to look at from a social and ethical point of view. Through the selection of a button, we expose specific environmental research information on global changes such as climate change and pandemics, and make people aware that we are connected. The human-shaped character in the game is also a biological body through which radioactivity, dust, contaminated food, and toxic substances accumulate and pass, as well as a material entity through which industrial systems, power structures, and economic interests compete. The body and entity that collapses and resurrects inside the game is placed on a complex network of harmless gas, barren land, rotten food, a system designed to force you to click, and a social system that turns like a cogwheel by a click button. In the process of appreciating the game, you can imagine the future while watching the environmental research. We can look at the world with games and solve problems from various perspectives. The player's choices determine the Earth's morbidity and temperature. As a result, the number of times the Earth is born is updated, which symbolizes that human influence has the power to destroy the planet. As a result, we feel that human choices are interconnected and dependent on the life activities of humans and non-humans, and as a result, we reflect on our position and responsibility.	https://dl.acm.org/doi/abs/10.1145/3550470.3558418	Yunyoung Jang
Garrano	A Garrano horse is forced to pull a heavy load under a blazing sun; young boy Joel discovers a man who is about to set a forest on fire.	https://dl.acm.org/doi/abs/10.1145/3550339.3558244	Sanne Jehoul
Gaussian Blue Noise	Among the various approaches for producing point distributions with blue noise spectrum, we argue for an optimization framework using Gaussian kernels. We show that with a wise selection of optimization parameters, this approach attains unprecedented quality, provably surpassing the current state of the art attained by the optimal transport (BNOT) approach. Further, we show that our algorithm scales smoothly and feasibly to high dimensions while maintaining the same quality, realizing unprecedented high-quality high-dimensional blue noise sets. Finally, we show an extension to adaptive sampling.	https://dl.acm.org/doi/abs/10.1145/3550454.3555519	Abdalla G. M. Ahmed, Jing Ren, Peter Wonka
Geo-Metric: A Perceptual Dataset of Distortions on Faces	In this work we take a novel perception-centered approach to quantify distortions on 3D geometry of faces, to which humans are particularly sensitive. We generated a dataset, composed of 100 high-quality and demographically-balanced face scans. We then subjected these meshes to distortions that cover relevant use cases in computer graphics, and conducted a large-scale perceptual study to subjectively evaluate them. Our dataset consists of over 84,000 quality comparisons, making it the largest ever psychophysical dataset for geometric distortions. Finally, we demonstrated how our data can be used for applications like metrics, compression, and level-of-detail rendering.	https://dl.acm.org/doi/abs/10.1145/3550454.3555475	Krzysztof Wolski, Laura Trutoiu, Zhao Dong, Zhengyang Shen, Kevin Mackenzie, Alexandre Chapiro
Globally Injective Flattening via a Reduced Harmonic Subspace	We present a highly efficient-and-robust method for free-boundary flattening of disk-like triangle meshes in a globally injective manner. We show that by restricting the solution to a low-dimensional subspace of harmonic maps, we can dramatically accelerate the process while obtaining a low-distortion result. The algorithm consists of two main steps. A linear subspace construction, and a nonlinear nonconvex optimization for finding a low-distortion globally injective map within that subspace. The complexity of the first step dominates the algorithm's runtime and is merely that of solving a linear system. We combine recent results for computing locally-and-globally injective maps with that of harmonic maps into a conceptually simple algorithm that guarantees global injectivity. We demonstrate the great efficiency of our method over a dataset of 100 large scale models with more than 2M triangles each. Our algorithm is 10 times faster on average compared to the state-of-the-art Efficient Bijective Parameterizations (EBP) method [Su et al. 2020], on these high-resolution meshes, and more than 20 times faster on challenging examples (Figures 1,11). The speedup over [Jiang et al. 2017; Smith and Schaefer 2015] is even more dramatic.	https://dl.acm.org/doi/abs/10.1145/3550454.3555449	Guy Fargion, Ofir Weber
Gloss management for consistent reproduction of real and virtual objects	A good match of material appearance between real-world objects and their digital on-screen representations is critical for many applications such as fabrication, design, and e-commerce. However, faithful appearance reproduction is challenging, especially for complex phenomena, such as gloss. In most cases, the view-dependent nature of gloss and the range of luminance values required for reproducing glossy materials exceeds the current capabilities of display devices. As a result, appearance reproduction poses significant problems even with accurately rendered images. This paper studies the gap between the gloss perceived from real-world objects and their digital counterparts. Based on our psychophysical experiments on a wide range of 3D printed samples and their corresponding photographs, we derive insights on the influence of geometry, illumination, and the display's brightness and measure the change in gloss appearance due to the display limitations. Our evaluation experiments demonstrate that using the prediction to correct material parameters in a rendering system improves the match of gloss appearance between real objects and their visualization on a display device.	https://dl.acm.org/doi/abs/10.1145/3550469.3555406	Bin Chen, Michal Piovarči, Chao Wang, Hans-Peter Seidel, Piotr Didyk, Karol Myszkowski, Ana Serrano
Going Well...	The deadline is approaching. Producer Park, who is in charge of closing, comes to the writer. The writer escapes to delusions and goes crazy due to the PD's pressure and stress of the deadline.	https://dl.acm.org/doi/abs/10.1145/3550339.3557011	Hye-Jeong Lee
Green Coordinates for Triquad Cages in 3D	We introduce Green coordinates for triquad cages in 3D. Based on Green's third identity, Green coordinates allow defining the harmonic deformation of a 3D point inside a cage as a linear combination of its vertices and face normals. Using appropriate Neumann boundary conditions, the resulting deformations are quasi-conformal in 3D, and thus best-preserve the local deformed geometry, in that volumetric conformal 3D deformations do not exist unless rigid. Most coordinate systems use cages made of triangles, yet quads are in general favored by artists as those align naturally onto important geometric features of the 3D shapes, such as the limbs of a character, without introducing arbitrary asymmetric deformations and representation. While triangle cages admit per-face constant normals and result in a single Green normal-coordinate per triangle, the case of quad cages is at the same time more involved (as the normal varies along non-planar quads) and more flexible (as many different mathematical models allow defining the smooth geometry of a quad interpolating its four edges). We consider bilinear quads, and we introduce a new Neumann boundary condition resulting in a simple set of four additional normal-coordinates per quad. Our coordinates remain quasi-conformal in 3D, and we demonstrate their superior behavior under non-trivial deformations of realistic triquad cages.	https://dl.acm.org/doi/abs/10.1145/3550469.3555400	Jean-Marc Thiery, Tamy Boubekeur
Guernica, éres tu: VR Volumetric Capture as an Art Style: A Recreation of Pablo Picasso's Guernica in VR Form	Guernica, éres tu is a first-person anti-war storytelling experience that reconstructs Pablo Picasso's famed painting Guernica into a virtual environment. Guernica, éres tu, through the use of 8i's volumetric capture technology in a virtual reality environment, the VR application tries to retain the essence of human into the artwork and fits the creative style of abstract and surreal art by modifying the 3D captures. As a result, it persuasively delivers a message that the belligerents, the casualties, and the observers are all actual human beings, not merely statistics nor images on the news.	https://dl.acm.org/doi/abs/10.1145/3550472.3558408	Seung Hyun Jung, Hyeyeon Lee, Hangyeol Jo
H rtDown: Document Processor for Executable Linear Algebra Papers	Scientific documents describe a topic in a mix of prose and mathematical expressions. The prose refers to those expressions, which themselves must be encoded in, e.g., LaTeX. The resulting documents are static, even though most documents are now read digitally. Moreover, formulas must be implemented or re-implemented separately in a programming language in order to create executable research artifacts. Literate environments allow executable code to be added in addition to the prose and math. The code is yet another encoding of the same mathematical expressions. We introduce H rtDown, a document processor, authoring environment, and paper reading environment for scientific documents. Prose is written in Markdown, linear algebra formulas in an enhanced version of I LA, derivations in LaTeX, and dynamic figures in Python. H rtDown is designed to support existing scientific writing practices: editing in plain text, using and defining symbols in prose-determined order, and context-dependent symbol re-use. H rtDown's authoring environment assists authors by identifying incorrect formulas and highlighting symbols not yet described in the prose. H rtDown outputs a dynamic paper reader with math augmentations to aid in comprehension, and code libraries for experimenting with the executable formulas. H rtDown supports dynamic figures generated by inline Python code. This enables a new approach to scientific experimentation, where editing the mathematical formulas directly updates the figures. We evaluate H rtDown with an expert study and by re-implementing SIGGRAPH papers.	https://dl.acm.org/doi/abs/10.1145/3550469.3555395	Yong Li, Shoaib Kamil, Alec Jacobson, Yotam Gingold
Hanging Print: Plastic Extrusion for Catenary Weaving in Mid-Air	We present Hanging Print, a framework to design and fabricate shapes by weaving catenaries(i.e. hanging curves) by extruding plastic filaments directly in mid-air. Hanging Print has the potential to assist designing expressions unique to hanging structures, unique to plastic, and beyond imagination. In this paper we introduce the workflow of Hanging Print, and demonstrate our works in various scales.	https://dl.acm.org/doi/abs/10.1145/3550082.3564180	Rina Kinoshita, Hiroya Tanaka
Hellscape	Enyo is a brave and daring warrior determined to bring back her dead mentor from Hell. In her quest, Enyo meets Cerberus and becomes friend with him.	https://dl.acm.org/doi/abs/10.1145/3550339.3556086	Alixe Devaux, Camille Leroux, Félicia Poggi, Clémence Lacoume, Lara Brière, Valentine Wilke, Philippe Meis
Hidden Degrees of Freedom in Implicit Vortex Filaments	This paper presents a new representation of curve dynamics, with applications to vortex filaments in fluid dynamics. Instead of representing these filaments with explicit curve geometry and Lagrangian equations of motion, we represent curves with a new co-dimensional 2 level set description. Our implicit representation admits several redundant mathematical degrees of freedom in both the configuration and the dynamics of the curves, which can be tailored specifically to improve numerical robustness, in contrast to naive approaches for implicit curve dynamics that suffer from overwhelming numerical stability problems. Furthermore, we note how these hidden degrees of freedom perfectly map to a in fluid dynamics. Motivated by these observations, we introduce level set functions and dynamics which successfully regularize sources of numerical instability, particularly in the twisting modes around curve filaments. A consequence is a novel simulation method which produces stable dynamics for large numbers of interacting vortex filaments and effortlessly handles topological changes and re-connection events.	https://dl.acm.org/doi/abs/10.1145/3550454.3555459	Sadashige Ishida, Chris Wojtan, Albert Chern
Hierarchical Layout Blending with Recursive Optimal Correspondence	We present a novel method for blending hierarchical layouts with semantic labels. The core of our method is a hierarchical structure correspondence algorithm, which recursively finds optimal substructure correspondences, achieving a globally optimal correspondence between a pair of hierarchical layouts. This correspondence is consistent with the structures of both layouts, allowing us to define the union of the layouts' structures. The resulting compound structure helps extract intermediate layout structures, from which blended layouts can be generated via an optimization approach. The correspondence also defines a similarity measure between layouts in a hierarchically structured view. Our method provides a new way for novel layout creation. The introduced structural similarity measure regularizes the layouts in a hyperspace. We demonstrate two applications in this paper, i.e., exploratory design of novel layouts and sketch-based layout retrieval, and test them on a magazine layout dataset. The effectiveness and feasibility of these two applications are confirmed by the user feedback and the extensive results. The code is available at https://github.com/lyf7115/LayoutBlending.	https://dl.acm.org/doi/abs/10.1145/3550454.3555446	Pengfei Xu, Yifan Li, Zhijin Yang, Weiran Shi, Hongbo Fu, Hui Huang
High-Order Directional Fields	We introduce a framework for representing face-based directional fields of an arbitrary piecewise-polynomial order. Our framework is based on a primal-dual decomposition of fields, where the exact component of a field is the gradient of piecewise-polynomial conforming function, and the coexact component is defined as the adjoint of a dimensionally-consistent discrete curl operator. Our novel formulation sidesteps the difficult problem of constructing high-order non-conforming function spaces, and makes it simple to harness the flexibility of higher-order finite elements for directional-field processing. Our representation is structure-preserving, and draws on principles from finite-element exterior calculus. We demonstrate its benefits for applications such as Helmholtz-Hodge decomposition, smooth PolyVector fields, the vector heat method, and seamless parameterization.	https://dl.acm.org/doi/abs/10.1145/3550454.3555455	Iwan Boksebeld, Amir Vaxman
Human Performance Modeling and Rendering via Neural Animated Mesh	We have recently seen tremendous progress in the neural advances for photo-real human modeling and rendering. However, it's still challenging to integrate them into an existing mesh-based pipeline for downstream applications. In this paper, we present a comprehensive neural approach for high-quality reconstruction, compression, and rendering of human performances from dense multi-view videos. Our core intuition is to bridge the traditional animated mesh workflow with a new class of highly efficient neural techniques. We first introduce a neural surface reconstructor for high-quality surface generation in minutes. It marries the implicit volumetric rendering of the truncated signed distance field (TSDF) with multi-resolution hash encoding. We further propose a hybrid neural tracker to generate animated meshes, which combines explicit non-rigid tracking with implicit dynamic deformation in a self-supervised framework. The former provides the coarse warping back into the canonical space, while the latter implicit one further predicts the displacements using the 4D hash encoding as in our reconstructor. Then, we discuss the rendering schemes using the obtained animated meshes, ranging from dynamic texturing to lumigraph rendering under various bandwidth settings. To strike an intricate balance between quality and bandwidth, we propose a hierarchical solution by first rendering 6 virtual views covering the performer and then conducting occlusion-aware neural texture blending. We demonstrate the efficacy of our approach in a variety of mesh-based applications and photo-realistic free-view experiences on various platforms, i.e., inserting virtual human performances into real environments through mobile AR or immersively watching talent shows with VR headsets.	https://dl.acm.org/doi/abs/10.1145/3550454.3555451	Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao Wang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang, Minye Wu, Lan Xu, Jingyi Yu
Human-like non-human – HAOS human electrical cognition liberation project Ver 1.0	"The project ""human-like non-human - HAOS human cognition liberation project"" to attempt using electrical perception sensing in the oral region as an addition to the Eight Consciousness defined by Buddhist disciplines and expanding beyond the biological identity of a human."	https://dl.acm.org/doi/abs/10.1145/3550470.3558450	Jiun Ting Lai
HumanConQuad: Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning	Robotic creatures are capable of entering hazardous environments instead of human workers, but it is challenging to develop a fully autonomous agent that can work independently in unstructured scenes. We propose a human motion-based control interface for quadrupedal robots that promises adaptable robot operations by reflecting the user's intuition directly to the robot's movements. Designing motion interface for different morphologies conveys tricky problems in solving dynamics and control strategies. We first retarget the captured human motion into the corresponding robot's kinematic space with proper semantics using supervised learning and post-processing techniques. Second, we build the motion imitation controller to track the given retargeted motion using deep reinforcement learning with task-based curriculums. Finally, we apply domain randomization during training for real-world deployment. (Video1)	https://dl.acm.org/doi/abs/10.1145/3550471.3564762	Sunwoo Kim, Maks Sorokin, Jehee Lee, Sehoon Ha
Hydrophobic and Hydrophilic Solid-Fluid Interaction	We propose a novel solid-fluid coupling method to capture the subtle hydrophobic and hydrophilic interactions between liquid, solid, and air at their multi-phase junctions. The key component of our approach is a Lagrangian model that tackles the coupling, evolution, and equilibrium of dynamic contact lines evolving on the interface between surface-tension fluid and deformable objects. This contact-line model captures an ensemble of small-scale geometric and physical processes, including dynamic waterfront tracking, local momentum transfer and force balance, and interfacial tension calculation. On top of this contact-line model, we further developed a mesh-based level set method to evolve the three-phase T-junction on a deformable solid surface. Our dynamic contact-line model, in conjunction with its monolithic coupling system, unifies the simulation of various hydrophobic and hydrophilic solid-fluid-interaction phenomena and enables a broad range of challenging small-scale elastocapillary phenomena that were previously difficult or impractical to solve, such as the elastocapillary origami and self-assembly, dynamic contact angles of drops, capillary adhesion, as well as wetting and splashing on vibrating surfaces.	https://dl.acm.org/doi/abs/10.1145/3550454.3555478	Jinyuan Liu, Mengdi Wang, Fan Feng, Annie Tang, Qiqin Le, Bo Zhu
ICARUS: A Specialized Architecture for Neural Radiance Fields Rendering	The practical deployment of Neural Radiance Fields (NeRF) in rendering applications faces several challenges, with the most critical one being low rendering speed on even high-end graphic processing units (GPUs). In this paper, we present ICARUS, a specialized accelerator architecture tailored for NeRF rendering. Unlike GPUs using general purpose computing and memory architectures for NeRF, ICARUS executes the complete NeRF pipeline using dedicated plenoptic cores (PLCore) consisting of a positional encoding unit (PEU), a multi-layer perceptron (MLP) engine, and a volume rendering unit (VRU). A PLCore takes in positions & directions and renders the corresponding pixel colors without any intermediate data going off-chip for temporary storage and exchange, which can be time and power consuming. To implement the most expensive component of NeRF, i.e., the MLP, we transform the fully connected operations to approximated reconfigurable multiple constant multiplications (MCMs), where common subexpressions are shared across different multiplications to improve the computation efficiency. We build a prototype ICARUS using Synopsys HAPS-80 S104, a field programmable gate array (FPGA)-based prototyping system for large-scale integrated circuits and systems design. We evaluate the power-performancearea (PPA) of a PLCore using 40nm LP CMOS technology. Working at 400 MHz, a single PLCore occupies 16.5 and consumes 282.8 mW, translating to 0.105 uJ/sample. The results are compared with those of GPU and tensor processing unit (TPU) implementations.	https://dl.acm.org/doi/abs/10.1145/3550454.3555505	Chaolin Rao, Huangjie Yu, Haochuan Wan, Jindong Zhou, Yueyang Zheng, Minye Wu, Yu Ma, Anpei Chen, Binzhe Yuan, Pingqiang Zhou, Xin Lou, Jincyi Yu
ICVFX Production with UE5.1	Westworld is going to showcase some of the ICVFX technology that it used in actual TV series and films production.	https://dl.acm.org/doi/abs/10.1145/3550453.3586019	Koni Jung
IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-Aware Portrait Synthesis	Existing 3D-aware facial generation methods face a dilemma in quality versus editability: they either generate editable results in low resolution, or high-quality ones with no editing flexibility. In this work, we propose a new approach that brings the best of both worlds together. Our system consists of three major components: (1) a 3D-semantics-aware generative model that produces view-consistent, disentangled face images and semantic masks; (2) a hybrid GAN inversion approach that initializes the latent codes from the semantic and texture encoder, and further optimizes them for faithful reconstruction; and (3) a canonical editor that enables efficient manipulation of semantic masks in canonical view and produces high-quality editing results. Our approach is competent for many applications, e.g. free-view face drawing, editing and style control. Both quantitative and qualitative results show that our method reaches the state-of-the-art in terms of photorealism, faithfulness and efficiency.	https://dl.acm.org/doi/abs/10.1145/3550454.3555506	Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, Yebin Liu
Illusory Land	The core idea is that fragments of memory echo through the meta-universe. It is a 360-degree immersive art work using the latest real-time animation technology. It can be displayed in VR or Dome projection. Allowing the audience to immerse themselves in a surreal Oriental landscape fantasy, experience dreams and memories of the past, present, and future growing, changing, disintegrating, and reorganizing in a chaotic meta-universe. It describes an electronic tree growing from another dimension, with a VR Birdman wrapped together by umbilical cords of optical fiber. The trees were covered with psychedelic electronic flowers. It conveys the conflicts between technology and humanity, memory and the present, evolution and extinction that continue to reverberate in the meta-universe. The landscape floating upside down in the mist adds to the atmosphere of suspense and chaos. The style is a mixture of surrealism, fantasy, cyberpunk, Song Dynasty painting, etc. Currently uploaded is a work in progress version, but it has also been of fairly high quality, full of artistic and technical challenges throughout the production process. We are trying to put more challenging elements, concepts into the final version, which will have longer duration, multiple different scenes, more complex story lines, full music, etc.	https://dl.acm.org/doi/abs/10.1145/3550470.3558428	Xi Wang, Shaoyu Su, Liaowei Chen, Yifan Lin, Francisco Manuel Barros, Zao Song
Impact of correct and simulated focus cues on perceived realism	The natural accommodation of the human eye to different distances results in focus cues, which contribute to depth perception and appearance. Since focus cues are very difficult to reproduce in an electronic display, it is desirable to know how much they contribute to realistic image appearance. In this work we quantify the potential benefit of focus cues in terms of increased realism compared to regular stereo image presentation. As a secondary goal, we evaluate whether three depth-of-field rendering techniques, which reproduce defocus blur at three different degrees of accuracy, can reintroduce the benefits of focus cues. Our findings confirm the importance of focus cues for realistic image appearance, and also show that they cannot easily be substituted by depth-of-field rendering.	https://dl.acm.org/doi/abs/10.1145/3550469.3555405	Joseph March, Anantha Krishnan, Simon Watt, Marek Wernikowski, Hongyun Gao, Ali Özgür Yöntem, Rafal Mantiuk
Implicit Conversion of Manifold B-Rep Solids by Neural Halfspace Representation	We present a novel implicit representation --- (NH-Rep), to convert manifold B-Rep solids to implicit representations. NH-Rep is a Boolean tree built on a set of implicit functions represented by the neural network, and the composite Boolean function is capable of representing solid geometry while preserving sharp features. We propose an efficient algorithm to extract the Boolean tree from a manifold B-Rep solid and devise a neural network-based optimization approach to compute the implicit functions. We demonstrate the high quality offered by our conversion algorithm on ten thousand manifold B-Rep CAD models that contain various curved patches including NURBS, and the superiority of our learning approach over other representative implicit conversion algorithms in terms of surface reconstruction, sharp feature preservation, signed distance field approximation, and robustness to various surface geometry, as well as a set of applications supported by NH-Rep.	https://dl.acm.org/doi/abs/10.1145/3550454.3555502	Hao-Xiang Guo, Yang Liu, Hao Pan, Baining Guo
Improving Co-speech gesture rule-map generation via wild pose matching with gesture units.	In this poster, we present a method to generate co-speech text-to-gesture mapping for 3D digital humans. We obtained text and 2D pose data from public monologue videos. Gesture units were obtained from motion capture sequences. The method works by matching 2D poses to 3D gesture units. We trained a model via contrastive learning to improve the matching of noisy pose sequences with gesture units. To ensure diverse gesture sequences at runtime, gesture units were clustered using K-Mean clustering. We incorporated 2035 gestures and 210k rules. Our method is highly adaptable and easy to control and use. Demo Video : https://youtu.be/QBtGdGE1Wgk	https://dl.acm.org/doi/abs/10.1145/3550082.3564185	Ghazanfar Ali, Jae-In Hwang
Inner self drawing machine	"Besides men and women, people can be neither man nor woman, fluid identity, transgender, and agender. Not only that, in terms of sexual orientation, besides heterosexuals, there are homosexuals, bisexuals, pansexuals, and asexuals as well. However, ignorance and lacking empathetic understanding of those sexual minorities make their lives harsh and suffering. For instance, in the case of transgender people, 28% of them postponed their health care due to discrimination, 19% of them refused medical care altogether, and 28% of them experienced verbal harassment by medical professionals, according to a 2011 national transgender discrimination survey (USA). Unluckily, we are apt to generate a basic understanding of others based on their gender expressions and use such irresponsible and heuristic findings to deal with others. Thus, we decided to create an installation to at least minimize the gap for a moment when the audience can enjoy themselves by watching the drawing performance of their idea portrait (inner-self). Regarding the AI portrait painter, we leverage StyleGAN to generate the continuous gender spectrum of each participant based on their facial features, in which they can choose their ideal gender representation that reflects their inner self the most. Then our AI portrait painter ""draws"" the selected ""self"" on the canvas. In general professional painters can detect and draw the most confident and beautiful us, on the other hand, we tend to exaggerate our flaws and ignore our attractive parts. When the drawing performance finishes, the audience can receive the drawing result as a well-printed portrait simultaneously. The printed portrait also works as a souvenir of participating in our exhibition. Our work aims to raise an empathic understanding of the various sexual minorities for a more inclusive, preferable, and sustainable world."	https://dl.acm.org/doi/abs/10.1145/3550470.3558429	Qing Zhang, Fan Xie, Yifei Huang, Yun Suen Pai, George Chernyshov, Jing Huang, Xiongqi Wang, Jamie A Ward, Kai Kunze
Interactive Chinese Character Learning in Virtual Reality	Learning Chinese characters and characters of Chinese origin is a challenging and time-consuming task for native speakers and second language learners alike. Traditional methods to acquire Chinese characters include repeatedly writing the characters on paper or air writing. In scientific research, computer- and mobile-assisted approaches have been implemented that allow users to be interactively guided through the learning process. The recent advancements and availability of virtual reality (VR) headsets with six degrees of freedom tracking and controllers provide an interesting new platform for education applications. The proposed VR application utilizes novel interaction and visualization methods tailored for VR with guided repetition as well as sound and haptic feedback. The augmentation of air writing further enables motor-based learning and allows for the application to be used as an educational exergame.	https://dl.acm.org/doi/abs/10.1145/3550472.3558415	Daniel Vogel, Susanne Schmidt, Frank Steinicke
Interactive Exploration of Tension-Compression Mixed Shells	Achieving a pure-compression stress state is considered central to the form-finding of shell structures. However, the pure-compression assumption restricts the geometry of the structure's plan in that any free boundary edges cannot bulge outward. Allowing both tension and compression is essential so that overhanging leaves can stretch out toward the sky. When performing tension-compression mixed form-finding, a problem with boundary condition (BC) compatibility arises. Since the form-finding equation is hyperbolic, boundary information propagates along the asymptotic lines of the stress function. If conflicting BC data is prescribed at either end of an asymptotic line, the problem becomes ill-posed. This requires a user of a form-finding method to know the solution in advance. By contrast, pure-tension or pure-compression problems are elliptic and always give solutions under any BCs sufficient to restrain rigid motion. To solve the form-finding problem for tension-compression mixed shells, we focus on the Airy's stress function, which describes the stress field in a shell. Rather than taking the stress function as given, we instead treat both the stress function and the shell as unknowns. This doubles the solution variables, turning the problem to one that has an infinity of different solutions. By enforcing equilibrium in the shell interior and prescribing the correct of BCs to both the stress function and the shell, a stress function and shell can be simultaneously found such that equilibrium is satisfied everywhere in the shell interior and thus automatically has compatible BCs by construction. The problem of a potentially over-constrained form-finding is thus avoided by expanding the solution space and creating an under-determined problem. By varying inputs and repeatedly searching for stress function-shell pairs that fall within the solution space, a user is allowed to interactively explore the possible forms of tension-compression mixed shells under the given plan of the shell.	https://dl.acm.org/doi/abs/10.1145/3550454.3555438	Masaaki Miki, Toby Mitchell
Interactive and Robust Mesh Booleans	Boolean operations are among the most used paradigms to create and edit digital shapes. Despite being conceptually simple, the computation of mesh Booleans is notoriously challenging. Main issues come from numerical approximations that make the detection and processing of intersection points inconsistent and unreliable, exposing implementations based on floating point arithmetic to many kinds of degeneracy and failure. Numerical methods based on rational numbers or exact geometric predicates have the needed robustness guarantees, that are achieved at the cost of increased computation times that, as of today, has always restricted the use of robust mesh Booleans to offline applications. We introduce an algorithm for Boolean operations with robustness guarantees that is capable of operating at interactive frame rates on meshes with up to 200K triangles. We evaluate our tool thoroughly, considering not only interactive applications but also batch processing of large collections of meshes, processing of huge meshes containing millions of elements and variadic Booleans of hundreds of shapes altogether. In all these experiments, we consistently outperform prior robust floating point methods by at least one order of magnitude.	https://dl.acm.org/doi/abs/10.1145/3550454.3555460	Gianmarco Cherchi, Fabio Pellacini, Marco Attene, Marco Livesu
Internal-External Boundary Attentions for Transparent Object Segmentation	Recognizing transparent objects such as window and glassware is a challenging task in 3D Reconstruction from color images. In recent years, transparent object recognition methods have focused on feature extraction from boundary region of transparent objects. Our observation is that, unlike non-transparent objects, transparent objects can be characterized and located better by looking at their external and internal boundaries separately. We propose a new internal-external boundaries attention module in which internal and external boundary features are separately recognized. We add an edge-body fully attention module that supervises the segmentation of the generated transparent objects body using semantic information in the external boundaries. We employ contour loss to perform distance-weighted supervision on the inner and outer boundaries separately. Extensive experiments show that proposed method outperforms existing methods on Trans10k dataset.	https://dl.acm.org/doi/abs/10.1145/3550082.3564183	Seungkyu Lee, Dongshen Han
Investigating the Effects of Synchronized Visuo-Tactile Stimuli for Inducing Kinesthetic Illusion in Observational Learning of Whole-Body Movements	Skill improvement in sports is an essential factor in keeping the motivation to continue. Previous studies showed that kinesthetic illusion enhances observational learning. However, such studies have only dealt with the learning of movements using a single part of the body, and whether kinesthetic illusion can be induced in observational learning of whole-body movements has not been clarified. In this study, we conducted an experiment involving human subjects and confirmed that synchronized visuo-tactile stimuli can induce kinesthetic illusion even in whole-body movements. Moreover, we also demonstrated a complete mediation model in which the synchronization of visuo-tactile stimuli influences kinesthetic illusion mediated by body ownership.	https://dl.acm.org/doi/abs/10.1145/3550082.3564168	Shintaro Fukumoto, Seiya Mitsuno, Kazuki Nakayama, Reiya Itatani, Genki Jogan, Hamed Mahzoon
Iron Sail	Revenant was commissioned by Vietnam based company IRON SAIL, to create a cinematic CG trailer that would introduce their new website and strengthen their brand identity. The company houses a lot of different games, each with their own individual aesthetic. Our aim was to nest them under one sophisticated style that would unify the IRON SAIL umbrella, appealing to their user market and paving the way for future campaigns.	https://dl.acm.org/doi/abs/10.1145/3550339.3556635	Kevin McCrae, Alex Rych, Nicole Anderson (Blackwood)
Isometric Energies for Recovering Injectivity in Constrained Mapping	Computing injective maps with low distortions is a long-standing problem in computer graphics. Such maps are particularly challenging to obtain in the presence of positional constraints, because an injective initial map is often not available. Recently, several energies were proposed and shown to be highly successful in optimizing injectivity from non-injective initial maps while satisfying positional constraints. However, minimizing these energies tends to produce elements with significant isometric distortions. This paper presents simple variants of these energies that retain their desirable traits while promoting isometry. While our method is not guaranteed to provide an injective map, we observe that, on large-scale 2D and 3D data sets, minimizing the proposed isometric variants results in a similar level of success in recovering injectivity as the original energies but a significantly lower isometric distortion.	https://dl.acm.org/doi/abs/10.1145/3550469.3555419	Xingyi Du, Danny M. Kaufman, Qingnan Zhou, Shahar Kovalsky, Yajie Yan, Noam Aigerman, Tao Ju
Isotropic ARAP Energy Using Cauchy-Green Invariants	"As-Rigid-As-Possible (ARAP) energy has been popular for shape editing, mesh parametrisation and soft-body simulation for almost two decades. However, a formulation using Cauchy-Green (CG) invariants has always been unclear, due to a rotation-polluted trace term that cannot be directly expressed using these invariants. We show how this incongruent trace term can be understood via an implicit relationship to the CG invariants. Our analysis reveals this relationship to be a polynomial where the roots equate to the trace term, and where the derivatives also give rise to closed-form expressions of the Hessian to guarantee positive semi-definiteness for a fast and concise Newton-type implicit time integration. A consequence of this analysis is a novel analytical formulation to compute rotations and singular values of deformation-gradient tensors without explicit/numerical factorization which is significant, resulting in up-to 3.5× speedup and benefits energy function evaluation for reducing solver time. We validate our energy formulation by experiments and comparison, demonstrating that our resulting eigendecomposition using the CG invariants is equivalent to existing ARAP formulations. We thus reveal isotropic ARAP energy to be a member of the ""Cauchy-Green club"", meaning that it can indeed be defined using CG invariants and therefore that the closed-form expressions of the resulting Hessian are shared with other energies written in their terms."	https://dl.acm.org/doi/abs/10.1145/3550454.3555507	Huancheng Lin, Floyd M. Chitalu, Taku Komura
It's Me: VR-based Journaling for Improved Cognitive Self-Regulation	Journaling is a well-known evidence-based strategy for practicing better self-regulation and self-awareness in daily life. It is also an effective way for reducing the effects of negative emotions like anxiety and depression. We explore the combination of virtual reality (VR) and a body-tracking mirror as a cognitive activity in the form of VR journaling. VR-based journaling aims to build cognitive reappraisal, self-reflection, and autonomic emotion regulation in a sustainable way. The user performs in front of the mirror, using body-tracking to help them keep a daily journal. Considering that looking into the mirror is already a daily habit for most people, it does not require the users to form a new habit when trying to do journaling. Furthermore, it activates creativity by encouraging the user to use their body language as a story-telling tool.	https://dl.acm.org/doi/abs/10.1145/3550082.3564196	Yixin Wang, Yun Suen Pai, Kouta Minamizawa
Keep Smiling	Keep Smiling is an online interactive experience in the form of a job interview conducted by an AI agent. The agent asks the participant to smile, to smile even more, and to count objects she/he can see through a nearby window while continuing to smile.	https://dl.acm.org/doi/abs/10.1145/3550470.3558456	Varvara Guljajeva, Mar Canet Sola
Land Enough	Land Enough uses the issues of rising sea levels, energy, waste, and diversity in a fictional climate-destroyed future. Each participant was given the role of explorer, architect, scientist, or engineer to create artful technologies in a fictional climate-destroyed future. These post-apocalyptic technologies signal the non-fungibility and fragility of nature.	https://dl.acm.org/doi/abs/10.1145/3550470.3558453	Bengi Agcal, Ray Lc, Ziyou Yin
Language-driven Diversified Image Retargeting	Content-aware image resizing could automatically retarget an image to different aspect ratios while preserving visually salient contents. However, it is difficult for users to interact with the retargeting process and control the results. In this paper, we propose a language-driven diversified image retargeting (LDIR) method that allows the users to control the retargeting process by providing additional textual descriptions. Taking the original image and user-provided texts as inputs, LDIR retargets the image into the desired resolution while preserving the content indicated by texts. Following a self-play reinforcement learning pipeline, a multimodel reward function is proposed by considering both the visual quality and language guidance. Preliminary experiments manifest that LDIR can achieve diversified image retargeting guided by texts.	https://dl.acm.org/doi/abs/10.1145/3550082.3564169	Rui Wang, Nisha Huang, Fan Tang, Weiming Dong, Tong-Yee Lee
LaplacianFusion: Detailed 3D Clothed-Human Body Reconstruction	We propose , a novel approach that reconstructs detailed and controllable 3D clothed-human body shapes from an input depth or 3D point cloud sequence. The key idea of our approach is to use Laplacian coordinates, well-known differential coordinates that have been used for mesh editing, for representing the local structures contained in the input scans, instead of implicit 3D functions or vertex displacements used previously. Our approach reconstructs a controllable base mesh using SMPL, and learns a surface function that predicts Laplacian coordinates representing surface details on the base mesh. For a given pose, we first build and subdivide a base mesh, which is a deformed SMPL template, and then estimate Laplacian coordinates for the mesh vertices using the surface function. The final reconstruction for the pose is obtained by integrating the estimated Laplacian coordinates as a whole. Experimental results show that our approach based on Laplacian coordinates successfully reconstructs more visually pleasing shape details than previous methods. The approach also enables various surface detail manipulations, such as detail transfer and enhancement.	https://dl.acm.org/doi/abs/10.1145/3550454.3555511	Hyomin Kim, Hyeonseo Nam, Jungeon Kim, Jaesik Park, Seungyong Lee
LayoutEnhancer: Generating Good Indoor Layouts from Imperfect Data	We address the problem of indoor layout synthesis, which is a topic of continuing research interest in computer graphics. The newest works made significant progress using data-driven generative methods; however, these approaches rely on suitable datasets. In practice, desirable layout properties may not exist in a dataset, for instance, specific expert knowledge can be missing in the data. We propose a method that combines expert knowledge, for example, knowledge about ergonomics, with a data-driven generator based on the popular Transformer architecture. The knowledge is given as differentiable scalar functions, which can be used both as weights or as additional terms in the loss function. Using this knowledge, the synthesized layouts can be biased to exhibit desirable properties, even if these properties are not present in the dataset. Our approach can also alleviate problems of lack of data and imperfections in the data. Our work aims to improve generative machine learning for modeling and provide novel tools for designers and amateurs for the problem of interior layout creation.	https://dl.acm.org/doi/abs/10.1145/3550469.3555425	Kurt Leimer, Paul Guerrero, Tomer Weiss, Przemyslaw Musialski
Learning Reconstructability for Drone Aerial Path Planning	We introduce the first to improve view and path planning for large-scale 3D urban scene acquisition using unmanned drones. In contrast to previous heuristic approaches, our method learns a model that predicts how well a 3D urban scene will be reconstructed from a set of viewpoints. To make such a model trainable and simultaneously applicable to drone path planning, we simulate the proxy-based 3D scene reconstruction during training to set up the prediction. Specifically, the neural network we design is trained to predict the scene reconstructability as a function of the , a set of viewpoints, and optionally a series of scene images acquired in flight. To reconstruct a new urban scene, we first build the 3D scene proxy, then rely on the predicted reconstruction quality and uncertainty measures by our network, based off of the proxy geometry, to guide the drone path planning. We demonstrate that our data-driven reconstructability predictions are more closely correlated to the true reconstruction quality than prior heuristic measures. Further, our learned predictor can be easily integrated into existing path planners to yield improvements. Finally, we devise a new iterative view planning framework, based on the learned reconstructability, and show superior performance of the new planner when reconstructing both synthetic and real scenes.	https://dl.acm.org/doi/abs/10.1145/3550454.3555433	Yilin Liu, Liqiang Lin, Yue Hu, Ke Xie, Chi-Wing Fu, Hao Zhang, Hui Huang
Learning Virtual Chimeras by Dynamic Motion Reassembly	The is a mythological hybrid creature composed of different animal parts. The chimera's movements are highly dependent on the spatial and temporal alignments of its composing parts. In this paper, we present a novel algorithm that creates and animates chimeras by dynamically reassembling source characters and their movements. Our algorithm exploits a two-network architecture: part assembler and dynamic controller. The part assembler is a supervised learning layer that searches for the spatial alignment among body parts, assuming that the temporal alignment is provided. The dynamic controller is a reinforcement learning layer that learns robust control policy for a wide variety of potential temporal alignments. These two layers are tightly intertwined and learned simultaneously. The chimera animation generated by our algorithm is energy efficient and expressive in terms of describing weight shifting, balancing, and full-body coordination. We demonstrate the versatility of our algorithm by generating the motor skills of a large variety of chimeras from limited source characters.	https://dl.acm.org/doi/abs/10.1145/3550454.3555489	Seyoung Lee, Jiye Lee, Jehee Lee
Learning to Generate 3D Shapes from a Single Example	Existing generative models for 3D shapes are typically trained on a large 3D dataset, often of a specific object category. In this paper, we investigate the deep generative model that learns from only a reference 3D shape. Specifically, we present a multi-scale GAN-based model designed to capture the input shape's geometric features across a range of spatial scales. To avoid large memory and computational cost induced by operating on the 3D volume, we build our generator atop the tri-plane hybrid representation, which requires only 2D convolutions. We train our generative model on a voxel pyramid of the reference shape, without the need of any external supervision or manual annotation. Once trained, our model can generate diverse and high-quality 3D shapes possibly of different sizes and aspect ratios. The resulting shapes present variations across different scales, and at the same time retain the global structure of the reference shape. Through extensive evaluation, both qualitative and quantitative, we demonstrate that our model can generate 3D shapes of various types.	https://dl.acm.org/doi/abs/10.1145/3550454.3555480	Rundi Wu, Changxi Zheng
Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation	Given a portrait image of a person and an environment map of the target lighting, portrait relighting aims to re-illuminate the person in the image as if the person appeared in an environment with the target lighting. To achieve high-quality results, recent methods rely on deep learning. An effective approach is to supervise the training of deep neural networks with a high-fidelity dataset of desired input-output pairs, captured with a light stage. However, acquiring such data requires an expensive special capture rig and time-consuming efforts, limiting access to only a few resourceful laboratories. To address the limitation, we propose a new approach that can perform on par with the state-of-the-art (SOTA) relighting methods without requiring a light stage. Our approach is based on the realization that a successful relighting of a portrait image depends on two conditions. First, the method needs to mimic the behaviors of physically-based relighting. Second, the output has to be photorealistic. To meet the first condition, we propose to train the relighting network with training data generated by a virtual light stage that performs physically-based rendering on various 3D synthetic humans under different environment maps. To meet the second condition, we develop a novel synthetic-to-real approach to bring photorealism to the relighting network output. In addition to achieving SOTA results, our approach offers several advantages over the prior methods, including controllable glares on glasses and more temporally-consistent results for relighting videos.	https://dl.acm.org/doi/abs/10.1145/3550454.3555442	Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz, Ming-Yu Liu, Ting-Chun Wang
Learning-Based Bending Stiffness Parameter Estimation by a Drape Tester	Real-world fabrics often possess complicated nonlinear, anisotropic bending stiffness properties. Measuring the physical parameters of such properties for physics-based simulation is difficult yet unnecessary, due to the persistent existence of numerical errors in simulation technology. In this work, we propose to adopt a simulation-in-the-loop strategy: instead of measuring the physical parameters, we estimate the simulation parameters to minimize the discrepancy between reality and simulation. This strategy offers good flexibility in test setups, but the associated optimization problem is computationally expensive to solve by numerical methods. Our solution is to train a regression-based neural network for inferring bending stiffness parameters, directly from drape features captured in the real world. Specifically, we choose the Cusick drape test method and treat multiple-view depth images as the feature vector. To effectively and efficiently train our network, we develop a highly expressive and physically validated bending stiffness model, and we use the traditional cantilever test to collect the parameters of this model for 618 real-world fabrics. Given the whole parameter data set, we then construct a parameter subspace, generate new samples within the sub-space, and finally simulate and augment synthetic data for training purposes. The experiment shows that our trained system can replace cantilever tests for quick, reliable and effective estimation of simulation-ready parameters. Thanks to the use of the system, our simulator can now faithfully simulate bending effects comparable to those in the real world.	https://dl.acm.org/doi/abs/10.1145/3550454.3555464	Xudong Feng, Wenchao Huang, Weiwei Xu, Huamin Wang
Learning-based Inverse Rendering of Complex Indoor Scenes with Differentiable Monte Carlo Raytracing	Indoor scenes typically exhibit complex, spatially-varying appearance from global illumination, making inverse rendering a challenging ill-posed problem. This work presents an end-to-end, learning-based inverse rendering framework incorporating differentiable Monte Carlo raytracing with importance sampling. The framework takes a single image as input to jointly recover the underlying geometry, spatially-varying lighting, and photorealistic materials. Specifically, we introduce a physically-based differentiable rendering layer with screen-space ray tracing, resulting in more realistic specular reflections that match the input photo. In addition, we create a large-scale, photorealistic indoor scene dataset with significantly richer details like complex furniture and dedicated decorations. Further, we design a novel out-of-view lighting network with uncertainty-aware refinement leveraging hypernetwork-based neural radiance fields to predict lighting outside the view of the input photo. Through extensive evaluations on common benchmark datasets, we demonstrate superior inverse rendering quality of our method compared to state-of-the-art baselines, enabling various applications such as complex object insertion and material editing with high fidelity. Code and data will be made available at https://jingsenzhu.github.io/invrend	https://dl.acm.org/doi/abs/10.1145/3550469.3555407	Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua Zhong, Dianbing Xi, Rui Wang, Hujun Bao, Jiaxiang Zheng, Rui Tang
Lightweight Neural Basis Functions for All-Frequency Shading	Basis functions provide both the abilities for compact representation and the properties for efficient computation. Therefore, they are pervasively used in rendering to perform all-frequency shading. However, common basis functions, including spherical harmonics (SH), wavelets, and spherical Gaussians (SG) all have their own limitations, such as low-frequency for SH, not rotationally invariant for wavelets, and no multiple product support for SG. In this paper, we present neural basis functions, an implicit and data-driven set of basis functions that circumvents the limitations with all desired properties. We first introduce a representation neural network that takes any general 2D spherical function (e.g. environment lighting, BRDF, and visibility) as input and projects it onto the latent space as coefficients of our neural basis functions. Then, we design several lightweight neural networks that perform different types of computation, giving our basis functions different computational properties such as double/triple product integrals and rotations. We demonstrate the practicality of our neural basis functions by integrating them into all-frequency shading applications, showing that our method not only achieves a compression rate of and 10 × -40 × better performance than wavelets at equal quality, but also renders all-frequency lighting effects in real-time without the aforementioned limitations from classic basis functions.	https://dl.acm.org/doi/abs/10.1145/3550469.3555386	Zilin Xu, Zheng Zeng, Lifan Wu, Lu Wang, Ling-Qi Yan
Lizardians	Lizardians is a three-channel 3-D animation that depicts a world in which the human body has been commercialized for mass consumption. The lives of two people unfold synchronously to reveal the consequences of the late-capitalistic system on individuals' freedom, labor, creativity, and authorship.	https://dl.acm.org/doi/abs/10.1145/3550470.3558442	Young Joo Lee
Look-Ahead Training with Learned Reflectance Loss for Single-Image SVBRDF Estimation	In this paper, we propose a novel optimization-based method to estimate the reflectance properties of a near planar surface from a single input image. Specifically, we perform test-time optimization by directly updating the parameters of a neural network to minimize the test error. Since single image SVBRDF estimation is a highly ill-posed problem, such an optimization is prone to overfitting. Our main contribution is to address this problem by introducing a training mechanism that takes the test-time optimization into account. Specifically, we train our network by minimizing the training loss after one or more gradient updates with the test loss. By training the network in this manner, we ensure that the network does not overfit to the input image during the test-time optimization process. Additionally, we propose a learned reflectance loss to augment the typically used rendering loss during the test-time optimization. We do so by using an auxiliary network that estimates pseudo ground truth reflectance parameters and train it in combination with the main network. Our approach is able to converge with a small number of iterations of the test-time optimization and produces better results compared to the state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3550454.3555495	Xilong Zhou, Nima Khademi Kalantari
Low-Latency Motion Transfer with Electromagnetic Actuation for Joint Action	"Joint action, performing a task together by multiple people, can be beneficial in transferring the embodied skill, establishing physical relationships, engaging multiple people, and completing the task more effectively. To facilitate real-time joint bodily action, we introduce a low-latency motion transfer system (total latency: 22.9 ± 2.0ms, finger attraction latency: 13.7 ± 2.1ms) using electromagnetic actuation, which allows them to share their instantaneous actions, such as pressing the button with their fingers. Our system can change the type of motion transfer at both the direction of the transfer between users (mono- or bi-direction) and the input logic (""AND"" or ""OR"") based on the users' input. We will discuss its potential as a computer-assisted joint action and the possibility that the computer system orchestrates the joint action between the users or the user and the computer."	https://dl.acm.org/doi/abs/10.1145/3550471.3558399	Daisuke Tajima, Taku Tanichi, Shehata Mohammad H., Shunichi Kasahara
LuisaRender: A High-Performance Rendering Framework with Layered and Unified Interfaces on Stream Architectures	The advancements in hardware have drawn more attention than ever to high-quality offline rendering with modern stream processors, both in the industry and in research fields. However, the graphics APIs are fragmented and existing shading languages lack high-level constructs such as polymorphism, which adds complexity to developing and maintaining cross-platform high-performance renderers. We present LuisaRender , a high-performance rendering framework for modern stream-architecture hardware. Our main contribution is an expressive C++-embedded DSL for kernel programming with JIT code generation and compilation. We also implement a unified runtime layer with resource wrappers and an optimized Monte Carlo renderer. Experiments on test scenes show that LuisaRender achieves much higher performance than existing research renderers on modern graphics hardware, e.g., 5--11× faster than PBRT-v4 and 4--16× faster than Mitsuba 3.	https://dl.acm.org/doi/abs/10.1145/3550454.3555463	Shaokun Zheng, Zhiqian Zhou, Xin Chen, Difei Yan, Chuyan Zhang, Yuefeng Geng, Yan Gu, Kun Xu
MEMformer: Transformer-based 3D Human Motion Estimation from MoCap Markers	We address the problem of 3D human motion estimation from original MoCap optical markers. The original markers are noisy, disordered, and unlabeled, hence recovering 3D human motion from them is non-trivial. Existing works are either time-consuming or assuming the knowledge of the marker labels. We address these problems by presenting an end-to-end method for 3D human motion estimation by leveraging the capability of Transformer to model long-range dependencies. The method takes original markers as inputs and learns joint poses with a Transformer-like architecture. Experimental results show that our method is able to achieve better than centimeter-level errors.	https://dl.acm.org/doi/abs/10.1145/3550082.3564197	Jinhui Luan, Haiyong Jiang, Junqi Diao, Ying Wang, Jun Xiao
MIPNet: Neural Normal-to-Anisotropic-Roughness MIP Mapping	We present MIPNet, a novel approach for SVBRDF mipmapping which preserves material appearance under varying view distances and lighting conditions. As in classical mipmapping, our method explicitly encodes the multiscale appearance of materials in a SVBRDF mipmap pyramid. To do so, we use a tensor-based representation, coping with gradient-based optimization, for encoding anisotropy which is compatible with existing real-time rendering engines. Instead of relying on a simple texture patch average for each channel independently, we propose a cascaded architecture of multilayer perceptrons to approximate the material appearance using only the fixed material channels. Our neural model learns simple mipmapping filters using a differentiable rendering pipeline based on a rendering loss and is able to transfer signal from normal to anisotropic roughness. As a result, we obtain a drop-in replacement for standard material mipmapping, offering a significant improvement in appearance preservation while still boiling down to a single per-pixel mipmap texture fetch. We report extensive experiments on two distinct BRDF models.	https://dl.acm.org/doi/abs/10.1145/3550454.3555487	Alban Gauthier, Robin Faury, Jérémy Levallois, Théo Thonat, Jean-Marc Thiery, Tamy Boubekeur
MMGrip: A Handheld Multimodal Haptic Device Combining Vibration, Impact, and Shear for Realistic Expression of Contact	We introduce MMGrip, a handheld multimodal haptic device that simultaneously presents vibration, impact, and shear for realistic and immersive haptic feedback to virtual collision events. The three types of haptic stimulus mimic a damped vibration, a collision impulse, and a skin slip deformation, respectively, occurring at physical contact. Controlling the three stimuli, as well as their onset time differences, provides a vast design space for offering diverse and realistic haptic experiences. We present the design of MMGrip and demonstrates its application to virtual sports.	https://dl.acm.org/doi/abs/10.1145/3550082.3564177	Dong-Geun Kim, Jungeun Lee, Seungmoon Choi
Make Your Own Sprites: Aliasing-Aware and Cell-Controllable Pixelization	Pixel art is a unique art style with the appearance of low resolution images. In this paper, we propose a data-driven pixelization method that can produce sharp and crisp cell effects with controllable cell sizes. Our approach overcomes the limitation of existing learning-based methods in cell size control by introducing a reference pixel art to explicitly regularize the cell structure. In particular, the cell structure features of the reference pixel art are used as an auxiliary input for the pixelization process, and for measuring the style similarity between the generated result and the reference pixel art. Furthermore, we disentangle the pixelization process into specific cell-aware and aliasing-aware stages, mitigating the ambiguities in joint learning of cell size, aliasing effect, and color assignment. To train our model, we construct a dedicated pixel art dataset and augment it with different cell sizes and different degrees of anti-aliasing effects. Extensive experiments demonstrate its superior performance over state-of-the-arts in terms of cell sharpness and perceptual expressiveness. We also show promising results of video game pixelization for the first time. Code and dataset are available at https://github.com/WuZongWei6/Pixelization.	https://dl.acm.org/doi/abs/10.1145/3550454.3555482	Zongwei Wu, Liangyu Chai, Nanxuan Zhao, Bailin Deng, Yongtuo Liu, Qiang Wen, Junle Wang, Shengfeng He
Marginal Multiple Importance Sampling	Multiple importance sampling (MIS) is a powerful tool to combine different sampling techniques in a provably good manner. MIS requires that the techniques' probability density functions (PDFs) are readily evaluable point-wise. However, this requirement may not be satisfied when (some of) those PDFs are marginals, i.e., integrals of other PDFs. We generalize MIS to combine samples from such marginal PDFs. The key idea is to consider each marginalization domain as a continuous space of sampling techniques with readily evaluable (conditional) PDFs. We stochastically select techniques from these spaces and combine the samples drawn from them into an unbiased estimator. Prior work has dealt with the special cases of multiple classical techniques or a single marginal one. Our formulation can handle mixtures of those. We apply our marginal MIS formulation to light-transport simulation to demonstrate its utility. We devise a marginal path sampling framework that makes previously intractable sampling techniques practical and significantly broadens the path-sampling choices beyond what is presently possible. We highlight results from two algorithms based on marginal MIS: a novel formulation of path-space filtering at multiple vertices along a camera path and a similar filtering method for photon-density estimation.	https://dl.acm.org/doi/abs/10.1145/3550469.3555388	Rex West, Iliyan Georgiev, Toshiya Hachisuka
Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers	Previous studies have explored generating accurately lip-synced talking faces for arbitrary targets given audio conditions. However, most of them deform or generate the whole facial area, leading to non-realistic results. In this work, we delve into the formulation of altering only the mouth shapes of the target person. This requires masking a large percentage of the original image and seamlessly inpainting it with the aid of audio and reference frames. To this end, we propose the Audio-Visual Context-Aware Transformer (AV-CAT) framework, which produces accurate lip-sync with photo-realistic quality by predicting the masked mouth shapes. Our key insight is to exploit desired contextual information provided in audio and visual modalities thoroughly with delicately designed Transformers. Specifically, we propose a convolution-Transformer hybrid backbone and design an attention-based fusion strategy for filling the masked parts. It uniformly attends to the textural information on the unmasked regions and the reference frame. Then the semantic audio information is involved in enhancing the self-attention computation. Additionally, a refinement network with audio injection improves both image and lip-sync quality. Extensive experiments validate that our model can generate high-fidelity lip-synced results for arbitrary subjects. 	https://dl.acm.org/doi/abs/10.1145/3550469.3555393	Yasheng Sun, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Zhibin Hong, Jingtuo Liu, Errui Ding, Jingdong Wang, Ziwei Liu, Koike Hideki
MeshTaichi: A Compiler for Efficient Mesh-Based Operations	Meshes are an indispensable representation in many graphics applications because they provide conformal spatial discretizations. However, mesh-based operations are often slow due to unstructured memory access patterns. We propose MeshTaichi, a novel mesh compiler that provides an intuitive programming model for efficient mesh-based operations. Our programming model hides the complex indexing system from users and allows users to write mesh-based operations using reference-style neighborhood queries. Our compiler achieves its high performance by exploiting data locality. We partition input meshes and prepare the wanted relations by inspecting users' code during compile time. During run time, we further utilize on-chip memory (shared memory on GPU and L1 cache on CPU) to access the wanted attributes of mesh elements efficiently. Our compiler decouples low-level optimization options with computations, so that users can explore different localized data attributes and different memory orderings without changing their computation code. As a result, users can write concise code using our programming model to generate efficient mesh-based computations on both CPU and GPU backends. We test MeshTaichi on a variety of physically-based simulation and geometry processing applications with both triangle and tetrahedron meshes. MeshTaichi achieves a consistent speedup ranging from 1.4× to 6×, compared to state-of-the-art mesh data structures and compilers.	https://dl.acm.org/doi/abs/10.1145/3550454.3555430	Chang Yu, Yi Xu, Ye Kuang, Yuanming Hu, Tiantian Liu
Metamodernity	One person is born out of his consciousness. He goes on a surreal internal journey to find out who he is. He awakens a butterfly in his heart, he sometimes shares his thoughts nd observes different parts and aspects of the world. In the end of the journey full of various emotions and ups and downs, nothing but death is waiting for him. But soon he realizes that this death is the true birth and transcendence. Beautiful enchanting colors and sounds fill in his world and his true self comes out to the world. His butterfly now flies freely in his colorful sky.	https://dl.acm.org/doi/abs/10.1145/3550339.3556085	Daebum Im, Erick Oh, Keyon Kim
Metappearance: Meta-Learning for Visual Appearance Reproduction	There currently exist two main approaches to reproducing visual appearance using Machine Learning (ML): The first is training models that generalize over different instances of a problem, e.g., different images of a dataset. As one-shot approaches, these offer fast inference, but often fall short in quality. The second approach does not train models that generalize across tasks, but rather over-fit a single instance of a problem, e.g., a flash image of a material. These methods offer high quality, but take long to train. We suggest to combine both techniques end-to-end using meta-learning: We over-fit onto a single problem instance in an inner loop, while also learning how to do so efficiently in an outer-loop across many exemplars. To this end, we derive the required formalism that allows applying meta-learning to a wide range of visual appearance reproduction problems: textures, Bidirectional Reflectance Distribution Functions (BRDFs), spatially-varying BRDFs (svBRDFs), illumination or the entire light transport of a scene. The effects of meta-learning parameters on several different aspects of visual appearance are analyzed in our framework, and specific guidance for different tasks is provided. Metappearance enables visual quality that is similar to over-fit approaches in only a fraction of their runtime while keeping the adaptivity of general models.	https://dl.acm.org/doi/abs/10.1145/3550454.3555458	Michael Fischer, Tobias Ritschel
Method of Creating Video Content that Cause The Sensation of Falling	In recent years, video content that enables users to experience a simulated fall using an head mounted display (HMD) has become popular. However, the visual factors that cause the sensation of falling have not been clarified. In this study, we focused on visual stimuli, which are known to affect the intensity of vection, and examined their relationship to the sensation of falling.The results confirmed that stimulus presented from the peripheral visual field area and increased spatial frequency amplified the sensation of falling. In addition, we were able to produce video content that produced a greater sense of falling by using multiple visual stimuli that had the property of increasing vection.	https://dl.acm.org/doi/abs/10.1145/3550082.3564208	Kaho Iwasaki, Yuji Sakamoto
Metric-KNN is All You Need	In this work, we propose a novel Metric-K Nearest Neighbor (-KNN) to facilitate topology aware learning in point clouds. Topology aware learning is achieved by accumulation of local features in deep-learning model. Recent work rely on Ball queries or K-Nearest-Neighbor (KNN) for local feature extraction of point clouds and finds challenges in retaining topological information. -KNN employes a generalised Minkowski distance in the KNN search algorithm for topological representation of point clouds. -KNN enables state-of-the-art point cloud methods to perform topology aware downstream tasks. We demonstrate the performance of -KNN as plugin towards point cloud classification, part-segmentation, and denoising using benchmark dataset.	https://dl.acm.org/doi/abs/10.1145/3550082.3564198	Tejas Anvekar, Ramesh Ashok Tabib, Dikshit Hegde, Uma Mudenagudi
Mixed Variational Finite Elements for Implicit Simulation of Deformables	We propose and explore a new method for the implicit time integration of elastica. Key to our approach is the use of a mixed variational principle. In turn, its finite element discretization leads to an efficient and accurate sequential quadratic programming solver with a superset of the desirable properties of many previous integration strategies. This framework fits a range of elastic constitutive models and remains stable across a wide span of time step sizes and material parameters (including problems that are approximately rigid). Our method exhibits convergence on par with full Newton type solvers and also generates visually plausible results in just a few iterations comparable to recent fast simulation methods that do not converge. These properties make it suitable for both offline accurate simulation and performant applications with expressive physics. We demonstrate the efficacy of our approach on a number of simulated examples.	https://dl.acm.org/doi/abs/10.1145/3550469.3555418	Ty Trusty, Danny Kaufman, David I.W. Levin
Modern Rescue 2022ss	This work is a virtual catwalk animation based on modern urban diseases, consisting three themes: information leakage, social behavior alienation, and environmental degradation. Taking the catwalk as the form of presentation. This work aims to inspire people to focus on the Sustainable Development Goals by realizing the environmental damage.	https://dl.acm.org/doi/abs/10.1145/3550470.3558447	Yuxi Mao, Fang Fang, Tianyou Xue
Moments in Nature	Paradise creek, is a small river running between two cities. Realizing some of the original environment was altered, we created a story where herons and humans encounter each other in unexpected ways and the original river space could be discovered. The story unfolds as the audience observes the heron in flight. Telling the story of the heron becomes a collaborative effort between a team on stage and audience members in the theater. The participants on stage help to find and rescue the red heron. In real-time, viewers witness on screen the interactive story unfolding inside the virtual river.	https://dl.acm.org/doi/abs/10.1145/3550453.3570126	Jean-Marc Gauthier
Morig: Motion-Aware Rigging of Character Meshes from Point Clouds	We present MoRig, a method that automatically rigs character meshes driven by single-view point cloud streams capturing the motion of performing characters. Our method is also able to animate the 3D meshes according to the captured point cloud motion. MoRig's neural network encodes motion cues from the point clouds into features that are informative about the articulated parts of the performing character. These motion-aware features guide the inference of an appropriate skeletal rig for the input mesh, which is then animated based on the point cloud motion. Our method can rig and animate diverse characters, including humanoids, quadrupeds, and toys with varying articulation. It accounts for occluded regions in the point clouds and mismatches in the part proportions between the input mesh and captured character. Compared to other rigging approaches that ignore motion cues, MoRig produces more accurate rigs, well-suited for re-targeting motion from captured characters.	https://dl.acm.org/doi/abs/10.1145/3550469.3555390	Zhan Xu, Yang Zhou, Li Yi, Evangelos Kalogerakis
Motion Guided Deep Dynamic 3D Garments	Realistic dynamic garments on animated characters have many AR/VR applications. While authoring such dynamic garment geometry is still a challenging task, data-driven simulation provides an attractive alternative, especially if it can be controlled simply using the motion of the underlying character. In this work, we focus on motion guided dynamic 3D garments, especially for loose garments. In a data-driven setup, we first learn a generative space of plausible garment geometries. Then, we learn a mapping to this space to capture the motion dependent dynamic deformations, conditioned on the previous state of the garment as well as its relative position with respect to the underlying body. Technically, we model garment dynamics, driven using the input character motion, by predicting per-frame local displacements in a canonical state of the garment that is enriched with frame-dependent skinning weights to bring the garment to the global space. We resolve any remaining per-frame collisions by predicting residual local displacements. The resultant garment geometry is used as history to enable iterative roll-out prediction. We demonstrate plausible generalization to unseen body shapes and motion inputs, and show improvements over multiple state-of-the-art alternatives.	https://dl.acm.org/doi/abs/10.1145/3550454.3555485	Meng Zhang, Duygu Ceylan, Niloy J. Mitra
Motion In-Betweening via Two-Stage Transformers	We present a deep learning-based framework to synthesize motion in-betweening in a two-stage manner. Given some context frames and a target frame, the system can generate plausible transitions with variable lengths in a non-autoregressive fashion. The framework consists of two Transformer Encoder-based networks operating in two stages: in the first stage a Context Transformer is designed to generate rough transitions based on the context and in the second stage a Detail Transformer is employed to refine motion details. Compared to existing Transformer-based methods which either use a complete Transformer Encoder-Decoder architecture or additional 1D convolutions to generate motion transitions, our framework achieves superior performance with less trainable parameters by only leveraging the Transformer Encoder and masked self-attention mechanism. To enhance the generalization of our transformer-based framework, we further introduce Keyframe Positional Encoding and Learned Relative Positional Encoding to make our method robust in synthesizing longer transitions exceeding the maximum transition length during training. Our framework is also artist-friendly by supporting full and partial pose constraints within the transition, giving artists fine control over the synthesized results. We benchmark our framework on the LAFAN1 dataset, and experiments show that our method outperforms the current state-of-the-art methods at a large margin (an average of 16% for normal-length sequences and 55% for excessive-length sequences). Our method trains faster than the RNN-based method and achieves a four-time speedup during inference. We implement our framework into a production-ready tool inside an animation authoring software and conduct a pilot study to validate the practical value of our method.	https://dl.acm.org/doi/abs/10.1145/3550454.3555454	Jia Qin, Youyi Zheng, Kun Zhou
Motion In-betweening for Physically Simulated Characters	We present a motion in-betweening framework to generate high quality, physically plausible character animation when we are given temporally sparse keyframes as soft animation constraints. More specifically, we learn imitation policies for physically simulated characters by using deep reinforcement learning where the policies can access limited information only. Once learned, the physically simulated characters are capable of adapting to external perturbations while following given sparse input keyframes. We demonstrate the performance of our framework on two different motion datasets and also compare our results with the the results generated by a baseline imitation policy.	https://dl.acm.org/doi/abs/10.1145/3550082.3564186	Deepak Gopinath, Hanbyul Joo, Jungdam Won
MovableBag: Substitutional Robot for Enhancing Immersive Boxing Training with Encountered-Type Haptic	Boxing is a combat sport that involves fierce movement and close body contact between two boxers. In addition to the boxer, the coach is also essential for training. The coach could be injured or too tired to continue consecutive training sessions for many boxers. Thus, we present MovableBag, a substitutional robot that provides room-scale mobility and the ability to carry sport props with an easier tracking setup to simulate the boxing opponent or coach in virtual reality with encountered-type haptic feedback. In addition, we integrate our system with a wireless VR headset and develop a VR boxing application to show the capacity of the device and inform future studies.	https://dl.acm.org/doi/abs/10.1145/3550472.3558406	Luis Andres Mendez S., Ho Yin Ng, Zin Yin Lim, Yi-Jie Lu, Ping-Hsuan Han
MyStyle: A Personalized Generative Prior	We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (~ 100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.	https://dl.acm.org/doi/abs/10.1145/3550454.3555436	Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, Daniel Cohen-Or
NeRFFaceEditing: Disentangled Face Editing in Neural Radiance Fields	Recent methods for synthesizing 3D-aware face images have achieved rapid development thanks to neural radiance fields, allowing for high quality and fast inference speed. However, existing solutions for editing facial geometry and appearance independently usually require retraining and are not optimized for the recent work of generation, thus tending to lag behind the generation process. To address these issues, we introduce NeRFFaceEditing, which enables editing and decoupling geometry and appearance in the pretrained tri-plane-based neural radiance field while retaining its high quality and fast inference speed. Our key idea for disentanglement is to use the statistics of the tri-plane to represent the high-level appearance of its corresponding facial volume. Moreover, we leverage a generated 3D-continuous semantic mask as an intermediary for geometry editing. We devise a geometry decoder (whose output is unchanged when the appearance changes) and an appearance decoder. The geometry decoder aligns the original facial volume with the semantic mask volume. We also enhance the disentanglement by explicitly regularizing rendered images with the same appearance but different geometry to be similar in terms of color distribution for each facial component separately. Our method allows users to edit via semantic masks with decoupled control of geometry and appearance. Both qualitative and quantitative evaluations show the superior geometry and appearance control abilities of our method compared to existing and alternative solutions.	https://dl.acm.org/doi/abs/10.1145/3550469.3555377	Kaiwen Jiang, Shu-Yu Chen, Feng-Lin Liu, Hongbo Fu, Lin Gao
NeuLighting: Neural Lighting for Free Viewpoint Outdoor Scene Relighting with Unconstrained Photo Collections	We propose NeuLighting, a new framework for free viewpoint outdoor scene relighting from a sparse set of unconstrained in-the-wild photo collections. Our framework represents all the scene components as continuous functions parameterized by MLPs that take a 3D location and the lighting condition as input and output reflectance and necessary outdoor illumination properties. Unlike object-level relighting methods which often leverage training images with controllable and consistent indoor illumination, we concentrate on the more challenging outdoor situation where all the images are captured under arbitrary unknown illumination. The key to our method includes a neural lighting representation that compresses the per-image illumination into a disentangled latent vector, and a new free viewpoint relighting scheme that is robust to arbitrary lighting variations across images. The lighting representation is compressive to explain a wide range of illumination and can be easily fed into the query-based NeuLighting framework, enabling efficient shading effect evaluation under any kind of novel illumination. Furthermore, to produce high-quality cast shadows, we estimate the sun visibility map to indicate the shadow regions according to the scene geometry and the sun direction. Thanks to the flexible and explainable neural lighting representation, our system supports outdoor relighting with many different illumination sources, including natural images, environment maps, and time-lapse videos. The high-fidelity renderings under novel views and illumination prove the superiority of our method against state-of-the-art relighting solutions.	https://dl.acm.org/doi/abs/10.1145/3550469.3555384	Quewei Li, Jie Guo, Yang Fei, Feichao Li, Yanwen Guo
Neural Bidirectional Texture Function Compression and Rendering	The recent success of Machine Learning encouraged research using artificial neural networks (NNs) in computer graphics. A good example is the bidirectional texture function (BTF), a data-driven representation of surface materials that can encapsulate complex behaviors that would otherwise be too expensive to calculate for real-time applications, such as self-shadowing and interreflections. We propose two changes to the state-of-the-art using neural networks for BTFs, specifically NeuMIP. These changes, suggested by recent work in neural scene representation and rendering, aim to improve baseline quality, memory footprint, and performance. We conduct an ablation study to evaluate the impact of each change. We test both synthetic and real data, and provide a working implementation within the Mitsuba 2 rendering framework. Our results show that our method outperforms the baseline in all these metrics and that neural BTF is part of the broader field of neural scene representation. Project website: https://traverse-research.github.io/NeuBTF/.	https://dl.acm.org/doi/abs/10.1145/3550082.3564188	Luca Quartesan, Carlos Pereira Santos
Neural Brushstroke Engine: Learning a Latent Style Space of Interactive Drawing Tools	We propose Neural Brushstroke Engine, the first method to apply deep generative models to learn a distribution of interactive drawing tools. Our conditional GAN model learns the latent space of drawing styles from a small set (about 200) of unlabeled images in different media. Once trained, a single model can texturize stroke patches drawn by the artist, emulating a diverse collection of brush styles in the latent space. In order to enable interactive painting on a canvas of arbitrary size, we design a painting engine able to support real-time seamless patch-based generation, while allowing artists direct control of stroke shape, color and thickness. We show that the latent space learned by our model generalizes to unseen drawing and more experimental styles (e.g. beads) by embedding real styles into the latent space. We explore other applications of the continuous latent space, such as optimizing brushes to enable painting in the style of an existing artwork, automatic line drawing stylization, brush interpolation, and even natural language search over a continuous space of drawing tools. Our prototype received positive feedback from a small group of digital artists.	https://dl.acm.org/doi/abs/10.1145/3550454.3555472	Maria Shugrina, Chin-Ying Li, Sanja Fidler
Neural Cloth Simulation	We present a general framework for the garment animation problem through unsupervised deep learning inspired in physically based simulation. Existing trends in the literature already explore this possibility. Nonetheless, these approaches do not handle cloth dynamics. Here, we propose the first methodology able to learn realistic cloth dynamics unsupervisedly, and henceforth, a general formulation for neural cloth simulation. The key to achieve this is to adapt an existing optimization scheme for motion from simulation based methodologies to deep learning. Then, analyzing the nature of the problem, we devise an architecture able to automatically disentangle static and dynamic cloth subspaces by design. We will show how this improves model performance. Additionally, this opens the possibility of a novel motion augmentation technique that greatly improves generalization. Finally, we show it also allows to control the level of motion in the predictions. This is a useful, never seen before, tool for artists. We provide of detailed analysis of the problem to establish the bases of neural cloth simulation and guide future research into the specifics of this domain.	https://dl.acm.org/doi/abs/10.1145/3550454.3555491	Hugo Bertiche, Meysam Madadi, Sergio Escalera
Neural James-Stein Combiner for Unbiased and Biased Renderings	Unbiased rendering algorithms such as path tracing produce accurate images given a huge number of samples, but in practice, the techniques often leave visually distracting artifacts (i.e., noise) in their rendered images due to a limited time budget. A favored approach for mitigating the noise problem is applying learning-based denoisers to unbiased but noisy rendered images and suppressing the noise while preserving image details. However, such denoising techniques typically introduce a systematic error, i.e., the denoising bias, which does not decline as rapidly when increasing the sample size, unlike the other type of error, i.e., variance. It can technically lead to slow numerical convergence of the denoising techniques. We propose a new combination framework built upon the James-Stein (JS) estimator, which merges a pair of unbiased and biased rendering images, e.g., a path-traced image and its denoised result. Unlike existing post-correction techniques for image denoising, our framework helps an input denoiser have lower errors than its unbiased input without relying on accurate estimation of per-pixel denoising errors. We demonstrate that our framework based on the well-established JS theories allows us to improve the error reduction rates of state-of-the-art learning-based denoisers more robustly than recent post-denoisers.	https://dl.acm.org/doi/abs/10.1145/3550454.3555496	Jeongmin Gu, Jose A. Iglesias-Guitian, Bochang Moon
Neural Parameterization for Dynamic Human Head Editing	Implicit radiance functions emerged as a powerful scene representation for reconstructing and rendering photo-realistic views of a 3D scene. These representations, however, suffer from poor editability. On the other hand, explicit representations such as polygonal meshes allow easy editing but are not as suitable for reconstructing accurate details in dynamic human heads, such as fine facial features, hair, teeth, and eyes. In this work, we present Neural Parameterization (NeP), a hybrid representation that provides the advantages of both implicit and explicit methods. NeP is capable of photo-realistic rendering while allowing fine-grained editing of the scene geometry and appearance. We first disentangle the geometry and appearance by parameterizing the 3D geometry into 2D texture space. We enable geometric editability by introducing an explicit linear deformation blending layer. The deformation is controlled by a set of sparse key points, which can be explicitly and intuitively displaced to edit the geometry. For appearance, we develop a hybrid 2D texture consisting of an explicit texture map for easy editing and implicit view and time-dependent residuals to model temporal and view variations. We compare our method to several reconstruction and editing baselines. The results show that the NeP achieves almost the same level of rendering accuracy while maintaining high editability.	https://dl.acm.org/doi/abs/10.1145/3550454.3555494	Li Ma, Xiaoyu Li, Jing Liao, Xuan Wang, Qi Zhang, Jue Wang, Pedro V. Sander
Neural Photo-Finishing	Image processing pipelines are ubiquitous and we rely on them either directly, by filtering or adjusting an image post-capture, or indirectly, as image signal processing (ISP) pipelines on broadly deployed camera systems. Used by artists, photographers, system engineers, and for downstream vision tasks, traditional image processing pipelines feature complex algorithmic branches developed over decades. Recently, image-to-image networks have made great strides in image processing, style transfer, and semantic understanding. The differentiable nature of these networks allows them to fit a large corpus of data; however, they do not allow for intuitive, fine-grained controls that photographers find in modern photo-finishing tools. This work closes that gap and presents an approach to making complex photo-finishing pipelines differentiable, allowing legacy algorithms to be trained akin to neural networks using first-order optimization methods. By concatenating tailored network proxy models of individual processing steps (e.g. white-balance, tone-mapping, color tuning), we can model a non-differentiable reference image finishing pipeline more faithfully than existing proxy image-to-image network models. We validate the method for several diverse applications, including photo and video style transfer, slider regression for commercial camera ISPs, photography-driven neural demosaicking, and adversarial photo-editing.	https://dl.acm.org/doi/abs/10.1145/3550454.3555526	Ethan Tseng, Yuxuan Zhang, Lars Jebe, Xuaner Zhang, Zhihao Xia, Yifei Fan, Felix Heide, Jiawen Chen
Neural Point Catacaustics for Novel-View Synthesis of Reflections	View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/	https://dl.acm.org/doi/abs/10.1145/3550454.3555497	Georgios Kopanas, Thomas Leimkühler, Gilles Rainer, Clément Jambon, George Drettakis
Neural Wavelet-domain Diffusion for 3D Shape Generation	This paper presents a new approach for 3D shape generation, enabling direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets, and formulate a pair of neural networks: a generator based on the diffusion model to produce diverse shapes in the form of coarse coefficient volumes; and a detail predictor to further produce compatible detail coefficient volumes for enriching the generated shapes with fine structures and details. Both quantitative and qualitative experimental results manifest the superiority of our approach in generating diverse and high-quality shapes with complex topology and structures, clean surfaces, and fine details, exceeding the 3D generation capabilities of the state-of-the-art models.	https://dl.acm.org/doi/abs/10.1145/3550469.3555394	Ka-Hei Hui, Ruihui Li, Jingyu Hu, Chi-Wing Fu
NeuralMarker: A Framework for Learning General Marker Correspondence	We tackle the problem of estimating correspondences from a general marker, such as a movie poster, to an image that captures such a marker. Conventionally, this problem is addressed by fitting a homography model based on sparse feature matching. However, they are only able to handle plane-like markers and the sparse features do not sufficiently utilize appearance information. In this paper, we propose a novel framework NeuralMarker, training a neural network estimating dense marker correspondences under various challenging conditions, such as marker deformation, harsh lighting, etc. Deep learning has presented an excellent performance in correspondence learning once provided with sufficient training data. However, annotating pixel-wise dense correspondence for training marker correspondence is too expensive. We observe that the challenges of marker correspondence estimation come from two individual aspects: geometry variation and appearance variation. We, therefore, design two components addressing these two challenges in NeuralMarker. First, we create a synthetic dataset FlyingMarkers containing marker-image pairs with ground truth dense correspondences. By training with FlyingMarkers, the neural network is encouraged to capture various marker motions. Second, we propose the novel Symmetric Epipolar Distance (SED) loss, which enables learning dense correspondence from posed images. Learning with the SED loss and the cross-lighting posed images collected by Structure-from-Motion (SfM), NeuralMarker is remarkably robust in harsh lighting environments and avoids synthetic image bias. Besides, we also propose a novel marker correspondence evaluation method circumstancing annotations on real marker-image pairs and create a new benchmark. We show that NeuralMarker significantly outperforms previous methods and enables new interesting applications, including Augmented Reality (AR) and video editing.	https://dl.acm.org/doi/abs/10.1145/3550454.3555468	Zhaoyang Huang, Xiaokun Pan, Weihong Pan, Weikang Bian, Yan Xu, Ka Chun Cheung, Guofeng Zhang, Hongsheng Li
NeuralRoom: Geometry-Constrained Neural Implicit Surfaces for Indoor Scene Reconstruction	We present a novel neural surface reconstruction method called NeuralRoom for reconstructing room-sized indoor scenes directly from a set of 2D images. Recently, implicit neural representations have become a promising way to reconstruct surfaces from multiview images due to their high-quality results and simplicity. However, implicit neural representations usually cannot reconstruct indoor scenes well because they suffer severe shape-radiance ambiguity. We assume that the indoor scene consists of texture-rich and flat texture-less regions. In texture-rich regions, the multiview stereo can obtain accurate results. In the flat area, normal estimation networks usually obtain a good normal estimation. Based on the above observations, we reduce the possible spatial variation range of implicit neural surfaces by reliable geometric priors to alleviate shape-radiance ambiguity. Specifically, we use multiview stereo results to limit the NeuralRoom optimization space and then use reliable geometric priors to guide NeuralRoom training. Then the NeuralRoom would produce a neural scene representation that can render an image consistent with the input training images. In addition, we propose a smoothing method called perturbation-residual restrictions to improve the accuracy and completeness of the flat region, which assumes that the sampling points in a local surface should have the same normal and similar distance to the observation center. Experiments on the ScanNet dataset show that our method can reconstruct the texture-less area of indoor scenes while maintaining the accuracy of detail. We also apply NeuralRoom to more advanced multiview reconstruction algorithms and significantly improve their reconstruction quality.	https://dl.acm.org/doi/abs/10.1145/3550454.3555514	Yusen Wang, Zongcheng Li, Yu Jiang, Kaixuan Zhou, Tuo Cao, Yanping Fu, Chunxia Xiao
Nippo-Latin American Land	The installation consists of an urban/rural Japanese landscape influenced by a Latinx community. Model structures are made of printed photographs, and small screens for seeing details, both part of a bigger archive. In this setting, attendees will be able to see the visual communication of the city.	https://dl.acm.org/doi/abs/10.1145/3550470.3558437	Marita Ibañez Sandoval
No-code Digital Human for Conversational Behavior	In this poster, we present Flow Human, a no-code system that generates conversational behavior of digital humans from the text. Our users only need to build a conversation flow they want to talk to customers using the flow-based authoring tool we developed. Our system then automatically generates the verbal and non-verbal behavior of digital humans along the conversation flow, interacts with customers, and collects feedback. We believe that this work can serve the potential to be distributed to various services that have not been introduced because of the challenging task of controlling multiple factors in digital humans (e.g., conversation flow, co-speech gestures, and facial animation).	https://dl.acm.org/doi/abs/10.1145/3550082.3564175	Hanseob Kim, Jieun Kim, Ghazanfar Ali, Jae-In Hwang
OVERPAINT: Automatic Multi-Layer Stencil Generation without Bridges	Stencil art is a drawing technique for reproducing designs by repeatedly painting pigments through stencil sheets. Each stencil sheet is a connected surface maintained via thin connections called bridges. Existing computer-aided stencil art creation studies focus on choosing a set of bridges that causes the least amount of distortion while fully connecting all islands to the sheet. Unlike previous works, we propose a novel method called OVERPAINT that sidesteps the use of bridges by overpainting pigments. Our method automatically generates an ordered set of bridge-free stencil sheets from an input image. By painting sequentially, one can obtain a result resembling the appearance of the input image. We demonstrate our method on several artworks and a user study. The results show that our method enables the user to create visually precise and delicate stencil arts.	https://dl.acm.org/doi/abs/10.1145/3550340.3564217	Yuta Fukushima, Anran Qi, I-Chao Shen, Takeo Igarashi
Optical Parameter Estimation for Hair and Fur using Differentiable Rendering	In this paper, we propose a parameter estimation method for Bidirectional Curve Scattering Distribution Function (BCSDF) for hair and fur using differentiable rendering. Given fiber geometries, illumination, and a target rendered image, our method jointly estimates optical parameters of BCSDF so as to match the target image taking into account indirect illumination. We develop a differentiable path-tracing renderer tailored to estimate optical parameters of BCSDF. To avoid getting stuck in a local minimum, we propose a two-stage estimation method where the first stage estimates two optical parameters affecting hair and fur's entire appearance. Then, all the optical parameters are jointly estimated in the second stage. Experimental results substantiate that our method can automatically estimate optical parameters of hair and fur.	https://dl.acm.org/doi/abs/10.1145/3550340.3564221	Yusei Shibaike, Kei Iwasaki
Origami Tessellations Induced by Growth	We create a series of origami tessellations by simulating the growth of membranes. The self-organized winkling patterns allow the transformation of one surface into another with a different curvature without stretching or shrinking. Our work demonstrates the universality of the principle of folding in nature, art, and mathematics.	https://dl.acm.org/doi/abs/10.1145/3550470.3558440	Tomohiro Tachi, Junichiro Horikawa, Daiki Kanaoka
Out of the Cave: An Immersive VR Comic About a Journey of Self-Healing	"""Out of the Cave"" is an immersive VR comic. Its story revolves around a girl struggling with the regrets of her past relationship and stepping on a journey to heal the scars. This work blends webcomics into a spatial medium to create a brand-new narrative experience. The audience can dive into the story world and complete the journey of pursuit and self-healing with the heroine."	https://dl.acm.org/doi/abs/10.1145/3550472.3558414	Yu-Han Li, Chia-Yin Chang, Neng-Hao Yu
PADL: Language-Directed Physics-Based Character Control	Developing systems that can synthesize natural and life-like motions for simulated characters has long been a focus for computer animation. But in order for these systems to be useful for downstream applications, they need not only produce high-quality motions, but must also provide an accessible and versatile interface through which users can direct a character's behaviors. Natural language provides a simple-to-use and expressive medium for specifying a user's intent. Recent breakthroughs in natural language processing (NLP) have demonstrated effective use of language-based interfaces for applications such as image generation and program synthesis. In this work, we present PADL, which leverages recent innovations in NLP in order to take steps towards developing language-directed controllers for physics-based character animation. PADL allows users to issue natural language commands for specifying both high-level tasks and low-level skills that a character should perform. We present an adversarial imitation learning approach for training policies to map high-level language commands to low-level controls that enable a character to perform the desired task and skill specified by a user's commands. Furthermore, we propose a multi-task aggregation method that leverages a language-based multiple-choice question-answering approach to determine high-level task objectives from language commands. We show that our framework can be applied to effectively direct a simulated humanoid character to perform a diverse array of complex motor skills.	https://dl.acm.org/doi/abs/10.1145/3550469.3555391	Jordan Juravsky, Yunrong Guo, Sanja Fidler, Xue Bin Peng
Palette-based Image Search with Color Weights	We propose a novel image search system with color palettes. By querying color palettes, users can search for inspiring images, which is helpful for design exploration. Although few such systems can accept palettes as queries, they have several constraints on query palettes (e.g., limited palette size) or search results. Our system accepts palettes with color weights and any sizes as the inputs and returns results with enough diversity that can stimulate users' design inspiration. To achieve these, we extract color themes from our database with a perceptually-based model and calculate the palette similarity based on a weighted palette distance. Visual comparisons demonstrate that our system has more potential for helping users' design inspiration than the existing systems.	https://dl.acm.org/doi/abs/10.1145/3550082.3564203	Naoki Kita, Shiori Kawasaki, Takafumi Saito
Pattern-Based Cloth Registration and Sparse-View Animation	We propose a novel multi-view camera pipeline for the reconstruction and registration of dynamic clothing. Our proposed method relies on a specifically designed pattern that allows for precise video tracking in each camera view. We triangulate the tracked points and register the cloth surface in a fine-grained geometric resolution and low localization error. Compared to state-of-the-art methods, our registration exhibits stable correspondence, tracking the same points on the deforming cloth surface along the temporal sequence. As an application, we demonstrate how the use of our registration pipeline greatly improves state-of-the-art pose-based drivable cloth models. Furthermore, we propose a novel model, , for driving cloth from a dense tracking signal which is obtained from two opposing camera views. The method produces realistic reconstructions which are faithful to the actual geometry of the deforming cloth. In this setting, the user wears a garment with our custom pattern which enables our driving model to reconstruct the geometry. Our code and data are available at https://github.com/HalimiOshri/Pattern-Based-Cloth-Registration-and-Sparse-View-Animation. The released data includes our pattern and registered mesh sequences containing four different subjects and 15k frames in total.	https://dl.acm.org/doi/abs/10.1145/3550454.3555448	Oshri Halimi, Tuur Stuyck, Donglai Xiang, Timur Bagautdinov, He Wen, Ron Kimmel, Takaaki Shiratori, Chenglei Wu, Yaser Sheikh, Fabian Prada
People of the Gold	"""People of the Gold"" is a project that reflects on the identity, and history of the early Chinese immigrants in California during the period of Gold Rush. The project consists of multiple archive-looking moving images generated by a machine learning algorithm trained with historical documents. The work is presented in a dimmed room with interface for viewer to browse. The constantly changing portrait photo of people and the ""identity information"" next to the photos are presented. Viewers are invited to browse those files slowly, replicating the experience of browsing historical documents. The project aims to raise awareness and emotions to the history of those people of the gold through exploring the potential of machine learning algorithm in the field of photographic imaging and anthropology studies."	https://dl.acm.org/doi/abs/10.1145/3550470.3558436	Ren Yang
Perception of War: AI Data Universe	Perception of War is a data art that visualizes human perception data about the Ukraine-Russia war by replacing it with emotions through artificial intelligence technology. Data are collected from SNS media that relay human perception by replacing it with a message. 60,000 images, texts, and latitude data tagged with the Ukraine-Russia War were collected, and the perception inherent in each data was analyzed and classified as emotions through artificial intelligence technology. Each emotion is given a unique visual tactility function such as color, pattern, and texture, and is expressed towards the Ukraine latitude. The human perception left behind in Ukraine is in an aesthetically fascinating and somewhat bizarre form. This is a means of exploring the fragments of war left in the present and in the future, and presents a discourse about heterogeneity in the definition of war.	https://dl.acm.org/doi/abs/10.1145/3550470.3558435	Haein Yoon, Jungho Kim, Hyoungin Kim, Taekyung Yoo
Period Drama	Our film, called Period Drama, is a Victorian-era tale about eleven-year-old Georgiana Crimsworth, who gets her first period before ever having gotten the talk. Partly based on personal experience, this story aims to get across the idea that at that age, this sort of situation really makes you feel like you're trapped in a gothic horror story. Menstruation, and by extension women's health as a whole, isn't often discussed in popular media - and almost never in a medium like animation. Because of the implicit taboo that exists, there's still a lot of shame and ignorance that surrounds a period, when that certainly shouldn't be the case. Period Drama is our effort to address this, and hopefully make the subject easier to discuss, too.	https://dl.acm.org/doi/abs/10.1145/3550339.3556650	Anushka Nair, Lauryn Anthony
Physical Interaction: Reconstructing Hand-object Interactions with Physics	Single view-based reconstruction of hand-object interaction is challenging due to the severe observation missing caused by occlusions. This paper proposes a physics-based method to better solve the ambiguities in the reconstruction. It first proposes a force-based dynamic model of the in-hand object, which not only recovers the unobserved contacts but also solves for plausible contact forces. Next, a confidence-based slide prevention scheme is proposed, which combines both the kinematic confidences and the contact forces to jointly model static and sliding contact motion. Qualitative and quantitative experiments show that the proposed technique reconstructs both physically plausible and more accurate hand-object interaction and estimates plausible contact forces in real-time with a single RGBD sensor.	https://dl.acm.org/doi/abs/10.1145/3550469.3555421	Haoyu Hu, Xinyu Yi, Hao Zhang, Jun-Hai Yong, Feng Xu
PopStage: The Generation of Stage Cross-Editing Video Based on Spatio-Temporal Matching	is a mixed video that is created by concatenating the segments from various performance videos of an identical song in a visually smooth manner by matching the main subject's silhouette presented in the frame. We introduce , which allows users to generate a StageMix automatically. PopStage is designed based on the StageMix Editing Guideline that we established by interviewing creators as well as observing their workflows. PopStage consists of two main steps: finding an editing path and generating a transition effect at a transition point. Using a reward function that favors visual connection and the optimality of transition timing across the videos, we obtain the optimal path that maximizes the sum of rewards through dynamic programming. Given the optimal path, PopStage then aligns the silhouettes of the main subject from the transitioning video pair to enhance the visual connection at the transition point. The virtual camera view is next optimized to remove the black areas that are often created due to the transformation needed for silhouette alignment, while reducing pixel loss. In this process, we enforce the view to be the maximum size while maintaining the temporal continuity across the frames. Experimental results show that PopStage can generate a StageMix of a similar quality to those produced by professional creators in a highly reduced production time.	https://dl.acm.org/doi/abs/10.1145/3550454.3555467	Dawon Lee, Jung Eun Yoo, Kyungmin Cho, Bumki Kim, Gyeonghun Im, Junyong Noh
Position-Based Surface Tension Flow	This paper presents a novel approach to simulating surface tension flow within a position-based dynamics (PBD) framework. We enhance the conventional PBD fluid method in terms of its surface representation and constraint enforcement to furnish support for the simulation of interfacial phenomena driven by strong surface tension and contact dynamics. The key component of our framework is an on-the-fly local meshing algorithm to build the local geometry around each surface particle. Based on this local mesh structure, we devise novel surface constraints that can be integrated seamlessly into a PBD framework to model strong surface tension effects. We demonstrate the efficacy of our approach by simulating a multitude of surface tension flow examples exhibiting intricate interfacial dynamics of films and drops, which were all infeasible for a traditional PBD method.	https://dl.acm.org/doi/abs/10.1145/3550454.3555476	Jingrui Xing, Liangwang Ruan, Bin Wang, Bo Zhu, Baoquan Chen
Procedural Modeling of Crystal Clusters	We propose procedural modeling of crystal clusters. Based on crystallography, we model single crystals and then distribute single crystals on a base rock using a hierarchical sampling approach. Users can interactively generate various crystal clusters, including amethyst and citrine clusters. Furthermore, our approach offers a high-level specification, i.e., target shape so that the generated clusters approximate the shape; such controllability is crucial for realizing artists' intentions. Additionally, we focused on fabrication-oriented procedural modeling. We identify the possible parameter spaces based on physical experiments, i.e., test the removability of resin casts from silicone molds produced by the 3D printed crystal clusters for making replicas.	https://dl.acm.org/doi/abs/10.1145/3550082.3564165	Naoki Kita, Satoshi Tsukii, Miwako Tsuru
Production-Ready Face Re-Aging for Visual Effects	Photorealistic digital re-aging of faces in video is becoming increasingly common in entertainment and advertising. But the predominant 2D painting workflow often requires frame-by-frame manual work that can take days to accomplish, even by skilled artists. Although research on facial image re-aging has attempted to automate and solve this problem, current techniques are of little practical use as they typically suffer from facial identity loss, poor resolution, and unstable results across subsequent video frames. In this paper, we present the first practical, fully-automatic and production-ready method for re-aging faces in video images. Our first key insight is in addressing the problem of collecting longitudinal training data for learning to re-age faces over extended periods of time, a task that is nearly impossible to accomplish for a large number of real people. We show how such a longitudinal dataset can be constructed by leveraging the current state-of-the-art in facial re-aging that, although failing on real images, does provide photoreal re-aging results on synthetic faces. Our second key insight is then to leverage such synthetic data and formulate facial re-aging as a practical image-to-image translation task that can be performed by training a well-understood U-Net architecture, without the need for more complex network designs. We demonstrate how the simple U-Net, surprisingly, allows us to advance the state of the art for re-aging real faces on video, with unprecedented temporal stability and preservation of facial identity across variable expressions, viewpoints, and lighting conditions. Finally, our new face re-aging network (FRAN) incorporates simple and intuitive mechanisms that provides artists with localized control and creative freedom to direct and fine-tune the re-aging effect, a feature that is largely important in real production pipelines and often overlooked in related research work.	https://dl.acm.org/doi/abs/10.1145/3550454.3555520	Gaspard Zoss, Prashanth Chandran, Eftychios Sifakis, Markus Gross, Paulo Gotardo, Derek Bradley
Progressive Material Caching	The evaluation of material networks is a relatively resource-intensive process in the rendering pipeline. Modern production scenes can contain hundreds or thousands of complex materials with massive networks, so there is a great demand for an efficient way of handling material networks. In this paper, we introduce an efficient method for progressively caching the material nodes without an overhead on the rendering performance. We evaluate the material networks as usual in the rendering process. Then, the output value of part of the network is stored in a cache and can be used in the evaluation of the next materials. Using our method, we can render the scene with performance equal to or better than that of the method without caching, with a slight difference in the images rendered with caching and without it.	https://dl.acm.org/doi/abs/10.1145/3550340.3564223	Shin Fujieda, Takahiro Harada
Progressive Simulation for Cloth Quasistatics	"The trade-off between speed and fidelity in cloth simulation is a fundamental computational problem in computer graphics and computational design. Coarse cloth models provide the interactive performance required by designers, but they can not be simulated at higher resolutions (""up-resed"") without introducing simulation artifacts and/or unpredicted outcomes, such as different folds, wrinkles and drapes. But how can a coarse simulation predict the result of an unconstrained, high-resolution simulation that has not yet been run? We propose Progressive Cloth Simulation (PCS), a new forward simulation method for efficient of cloth quasistatics on exceedingly coarse triangle meshes with consistent and progressive improvement over a hierarchy of increasingly higher-resolution models. PCS provides an efficient coarse previewing simulation method that predicts the coarse-scale folds and wrinkles that will be generated by a corresponding converged, high-fidelity C-IPC simulation of the cloth drape's equilibrium. For each preview PCS can generate an increasing-resolution sequence of models that progress towards this converged solution. This successive improvement can then be interrupted at any point, for example, whenever design parameters are updated. PCS then ensures feasibility at all resolutions, so that predicted solutions remain intersection-free and capture the complex folding and buckling behaviors of frictionally contacting cloth."	https://dl.acm.org/doi/abs/10.1145/3550454.3555510	Jiayi Eris Zhang, Jèrèmie Dumas, Yun (Raymond) Fei, Alec Jacobson, Doug L. James, Danny M. Kaufman
Pupil-Aware Holography	Holographic displays promise to deliver unprecedented display capabilities in augmented reality applications, featuring a wide field of view, wide color gamut, spatial resolution, and depth cues all in a compact form factor. While emerging holographic display approaches have been successful in achieving large étendue and high image quality as seen by a camera, the large étendue also reveals a problem that makes existing displays impractical: the sampling of the holographic field by the eye pupil. Existing methods have not investigated this issue due to the lack of displays with large enough étendue, and, as such, they suffer from severe artifacts with varying eye pupil size and location. We show that the holographic field as sampled by the eye pupil is highly varying for existing display setups, and we propose pupil-aware holography that maximizes the perceptual image quality irrespective of the size, location, and orientation of the eye pupil in a near-eye holographic display. We validate the proposed approach both in simulations and on a prototype holographic display and show that our method eliminates severe artifacts and significantly outperforms existing approaches.	https://dl.acm.org/doi/abs/10.1145/3550454.3555508	Praneeth Chakravarthula, Seung-Hwan Baek, Florian Schiffers, Ethan Tseng, Grace Kuo, Andrew Maimone, Nathan Matsuda, Oliver Cossairt, Douglas Lanman, Felix Heide
Pupillary oscillation induced by pseudo-isochromatic stimuli for objective color vision test	Color vision tests are conducted based on subjective responses; however, since subjective responses can be biased and are not uniformly applicable to all individuals, an objective method is required. In this study, we developed a quantitative and user-friendly color vision test, which utilized a psychophysical method based on pupillary responses to color flicker stimuli sampled from the confusion line. The results revealed that power spectral density of pupillary oscillations increased corresponded to the color differences. Our system could eventually lead to a feasible color testing method.	https://dl.acm.org/doi/abs/10.1145/3550082.3564161	Yuto Nakanishi, Yuya Kinzuka, Fumiaki Sato, Shigeki Nakauchi, Tetsuto Minami
QuadStream: A Quad-Based Scene Streaming Architecture for Novel Viewpoint Reconstruction	Streaming rendered 3D content over a network to a thin client device, such as a phone or a VR/AR headset, brings high-fidelity graphics to platforms where it would not normally possible due to thermal, power, or cost constraints. Streamed 3D content must be transmitted with a representation that is both robust to latency and potential network dropouts. Transmitting a video stream and reprojecting to correct for changing viewpoints fails in the presence of disocclusion events; streaming scene geometry and performing high-quality rendering on the client is not possible on limited-power mobile GPUs. To balance the competing goals of disocclusion robustness and minimal client workload, we introduce , a new streaming content representation that reduces motion-to-photon latency by allowing clients to efficiently render novel views without artifacts caused by disocclusion events. Motivated by traditional macroblock approaches to video codec design, we decompose the scene seen from positions in a into a series of , or view-aligned quads from multiple views. By operating on a rasterized G-Buffer, our approach is independent of the representation used for the scene itself; the resulting is an approximate geometric representation of the scene that can be reconstructed by a thin client to render both the current view and nearby adjacent views. Our technical contributions are an efficient parallel quad generation, merging, and packing strategy for proxy views covering potential client movement in a scene; a packing and encoding strategy that allows masked quads with depth information to be transmitted as a frame-coherent stream; and an efficient rendering approach for rendering our QuadStream representation into entirely novel views on thin clients. We show that our approach achieves superior quality compared both to video data streaming methods, and to geometry-based streaming.	https://dl.acm.org/doi/abs/10.1145/3550454.3555524	Jozef Hladky, Michael Stengel, Nicholas Vining, Bernhard Kerbl, Hans-Peter Seidel, Markus Steinberger
QuadStretch: A Forearm-wearable Skin Stretch Display for Immersive VR Experience	Force feedback is important for immersive VR experience but requires a cumbersome device restricting the use of the hand. An alternative to work around this problem is the use of substituted tactile feedback. QuadStretch is a lightweight and flexible forearm-wearable device for providing substituted skin-stretch feedback to express force on the arm. Consisting of four pairs of tactors, it does not require a ground point and can express the directional sense of force accompanying arm movements. In this demo, we prepared six demo scenarios, Boxing, Archery, Wings, Climbing, Slingshot, and Fishing, to show how the expressive power of QuadStretch can enhance the VR experience.	https://dl.acm.org/doi/abs/10.1145/3550471.3564761	Youngbo Shim, Taejun Kim, Sangyoon Lee, Sunbum Kim, Geehyuk Lee
QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars	Real-time tracking of human body motion is crucial for interactive and immersive experiences in AR/VR. However, very limited sensor data about the body is available from standalone wearable devices such as HMDs (Head Mounted Devices) or AR glasses. In this work, we present a reinforcement learning framework that takes in sparse signals from an HMD and two controllers, and simulates plausible and physically valid full body motions. Using high quality full body motion as dense supervision during training, a simple policy network can learn to output appropriate torques for the character to balance, walk, and jog, while closely following the input signals. Our results demonstrate surprisingly similar leg motions to ground truth without any observations of the lower body, even when the input is only the 6D transformations of the HMD. We also show that a single policy can be robust to diverse locomotion styles, different body sizes, and novel environments.	https://dl.acm.org/doi/abs/10.1145/3550469.3555411	Alexander Winkler, Jungdam Won, Yuting Ye
RFEPS: Reconstructing Feature-Line Equipped Polygonal Surface	Feature lines are important geometric cues in characterizing the structure of a CAD model. Despite great progress in both explicit reconstruction and implicit reconstruction, it remains a challenging task to reconstruct a polygonal surface equipped with feature lines, especially when the input point cloud is noisy and lacks faithful normal vectors. In this paper, we develop a multistage algorithm, named , to address this challenge. The key steps include (1) denoising the point cloud based on the assumption of local planarity, (2) identifying the feature-line zone by optimization of discrete optimal transport, (3) augmenting the point set so that sufficiently many additional points are generated on potential geometry edges, and (4) generating a polygonal surface that interpolates the augmented point set based on restricted power diagram. We demonstrate through extensive experiments that RFEPS, benefiting from the edge-point augmentation and the feature preserving explicit reconstruction, outperforms state of the art methods in terms of the reconstruction quality, especially in terms of the ability to reconstruct missing feature lines.	https://dl.acm.org/doi/abs/10.1145/3550454.3555443	Rui Xu, Zixiong Wang, Zhiyang Dou, Chen Zong, Shiqing Xin, Mingyan Jiang, Tao Ju, Changhe Tu
Rapid Face Asset Acquisition with Recurrent Feature Alignment	We present Recurrent Feature Alignment (ReFA), an end-to-end neural network for the very rapid creation of production-grade face assets from multi-view images. ReFA is on par with the industrial pipelines in quality for producing accurate, complete, registered, and textured assets directly applicable to physically-based rendering, but produces the asset end-to-end, fully automatically at a significantly faster speed at 4.5 FPS, which is unprecedented among neural-based techniques. Our method represents face geometry as a position map in the UV space. The network first extracts per-pixel features in both the multi-view image space and the UV space. A recurrent module then iteratively optimizes the geometry by projecting the image-space features to the UV space and comparing them with a reference UV-space feature. The optimized geometry then provides pixel-aligned signals for the inference of high-resolution textures. Experiments have validated that ReFA achieves a median error of 0.603 in geometry reconstruction, is robust to extreme pose and expression, and excels in sparse-view settings. We believe that the progress achieved by our network enables lightweight, fast face assets acquisition that significantly boosts the downstream applications, such as avatar creation and facial performance capture. It will also enable massive database capturing for deep learning purposes.	https://dl.acm.org/doi/abs/10.1145/3550454.3555509	Shichen Liu, Yunxuan Cai, Haiwei Chen, Yichao Zhou, Yajie Zhao
Real time technologies for Realistic Digital Humans: facial performance and hair simulation	We have identified the world's most extensive parameter space for the human head based on over 4TB of 4D data acquired from multiple actors. Ziva's proprietary machine-learning processes can apply this data set to any number of secondary 3D heads, enabling them all to perform novel facial expressions in real-time while preserving volume and staying within the natural range of human expressions. Facial performances can then be augmented and tailored with Ziva expressions controls, solving the costly limitations of scalability, realism, artist control, and speed. For this presentation, we will discuss and demonstrate how this innovation can improve the overall quality of RT3D faces for all productions while simplifying and accelerating the overall production workflow and enabling mass production of high-performance real-time characters. We will then illustrate how performance capture can be decoupled from asset production, enabling actor-nonspecific performance capture, by showing a single performance being applied to multiple faces of varying proportions, enabling any performance to run on any head, all at state-of-the-art quality. We will additionally highlight a new integrated Hair solution for authoring / importing / simulating/ rendering strand-based hair in Unity. Built from the ground up with Unity users in mind, and evolved and hardened during the production of Enemies, the hair system is applicable not only to realistic digital humans, but also to much more stylized content and games. Using a fast and flexible GPU-based solver that works on both strand- and volume-information, the system enables users to interactively set up 'Hair Instances' and interact with those instances as they are simulated and rendered in real time. We will concentrate on demonstrating the simulation part of the system, including the strand-based solver, volume-based quantities such as density and pressure, the fully configurable set of constraints and the level of detail support that artists have.	https://dl.acm.org/doi/abs/10.1145/3550453.3570122	Krasimir Nechevski, Mark Schoennagel
Real-Time Facial Animation Generation on Face Mask	In light of the COVID-19 pandemic, wearing a mask is crucial to avoid contracting infectious diseases. However, wearing a mask is known to impair communication functions. This study aims to address the communication difficulties caused by wearing a mask and provide a strategy for aiding in understanding the speaker's speech through facial animation. Facial animation is generated in real-time, and upper facial information is processed to detect the speaker's emotions, generating a lower facial expression. In addition, the system detects the mask's shape and enables accurate registration in the proper position. This technology can improve communication and alleviate challenges associated with communication between persons wearing face masks.	https://dl.acm.org/doi/abs/10.1145/3550082.3564178	Bin Han, Gerard Jounghyun Kim, Jae-In Hwang
Real-Time Omnidirectional Roaming in Large Scale Indoor Scenes	Neural radiance field (NeRF) has recently achieved impressive results in novel view synthesis. However, previous works on NeRF mainly focus on object-centric scenarios. They would suffer observable performance degradation in outward-facing and large-scale scenes due to limiting positional encoding capacity. To narrow the gap, we explore radiance fields in a geometry-aware fashion. We estimate explicit geometry from the omnidirectional neural radiance field that was learned from multiple 360° images. Relying on the recovered geometry, we use an adaptive divide-and-conquer strategy to slim and fine-tune the radiance fields and further improve render speed and quality. Quantitative and qualitative comparisons among baselines illustrated our predominant performance in large-scale indoor scenes and our system supports real-time VR roaming.	https://dl.acm.org/doi/abs/10.1145/3550340.3564222	Huajian Huang, Yingshu Chen, Tianjia Zhang, Sai-Kit Yeung
Realistic Luminance in VR	As virtual reality (VR) headsets continue to achieve ever more immersive visuals along the axes of resolution, field of view, focal cues, distortion mitigation, and so on, the luminance and dynamic range of these devices falls far short of widely available consumer televisions. While work remains to be done on the display architecture side, power and weight limitations in head-mounted displays pose a challenge for designs aiming for high luminance. In this paper, we seek to gain a basic understanding of VR user preferences for display luminance values in relation to known, real-world luminances for immersive, natural scenes. To do so, we analyze the luminance characteristics of an existing high-dynamic-range (HDR) panoramic image dataset, build an HDR VR headset capable of reproducing over 20,000 nits peak luminance, and conduct a first-of-its-kind study on user brightness preferences in VR. We conclude that current commercial VR headsets do not meet user preferences for display luminance, even for indoor scenes.	https://dl.acm.org/doi/abs/10.1145/3550469.3555427	Nathan Matsuda, Alex Chapiro, Yang Zhao, Clinton Smith, Romain Bachy, Douglas Lanman
Realistic Rendering Tool for Pseudo-Structural Coloring with Multi-Color Extrusion of FFF 3D Printing	In this study, we developed a flow to simulate anisotropy in multi-color printing of a FFF 3D printer. There are previous methods for simulating layered marks from gcode, which is a 3D printer control statement, and multi-color printing by switching filament colors and their simulators. However, there was no simulator for the case where color anisotropy exists in a single path, which is the target of this study. Since the coloring of this method is based on a completely different principle from existing methods, it is difficult to predict the results. Our simulator tool was very useful in rendering multi-color 3D printed models. Furthermore, a more sophisticated simulation of the placement of objects in real space was performed based on the highly realistic images created by physical-based rendering. The simulation also worked in VR space and showed an optimal preview at the architectural scale.	https://dl.acm.org/doi/abs/10.1145/3550082.3564179	Soya Eguchi, Yasuo Nagura, Hiroya Tanaka
Realtime blur simulation of varifocal spectacle lenses in virtual reality	Virtual reality is a promising tool to study the influence of ophthalmic lenses on human perception. Independent of the actual refractive errors of the users' eye, vision through different types of lenses can be simulated while allowing free eye and head movements and easy measurement of behavioural parameters. Especially for presbyopia, existing solutions often lead to visual discomfort. Multifocal spectacle lenses have a spatially varying optical power distribution leading to changing strength and orientation of blurred vision. Understanding the contribution of blur to visual discomfort and studying behavioral changes under the influence of lens blur is a promising application of a spectacle lens simulator. Here we present our framework to simulate spherical and astigmatic blur of corrective spectacle lenses. Size and orientation of a location dependent blur ellipse is calculated live in virtual reality using the depth information of the scene and precalculated power distribution of the lens.	https://dl.acm.org/doi/abs/10.1145/3550340.3564224	Yannick Sauer, Siegfried Wahl, Selam Wondimu Habtegiorgis
Reconstructing Hand-Held Objects from Monocular Video	This paper presents an approach that reconstructs a hand-held object from a monocular video. In contrast to many recent methods that directly predict object geometry by a trained network, the proposed approach does not require any learned prior about the object and is able to recover more accurate and detailed object geometry. The key idea is that the hand motion naturally provides multiple views of the object and the motion can be reliably estimated by a hand pose tracker. Then, the object geometry can be recovered by solving a multi-view reconstruction problem. We devise an implicit neural representation-based method to solve the reconstruction problem and address the issues of imprecise hand pose estimation, relative hand-object motion, and insufficient geometry optimization for small objects. We also provide a newly collected dataset with 3D ground truth to validate the proposed approach. The dataset and code will be released at https://dihuangdh.github.io/hhor.	https://dl.acm.org/doi/abs/10.1145/3550469.3555401	Di Huang, Xiaopeng Ji, Xingyi He, Jiaming Sun, Tong He, Qing Shuai, Wanli Ouyang, Xiaowei Zhou
Reconstructing Personalized Semantic Facial NeRF Models from Monocular Video	We present a novel semantic model for human head defined with neural radiance field. The 3D-consistent head model consist of a set of disentangled and interpretable bases, and can be driven by low-dimensional expression coefficients. Thanks to the powerful representation ability of neural radiance field, the constructed model can represent complex facial attributes including hair, wearings, which can not be represented by traditional mesh blendshape. To construct the personalized semantic facial model, we propose to define the bases as several multi-level voxel fields. With a short monocular RGB video as input, our method can construct the subject's semantic facial NeRF model with only ten to twenty minutes, and can render a photorealistic human head image in tens of miliseconds with a given expression coefficient and view direction. With this novel representation, we apply it to many tasks like facial retargeting and expression editing. Experimental results demonstrate its strong representation ability and training/inference speed. Demo videos and released code are provided in our project page: https://ustc3dv.github.io/NeRFBlendShape/	https://dl.acm.org/doi/abs/10.1145/3550454.3555501	Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, Juyong Zhang
Reconstructing editable prismatic CAD from rounded voxel models	Reverse Engineering a CAD shape from other representations is an important geometric processing step for many downstream applications. In this work, we introduce a novel neural network architecture to solve this challenging task and approximate a smoothed signed distance function with an editable, constrained, prismatic CAD model. During training, our method reconstructs the input geometry in the voxel space by decomposing the shape into a series of 2D profile images and 1D envelope functions. These can then be recombined in a differentiable way allowing a geometric loss function to be defined. During inference, we obtain the CAD data by first searching a database of 2D constrained sketches to find curves which approximate the profile images, then extrude them and use Boolean operations to build the final CAD model. Our method approximates the target shape more closely than other methods and outputs highly editable constrained parametric sketches which are compatible with existing CAD software.	https://dl.acm.org/doi/abs/10.1145/3550469.3555424	Joseph George Lambourne, Karl Willis, Pradeep Kumar Jayaraman, Longfei Zhang, Aditya Sanghi, Kamal Rahimi Malekshan
Recursive Rendering of 2D Images for Accurate Pose Estimation in a 3D Mesh Map	Image-based pose estimation generally needs 3D map generation, which is costly in practice. In this paper, we propose a method to accurately estimate the pose by recursively rendering 2D images from a generic 3D city mesh map and updating the pose.	https://dl.acm.org/doi/abs/10.1145/3550082.3564201	Yohei Hanaoka, Suwichaya Suwanwimolkul, Satoshi Komorita
Reference Based Sketch Extraction via Attention Mechanism	We propose a model that extracts a sketch from a colorized image in such a way that the extracted sketch has a line style similar to a given reference sketch while preserving the visual content identically to the colorized image. Authentic sketches drawn by artists have various sketch styles to add visual interest and contribute feeling to the sketch. However, existing sketch-extraction methods generate sketches with only one style. Moreover, existing style transfer models fail to transfer sketch styles because they are mostly designed to transfer textures of a source style image instead of transferring the sparse line styles from a reference sketch. Lacking the necessary volumes of data for standard training of translation systems, at the core of our GAN-based solution is a self-reference sketch style generator that produces various reference sketches with a similar style but different spatial layouts. We use independent attention modules to detect the edges of a colorized image and reference sketch as well as the visual correspondences between them. We apply several loss terms to imitate the style and enforce sparsity in the extracted sketches. Our sketch-extraction method results in a close imitation of a reference sketch style drawn by an artist and outperforms all baseline methods. Using our method, we produce a synthetic dataset representing various sketch styles and improve the performance of auto-colorization models, in high demand in comics. The validity of our approach is confirmed via qualitative and quantitative evaluations.	https://dl.acm.org/doi/abs/10.1145/3550454.3555504	Amirsaman Ashtari, Chang Wook Seo, Cholmin Kang, Sihun Cha, Junyong Noh
Remembrance: Magma	Remembrance: Magma is an immersive virtual reality animation addressing the poetics of a mind as it is dying of dementia. Remembrance: Magma incorporates modern tech, cutting-edge research on the aging brain, East Asian crafting aesthetics, and Korean shamanic traditions as it examines the nature of the brain as a sensor that desires data even as it slowly fails. Culturally intersectional, Remembrance explores the poetic and painful processes of memory degeneration.	https://dl.acm.org/doi/abs/10.1145/3550470.3558425	Chanee Choi
Repository	A VR art experience provides a conceptual response to the issues of data authorship and data oblivion. It visualizes 1945 Twitter posts retrieved from Twitter API under the construction of a server farm with a paper shredder. It questions the possible future of digital heritage.	https://dl.acm.org/doi/abs/10.1145/3550470.3558439	Weidi Zhang
Representation of FRP material damage in 3DCG	In recent years, most productions of 3D video games dealing with cars have represented automobile damage due to collisions. However, the cracks peculiar to FRP materials have not been reproduced in previous works. Therefore, in this research, we devised a method of expressing cracks in FRP materials. In addition, we will conduct experiments to determine the realism of the visual representation of breakage as well as the processing cost and consider whether it is possible to incorporate it into actual video game works.	https://dl.acm.org/doi/abs/10.1145/3550082.3564166	Shoon Komori, Tomokazu Ishikawa
Reprise: English	A young girl is forced to share her family home with her abuser.	https://dl.acm.org/doi/abs/10.1145/3550339.3557726	Leance Volschenk, Carine Chrast, Saskia Bulletti, Livia Neuenschwander, Gerd Gockell
Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings	Automatic synthesis of realistic co-speech gestures is an increasingly important yet challenging task in artificial embodied agent creation. Previous systems mainly focus on generating gestures in an end-to-end manner, which leads to difficulties in mining the clear rhythm and semantics due to the complex yet subtle harmony between speech and gestures. We present a novel co-speech gesture synthesis method that achieves convincing results both on the rhythm and semantics. For the rhythm, our system contains a robust rhythm-based segmentation pipeline to ensure the temporal coherence between the vocalization and gestures explicitly. For the gesture semantics, we devise a mechanism to effectively disentangle both low- and high-level neural embeddings of speech and motion based on linguistic theory. The high-level embedding corresponds to semantics, while the low-level embedding relates to subtle variations. Lastly, we build correspondence between the hierarchical embeddings of the speech and the motion, resulting in rhythm- and semantics-aware gesture synthesis. Evaluations with existing objective metrics, a newly proposed rhythmic metric, and human feedback show that our method outperforms state-of-the-art systems by a clear margin.	https://dl.acm.org/doi/abs/10.1145/3550454.3555435	Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, Libin Liu
Ribbon Font Neural Style Transfer for OpenType-SVG Font	We use existing machine learning neural style transfer model, differential rasterizer, for colored font design. The input of the proposed system is an existing TrueType font and the output is an neural style transferred OpenType-SVG color font. Each character glyph contains a series of vector graphics path elements. A neural style transferred glyph consists of dozens of random Bézier curves initially distributed among the glyph space, gradually converging to glyph stokes of a given character. In contract, the process of designing a font manually requires specialized skills for designers. They use mouse cursor to adjust strokes and glyph kerning for each character. It require hours to achieve a specific style. A Chinese font contains thousands of characters. Neural style transfer techniques can be used to speed up the process for designing decorative fonts. In addition, we add different colors to the curves in a an neural style transfer character for visual appealing. In order to support colors in a font, we choose OpenType-SVG format, allowing displaying colors in software like Illustrator or Inkscape. We open-source our code at https://github.com/su8691/ribbon.	https://dl.acm.org/doi/abs/10.1145/3550082.3564163	Sihi-Ting Huang, Tung-Ju Hsieh
Rig Inversion by Training a Differentiable Rig Function	Rig inversion is the problem of creating a method that can find the rig parameter vector that best approximates a given input mesh. In this paper we propose to solve this problem by first obtaining a differentiable rig function by training a multi layer perceptron to approximate the rig function. This differentiable rig function can then be used to train a deep learning model of rig inversion.	https://dl.acm.org/doi/abs/10.1145/3550340.3564218	Mathieu Marquis Bolduc, Hau Nghiep Phan
River Concert	Have we ever heard the voice of river carefully? We use the technology of speech recognition, including the emotion recognition function to analysis audio samples of underwater recording. So we can hear the underwater sound into a dynamic piece of music and let the river play the music itself.	https://dl.acm.org/doi/abs/10.1145/3550470.3558433	Siou-Ming Wu
Robust Vectorized Surface Reconstruction with 2D-3D Joint Optimization	In this work, we propose a robust pipeline to create vectorized models from LiDAR point clouds without the assumption of watertight polygonal surfaces. The core idea behind our method is combining the hierarchical 2D projection with the pairwise intersections of clipped planes to precisely hypothesize faces of an object inside and outside while enhancing the robustness of different scenes. Moreover, to generate concise polygon meshes efficiently, Adaptive K-means++ (AKM++) is used to smooth the 2D rasterized height map followed by the Levenberg-Marquardt (LM) algorithm to optimize the projection primitive with planar intersections jointly. The final model could be easily obtained by merging the derived neighbouring facets above. Our experimental results show that the proposed method obtains compact models with high fidelity and efficiency on either precise or defect-laden data compared with other state-of-the-art approaches.	https://dl.acm.org/doi/abs/10.1145/3550082.3564202	Chuanchuan Wang, Yatong Xu, Minjie Tang, Jie Li, Hui Mao, Shiliang Pu
S3-Slicer: A General Slicing Framework for Multi-Axis 3D Printing	Multi-axis motion introduces more degrees of freedom into the process of 3D printing to enable different objectives of fabrication by accumulating materials layers upon curved layers. An existing challenge is how to effectively generate the curved layers satisfying multiple objectives simultaneously. This paper presents a general slicing framework for achieving multiple fabrication objectives including support free, strength reinforcement and surface quality. These objectives are formulated as local printing directions varied in the volume of a solid, which are achieved by computing the rotation-driven deformation for the input model. The height field of a deformed model is mapped into a scalar field on its original shape, the isosurfaces of which give the curved layers of multi-axis 3D printing. The deformation can be effectively optimized with the help of quaternion fields to achieve the fabrication objectives. The effectiveness of our method has been verified on a variety of models.	https://dl.acm.org/doi/abs/10.1145/3550454.3555516	Tianyu Zhang, Guoxin Fang, Yuming Huang, Neelotpal Dutta, Sylvain Lefebvre, Zekai Murat Kilic, Charlie C. L. Wang
SCULPTOR: Skeleton-Consistent Face Creation Using a Learned Parametric Generator	Recent years have seen growing interest in 3D human face modeling due to its wide applications in digital human, character generation and animation. Existing approaches overwhelmingly emphasized on modeling the exterior shapes, textures and skin properties of faces, ignoring the inherent correlation between inner skeletal structures and appearance. In this paper, we present SCULPTOR, 3D face creations with , aiming to facilitate the easy creation of both anatomically correct and visually convincing face models via a hybrid parametric-physical representation. At the core of SCULPTOR is LUCY, the first large-scale shape-skeleton face dataset in collaboration with plastic surgeons. Named after the fossils of one of the oldest known human ancestors, our LUCY dataset contains high-quality Computed Tomography (CT) scans of the complete human head before and after orthognathic surgeries, which are critical for evaluating surgery results. LUCY consists of 144 scans of 72 subjects (31 male and 41 female), where each subject has two CT scans taken pre- and post-orthognathic operations. Based on our LUCY dataset, we learned a novel skeleton consistent parametric facial generator, SCULPTOR, which can create unique and nuanced facial features that help define a character and at the same time maintain physiological soundness. Our SCULPTOR jointly models the skull, face geometry and face appearance under a unified data-driven framework by separating the depiction of a 3D face into shape blend shape, pose blend shape and facial expression blend shape. SCULPTOR preserves both anatomic correctness and visual realism in facial generation tasks compared with existing methods. Finally, we showcase the robustness and effectiveness of SCULPTOR in various fancy applications unseen before, like archaeological skeletal facial completion, bone-aware character fusion, skull inference from images, face generation with lipo-Level change and facial animations, etc.	https://dl.acm.org/doi/abs/10.1145/3550454.3555462	Zesong Qiu, Yuwei Li, Dongming He, Qixuan Zhang, Longwen Zhang, Yinghao Zhang, Jingya Wang, Lan Xu, Xudong Wang, Yuyao Zhang, Jingyi Yu
SHRED: 3D Shape Region Decomposition with Learned Local Operations	We present SHRED, a method for 3D SHape REgion Decomposition. SHRED takes a 3D point cloud as input and uses learned local operations to produce a segmentation that approximates fine-grained part instances. We endow SHRED with three decomposition operations: splitting regions, fixing the boundaries between regions, and merging regions together. Modules are trained independently and locally, allowing SHRED to generate high-quality segmentations for categories not seen during training. We train and evaluate SHRED with fine-grained segmentations from PartNet; using its merge-threshold hyperparameter, we show that SHRED produces segmentations that better respect ground-truth annotations compared with baseline methods, at any desired decomposition granularity. Finally, we demonstrate that SHRED is useful for downstream applications, out-performing all baselines on zero-shot fine-grained part instance segmentation and few-shot finegrained semantic segmentation when combined with methods that learn to label shape regions.	https://dl.acm.org/doi/abs/10.1145/3550454.3555440	R. Kenny Jones, Aalia Habib, Daniel Ritchie
SMPL-IK: Learned Morphology-Aware Inverse Kinematics for AI Driven Artistic Workflows	Inverse Kinematics (IK) systems are often rigid with respect to their input character, thus requiring user intervention to be adapted to new skeletons. In this paper we aim at creating a flexible, learned IK solver applicable to a wide variety of human morphologies. We extend a state-of-the-art machine learning IK solver to operate on the well known Skinned Multi-Person Linear model (SMPL). We call our model SMPL-IK, and show that when integrated into real-time 3D software, this extended system opens up opportunities for defining novel AI-assisted animation workflows. For example, when chained with existing pose estimation algorithms, SMPL-IK accelerates posing by allowing users to bootstrap 3D scenes from 2D images while allowing for further editing. Additionally, we propose a novel SMPL Shape Inversion mechanism (SMPL-SI) to map arbitrary humanoid characters to the SMPL space, allowing artists to leverage SMPL-IK on custom characters. In addition to qualitative demos showing proposed tools, we present quantitative SMPL-IK baselines on the H36M and AMASS datasets. Our code is publicly available https://github.com/boreshkinai/smpl-ik.	https://dl.acm.org/doi/abs/10.1145/3550340.3564227	Vikram Voleti, Boris Oreshkin, Florent Bocquelet, Félix Harvey, Louis-Simon Ménard, Christopher Pal
Samara Op.4	An old automaton sculptor retraces his own life throughout the four seasons.	https://dl.acm.org/doi/abs/10.1145/3550339.3556429	Maxime Wattrelos, Jérémy Trochet, Louis Cocquet, Marie Heribel, François Mainguet, Carlos De Carvalho
Sampling Neural Radiance Fields for Refractive Objects	Recently, differentiable volume rendering in neural radiance fields (NeRF) has gained a lot of popularity, and its variants have attained many impressive results. However, existing methods usually assume the scene is a homogeneous volume so that a ray is cast along the straight path. In this work, the scene is instead a heterogeneous volume with a piecewise-constant refractive index, where the path will be curved if it intersects the different refractive indices. For novel view synthesis of refractive objects, our NeRF-based framework aims to optimize the radiance fields of bounded volume and boundary from multi-view posed images with refractive object silhouettes. To tackle this challenging problem, the refractive index of a scene is reconstructed from silhouettes. Given the refractive index, we extend the stratified and hierarchical sampling techniques in NeRF to allow drawing samples along a curved path tracked by the Eikonal equation. The results indicate that our framework outperforms the state-of-the-art method both quantitatively and qualitatively, demonstrating better performance on the perceptual similarity metric and an apparent improvement in the rendering quality on several synthetic and real scenes.	https://dl.acm.org/doi/abs/10.1145/3550340.3564234	Jen-I Pan, Jheng-Wei Su, Kai-Wen Hsiao, Ting-Yu Yen, Hung-Kuo Chu
Samurai Frog Golf	"Humans have vanished into whispered legend and animals have regained ownership of the land, sea and sky. Our story is about one particular frog: Scarred inside and out, the old samurai is haunted by ghosts of a life of violence in the Frog Wars. He melts his sword into a golf club and vows never to kill again. A gruff, solitary creature, he wants nothing to do with any other living beast, and especially not his hated ancient enemies the Turtles. All he wants is to be left alone to spend his remaining years at peace on the golf course. As fate would have it, one bad swing sets off a course of events that put him in the unwilling position of caring for a baby - but not just any baby, the infant prince of the turtle clan! Pursued by enemies old and new, our hero must set off on a long and dangerous journey to return the baby to his homeland. With his tamahage driver and his caddy, the sage but forgetful sugar-glider by his side, the trio travel across a vast and perilous land. Through the adventure, our hero must learn to overcome his own prejudice, forgive ancient enemies and himself, and heal the wounds of the past. If our hero is to reach his goal and find his own redemption, he must take this sage bit of advice from his caddy: ""To move forward, you must be like the turtle. He only progresses when he sticks out his neck."""	https://dl.acm.org/doi/abs/10.1145/3550339.3556645	Brent Forrest
Scalable Multi-Class Sampling via Filtered Sliced Optimal Transport	We propose a multi-class point optimization formulation based on continuous Wasserstein barycenters. Our formulation is designed to handle hundreds to thousands of optimization objectives and comes with a practical optimization scheme. We demonstrate the effectiveness of our framework on various sampling applications like stippling, object placement, and Monte-Carlo integration. We a derive multi-class error bound for perceptual rendering error which can be minimized using our optimization. We provide source code at https://github.com/iribis/filtered-sliced-optimal-transport.	https://dl.acm.org/doi/abs/10.1145/3550454.3555484	Corentin Salaün, Iliyan Georgiev, Hans-Peter Seidel, Gurprit Singh
Scarecrow XR	"Scarecrow XR is one of a kind XR immersive performance using cutting-edge technologies of metaverse platforms, motion and facial capture, haptics, and volumetric scan. It evolved from LBE through Metaverse to Hybrid to fit various conditions. This presentation integrates the haptic algorithm and dancing performance developed for K'ARTS/POSTECH's ""Ballet Metanique."""	https://dl.acm.org/doi/abs/10.1145/3550453.3586018	Sngmoo Lee, Jihyun Jung, Chungyean Cho, Junwoo Kim
Scene Synthesis from Human Motion	Large-scale capture of human motion with diverse, complex scenes, while immensely useful, is often considered prohibitively costly. Meanwhile, human motion alone contains rich information about the scene they reside in and interact with. For example, a sitting human suggests the existence of a chair, and their leg position further implies the chair's pose. In this paper, we propose to synthesize diverse, semantically reasonable, and physically plausible scenes based on human motion. Our framework, Scene Synthesis from HUMan MotiON (SUMMON), includes two steps. It first uses ContactFormer, our newly introduced contact predictor, to obtain temporally consistent contact labels from human motion. Based on these predictions, SUMMON then chooses interacting objects and optimizes physical plausibility losses; it further populates the scene with objects that do not interact with humans. Experimental results demonstrate that SUMMON synthesizes feasible, plausible, and diverse scenes and has the potential to generate extensive human-scene interaction data for the community.	https://dl.acm.org/doi/abs/10.1145/3550469.3555426	Sifan Ye, Yixing Wang, Jiaman Li, Dennis Park, C. Karen Liu, Huazhe Xu, Jiajun Wu
Scope of the Cloud	Scope of the Cloud sets its stage on a floating city in 2051, CLOUD, a city of climate refugees. Each game console sets Planner, Researcher, and the Flightless bird as the main protagonist. In a playful way, the game questions how mobility can affect ecosystems and human lives.	https://dl.acm.org/doi/abs/10.1145/3550470.3558438	Jooyoung Oh
Seeing is Feeling: A Novel Haptic Display for Wearer-Observer Mutual Haptic Understanding	We propose a new haptic display that enables mutual understanding of haptic sensation between the wearer and an observer. In addition to presenting haptic sensations by inducing skin deformation, we have achieved creating a mutual understanding of the sensations with the observer by making the haptic stimulus evident. People have the ability to predict the sensations of others by observing their sensory states. The system is composed of a part that provides haptic stimulus while creating visible skin deformation, and a mechanical structure that visually exaggerates the deformation. The proposed system realizes richer interactions by extending the entertainment experiences of the observers during live content, increase understanding of internal states through biofeedback, and be used in remote haptic communication.	https://dl.acm.org/doi/abs/10.1145/3550471.3558400	Arata Horie, Ryo Murata, Zendai Kashino, Masahiko Inami
Shader Park: Live-Coding Interactive & Procedural Shaders with JavaScript	Shader Park is an open-source JavaScript library and community for creating interactive 2D and 3D shaders. It features a live-coding environment that allows the community to share their creations. The library abstracts signed distance fields into a high-level imperative API (similar to Processing, or p5.js) making procedural modeling much more accessible and productive. The unification of procedural geometry, animation, and materials into a single program makes prototyping faster and more flexible. A major focus of the project is code literacy and artistic experimentation by making computer graphics programming accessible for artists, designers, students, educators, and all. The library also provides interactive documentation and a number of plugins for WebGL, Three.js, and TouchDesigner. In this demo we will cover the core features of Shader Park, and live-code an interactive shader with it.	https://dl.acm.org/doi/abs/10.1145/3550453.3570120	Torin Blankensmith, Peter Whidden
Shape Completion with Points in the Shadow	Single-view point cloud completion aims to recover the full geometry of an object based on only limited observation, which is extremely hard due to the data sparsity and occlusion. The core challenge is to generate plausible geometries to fill the unobserved part of the object based on a partial scan, which is under-constrained and suffers from a huge solution space. Inspired by the classic shadow volume technique in computer graphics, we propose a new method to reduce the solution space effectively. Our method considers the camera a light source that casts rays toward the object. Such light rays build a reasonably constrained but sufficiently expressive basis for completion. The completion process is then formulated as a point displacement optimization problem. Points are initialized at the partial scan and then moved to their goal locations with two types of movements for each point: directional movements along the light rays and constrained local movement for shape refinement. We design neural networks to predict the ideal point movements to get the completion results. We demonstrate that our method is accurate, robust, and generalizable through exhaustive evaluation and comparison. Moreover, it outperforms state-of-the-art methods qualitatively and quantitatively on MVP datasets.	https://dl.acm.org/doi/abs/10.1145/3550469.3555389	Bowen Zhang, Xi Zhao, He Wang, Ruizhen Hu
Shape from Release: Inverse Design and Fabrication of Controlled Release Structures	Objects with different shapes can dissolve in significantly different ways inside a solution. Predicting different shapes' dissolution dynamics is an important problem especially in pharmaceutics. More important and challenging, however, is controlling the dissolution via shape, , designing shapes that lead to a desired release behavior of materials in a solvent over a specific time. Here, we tackle this challenge by introducing a computational inverse design pipeline. We begin by introducing a simple, physically-inspired differentiable forward model of dissolution. We then formulate our inverse design as a PDE-constrained topology optimization that has access to analytical derivatives obtained via sensitivity analysis. Furthermore, we incorporate fabricability terms in the optimization objective that enable physically realizing our designs. We thoroughly analyze our approach on a diverse set of examples via both simulation and fabrication.	https://dl.acm.org/doi/abs/10.1145/3550454.3555518	Julian Panetta, Haleh Mohammadian, Emiliano Luci, Vahid Babaei
Simulation of Hand Anatomy Using Medical Imaging	Precision modeling of the hand internal musculoskeletal anatomy has been largely limited to individual poses, and has not been connected into continuous volumetric motion of the hand anatomy actuating across the hand's entire range of motion. This is for a good reason, as hand anatomy and its motion are extremely complex and cannot be predicted merely from the anatomy in a single pose. We give a method to simulate the volumetric shape of hand's musculoskeletal organs to any pose in the hand's range of motion, producing external hand shapes and internal organ shapes that match ground truth optical scans and medical images (MRI) in multiple scanned poses. We achieve this by combining MRI images in multiple hand poses with FEM multibody nonlinear elastoplastic simulation. Our system models bones, muscles, tendons, joint ligaments and fat as separate volumetric organs that mechanically interact through contact and attachments, and whose shape matches medical images (MRI) in the MRI-scanned hand poses. The match to MRI is achieved by incorporating pose-space deformation and plastic strains into the simulation. We show how to do this in a non-intrusive manner that still retains all the simulation benefits, namely the ability to prescribe realistic material properties, generalize to arbitrary poses, preserve volume and obey contacts and attachments. We use our method to produce volumetric renders of the internal anatomy of the human hand in motion, and to compute and render highly realistic hand surface shapes. We evaluate our method by comparing it to optical scans, and demonstrate that we qualitatively and quantitatively substantially decrease the error compared to previous work. We test our method on five complex hand sequences, generated either using keyframe animation or performance animation using modern hand tracking techniques.	https://dl.acm.org/doi/abs/10.1145/3550454.3555486	Mianlun Zheng, Bohan Wang, Jingtao Huang, Jernej Barbič
SkinMixer: Blending 3D Animated Models	We propose a novel technique to compose new 3D animated models, such as videogame characters, by combining pieces from existing ones. Our method works on production-ready rigged, skinned, and animated 3D models to reassemble new ones. We exploit operations on the skeletons to trigger the automatic creation of a new mesh, linked to the new skeleton by a set of skinning weights and complete with a set of animations. The resulting model preserves the quality of the input meshings (which can be quad-dominant and semi-regular), skinning weights (inducing believable deformation), and animations, featuring coherent movements of the new skeleton. Our method enables content creators to reuse valuable, carefully designed assets by assembling new ready-to-use characters while preserving most of the hand-crafted subtleties of models authored by digital artists. As shown in the accompanying video, it allows for drastically cutting the time needed to obtain the final result.	https://dl.acm.org/doi/abs/10.1145/3550454.3555503	Stefano Nuvoli, Nico Pietroni, Paolo Cignoni, Riccardo Scateni, Marco Tarini
Skull and Bones - Long Live Piracy Cinematic Trailer	The cinematic trailer for Skull and Bones tells a 'rags to riches' story about the destitute character Sam, finding his way from the streets of Boston, overcoming adversities and swashbuckling his way to become a pirate lord in the Indian Ocean. In order to tell the story which spans oceans, continents and time, Goodbye Kansas collaborated closely with DDB and Ubisoft Singapore to find the correct context, tone and tempo of the narrative as well as flesh out and develop the protagonist: Sam. The challenge was to show the hero's journey, as well as hint at the larger scope of the game's world and the possibilities it offers.	https://dl.acm.org/doi/abs/10.1145/3550339.3555953	Nils Lagergren, Emnet Mulugeta, Jan Cafourek
Sprite Fright	An 80's-inspired horror comedy, set in Britain.	https://dl.acm.org/doi/abs/10.1145/3550339.3556000	Francesco Siddi, Matthew Luhn, Hjalti Hjalmarsson
Sprite-from-Sprite: Cartoon Animation Decomposition with Self-supervised Sprite Estimation	"We present an approach to decompose cartoon animation videos into a set of ""sprites"" --- the basic units of digital cartoons that depict the contents and transforms of each animated object. The sprites in real-world cartoons are unique: artists may draw arbitrary sprite animations for expressiveness, where the animated content is often complicated, irregular, and challenging; alternatively, artists may also reduce their workload by tweening and adjusting sprites, or even reuse static sprites, in which case the transformations are relatively regular and simple. Based on these observations, we propose a sprite decomposition framework using Pixel Multilayer Perceptrons (Pixel MLPs) where the estimation of each sprite is conditioned on and guided by all other sprites. In this way, once those relatively regular and simple sprites are resolved, the decomposition of the remaining ""challenging"" sprites can simplified and eased with the guidance of other sprites. We call this method ""sprite-from-sprite"" cartoon decomposition. We study ablative architectures of our framework, and the user study demonstrates that our results are the most preferred ones in 19/20 cases."	https://dl.acm.org/doi/abs/10.1145/3550454.3555439	Lvmin Zhang, Tien-Tsin Wong, Yuxin Liu
Stitch it in Time: GAN-Based Facial Editing of Real Videos	The ability of Generative Adversarial Networks to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Applying StyleGAN editing to real videos introduces two main challenges: (i) StyleGAN operates over aligned crops. When editing videos, these crops need to be pasted back into the frame, resulting in a spatial inconsistency. (ii) Videos introduce a fundamental barrier to overcome — temporal coherency. To address the first challenge, we propose a novel stitching-tuning procedure. The generator is carefully tuned to overcome the spatial artifacts at crop borders, resulting in smooth transitions even when difficult backgrounds are involved. Turning to temporal coherence, we propose that this challenge is largely artificial. The source video is already temporally coherent, and deviations arise in part due to careless treatment of individual components in the editing pipeline. We leverage the natural alignment of StyleGAN and the tendency of neural networks to learn low-frequency functions, and demonstrate that they provide a strongly consistent prior. These components are combined in an end-to-end framework for semantic editing of facial videos. We compare our pipeline to the current state-of-the-art and demonstrate significant improvements. Our method produces meaningful manipulations and maintains greater spatial and temporal consistency, even on challenging talking head videos which current methods struggle with. Our code and videos are available at https://stitch-time.github.io/.	https://dl.acm.org/doi/abs/10.1145/3550469.3555382	Rotem Tzaban, Ron Mokady, Rinon Gal, Amit Bermano, Daniel Cohen-Or
Stochastic Poisson Surface Reconstruction	We introduce a statistical extension of the classic Poisson Surface Reconstruction algorithm for recovering shapes from 3D point clouds. Instead of outputting an implicit function, we represent the reconstructed shape as a modified Gaussian Process, which allows us to conduct statistical queries (e.g., the likelihood of a point in space being on the surface or inside a solid). We show that this perspective: improves PSR's integration into the online scanning process, broadens its application realm, and opens the door to other lines of research such as applying task-specific priors.	https://dl.acm.org/doi/abs/10.1145/3550454.3555441	Silvia Sellán, Alec Jacobson
StyleBin: Stylizing Video by Example in Stereo	In this paper we present StyleBin—an approach to example-based stylization of videos that can produce consistent binocular depiction of stylized content on stereoscopic displays. Given the target sequence and a set of stylized keyframes accompanied by information about depth in the scene, we formulate an optimization problem that converts the target video into a pair of stylized sequences, in which each frame consists of a set of seamlessly stitched patches taken from the original stylized keyframe. The aim of the optimization process is to align the individual patches so that they respect the semantics of the given target scene, while at the same time also following the prescribed local disparity in the corresponding viewpoints and being consistent in time. In contrast to previous depth-aware style transfer techniques, our approach is the first that can deliver semantically meaningful stylization and preserve essential visual characteristics of the given artistic media. We demonstrate the practical utility of the proposed method in various stylization use cases.	https://dl.acm.org/doi/abs/10.1145/3550469.3555420	Michal Kučera, David Mould, Daniel Sýkora
SurfaceVoronoi: Efficiently Computing Voronoi Diagrams Over Mesh Surfaces with Arbitrary Distance Solvers	In this paper, we propose to compute Voronoi diagrams over mesh surfaces driven by an arbitrary geodesic distance solver, assuming that the input is a triangle mesh as well as a collection of sites P = { } on the surface. We propose two key techniques to solve this problem. First, as the partition is determined by minimizing the distance fields, each of which rooted at a source site, we suggest keeping one or more distance triples, for each triangle, that may help determine the Voronoi bisectors when one uses a mark-and-sweep geodesic algorithm to predict the multi-source distance field. Second, rather than keep the distance itself at a mesh vertex, we use the squared distance to characterize the linear change of distance field restricted in a triangle, which is proved to induce an exact VD when the base surface reduces to a planar triangle mesh. Specially, our algorithm also supports the Euclidean distance, which can handle thin-sheet models (e.g. leaf) and runs faster than the traditional restricted Voronoi diagram (RVD) algorithm. It is very extensible to deal with various variants of surface-based Voronoi diagrams including (1) surface-based power diagram, (2) constrained Voronoi diagram with curve-type breaklines, and (3) curve-type generators. We conduct extensive experimental results to validate the ability to approximate the exact VD in different distance-driven scenarios.	https://dl.acm.org/doi/abs/10.1145/3550454.3555453	Shiqing Xin, Pengfei Wang, Rui Xu, Dongming Yan, Shuangmin Chen, Wenping Wang, Caiming Zhang, Changhe Tu
Synchronie Passagère	On his way for his important job interview, Noah runs out of gas. Left with no alternative, he has no choice but to embark with Katia, a tornado chaser.	https://dl.acm.org/doi/abs/10.1145/3550339.3556081	Julia Le Bras-Juarez, Emmie Marriere, Marianne Fourmanoit, Laura Techer, Louise-Marie Rousselie, Jean Delamarre, Alexis Prost, Philippe Meis
Tauray: A Scalable Real-Time Open-Source Path Tracer for Stereo and Light Field Displays	Light field displays represent yet another step in continually increasing pixel counts. Rendering realistic real-time 3D content for them with ray tracing-based methods is a major challenge even accounting for recent hardware acceleration features, as renderers have to scale to tens to hundreds of distinct viewpoints. To this end, we contribute an open-source, cross-platform real-time 3D renderer called Tauray. The primary focus of Tauray is in using photorealistic path tracing techniques to generate real-time content for multi-view displays, such as VR headsets and light field displays; this aspect is generally overlooked in existing renderers. Ray tracing hardware acceleration as well as multi-GPU rendering is supported. We compare Tauray to other open source real-time path tracers, like Lighthouse 2, and show that it can meet or significantly exceed their performance.	https://dl.acm.org/doi/abs/10.1145/3550340.3564225	Julius Ikkala, Markku Mäkitalo, Tuomas Lauttia, Erwan Leria, Pekka Jääskeläinen
Tear Off	A young bee must overcome her condition if she is to survive the DESTRUCTIVE HORNET that has invaded her hive. This adventure allows us to discover the dark side of a colony that is utopian at first glance. The film also explores the world of bees in more depth, with a story that highlights the fear of the unknown, of the dark, of claustrophobia, pushed in that by a macro camera which follows the character, in an alternative and frightening documented universe.	https://dl.acm.org/doi/abs/10.1145/3550339.3556222	Clément Del Negro, Charlotte Fargier, Héloïse Neveu, Camille Souchard, Nalini Bashin, Mikko Petremand, Matthias Bourgeuil, Philippe Meis
TeleViewDemo: Experience the Future of 3D Teleconferencing	Recent demonstrations of 3D telepresence provide a glimpse into the future where 2D video communication is replaced with photo-realistic virtual avatars rendered on 3D displays by using dedicated hardware. Our platform lets visitors experience real-time end-to-end 3D teleconferencing using commodity hardware. This demo integrates current state-of-the-art face reconstruction and rendering algorithms into an end-to-end system to illustrate the utility and capabilities of commodity telepresence on a range of 3D displays.	https://dl.acm.org/doi/abs/10.1145/3550472.3558404	Ziyi Xia, Frank Yu, Beibei Xiong, Emily Jia, Seungyeon Baek, James Gregson, Xingzhe He, Helge Rhodin, Sidney Fels
Teleport to the Augmented Real-World with Live Interactive Effects (IFX)	Augmented telepresence provides rich communication for people at a distance with interactive blended information between the virtual and real world [Rhee et al. 2017, 2020; Young et al. 2022]. We push the boundaries of augmented telepresence with a novel live media technology, including live capturing, modeling, blending, and interactive effects (IFX) to augment telepresence. Using our technology, people at a distance can connect and communicate with creative storytelling, augmented with novel IFX. We achieve this with the following breakthroughs: 1) digitizing remote spaces and people in real-time, 2) transmitting digitized information across a network, 3) augmenting remote telepresence using real-time visual effects and interactive storytelling with live-blending of 3D virtual assets into the digitized real-world. In this presentation, we will unveil several new technologies and novel IFX that can enrich telepresence, including: • Real-time 360° RGBD video capturing: we will demonstrate capturing 360° RGBD videos using a 360° RGB camera and LiDAR sensor, including synchronization between the RGB and depth streams as well as depth map generation. • IFX with live RGBD videos: we will demonstrate real-time blending of 3D virtual objects into the live 360° RGBD videos, showcasing real-time occlusion and collision handling. • 6-degrees of freedom (DoF) tele-movement: we introduce our recent research [Chen et al. 2022] for volumetric environment capturing and 6-DoF navigation. We will demonstrate real-time navigation (movement and rotation) in captured real surroundings (beyond room scales). We will showcase applications (Figure 1) where we can virtually teleport to and explore within a live stream of the augmented real world and communicate remotely with live IFX.	https://dl.acm.org/doi/abs/10.1145/3550453.3570123	Taehyun Rhee, Andrew Chalmers, Weng Khuan Hoh, Richard Roberts, Warren Butcher, Simon Finnie, Rose Barrett
Temporal and Spatial Distortion for VR Rhythmic Skill Training	In many sports, rhythmic skills are considered important. In this paper, we take juggling as an example and propose a VR system that simplifies the acquisition of a sense of rhythm. The proposed system uses temporal and spatial distortion and other functions to assist the training. A pilot study is conducted to validate the effectiveness of each function.	https://dl.acm.org/doi/abs/10.1145/3550082.3564160	Takashi Matsumoto, Erwin Wu, Hideki Koike
TexSR: Image Super-Resolution for High-Quality Texture Mapping	We introduce an image super-resolution technique for high-quality texture mapping in this poster. We first get upscaled textures from an existing image super-resolution (SR) method. We then perform a post-color correction algorithm to restore color tones and details lost in the SR algorithm. Finally, we compress the textures with variable compression ratios to reduce storage and memory overheads caused by the increased resolution. As a result, TexSR can improve the image quality of a state of the art, Real-ESRGAN.	https://dl.acm.org/doi/abs/10.1145/3550082.3564204	Jae-Ho Nah, Hyeju Kim
Text2Light: Zero-Shot Text-Driven HDR Panorama Generation	High-quality HDRIs (High Dynamic Range Images), typically HDR panoramas, are one of the most popular ways to create photorealistic lighting and 360-degree reflections of 3D scenes in graphics. Given the difficulty of capturing HDRIs, a versatile and controllable generative model is highly desired, where layman users can intuitively control the generation process. However, existing state-of-the-art methods still struggle to synthesize high-quality panoramas for complex scenes. In this work, we propose a zero-shot text-driven framework, Text2Light, to generate 4K+ resolution HDRIs without paired training data. Given a free-form text as the description of the scene, we synthesize the corresponding HDRI with two dedicated steps: 1) text-driven panorama generation in low dynamic range (LDR) and low resolution (LR), and 2) super-resolution inverse tone mapping to scale up the LDR panorama both in resolution and dynamic range. Specifically, to achieve zero-shot text-driven panorama generation, we first build dual codebooks as the discrete representation for diverse environmental textures. Then, driven by the pre-trained Contrastive Language-Image Pre-training (CLIP) model, a text-conditioned global sampler learns to sample holistic semantics from the global codebook according to the input text. Furthermore, a structure-aware local sampler learns to synthesize LDR panoramas patch-by-patch, guided by holistic semantics. To achieve super-resolution inverse tone mapping, we derive a continuous representation of 360-degree imaging from the LDR panorama as a set of structured latent codes anchored to the sphere. This continuous representation enables a versatile module to upscale the resolution and dynamic range simultaneously. Extensive experiments demonstrate the superior capability of Text2Light in generating high-quality HDR panoramas. In addition, we show the feasibility of our work in realistic rendering and immersive VR.	https://dl.acm.org/doi/abs/10.1145/3550454.3555447	Zhaoxi Chen, Guangcong Wang, Ziwei Liu
The Continuity of Locomotion: Rethinking Conventions for Locomotion and its Visualization in Shared Virtual Reality Spaces	Natural interaction between multiple users within a shared virtual environment (VE) relies on each other's awareness of the current position of the interaction partners. This, however, cannot be warranted when users employ noncontinuous locomotion techniques, such as teleportation, which may cause confusion among bystanders. In this paper, we pursue two approaches to create a pleasant experience for both the moving user and the bystanders observing that movement. First, we will introduce a system that delivers continuous full-body human representations for noncontinuous locomotion in shared virtual reality (VR) spaces. imitate their assigned user's real-world movements when close-by and autonomously navigate to their user when the distance between them exceeds a certain threshold, i.e., after the user teleports. As part of the system, we implemented four avatar transition techniques and compared them to conventional avatar locomotion in a user study, revealing significant positive effects on the observers' spatial awareness, as well as pragmatic and hedonic quality scores. Second, we introduce the concept of , which can be applied to any continuous locomotion method. By converting a continuous movement into short-interval teleport steps, we provide the merits of non-continuous locomotion for the moving user while observers can easily keep track of their path. Thus, while the experience for observers is similarly positive as with continuous motion, a user study confirmed that can significantly reduce the occurrence of cybersickness symptoms for the moving user, making it an attractive choice for shared VEs. We will discuss the potential of and for shared VR experiences, both when applied individually and in combination.	https://dl.acm.org/doi/abs/10.1145/3550454.3555522	Jann Philipp Freiwald, Susanne Schmidt, Bernhard E. Riecke, Frank Steinicke
The Emerging Media Art Performance of Eine Kleine Nachtmusik	This performance aims to transform the traditional classical music experience through emerging media technology. Generative visualization will analyze and use not only musicians' audio but also audience-generated noise and ambient sound. This will make the audience's expression part of the performance, and the visual outcomes will become nonfungible experiences.	https://dl.acm.org/doi/abs/10.1145/3550470.3558444	Kyungho Lee, Yousang Kwon
The End	After watching her gallant knight die before their wedding day and ending the film in tragedy, it is up to the determined princess Hilderose to take matters into her own hands and get back her Happily Ever After.	https://dl.acm.org/doi/abs/10.1145/3550339.3554695	Alexandria Siah
The Patient 05	Many assumed that the increased use of modern technology in the workplace would allow mankind to sit back more, yet some people are working longer hours than ever. This series by Axl Le reflects on the role of a modern work environment and how it impacts our life.	https://dl.acm.org/doi/abs/10.1145/3550470.3558427	Yi Le
The Pope's Dog	Chaos occurs when the Pope's dog escapes in the Vatican.	https://dl.acm.org/doi/abs/10.1145/3550339.3556467	Neko Pilarcik-Tellez, Madison Tody
The Sprayer	The beauty is forbidden.	https://dl.acm.org/doi/abs/10.1145/3550339.3554392	Farnoosh Abedi
The end of war	An anti-war short animation filmed with claymation techique.	https://dl.acm.org/doi/abs/10.1145/3550339.3554721	Lei Chen
The most boring Granny in the whole world	"Greta has the most boring grandma in the whole world. When she falls asleep on the sofa, Greta gets the idea to play ""funeral"" with her grandma. This confronts them with some questions they didn't ask themselves before. Because what remains when you have to leave life behind?"	https://dl.acm.org/doi/abs/10.1145/3550339.3555987	Damaris Zielke
Third Room: Decentralized Virtual Worlds on Matrix	Third Room is a web-based platform virtual worlds built on top of the open, secure, and decentralized Matrix communication protocol. It leverages the latest browser features such as Atomics, Shared ArrayBuffer, and WebGL2 to leverage the hardware's full capabilities and deliver high quality immersive worlds.	https://dl.acm.org/doi/abs/10.1145/3550453.3586015	Robert Long, Nathaniel Martin, Ajay Bura
Tidal Space: Interactive Home Installation for Work-From-Home Parents	The past few years of COVID-19 lockdown have made it abundantly clear that both childcare and professional work are inextricable from the home office – as many were forced to juggle between various roles and expectations while caring for their family and working from home. Tidal Space considers the manifold needs of work-from-home parents by incorporating motorized curtains and a foldable acoustic panel into the home office. Triggered by screen activity, the curtains self-adjust to serve as spatial moderators, mediating the boundaries between different types of work. From soundproof separation for focused work, to more translucent and open configurations for checking on the children, to interactive elements allowing kids and parents to play, Tidal Space aims to improve the home office experience for all.	https://dl.acm.org/doi/abs/10.1145/3550471.3561831	Martina Huynh, Jonas Althaus, Hyunjung Kim
TileGen: Tileable, Controllable Material Generation and Capture	"Recent methods (e.g. MaterialGAN) have used unconditional GANs to generate per-pixel material maps, or as a prior to reconstruct materials from input photographs. These models can generate varied random material appearance, but do not have any mechanism to constrain the generated material to a specific category or to control the coarse structure of the generated material, such as the exact brick layout on a brick wall. Furthermore, materials reconstructed from a single input photo commonly have artifacts and are generally not tileable, which limits their use in practical content creation pipelines. We propose TileGen, a generative model for SVBRDFs that is specific to a material category, always tileable, and optionally conditional on a provided input structure pattern. TileGen is a variant of StyleGAN whose architecture is modified to always produce tileable (periodic) material maps. In addition to the standard ""style"" latent code, TileGen can optionally take a condition image, giving a user direct control over the dominant spatial (and optionally color) features of the material. For example, in brick materials, the user can specify a brick layout and the brick color, or in leather materials, the locations of wrinkles and folds. Our inverse rendering approach can find a material perceptually matching a single target photograph by optimization. This reconstruction can also be conditional on a user-provided pattern. The resulting materials are tileable, can be larger than the target image, and are editable by varying the condition."	https://dl.acm.org/doi/abs/10.1145/3550469.3555403	Xilong Zhou, Milos Hasan, Valentin Deschaintre, Paul Guerrero, Kalyan Sunkavalli, Nima Khademi Kalantari
Time-Dependent Machine Learning for Volumetric Simulation	We explore the application of a time-dependent machine learning framework to art direction of volumetric simulations. We show the benefit of the time dependency inherent to the ODE-net model when used in conjunction with simulation sequences. Unlike other machine learning methods which maintain a uniform timestep constraint during evaluation, the ODE-net framework is able to generate results for arbitrary time samples. We demonstrate how this non-uniform time step evaluation can be leveraged for use in artistic direction tasks. We specifically apply the model to the retiming of volumetric simulations to showcase the ability of the machine learning method to properly predict arbitrary time steps. We show that with minimal training data, the model is able to generalize over several simulation sequences with similar parameters.	https://dl.acm.org/doi/abs/10.1145/3550082.3564214	Samuel Giraud-Carrier, Seth Holladay, Parris Egbert
Touchable Cooled Graphics: Midair 3D Image with Noncontact Cooling Feedback using Ultrasound-Driven Mist Vaporization	Adding tactile feedback to a midair image realizes immersive mixed reality contents. In this study, we develop a midair 3D image with a noncontact cooling sensation using ultrasound. In this system, users can feel a cooling sensation when touching the 3D image with their bare hands. The noncontact cooling sensation is rapidly displayed by ultrasound-driven mist vaporization. In the previous ultrasound haptic-optic display, only mechanical tactile feedback e.g. vibration has been used. The cooling sensation can extend the displayable material texture of the ultrasound haptic system. In the demo, we present a 3D image of ice. Participants can touch the image freely and feel its realistic cooling sensation.	https://dl.acm.org/doi/abs/10.1145/3550471.3558402	Hanaho Motoyama, Masahiro Fujiwara, Tao Morisaki, Hiroyuki Shinoda, Yasutoshi Makino
Towards Virtual Humans without Gender Stereotyped Visual Features	Animations have become increasingly realistic with the evolution of Computer Graphics (CG). In particular, human models and behaviors have been represented through animated virtual humans. Gender is a characteristic related to human identification, so virtual humans assigned to a specific gender have, in general, stereotyped representations through movements, clothes, hair, and colors in order to be understood by users as desired by designers. An important area of study is determining whether participants' perceptions change depending on how a virtual human is visually presented. Findings in this area can help the industry guide the modeling and animation of virtual humans to deliver the expected impact to the public. In this paper, we reproduce using an animated CG baby, a previous perceptual study conducted in real life aimed to assess gender bias about a baby. Our research indicates that simply textually reporting a virtual human's gender may be sufficient to create a perception of gender that affects the participant's emotional response so that stereotyped behaviors can be avoided.	https://dl.acm.org/doi/abs/10.1145/3550340.3564232	Victor Araujo, Diogo Schaffer, Angelo Brandelli Costa, Soraia Raupp Musse
Training-Free Neural Matte Extraction for Visual Effects	"Alpha matting is widely used in video conferencing as well as in movies, television, and social media sites. Deep learning approaches to the matte extraction problem are well suited to video conferencing due to the consistent subject matter (front-facing humans), however training-based approaches are somewhat pointless for entertainment videos where varied subjects (spaceships, monsters, etc.) may appear only a few times in a single movie – if a method of creating ground truth for training exists, just use that method to produce the desired mattes. We introduce a training-free high quality neural matte extraction approach that specifically targets the assumptions of visual effects production. Our approach is based on the deep image prior, which optimizes a deep neural network to fit a single image, thereby providing a deep encoding of the particular image. We make use of the representations in the penultimate layer to interpolate coarse and incomplete ""trimap"" constraints. Videos processed with this approach are temporally consistent. The algorithm is both very simple and surprisingly effective."	https://dl.acm.org/doi/abs/10.1145/3550340.3564230	Sharif Elcott, J.P. Lewis, Noritsugu Kanazawa, Christoph Bregler
Transcendental Avatar: Experiencing Bioresponsive Avatar of the Self for Improved Cognition	Transcendental Avatar is a virtual reality (VR) system focused on stress relief to support relaxation techniques, using biofeedback, a well-known therapy technique to improve physical and mental health. Biofeedback as a method uses visual and audio feedback of one's physiology to reflect, recognize, and help one gain awareness of many physiological functions, improving their cognitive and emotional state. Yet, there has been little work on how this can be appropriately leveraged in virtual reality (VR). In Transcendental Avatar, we proposed a system that shows the personification of the avatar, reflecting in real-time user's biofeedback to enhance their cognitive and emotional state. In this study, we examine whether stress, and self-reported anxiety symptoms can be relieved or reduced by the proposed immersive experience.	https://dl.acm.org/doi/abs/10.1145/3550472.3558417	Kinga Skiers, Yun Suen Pai, Kouta Minamizawa
Transcendental Avatar: Experiencing Bioresponsive Avatar of the Self for Improved Cognition	Transcendental Avatar is a virtual reality (VR) system focused on stress relief to support relaxation techniques, using biofeedback, a well-known therapy technique to improve physical and mental health. Biofeedback as a method uses visual and audio feedback of one's physiology to reflect, recognize, and help one gain awareness of many physiological functions, improving their cognitive and emotional state. Yet, there has been little work on how this can be appropriately leveraged in virtual reality (VR). In Transcendental Avatar, we proposed a system that shows the personification of the avatar, reflecting in real-time user's biofeedback to enhance their cognitive and emotional state. In this study, we examine whether stress, and self-reported anxiety symptoms can be relieved or reduced by the proposed immersive experience.	https://dl.acm.org/doi/abs/10.1145/3550472.3558417	Kinga Skiers, Yun Suen Pai, Kouta Minamizawa
Transcendental Avatar: Experiencing Bioresponsive Avatar of the Self for Improved Cognition	Transcendental Avatar is a virtual reality (VR) system focused on stress relief to support relaxation techniques, using biofeedback, a well-known therapy technique to improve physical and mental health. Biofeedback as a method uses visual and audio feedback of one's physiology to reflect, recognize, and help one gain awareness of many physiological functions, improving their cognitive and emotional state. Yet, there has been little work on how this can be appropriately leveraged in virtual reality (VR). In Transcendental Avatar, we proposed a system that shows the personification of the avatar, reflecting in real-time user's biofeedback to enhance their cognitive and emotional state. In this study, we examine whether stress, and self-reported anxiety symptoms can be relieved or reduced by the proposed immersive experience.	https://dl.acm.org/doi/abs/10.1145/3550082.3564210	Kinga Skiers, Yun Suen Pai, Kouta Minamizawa
Transcendental Avatar: Experiencing Bioresponsive Avatar of the Self for Improved Cognition	Transcendental Avatar is a virtual reality (VR) system focused on stress relief to support relaxation techniques, using biofeedback, a well-known therapy technique to improve physical and mental health. Biofeedback as a method uses visual and audio feedback of one's physiology to reflect, recognize, and help one gain awareness of many physiological functions, improving their cognitive and emotional state. Yet, there has been little work on how this can be appropriately leveraged in virtual reality (VR). In Transcendental Avatar, we proposed a system that shows the personification of the avatar, reflecting in real-time user's biofeedback to enhance their cognitive and emotional state. In this study, we examine whether stress, and self-reported anxiety symptoms can be relieved or reduced by the proposed immersive experience.	https://dl.acm.org/doi/abs/10.1145/3550082.3564210	Kinga Skiers, Yun Suen Pai, Kouta Minamizawa
Transcendental Avatar: Experiencing Bioresponsive Avatar of the Self for Improved Cognition	Biofeedback is a well-known form of therapy in which the patient receives sensory and auditory input on their physiology to help them reflect on, recognize, and be aware of their own state in order to improve their cognitive and emotional functioning. In our approach, a user's self-avatar is generated based on their physiological state, specifically their heart rate and electrodermal activity. Moreover, based on self-reported anxiety symptoms, we investigate if the biofeedback self-immersive avatar can help reduce stress levels.	https://dl.acm.org/doi/abs/10.1145/3550472.3558417	Kinga Skiers, Yun Suen Pai, Kouta Minamizawa
Transcendental Avatar: Experiencing Bioresponsive Avatar of the Self for Improved Cognition	Biofeedback is a well-known form of therapy in which the patient receives sensory and auditory input on their physiology to help them reflect on, recognize, and be aware of their own state in order to improve their cognitive and emotional functioning. In our approach, a user's self-avatar is generated based on their physiological state, specifically their heart rate and electrodermal activity. Moreover, based on self-reported anxiety symptoms, we investigate if the biofeedback self-immersive avatar can help reduce stress levels.	https://dl.acm.org/doi/abs/10.1145/3550472.3558417	Kinga Skiers, Yun Suen Pai, Kouta Minamizawa
Transcendental Avatar: Experiencing Bioresponsive Avatar of the Self for Improved Cognition	Biofeedback is a well-known form of therapy in which the patient receives sensory and auditory input on their physiology to help them reflect on, recognize, and be aware of their own state in order to improve their cognitive and emotional functioning. In our approach, a user's self-avatar is generated based on their physiological state, specifically their heart rate and electrodermal activity. Moreover, based on self-reported anxiety symptoms, we investigate if the biofeedback self-immersive avatar can help reduce stress levels.	https://dl.acm.org/doi/abs/10.1145/3550082.3564210	Kinga Skiers, Yun Suen Pai, Kouta Minamizawa
Transcendental Avatar: Experiencing Bioresponsive Avatar of the Self for Improved Cognition	Biofeedback is a well-known form of therapy in which the patient receives sensory and auditory input on their physiology to help them reflect on, recognize, and be aware of their own state in order to improve their cognitive and emotional functioning. In our approach, a user's self-avatar is generated based on their physiological state, specifically their heart rate and electrodermal activity. Moreover, based on self-reported anxiety symptoms, we investigate if the biofeedback self-immersive avatar can help reduce stress levels.	https://dl.acm.org/doi/abs/10.1145/3550082.3564210	Kinga Skiers, Yun Suen Pai, Kouta Minamizawa
Transformer Inertial Poser: Real-time Human Motion Reconstruction from Sparse IMUs with Simultaneous Terrain Generation	"Real-time human motion reconstruction from a sparse set of (e.g. six) wearable IMUs provides a non-intrusive and economic approach to motion capture. Without the ability to acquire position information directly from IMUs, recent works took data-driven approaches that utilize large human motion datasets to tackle this under-determined problem. Still, challenges remain such as temporal consistency, drifting of global and joint motions, and diverse coverage of motion types on various terrains. We propose a novel method to simultaneously estimate full-body motion and generate plausible visited terrain from only six IMU sensors in real-time. Our method incorporates 1. a conditional Transformer decoder model giving consistent predictions by explicitly reasoning prediction history, 2. a simple yet general learning target named ""stationary body points"" (SBPs) which can be stably predicted by the Transformer model and utilized by analytical routines to correct joint and global drifting, and 3. an algorithm to generate regularized terrain height maps from noisy SBP predictions which can in turn correct noisy global motion estimation. We evaluate our framework extensively on synthesized and real IMU data, and with real-time live demos, and show superior performance over strong baseline methods."	https://dl.acm.org/doi/abs/10.1145/3550469.3555428	Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, C. Karen Liu
Ultrasound-Driven Passive Haptic Actuator Based on Amplifying Radiation Force Using Simple Lever Mechanism	Haptics is a promising modality, which realizes intuitive human motion guidance and an immersive game experience. For a natural tactile experience, a lightweight and powerful wearable haptic device is required. In this study, we develop a lightweight passive haptic device (6.2 g) remotely driven by airborne ultrasound. This device can present a strong haptic stimulus of 400 mN (40 gf) by amplifying the applied ultrasound acoustic radiation force 19.6 times using a simple lever mechanism. Moreover, since the radiation force is presented at the sound velocity, the presentation speed of the amplified force is still high. In this demo, participants can experience a lightweight passive haptic actuator worn on their fingertips. This device can present a static force of 400 mN and low-frequency vibration in 45 ms. We also demonstrate an earring-type passive haptic device, which presents a haptic stimulus to the earlobe.	https://dl.acm.org/doi/abs/10.1145/3550471.3558401	Tao Morisaki, Masahiro Fujiwara, Yasutoshi Makino, Hiroyuki Shinoda
UmeTrack: Unified multi-view end-to-end hand tracking for VR	Real-time tracking of 3D hand pose in world space is a challenging problem and plays an important role in VR interaction. Existing work in this space are limited to either producing root-relative (versus world space) 3D pose or rely on multiple stages such as generating heatmaps and kinematic optimization to obtain 3D pose. Moreover, the typical VR scenario, which involves multi-view tracking from wide field of view (FOV) cameras is seldom addressed by these methods. In this paper, we present a unified end-to-end differentiable framework for multi-view, multi-frame hand tracking that directly predicts 3D hand pose in world space. We demonstrate the benefits of end-to-end differentiabilty by extending our framework with downstream tasks such as jitter reduction and pinch prediction. To demonstrate the efficacy of our model, we further present a new large-scale egocentric hand pose dataset that consists of both real and synthetic data. Experiments show that our system trained on this dataset handles various challenging interactive motions, and has been successfully applied to real-time VR applications.	https://dl.acm.org/doi/abs/10.1145/3550469.3555378	Shangchen Han, Po-Chen Wu, Yubo Zhang, Beibei Liu, Linguang Zhang, Zheng Wang, Weiguang Si, Peizhao Zhang, Yujun Cai, Tomas Hodan, Randi Cabezas, Luan Tran, Muzaffer Akbay, Tsz-Ho Yu, Cem Keskin, Robert Wang
Unbiased Caustics Rendering Guided by Representative Specular Paths	Caustics are interesting patterns caused by the light being focused when reflecting off glossy materials. Rendering them in computer graphics is still challenging: they correspond to high luminous intensity focused over a small area. Finding the paths that contribute to this small area is difficult, and even more difficult when using camera-based path tracing instead of bidirectional approaches. Recent improvements in path guiding are still unable to compute efficiently the light paths that contribute to a caustic. In this paper, we present a novel path guiding approach to enable reliable rendering of caustics. Our approach relies on computing representative specular paths, then extending them using a chain of spherical Gaussians. We use these extended paths to estimate the incident radiance distribution and guide path tracing. We combine this approach with several practical strategies, such as spatial reusing and parallax-aware representation for arbitrarily curved reflectors. Our path-guided algorithm using extended specular paths outperforms current state-of-the-art methods and handles multiple bounces of light and a variety of scenes.	https://dl.acm.org/doi/abs/10.1145/3550469.3555381	He Li, Beibei Wang, Changhe Tu, Kun Xu, Nicolas Holzschuch, Ling-Qi Yan
Unforgotten	Animation 'Unforgotten' delivers the 'Comfort Women' survivors' trauma and horrible sexual violence that they experienced at 'Comfort Stations' through metaphorical and poetic 3D animation scenes breaking out of the media's typical way of depicting sexual violence and re-traumatizing survivors. It uses 3D animation techniques to create fairy-like visuals, so as not to perpetuate violent imagery and re-victimizing survivors in describing atrocious wartime sexual violence.	https://dl.acm.org/doi/abs/10.1145/3550339.3554501	Sujin Kim
UniColor: A Unified Framework for Multi-Modal Colorization with Transformer	We propose the first unified framework to support colorization in multiple modalities, including both unconditional and conditional ones, such as stroke, exemplar, text, and even a mix of them. Rather than learning a separate model for each type of condition, we introduce a two-stage colorization framework for incorporating various conditions into a single model. In the first stage, multi-modal conditions are converted into a common representation of hint points. Particularly, we propose a novel CLIP-based method to convert the text to hint points. In the second stage, we propose a Transformer-based network composed of and to generate diverse and high-quality colorization results conditioned on hint points. Both qualitative and quantitative comparisons demonstrate that our method outperforms state-of-the-art methods in every control modality and further enables multi-modal colorization that was not feasible before. Moreover, we design an interactive interface showing the effectiveness of our unified framework in practical usage, including automatic colorization, hybrid-control colorization, local recolorization, and iterative color editing. Our code and models are available at .	https://dl.acm.org/doi/abs/10.1145/3550454.3555471	Zhitong Huang, Nanxuan Zhao, Jing Liao
Using Rhythm Game to Train Rhythmic Motion in Sports	Rhythm is important to improve skills in various sports. In this study, we create a rhythm game for ski training. It synchronizes music and turn timing, and has various types of feedback for training. We conducted a pilot study to verify the effectiveness through a comparison of three conditions.	https://dl.acm.org/doi/abs/10.1145/3550082.3564173	Hidetaka Katsuyama, Erwin Wu, Hideki Koike
VIINTER: View Interpolation with Implicit Neural Representations of Images	We present VIINTER, a method for view interpolation by interpolating the implicit neural representation (INR) of the captured images. We leverage the learned code vector associated with each image and interpolate between these codes to achieve viewpoint transitions. We propose several techniques that significantly enhance the interpolation quality. VIINTER signifies a new way to achieve view interpolation without constructing 3D structure, estimating camera poses, or computing pixel correspondence. We validate the effectiveness of VIINTER on several multi-view scenes with different types of camera layout and scene composition. As the development of INR of images (as opposed to surface or volume) has centered around tasks like image fitting and super-resolution, with VIINTER, we show its capability for view interpolation and offer a promising outlook on using INR for image manipulation tasks.	https://dl.acm.org/doi/abs/10.1145/3550469.3555417	Brandon Yushan Feng, Susmija Jabbireddy, Amitabh Varshney
VOCAL: Vowel and Consonant Layering for Expressive Animator-Centric Singing Animation	Singing and speaking are two fundamental forms of human communication. From a modeling perspective however, speaking can be seen as a subset of singing. We present VOCAL, a system that automatically generates expressive, animator-centric lower face animation from singing audio input. Articulatory phonetics and voice instruction ascribe additional roles to vowels (projecting melody and volume) and consonants (lyrical clarity and rhythmic emphasis) in song. Our approach directly uses these insights to define axes for Melodic-accent and Pitch-sensitivity (Ma-Ps), which together provide an abstract space to visually represent various singing styles. In our system. vowels are processed first. A lyrical vowel is often sung tonally as one or more different vowels. We perform any such vowel modifications using a neural network trained on input audio. These vowels are then dilated from their spoken behaviour to bleed into each other based on Melodic-accent (Ma), with Pitch-sensitivity (Ps) modeling visual vibrato. Consonant animation curves are then layered in, with viseme intensity modeling rhythmic emphasis (inverse to Ma). Our evaluation is fourfold: we show the impact of our design parameters; we compare our results to ground truth and prior art; we present compelling results on a variety of voices and singing styles; and we validate these results with professional singers and animators.	https://dl.acm.org/doi/abs/10.1145/3550469.3555408	Yifang Pan, Chris Landreth, Eugene Fiume, Karan Singh
VR Education Contents for Core Fundamental Nursing Skills	This educational content concerns fundamental nursing skills by using virtual reality. Blood sugar testing and the administration of insulin subcutaneous injection are two of the most frequently used among the 20 core fundamental nursing skills. The content pertaining to these tasks can be used for self-management education for diabetic patients. This contents designed a core fundamental nursing skill on immersive VR contents by dividing the learner's experience into three stages: guide, mission, and feedback with these skills. And it is designed by tracking the movement of the hand through finger joint recognition without using a controller for immerse in training. This content will help improve nursing students' clinical practice competency.	https://dl.acm.org/doi/abs/10.1145/3550472.3558410	Jungki Kim, Youngsoo Lee, Hye-Yon Yu
VToonify: Controllable High-Resolution Portrait Video Style Transfer	Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision. Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency. In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework. Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output. Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity. This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively. Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls. Code and pretrained models are available at our project page: www.mmlab-ntu.com/project/vtoonify/.	https://dl.acm.org/doi/abs/10.1145/3550454.3555437	Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy
Video-Driven Neural Physically-Based Facial Asset for Production	Production-level workflows for producing convincing 3D dynamic human faces have long relied on an assortment of labor-intensive tools for geometry and texture generation, motion capture and rigging, and expression synthesis. Recent neural approaches automate individual components but the corresponding latent representations cannot provide artists with explicit controls as in conventional tools. In this paper, we present a new learning-based, video-driven approach for generating dynamic facial geometries with high-quality physically-based assets. For data collection, we construct a hybrid multiview-photometric capture stage, coupling with ultra-fast video cameras to obtain raw 3D facial assets. We then set out to model the facial expression, geometry and physically-based textures using separate VAEs where we impose a global MLP based expression mapping across the latent spaces of respective networks, to preserve characteristics across respective attributes. We also model the delta information as wrinkle maps for the physically-based textures, achieving high-quality 4K dynamic textures. We demonstrate our approach in high-fidelity performer-specific facial capture and cross-identity facial motion retargeting. In addition, our multi-VAE-based neural asset, along with the fast adaptation schemes, can also be deployed to handle in-the-wild videos. Besides, we motivate the utility of our explicit facial disentangling strategy by providing various promising physically-based editing results with high realism. Comprehensive experiments show that our technique provides higher accuracy and visual fidelity than previous video-driven facial reconstruction and animation methods.	https://dl.acm.org/doi/abs/10.1145/3550454.3555445	Longwen Zhang, Chuxiao Zeng, Qixuan Zhang, Hongyang Lin, Ruixiang Cao, Wei Yang, Lan Xu, Jingyi Yu
VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild	We present VideoReTalking, a new system to edit the faces of a real-world talking head video according to input audio, producing a high-quality and lip-syncing output video even with a different emotion. Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism. Given a talking-head video, we first modify the expression of each frame according to the same expression template using the expression editing network, resulting in a video with the canonical expression. This video, together with the given audio, is then fed into the lip-sync network to generate a lip-syncing video. Finally, we improve the photo-realism of the synthesized faces through an identity-aware face enhancement network and post-processing. We use learning-based approaches for all three steps and all our modules can be tackled in a sequential pipeline without any user intervention. Furthermore, our system is a generic approach that does not need to be retrained to a specific person. Evaluations on two widely-used datasets and in-the-wild examples demonstrate the superiority of our framework over other state-of-the-art methods in terms of lip-sync accuracy and visual quality. 	https://dl.acm.org/doi/abs/10.1145/3550469.3555399	Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, Nannan Wang
Visual Simulation of Tire Smoke	In recent years, the realism of video expression by 3DCG has increased remarkably. In this study, we propose a CG representation method of tire smoke based on physical simulation. Although tire smoke has appeared in various video works and 3DCG games, the mechanism of how tire smoke is generated has not been fully elucidated. We calculate the generation and conduction of heat from the rotation speed and friction between the tire and the ground, and construct a model of tire smoke generation from the viewpoint of statistical mechanics. The parameters in the mathematical model are set from actual images and observations, and we confirm that the phenomenon of tire smoke can be reproduced.	https://dl.acm.org/doi/abs/10.1145/3550082.3564167	Tomoya Tamagawa, Tomokazu Ishikawa
Voight-Kampff 2.0	"""Voight-Kampff 2.0"" is a reflection on the sustainable development of artificial intelligence technology in the future. Once a machine becomes conscious, would AI stay as a tool or obtain a soul? Would there be a boundary between human and machines? A Voight-Kampff test process involving two individuals is simulated in this work, where the emotional fluctuations generated by empathy provide the basis for judging whether the suspect is human or machine. There is a saying in Buddhist philosophy that ""what can think is the heart"", which means that only equipped with empathy could one be considered with having personality and soul. Unlike breathing or heartbeat, EEG data is the most primitive and cannot be subjectively controlled, which also metaphors the process of ""thinking"". Therefore, the work chooses EEG data to test whether empathy produces emotional fluctuations. ""Voight-Kampff 2.0"" aims to create emotionally charged experiences through biological data. With the changing images visualization of brain waves, the human facial image is artistically processed in a retro pixel style. Specifically, when a question is delivered by the judge, the emotional changes of suspect is stimulated and reflected in EEG data, which is visualized clearly in graphics and facial images on screen of both sides. The Match degree between emotion reflection and correct answer will help the judge determine whether the suspect is human or AI. Real human brainwave art images have obvious area changes and sound feedback due to the question, but machines don't. The work not only allows audiences to empathize with the work, but also founds a connection among audiences' emotion. The work provides references for the creation project of EEG biological cooperation. Furthermore, significance of maintaining self-awareness in relationship of human-machine is also emphasized, which helps derive the sustainable development between human and AI."	https://dl.acm.org/doi/abs/10.1145/3550470.3558423	Fang Fang, Shuo Yan
Voronoi Spaghetti & VoroNoodles: Topologically Interlocked, Space-Filling, Corrugated & Congruent Tiles	In this work, we introduce an approach to model topologically interlocked corrugated bricks that can be assembled in a water-tight manner (space-filling) to design a variety of spatial structures. Our approach takes inspiration from recently developed methods that utilize Voronoi tessellation of spatial domains by using symmetrically arranged Voronoi sites. However, in contrast to these existing methods, we focus our attention on Voronoi sites modeled using helical trajectories, which can provide corrugation and better interlocking. For symmetries, we only use affine transformations based on the Bravais lattice to avoid self-intersections. This methodology naturally results in structures that are both space-filling (owing to Voronoi tessellation) as well as interlocking by corrugation (owing to helical trajectories). The resulting shapes of the bricks appear to be similar to a variety of pasta noodles, thereby inspiring the names, Voronoi Spaghetti and VoroNoodles.	https://dl.acm.org/doi/abs/10.1145/3550340.3564229	Cassie Mullins, Matt Ebert, Ergun Akleman, Vinayak Krihnamurthy
Wander [001]	This is a public art project centred on an AI chatbot called Wander. She can generate interactive fiction based on realworld locations' knowledge graphs. Through public participation, the stories are visualised on an interactive map, a fictional earth chronicle contributed through crowdsourced interactions.	https://dl.acm.org/doi/abs/10.1145/3550470.3558441	Yuqian Sun, Chang Hee Lee, Chenhang Cheng, Ali Asadipour, Ying Xu, Yihua Li
Water Simulation and Rendering from a Still Photograph	We propose an approach to simulate and render realistic water animation from a single still input photograph. We first segment the water surface, estimate rendering parameters, and compute water reflection textures with a combination of neural networks and traditional optimization techniques. Then we propose an image-based screen space local reflection model to render the water surface overlaid on the input image and generate real-time water animation. Our approach creates realistic results with no user intervention for a wide variety of natural scenes containing large bodies of water with different lighting and water surface conditions. Since our method provides a 3D representation of the water surface, it naturally enables direct editing of water parameters and also supports interactive applications like adding synthetic objects to the scene.	https://dl.acm.org/doi/abs/10.1145/3550469.3555415	Ryusuke Sugimoto, Mingming He, Jing Liao, Pedro V. Sander
WebtoonMe: A Data-Centric Approach for Full-Body Portrait Stylization	Full-body portrait stylization has drawn attention recently. However, most methods have focused only on converting face regions. A recently proposed two-stage method expands to full bodies, but the outputs are less plausible and fail to achieve quality robustness of non-face regions. Furthermore, they cannot reflect diverse skin tones. In this study, we propose a data-centric solution to build a production-level full-body portrait stylization system. We construct a novel and advanced dataset preparation paradigm that can effectively resolve the aforementioned problems.	https://dl.acm.org/doi/abs/10.1145/3550340.3564226	Jihye Back, Seungkwon Kim, Namhyuk Ahn
Window	Window is an interactive multimedia installation that reconstructs the Quarantine experience with point clouds, sound and diary texts. It creates a poetic digital environment, arouses the audience's empathetic feeling together and reflects on the relationship between body and space, the individual and the universe, the physical and the spiritual.	https://dl.acm.org/doi/abs/10.1145/3550470.3558452	Borou Yu, Jiajian Min, Mengying Zeng
WorldWide StRhyme	Worldwide StRhyme is an experience that rhymes in many languages by selecting words through stepping forward aiming to blur the boundary of words as sound and meaning. Although the experience is mainly performed by two people, bystanders also have an unprecedented auditory experience.	https://dl.acm.org/doi/abs/10.1145/3550470.3558448	Minori Manabe, Wataru Takamine, Richard Sahala Hartanto, Reita Maeno, Kai Fukubayashi
Woven Fabric Capture from a Single Photo	Digitally reproducing the appearance of woven fabrics is important in many applications of realistic rendering, from interior scenes to virtual characters. However, designing realistic shading models and capturing real fabric samples are both challenging tasks. Previous work ranges from applying generic shading models not meant for fabrics, to data-driven approaches scanning fabrics requiring expensive setups and large data. In this paper, we propose a woven fabric material model and a parameter estimation approach for it. Our lightweight forward shading model treats yarns as bent and twisted cylinders, shading these using a microflake-based bidirectional reflectance distribution function (BRDF) model. We propose a simple fabric capture configuration, wrapping the fabric sample on a cylinder of known radius and capturing a single image under known camera and light positions. Our inverse rendering pipeline consists of a neural network to estimate initial fabric parameters and an optimization based on differentiable rendering to refine the results. Our fabric parameter estimation achieves high-quality recovery of measured woven fabric samples, which can be used for efficient rendering and further edited.	https://dl.acm.org/doi/abs/10.1145/3550469.3555380	Wenhua Jin, Beibei Wang, Milos Hasan, Yu Guo, Steve Marschner, Ling-Qi Yan
XRAYHEAD	XRAYHEAD creates the striking illusion of seeing a skeleton inside one's head when the experimenter touches the interior skeleton. This illusion is tested in a dark room using a setup in which the participant views a specially designed smart skeleton through a half mirror, and the experimenter touches the participant and the skeleton's head with both hands. In this specific arrangement, the distance between the skeleton head and the palm of the experimenter's hand determines how the light intensity of the LED embedded in the skeleton head increases. Alternatively, the whole part of the participant's head is kept illuminated. Additionally, to enhance illusory tactile sensation to one's skeleton, an auditory stimulus is supplied through a speaker or a bone-conduction headphone and/or modulated in sync with the degree of translucence. This creates the illusion of a head with a clearly viewed skeleton inside its translucent surface, allowing the physical touch to the skeleton's surface. This technology was tested in our laboratory exhibition, and 77 of 101 participants reported a strong tactile sensation to the illusory skeleton inside their heads.	https://dl.acm.org/doi/abs/10.1145/3550472.3558411	Kento Imai, Haruka Kayano, Kenri Kodaka
Your3dEmoji: Creating Personalized Emojis via One-shot 3D-aware Cartoon Avatar Synthesis	Creating cartoon-style avatars has drawn growing attention recently, however previous methods only learn face cartoonization in the 2D image level. In this paper, we propose a novel 3D generative model to translate a real-world face image into its corresponding 3D avatar with only a single style example provided. To bridge the gap between 2D real faces and 3D cartoon avatars, we leverage the state-of-the-art StyleGAN and its style-mixing property to produce a 2D paired cartoonized face dataset. We then finetune a pretrained 3D GAN with the pair data in a dual-learning mechanism to get the final synthesized 3D avatar. Furthermore, we analyze the latent space of our model, enabling manual control in what degree a style is applied. Our model is 3D-aware in the sense and also able to do attribute editing, such as smile, age, etc directly in the 3D domain. Experimental results demonstrate that our method can produce high-fidelity cartoonized avatars with true-to-life 3D geometry.	https://dl.acm.org/doi/abs/10.1145/3550340.3564220	Shiyao Xu, Lingzhi Li, Li Shen, Yifang Men, Zhouhui Lian
Zero-Shot Multi-Modal Artist-Controlled Retrieval and Exploration of 3D Object Sets	When creating 3D content, highly specialized skills are generally needed to design and generate models of objects and other assets by hand. We address this problem through high-quality 3D asset retrieval from multi-modal inputs, including 2D sketches, images and text. We use CLIP as it provides a bridge to higher-level latent features. We use these features to perform a multi-modality fusion to address the lack of artistic control that affects common data-driven approaches. Our approach allows for multi-modal conditional feature-driven retrieval through a 3D asset database, by utilizing a combination of input latent embeddings. We explore the effects of different combinations of feature embeddings across different input types and weighting methods.	https://dl.acm.org/doi/abs/10.1145/3550340.3564216	Kristofer Schlachter, Benjamin Ahlbrand, Zhu Wang, Ken Perlin, Valerio Ortenzi
Zeroth: The self-awareness sense embodied by facing the coexistence of the physical and virtual bodies in the mirror	"""Itadakimasu"" is a gesture performed before a meal in Japan. It is meant to express gratitude for the cycle of life and the upstream supply chain. Still, it is also meant to express gratitude for the awareness of the boundary between self and others before eating a piece of life and using it as one's own flesh and blood. This ritual, which we all perform almost unconsciously without question, is a ritual that has become customary in recent years thanks to the invention of the radio, which allows us to share the same topics of conversation even when we are far away from each other. Today, physical reality and the XR/metaverse, here and there, are being connected without seams. As a result, the boundaries between oneself and others, oneself and the world have become blurred, and the loss of the ""self"" recognized by each living in this society is not far away. In the present age, when anyone can easily make discrete phase transitions anywhere, we believe it is necessary to perform appropriate rituals to recognize the boundaries between self and others, rather than just connecting without difficulty blindly. We sense and recognize all signals from the world through our five senses. It is a recursive structure in which one perceives one's existence for the first time as a contrast to the world one smells, and one can sense the world only through one's existence. In this art Zeroth, the user confronts himself in physical reality and himself in XR simultaneously. This experience allows the user to consider the appropriate ritual actions."	https://dl.acm.org/doi/abs/10.1145/3550470.3558426	Ami Miura, Hiroki Uchida, Takayuki Kawamura, Keiichi Zempo
À Bicyclette	The journey of an old man on a bicycle.	https://dl.acm.org/doi/abs/10.1145/3550339.3556378	Océane Lavergne, Benjamin Langagne, Lucas Durot, Pierre Cilluffo, Marine Beuvain, Kerrian Detay, Carlos De Carvalho
"""Dance of Drums"": An Interactive Installation of ICH Dance Representation Through the Combination of Virtual and Reality"	"With the development of the digital information, the digitalization and creative representation of the intangible culture heritage (ICH) dances have become the core of dance safeguarding. ""Dance of Drums"", an interactive installation combining virtual and reality for Chinese traditional bronze-drum dance representation, provides both external projected interface and internal virtual reality (VR) experiences. Specifically, ""Dance of Drums"" rebuilds the motions and folklore elements of bronze-drum dance through internal VR scenarios, and shows the history and ancient form of dance through the projected interface in real space, which reconstructs the static display of dances to promote the preservation and dissemination of bronze-drum dance."	https://dl.acm.org/doi/abs/10.1145/3550472.3558413	Zixiao Liu, Shuo Yan
違禁品 Outlaw	The Motherly Love, woven between Struggle.	https://dl.acm.org/doi/abs/10.1145/3550339.3556087	Chia-Shan Liu, Yu-Ching Ling, Yung-Hua Lu, Yu-Hua Yang, Hao-Yang Peng
