title	abstract	url	authors
"""Rebooting memories"": creating ""flow"" and inheriting memories from colorized photographs"	"In this paper, we explain the creation of ""flow"" in social media and real spaces using AI technology to colorize black-and-white stock photographs from digital archives and other locations. When visualizing the colors that the photographs should have had, the impressions of ""freezing"" in black-and-white photographs are ""rebooted,"" and viewers can more easily imagine the events depicted. This bridges the psychological gap between past events and modern daily life, sparking conversations. The ""flow"" generated here causes the emergence of lively communication and increases the value of information. This method can help to pass precious materials and memories of past events into the future."	https://dl.acm.org/authorize?N690067	Anju Niwata, Hidenori Watanave
3D Ken Burns effect from a single image	The Ken Burns effect allows animating still images with a virtual camera scan and zoom. Adding parallax, which results in the 3D Ken Burns effect, enables significantly more compelling results. Creating such effects manually is time-consuming and demands sophisticated editing skills. Existing automatic methods, however, require multiple input images from varying viewpoints. In this paper, we introduce a framework that synthesizes the 3D Ken Burns effect from a single image, supporting both a fully automatic mode and an interactive mode with the user controlling the camera. Our framework first leverages a depth prediction pipeline, which estimates scene depth that is suitable for view synthesis tasks. To address the limitations of existing depth estimation methods such as geometric distortions, semantic distortions, and inaccurate depth boundaries, we develop a semantic-aware neural network for depth prediction, couple its estimate with a segmentation-based depth adjustment process, and employ a refinement neural network that facilitates accurate depth predictions at object boundaries. According to this depth estimate, our framework then maps the input image to a point cloud and synthesizes the resulting video frames by rendering the point cloud from the corresponding camera positions. To address disocclusions while maintaining geometrically and temporally coherent synthesis results, we utilize context-aware color- and depth-inpainting to fill in the missing information in the extreme views of the camera path, thus extending the scene geometry of the point cloud. Experiments with a wide variety of image content show that our method enables realistic synthesis results. Our study demonstrates that our system allows users to achieve better results while requiring little effort compared to existing solutions for the 3D Ken Burns effect creation.	https://dl.acm.org/doi/abs/10.1145/3355089.3356528	Simon Niklaus, Long Mai, Jimei Yang, Feng Liu
3D hodge decompositions of edge- and face-based vector fields	We present a compendium of Hodge decompositions of vector fields on tetrahedral meshes embedded in the 3D Euclidean space. After describing the foundations of the Hodge decomposition in the continuous setting, we describe how to implement a five-component orthogonal decomposition that generically splits, for a variety of boundary conditions, any given discrete vector field expressed as discrete differential forms into two potential fields, as well as three additional harmonic components that arise from the topology or boundary of the domain. The resulting decomposition is proper and mimetic, in the sense that the theoretical dualities on the kernel spaces of vector Laplacians valid in the continuous case (including correspondences to cohomology and homology groups) are exactly preserved in the discrete realm. Such a decomposition only involves simple linear algebra with symmetric matrices, and can thus serve as a basic computational tool for vector field analysis in graphics, electromagnetics, fluid dynamics and elasticity.	https://dl.acm.org/doi/abs/10.1145/3355089.3356546	Rundong Zhao, Mathieu Desbrun, Guo-Wei Wei, Yiying Tong
400 MPH	On the Bonneville salt flats, the sound of roaring engines can be heard in the distance. At the wheel is Icarus, a chimpanzee hell-bent on transcending the limits of speed. Aboard iconic and increasingly powerful vehicles, Icarus will attempt to reach the ultimate speed of 400 Mph, thought to be impossible for land vehicles. As Icarus reaches for the sky, will he prevail or be consumed by his self-destructive quest?	https://dl.acm.org/authorize?N690903	Paul-Eugène Dannaud, Julia Chaix, Lorraine Desserre, Alice Lefort, Natacha Pianeti, Quentin Tireloque
A differential theory of radiative transfer	Physics-based differentiable rendering is the task of estimating the derivatives of radiometric measures with respect to scene parameters. The ability to compute these derivatives is necessary for enabling gradient-based optimization in a diverse array of applications: from solving analysis-by-synthesis problems to training machine learning pipelines incorporating forward rendering processes. Unfortunately, physics-based differentiable rendering remains challenging, due to the complex and typically nonlinear relation between pixel intensities and scene parameters. We introduce a differential theory of radiative transfer, which shows how individual components of the radiative transfer equation (RTE) can be differentiated with respect to arbitrary differentiable changes of a scene. Our theory encompasses the same generality as the standard RTE, allowing differentiation while accurately handling a large range of light transport phenomena such as volumetric absorption and scattering, anisotropic phase functions, and heterogeneity. To numerically estimate the derivatives given by our theory, we introduce an unbiased Monte Carlo estimator supporting arbitrary surface and volumetric configurations. Our technique differentiates path contributions symbolically and uses additional boundary integrals to capture geometric discontinuities such as visibility changes. We validate our method by comparing our derivative estimations to those generated using the finite-difference method. Furthermore, we use a few synthetic examples inspired by real-world applications in inverse rendering, non-line-of-sight (NLOS) and biomedical imaging, and design, to demonstrate the practical usefulness of our technique.	https://dl.acm.org/doi/abs/10.1145/3355089.3356522	Cheng Zhang, Lifan Wu, Changxi Zheng, Ioannis Gkioulekas, Ravi Ramamoorthi, Shuang Zhao
A multi-scale model for coupling strands with shear-dependent liquid	We propose a framework for simulating the complex dynamics of strands interacting with compressible, shear-dependent liquids, such as oil paint, mud, cream, melted chocolate, and pasta sauce. Our framework contains three main components: the strands modeled as discrete rods, the bulk liquid represented as a continuum (material point method), and a reduced-dimensional flow of liquid on the surface of the strands with detailed elastoviscoplastic behavior. These three components are tightly coupled together. To enable discrete strands interacting with continuum-based liquid, we develop models that account for the volume change of the liquid as it passes through strands and the momentum exchange between the strands and the liquid. We also develop an extended constraint-based collision handling method that supports cohesion between strands. Furthermore, we present a principled method to preserve the total momentum of a strand and its surface flow, as well as an analytic plastic flow approach for Herschel-Bulkley fluid that enables stable semi-implicit integration at larger time steps. We explore a series of challenging scenarios, involving splashing, shaking, and agitating the liquid which causes the strands to stick together and become entangled.	https://dl.acm.org/doi/abs/10.1145/3355089.3356532	Yun (Raymond) Fei, Christopher Batty, Eitan Grinspun, Changxi Zheng
A novel framework for inverse procedural texture modeling	Procedural textures are powerful tools that have been used in graphics for decades. In contrast to the alternative exemplar-based texture synthesis techniques, procedural textures provide user control and fast texture generation with low-storage cost and unlimited texture resolution. However, creating procedural models for complex textures requires a time-consuming process of selecting a combination of procedures and parameters. We present an example-based framework to automatically select procedural models and estimate parameters. In our framework, we consider textures categorized by commonly used high level classes. For each high level class we build a data-driven inverse modeling system based on an extensive collection of real-world textures and procedural texture models in the form of node graphs. We use unsupervised learning on collected real-world images in a texture class to learn sub-classes. We then classify the output of each of the collected procedural models into these sub-classes. For each of the collected models we train a convolutional neural network (CNN) to learn the parameters to produce a specific output texture. To use our framework, a user provides an exemplar texture image within a high level class. The system first classifies the texture into a sub-class, and selects the procedural models that produce output in that sub-class. The pre-trained CNNs of the selected models are used to estimate the parameters of the texture example. With the predicted parameters, the system can generate appropriate procedural textures for the user. The user can easily edit the textures by adjusting the node graph parameters. In a last optional step, style transfer augmentation can be applied to the fitted procedural textures to recover details lost in the procedural modeling process. We demonstrate our framework for four high level classes and show that our inverse modeling system can produce high-quality procedural textures for both structural and non-structural textures.	https://dl.acm.org/doi/abs/10.1145/3355089.3356516	Yiwei Hu, Julie Dorsey, Holly Rushmeier
A scalable galerkin multigrid method for real-time simulation of deformable objects	We propose a simple yet efficient multigrid scheme to simulate high-resolution deformable objects in their full spaces at interactive frame rates. The point of departure of our method is the Galerkin projection which is simple to construct. However, a naïve Galerkin multigrid does not scale well for large and irregular grids because it trades-off matrix sparsity for smaller sized linear systems which eventually stops improving the performance. Given that observation, we design our special projection criterion which is based on skinning space coordinates with piecewise constant weights, to make our Galerkin multigrid method scale for high-resolution meshes without suffering from dense linear solves. The usage of skinning space coordinates enables us to reduce the resolution of grids more aggressively, and our piecewise constant weights further ensure us to always deal with reasonably-sparse linear solves. Our projection matrices also help us to manage multi-level linear systems efficiently. Therefore, our method can be applied to different optimization schemes such as Newton's method and Projective Dynamics, pushing the resolution of a real-time simulation to orders of magnitudes higher. Our final GPU implementation outperforms the other state-of-the-art GPU deformable body simulators, enabling us to simulate large deformable objects with hundred thousands of degrees of freedom in real-time.	https://dl.acm.org/doi/abs/10.1145/3355089.3356486	Zangyueyang Xian, Xin Tong, Tiantian Liu
A thermomechanical material point method for baking and cooking	We present a Material Point Method for visual simulation of baking breads, cookies, pancakes and similar materials that consist of dough or batter (mixtures of water, flour, eggs, fat, sugar and leavening agents). We develop a novel thermomechanical model using mixture theory to resolve interactions between individual water, gas and dough species. Heat transfer with thermal expansion is used to model thermal variations in material properties. Water-based mass transfer is resolved through the porous mixture, gas represents carbon dioxide produced by leavening agents in the baking process and dough is modeled as a viscoelastoplastic solid to represent its varied and complex rheological properties. Water content in the mixture reduces during the baking process according to Fick's Law which contributes to drying and cracking of crust at the material boundary. Carbon dioxide gas produced by leavening agents during baking creates internal pressure that causes rising. The viscoelastoplastic model for the dough is temperature dependent and is used to model melting and solidification. We discretize the governing equations using a novel Material Point Method designed to track the solid phase of the mixture.	https://dl.acm.org/doi/abs/10.1145/3355089.3356537	Mengyuan Ding, Xuchen Han, Stephanie Wang, Theodore F. Gast, Joseph M. Teran
A whirlwind introduction to computer graphics	Provide a background for the amazing things you will see at SIGGRAPH Asia 2019 Create an understanding of common computer graphics vocabulary Help you understand the significance of the images and animations that you will see Provide references for further study	https://dl.acm.org/doi/abs/10.1145/3355047.3359404	Mike Bailey
AR-ia: Volumetric Opera for Mobile Augmented Reality	Motivated by recent availability of augmented and virtual reality platforms, we tackle the challenging problem of immersive storytelling experiences on mobile devices. In particular, we show an end-to-end system to generate 3D assets that enable real-time rendering of an opera on high end mobile phones. We call our system AR-ia and in this paper we walk through the main components and technical challenges of such a system, showing how to deliver an immersive mixed reality experience in every user's living room.	https://dl.acm.org/doi/abs/10.1145/3355355.3361890	Sean Kelly, Samantha Cordingley, Patrick Nolan, Christoph Rhemann, Sean Fanello, Danhang Tang, Jude Osborn, Jay Busch, Philip Davidson, Paul Debevec, Peter Denny, Graham Fyffe, Kaiwen Guo, Geoff Harvey, Shahram Izadi, Peter Lincoln, Wan-Chun Alex Ma, Jonathan Taylor, Xueming Yu, Matt Whalen, Jason Dourgarian, Genevieve Blanchett, Narelle French, Kirstin Sillitoe, Tea Uglow, Brenton Spiteri, Emma Pearson, Wade Kernot, Jonathan Richards
Accelerated complex-step finite difference for expedient deformable simulation	In deformable simulation, an important computing task is to calculate the gradient and derivative of the strain energy function in order to infer the corresponding internal force and tangent stiffness matrix. The standard numerical routine is the finite difference method, which evaluates the target function multiple times under a small real-valued perturbation. Unfortunately, the subtractive cancellation prevents us from setting this perturbation sufficiently small, and the regular finite difference is doomed for computing problems requiring a high-accuracy derivative evaluation. In this paper, we graft a new finite difference scheme, namely the complex-step finite difference (CSFD), with physics-based animation. CSFD is based on the complex Taylor series expansion, which avoids subtractions in first-order derivative approximation. As a result, one can use a very small perturbation to calculate the numerical derivative that is as accurate as its analytic counterpart. We accelerate the original CSFD method so that it is also as efficient as the analytic derivative. This is achieved by discarding high-order error terms, decoupling real and imaginary calculations, replacing costly functions based on the theory of equivalent infinitesimal, and isolating the propagation of the perturbation in composite/nesting functions. CSFD can be further augmented with multicomplex Taylor expansion and Cauchy-Riemann formula to handle higher-order derivatives and tensor-valued functions. We demonstrate the accuracy, convenience, and efficiency of this new numerical routine in the context of deformable simulation - one can easily deploy a robust simulator for general hyperelastic materials, including user-crafted ones to cater specific needs in different applications. Higher-order derivatives of the energy can be readily computed to construct modal derivative bases for reduced real-time simulation. Inverse simulation problems can also be conveniently solved using gradient/Hessian-based optimization procedures.	https://dl.acm.org/doi/abs/10.1145/3355089.3356493	Ran Luo, Weiwei Xu, Tianjia Shao, Hongyi Xu, Yin Yang
Accelerating ADMM for efficient simulation and optimization	The alternating direction method of multipliers (ADMM) is a popular approach for solving optimization problems that are potentially non-smooth and with hard constraints. It has been applied to various computer graphics applications, including physical simulation, geometry processing, and image processing. However, ADMM can take a long time to converge to a solution of high accuracy. Moreover, many computer graphics tasks involve non-convex optimization, and there is often no convergence guarantee for ADMM on such problems since it was originally designed for convex optimization. In this paper, we propose a method to speed up ADMM using Anderson acceleration, an established technique for accelerating fixed-point iterations. We show that in the general case, ADMM is a fixed-point iteration of the second primal variable and the dual variable, and Anderson acceleration can be directly applied. Additionally, when the problem has a separable target function and satisfies certain conditions, ADMM becomes a fixed-point iteration of only one variable, which further reduces the computational overhead of Anderson acceleration. Moreover, we analyze a particular non-convex problem structure that is common in computer graphics, and prove the convergence of ADMM on such problems under mild assumptions. We apply our acceleration technique on a variety of optimization problems in computer graphics, with notable improvement on their convergence speed.	https://dl.acm.org/doi/abs/10.1145/3355089.3356491	Juyong Zhang, Yue Peng, Wenqing Ouyang, Bailin Deng
Acoustic texture rendering for extended sources in complex scenes	Extended stochastic sources, like falling rain or a flowing waterway, provide an immersive ambience in virtual environments. In complex scenes, the rendered sound should vary naturally with listener position, differing not only in overall loudness but also in texture, to capture the indistinct murmur of a faraway brook versus the bright babbling of one up close. Modeling an ambient sound as a collection of random events such as individual raindrop impacts or water bubble oscillations, this variation can be seen as a change in the statistical distribution of events heard by the listener: the arrival rate of nearby, louder events relative to more distant or occluded, quieter ones. Reverberation and edge diffraction from scene geometry multiply and mix events more extensively compared to an empty scene and introduce salient spatial variation in texture. We formalize the notion of acoustic texture by introducing the (ELD), which relates the rapidity of received events to their loudness. To model spatial variation in texture, the ELD is made a function of listener location in the scene. We show that this ELD field can be extracted from a single wave simulation for each extended source and rendered flexibly using a granular synthesis pipeline, with grains derived procedurally or from recordings. Our system yields believable, realtime changes in acoustic texture as the listener moves, driven by sound propagation in the scene.	https://dl.acm.org/doi/abs/10.1145/3355089.3356566	Zechen Zhang, Nikunj Raghuvanshi, John Snyder, Steve Marschner
Adversarial Monte Carlo denoising with conditioned auxiliary feature modulation	Denoising Monte Carlo rendering with a very low sample rate remains a major challenge in the photo-realistic rendering research. Many previous works, including regression-based and learning-based methods, have been explored to achieve better rendering quality with less computational cost. However, most of these methods rely on handcrafted optimization objectives, which lead to artifacts such as blurs and unfaithful details. In this paper, we present an adversarial approach for denoising Monte Carlo rendering. Our key insight is that generative adversarial networks can help denoiser networks to produce more realistic high-frequency details and global illumination by learning the distribution from a set of high-quality Monte Carlo path tracing images. We also adapt a novel feature modulation method to utilize auxiliary features better, including normal, albedo and depth. Compared to previous state-of-the-art methods, our approach produces a better reconstruction of the Monte Carlo integral from a few samples, performs more robustly at different sample rates, and takes only a second for megapixel images.	https://dl.acm.org/doi/abs/10.1145/3355089.3356547	Bing Xu, Junfei Zhang, Rui Wang, Kun Xu, Yong-Liang Yang, Chuan Li, Rui Tang
An integrated 6DoF video camera and system design	Designing a fully integrated 360° video camera supporting 6DoF head motion parallax requires overcoming many technical hurdles, including camera placement, optical design, sensor resolution, system calibration, real-time video capture, depth reconstruction, and real-time novel view synthesis. While there is a large body of work describing various system components, such as multi-view depth estimation, our paper is the first to describe a complete, reproducible system that considers the challenges arising when designing, building, and deploying a full end-to-end 6DoF video camera and playback environment. Our system includes a computational imaging software pipeline supporting online markerless calibration, high-quality reconstruction, and real-time streaming and rendering. Most of our exposition is based on a professional 16-camera configuration, which will be commercially available to film producers. However, our software pipeline is generic and can handle a variety of camera geometries and configurations. The entire calibration and reconstruction software pipeline along with example datasets is open sourced to encourage follow-up research in high-quality 6DoF video reconstruction and rendering .	https://dl.acm.org/doi/abs/10.1145/3355089.3356555	Albert Parra Pozo, Michael Toksvig, Terry Filiba Schrager, Joyce Hsu, Uday Mathur, Alexander Sorkine-Hornung, Rick Szeliski, Brian Cabral
An interactive introduction to WEBGL	Explosion of interest in 3D graphics through a browser makes use of local hardware little loss of performance no platform dependence Answer three questions Which API should I use? What do I need to know? How do I get started?	https://dl.acm.org/doi/abs/10.1145/3355047.3359420	ED Angel, Dave Shreiner
Animating landscape: self-supervised learning of decoupled motion and appearance for single-image video synthesis	Automatic generation of a high-quality video from a single image remains a challenging task despite the recent advances in deep generative models. This paper proposes a method that can create a high-resolution, long-term animation using convolutional neural networks (CNNs) from a single landscape image where we mainly focus on skies and waters. Our key observation is that the (e.g., moving clouds) and (e.g., time-varying colors in the sky) in natural scenes have different time scales. We thus learn them separately and predict them with decoupled control while handling future uncertainty in both predictions by introducing latent codes. Unlike previous methods that infer output frames directly, our CNNs predict spatially-smooth intermediate data, i.e., for motion, flow fields for warping, and for appearance, color transfer maps, via self-supervised learning, i.e., without explicitly-provided ground truth. These intermediate data are applied not to each previous output frame, but to the input image only once for each output frame. This design is crucial to alleviate error accumulation in long-term predictions, which is the essential problem in previous recurrent approaches. The output frames can be looped like cinemagraph, and also be controlled directly by specifying latent codes or indirectly via visual annotations. We demonstrate the effectiveness of our method through comparisons with the state-of-the-arts on video prediction as well as appearance manipulation. Resultant videos, codes, and datasets will be available at http://www.cgg.cs.tsukuba.ac.jp/~endo/projects/AnimatingLandscape.	https://dl.acm.org/doi/abs/10.1145/3355089.3356523	Yuki Endo, Yoshihiro Kanamori, Shigeru Kuriyama
Artistic glyph image synthesis via one-stage few-shot learning	Automatic generation of artistic glyph images is a challenging task that attracts many research interests. Previous methods either are specifically designed for shape synthesis or focus on texture transfer. In this paper, we propose a novel model, AGIS-Net, to transfer both shape and texture styles in one-stage with only a few stylized samples. To achieve this goal, we first disentangle the representations for content and style by using two encoders, ensuring the multi-content and multi-style generation. Then we utilize two collaboratively working decoders to generate the glyph shape image and its texture image simultaneously. In addition, we introduce a local texture refinement loss to further improve the quality of the synthesized textures. In this manner, our one-stage model is much more efficient and effective than other multi-stage stacked methods. We also propose a large-scale dataset with Chinese glyph images in various shape and texture styles, rendered from 35 professional-designed artistic fonts with 7,326 characters and 2,460 synthetic artistic fonts with 639 characters, to validate the effectiveness and extendability of our method. Extensive experiments on both English and Chinese artistic glyph image datasets demonstrate the superiority of our model in generating high-quality stylized glyph images against other state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3355089.3356574	Yue Gao, Yuan Guo, Zhouhui Lian, Yingmin Tang, Jianguo Xiao
Artistic license in heritage visualisation: VR Sydney cove circa 1800	Heritage visualisations are works of the cultural imaginary and this paper examines the artwork which foregrounds the interpretive nature of heritage visualisation. It is a re-imagining in virtual reality of a contemporaneous print of Sydney Cove. Existing in the liminal space between accuracy and authenticity it is both art object and heritage visualisation. The dual nature of this work supports engagement with wider audiences, fostering and broadening debate at individual, institutional, academic and societal levels about the nature and role of heritage.	https://dl.acm.org/authorize?N690054	Kit Devine
Automatically translating image processing libraries to halide	This paper presents Dexter, a new tool that automatically translates image processing functions from a low-level general-purpose language to a high-level domain-specific language (DSL), allowing them to leverage cross-platform optimizations enabled by DSLs. Rather than building a classical syntax-driven compiler to do this translation, Dexter leverages recent advances in program synthesis and program verification, along with a new domain-specific synthesis algorithm, to translate C++ image processing code to the Halide DSL, while guaranteeing semantic equivalence. This new synthesis algorithm scales and generalizes to much larger and more complex functions than prior work, including the ability to handle tiling, conditionals, and multi-stage pipelines in the original low-level code. To demonstrate the effectiveness of our approach, we evaluate Dexter using real-world image processing functions from Adobe Photoshop, a widely used multi-platform image processing program. Our results show that Dexter can translate 264 out of 353 functions in our test set, with the original implementations ranging from 20 to 150 lines of code. By leveraging Halide's advanced auto-scheduling capabilities, we get median speedups of 7.03× and 4.52× for Dexter-translated functions as compared to the original implementations on Intel and ARM architectures, respectively.	https://dl.acm.org/doi/abs/10.1145/3355089.3356549	Maaz Bin Safeer Ahmad, Jonathan Ragan-Kelley, Alvin Cheung, Shoaib Kamil
Best friend	"In a near future, a lonely man is addicted to a product called ""Best Friend"" which offers him perfect virtual friends."	https://dl.acm.org/authorize?N690910	Nicholas Olivieri, Yi Shen, Juliana De Lucca, Varun Nair, David Feliu
Biomimetic eye modeling & deep neuromuscular oculomotor control	"We present a novel, biomimetic model of the eye for realistic virtual human animation. We also introduce a deep learning approach to oculomotor control that is compatible with our biomechanical eye model. Our eye model consists of the following functional components: (i) submodels of the 6 extraocular muscles that actuate realistic eye movements, (ii) an iris submodel, actuated by pupillary muscles, that accommodates to incoming light intensity, (iii) a corneal submodel and a deformable, ciliary-muscle-actuated lens submodel, which refract incoming light rays for focal accommodation, and (iv) a retina with a multitude of photoreceptors arranged in a biomimetic, foveated distribution. The light intensity captured by the photoreceptors is computed using ray tracing from the photoreceptor positions through the finite aperture pupil into the 3D virtual environment, and the visual information from the retina is output via an optic nerve vector. Our oculomotor control system includes a foveation controller implemented as a locally-connected, irregular Deep Neural Network (DNN), or ""LiNet"", that conforms to the nonuniform retinal photoreceptor distribution, and a neuromuscular motor controller implemented as a fully-connected DNN, plus auxiliary Shallow Neural Networks (SNNs) that control the accommodation of the pupil and lens. The DNNs are trained offline through deep learning from data synthesized by the eye model itself. Once trained, the oculomotor control system operates robustly and efficiently online. It innervates the intraocular muscles to perform illumination and focal accommodation and the extraocular muscles to produce natural eye movements in order to foveate and pursue moving visual targets. We additionally demonstrate the operation of our eye model (binocularly) within our recently introduced sensorimotor control framework involving an anatomically-accurate biomechanical human musculoskeletal model."	https://dl.acm.org/doi/abs/10.1145/3355089.3356558	Masaki Nakada, Arjun Lakshmipathy, Honglin Chen, Nina Ling, Tao Zhou, Demetri Terzopoulos
Birth of planet earth fulldome excerpt: photosynthesis in a chromatophore	This visualization of the quantum-mechanical process of photosynthesis involved combining structural models from atomic, protein, organelle, and cell scales. We descend into a prehistoric hot spring, revealing primitive bacteria which use an early form of photosynthesis to turn sunlight into chemical energy in structures called chromatophores. This fulldome animation follows the energy through progressively more stable forms: a captured photon, electronic excitation, pumped protons, and finally, synthesis of ATP molecules. The team used VMD, a GPU-accelerated molecular visualization tool, to create geometric abstractions from a 5-million-atom static snapshot of the scientific research model. To achieve an organic flight path through the 3D data, they adapted their Virtual Director camera choreography software to interface with VMD. In Houdini, they choreographed particle effects which were then exported to VMD for rendering of the hero chromatophore. Nine more layers were rendered in Houdini's Mantra, including 500,000 instanced chromatophores and particle effects, 300 high-resolution cell membranes, and depth passes to control luminance and depth effects. The visualization treatments from Houdini and VMD were combined in Nuke for a seamless flight through this dynamic microscopic light show.	https://dl.acm.org/authorize?N690913	Donna Cox, Robert Patterson, Stuart Levy, AJ Christensen, Kalina Borkiewicz, Jeff Carpenter, Melih Sener, John Stone, Barry Isralewitz, Donna Cox, Robert Patterson
Birth of planet earth fulldome excerpt: the collision that formed the moon	"This visualization of a scientific simulation shows the giant impact that formed the Moon. 4.5 billion years ago, the early Earth collided with a Mars-sized rock called ""Theia"", resulting in a spinning disk of lava and vaporized rock hotter than the surface of the sun. In this sequence, we see the first 24 hours of the 100-year process of our Moon's formation. This 1.1 million-point smoothed particle hydrodynamics model was 32 gigabytes and featured 492 data snapshots, which were interpolated by a factor of ten to play back smoothly on a 4K fulldome cinema screen. Using Houdini, special sprites used instanced spheres with a procedural falloff shader to face the hemispherical camera lens, to prevent intersection artifacts as particles evolved from solid to liquid to gas, and to avoid camera intersection. Other particles were surfaced as solids and liquids, and dynamic geometry lights allowed the emissive surfaces to illuminate diffuse surfaces. Processing and rendering a million translucent sprites was prohibitive on the AVL's 30-machine cluster, so the team used their custom Blurend pipeline to process imagery on the Blue Waters supercomputer. The shot was completed in Nuke, adding a Milky Way background and grading the mixing planet-forming material."	https://dl.acm.org/authorize?N690921	Donna Cox, Robert Patterson, AJ Christensen, Kalina Borkiewicz, Stuart Levy, Jeff Carpenter, Donna Cox, Robert Patterson
Blind image super-resolution with spatially variant degradations	Existing deep learning approaches to single image super-resolution have achieved impressive results but mostly assume a setting with fixed pairs of high resolution and low resolution images. However, to robustly address realistic upscaling scenarios where the relation between high resolution and low resolution images is unknown, blind image super-resolution is required. To this end, we propose a solution that relies on three components: First, we use a degradation aware SR network to synthesize the HR image given a low resolution image and the corresponding blur kernel. Second, we train a kernel discriminator to analyze the generated high resolution image in order to predict errors present due to providing an incorrect blur kernel to the generator. Finally, we present an optimization procedure that is able to recover both the degradation kernel and the high resolution image by minimizing the error predicted by our kernel discriminator. We also show how to extend our approach to spatially variant degradations that typically arise in visual effects pipelines when compositing content from different sources and how to enable both local and global user interaction in the upscaling process.	https://dl.acm.org/doi/abs/10.1145/3355089.3356575	Victor Cornillère, Abdelaziz Djelouah, Wang Yifan, Olga Sorkine-Hornung, Christopher Schroers
Bound	Two young brothers break their relationship out of pride.	https://dl.acm.org/authorize?N690946	Lesego Vorster, Pierre-Yves Vauzelle, Diego Torres, Venkatram Viswanathan, Kirill Blumenkrants
Bounded distortion tetrahedral metric interpolation	We present a method for volumetric shape interpolation with unique shape preserving features. The input to our algorithm are two or more 3-manifolds, immersed into R and discretized as tetrahedral meshes with shared connectivity. The output is a continuum of shapes that naturally blends the input shapes, while striving to preserve the geometric character of the input. The basis of our approach relies on the fact that the space of metrics with bounded isometric and angular distortion is convex [Chien et al. 2016b]. We show that for high dimensional manifolds, the bounded distortion metrics form a positive semidefinite cone product space. Our method can be seen as a generalization of the bounded distortion interpolation technique of [Chen et al. 2013] from planar shapes immersed in R to solids in R . The convexity of the space implies that a linear blend of the (squared) edge lengths of the input tetrahedral meshes is a simple yet powerful-and-natural choice. Linearly blending flat metrics results in a new metric which is, in general, not flat, and cannot be immersed into three-dimensional space. Nonetheless, the amount of curvature that is introduced in the process tends to be very low in practical settings. We further design an extremely robust nonconvex optimization procedure that efficiently flattens the metric. The flattening procedure strives to preserve the low distortion exhibited in the blended metric while guaranteeing the validity of the metric, resulting in a locally injective map with bounded distortion. Our method leads to volumetric interpolation with superb quality, demonstrating significant improvement over the state-of-the-art and qualitative properties which were obtained so far only in interpolating manifolds of lower dimensions.	https://dl.acm.org/doi/abs/10.1145/3355089.3356569	Ido Aharon, Renjie Chen, Denis Zorin, Ofir Weber
Bridging knowledge between craftsman and learner in Chinese intangible cultural heritage through WebAR	The purpose of this paper is to explore new perspectives to learning Intangible Cultural Heritage (ICH) through embodied interaction with focus on learning and experience with traditional Cantonese Porcelain crafting. This research developed a WebAR application where various processes are presented through the tangible interaction of virtual porcelain represented by physical objects. The learner is able to directly interact with the plate that bridges the tangible materials and making processes of ICH utilizing WebAR. Empirical studies found that the WebAR and embodied interaction can enhance student's tangible learning experience to transfer knowledge between craftsman and student.	https://dl.acm.org/authorize?N690068	Peng Tan, Yi Ji, Damian Hills, Tieming Fu
BxDF material acquisition, representation, and rendering for VR and design	Photorealistic and physically-based rendering of real-world environments with high fidelity materials is important to a range of applications, including special effects, architectural modelling, cultural heritage, computer games, automotive design, and virtual reality (VR). Our perception of the world depends on lighting and surface material characteristics, which determine how the light is reflected, scattered, and absorbed. In order to reproduce appearance, we must therefore understand all the ways objects interact with light, and the acquisition and representation of materials has thus been an important part of computer graphics from early days. Nevertheless, no material model nor acquisition setup is without limitations in terms of the variety of materials represented, and different approaches vary widely in terms of compatibility and ease of use. In this course, we describe the state of the art in material appearance acquisition and modelling, ranging from mathematical BSDFs to data-driven capture and representation of anisotropic materials, and volumetric/thread models for patterned fabrics. We further address the problem of material appearance constancy across different rendering platforms. We present two case studies in architectural and interior design. The first study demonstrates Yulio, a new platform for the creation, delivery, and visualization of acquired material models and reverse engineered cloth models in immersive VR experiences. The second study shows an end-to-end process of capture and data-driven BSDF representation using the physically-based Radiance system for lighting simulation and rendering.	https://dl.acm.org/doi/abs/10.1145/3355047.3362092	Giuseppe Claudio Guarnera, Dar'ya Guarnera, Gregory J. Ward, Mashhuda Glencross, Ian Hall
Carpentry compiler	Traditional manufacturing workflows strongly decouple design and fabrication phases. As a result, fabrication-related objectives such as manufacturing time and precision are difficult to optimize in the design space, and vice versa. This paper presents HL-HELM, a high-level, domain-specific language for expressing abstract, parametric fabrication plans; it also introduces LL-HELM, a low-level language for expressing concrete fabrication plans that take into account the physical constraints of available manufacturing processes. We present a new compiler that supports the real-time, unoptimized translation of high-level, geometric fabrication operations into concrete, tool-specific fabrication instructions; this gives users immediate feedback on the physical feasibility of plans as they design them. HELM offers novel optimizations to improve accuracy and reduce fabrication time as well as material costs. Finally, optimized low-level plans can be interpreted as step-by-step instructions for users to actually fabricate a physical product. We provide a variety of example fabrication plans in the carpentry domain that are designed using our high-level language, show how the compiler translates and optimizes these plans to generate concrete low-level instructions, and present the final physical products fabricated in wood.	https://dl.acm.org/doi/abs/10.1145/3355089.3356518	Chenming Wu, Haisen Zhao, Chandrakana Nandi, Jeffrey I. Lipton, Zachary Tatlock, Adriana Schulz
CharActor	The personality of living things is beautiful. Is it possible to have a personality in the digital world? CharActor is a video work that produces animation by recognizing shader programs as genes and by changing the mathematical expression itself using evolutionary computation.	https://dl.acm.org/authorize?N690061	Masaru Mizuochi
Chebyshev nets from commuting PolyVector fields	We propose a method for computing global Chebyshev nets on triangular meshes. We formulate the corresponding global parameterization problem in terms of PolyVector fields, and design an efficient optimization method to solve it. We compute, for the first time, Chebyshev nets with automatically-placed singularities, and demonstrate the realizability of our approach using real material.	https://dl.acm.org/doi/abs/10.1145/3355089.3356564	Andrew O. Sageman-Furnas, Albert Chern, Mirela Ben-Chen, Amir Vaxman
Checkerboard patterns with black rectangles	Checkerboard patterns with black rectangles can be derived from quad meshes with orthogonal diagonals. First, we present an initial theoretical analysis of these quad meshes. The analysis reveals many possible applications in geometry processing and also motivates the numerical optimization for aesthetic and functional checkerboard pattern design. Second, we describe an optimization algorithm that transforms initial 2D and 3D quad meshes into quad meshes with orthogonal diagonals. Third, we present a 2D checkerboard pattern design framework based on integer programming inspired by the logo design of the 2020 Olympic games. Our results show a variety of 2D and 3D checkerboard patterns that can be derived from 2D or 3D quad meshes with orthogonal diagonals.	https://dl.acm.org/doi/abs/10.1145/3355089.3356514	Chi-Han Peng, Caigui Jiang, Peter Wonka, Helmut Pottmann
Cinematic scientific visualization: the art of communicating science	"The Advanced Visualization Lab (AVL) is part of the the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign. The AVL is led by Professor Donna Cox, who coined the term ""Renaissance Team"", with the belief that bringing together specialists of diverse backgrounds creates a team that is greater than the sum of its parts, and members of the AVL team reflect that in our interdisciplinarity. We specialize in creating high-quality cinematic scientific visualizations of supercomputer simulations for public outreach."	https://dl.acm.org/doi/abs/10.1145/3355047.3359398	Kalina Borkiewicz, AJ Christensen, Drew Berry, Christopher Fluke, Greg Shirah, Kel Elkins
Colorblind-shareable videos by synthesizing temporal-coherent polynomial coefficients	To share the same visual content between color vision deficiencies (CVD) and normal-vision people, attempts have been made to allocate the two visual experiences of a binocular display (wearing and not wearing glasses) to CVD and normal-vision audiences. However, existing approaches only work for still images. Although state-of-the-art temporal filtering techniques can be applied to smooth the per-frame generated content, they may fail to maintain the multiple binocular constraints needed in our applications, and even worse, sometimes introduce color inconsistency (same color regions map to different colors). In this paper, we propose to train a neural network to predict the temporal coherent polynomial coefficients in the domain of global color decomposition. This indirect formulation solves the color inconsistency problem. Our key challenge is to design a neural network to predict the temporal coherent coefficients, while maintaining all required binocular constraints. Our method is evaluated on various videos and all metrics confirm that it outperforms all existing solutions.	https://dl.acm.org/doi/abs/10.1145/3355089.3356534	Hu Xinghong, Liu Xueting, Zhang Zhuming, Xia Menghan, Li Chengze, Wong Tien-Tsin
Come to the Table! Haere mai ki te tēpu!	Come to the Table! explores whether extended realities (XR) can create a bridge between indigenous people (Māori), descendants from European settlers (Pākehā) and people from other ethnicities, by practicing social inclusion. The experience uses real time depth sensing technology and AR/VR displays to enable participants to view and be part of tabletop conversations with people from different cultural backgrounds, in a playful, explorative and powerful way.	https://dl.acm.org/doi/abs/10.1145/3355355.3361898	Mairi Gunn, Huidong Bai, Prasanth Sasikumar
Comic-guided speech synthesis	We introduce a novel approach for synthesizing realistic speeches for comics. Using a comic page as input, our approach synthesizes speeches for each comic character following the reading flow. It adopts a cascading strategy to synthesize speeches in two stages: Comic Visual Analysis and Comic Speech Synthesis. In the first stage, the input comic page is analyzed to identify the gender and age of the characters, as well as texts each character speaks and corresponding emotion. Guided by this analysis, in the second stage, our approach synthesizes realistic speeches for each character, which are consistent with the visual observations. Our experiments show that the proposed approach can synthesize realistic and lively speeches for different types of comics. Perceptual studies performed on the synthesis results of multiple sample comics validate the efficacy of our approach.	https://dl.acm.org/doi/abs/10.1145/3355089.3356487	Yujia Wang, Wenguan Wang, Wei Liang, Lap-Fai Yu
Consistent shepard interpolation for SPH-based fluid animation	We present a novel technique to correct errors introduced by the discretization of a fluid body when animating it with smoothed particle hydrodynamics (SPH). Our approach is based on the Shepard correction, which reduces the interpolation errors from irregularly spaced data. With Shepard correction, the smoothing kernel function is normalized using the weighted sum of the kernel function values in the neighborhood. To compute the correction factor, densities of neighboring particles are needed, which themselves are computed with the uncorrected kernel. This results in an inconsistent formulation and an error-prone correction of the kernel. As a consequence, the density computation may be inaccurate, thus the pressure forces are erroneous and may cause instabilities in the simulation process. We present a consistent formulation by using the corrected densities to compute the exact kernel correction factor and, thereby, increase the accuracy of the simulation. Employing our method, a smooth density distribution is achieved, i.e., the noise in the density field is reduced by orders of magnitude. To show that our method is independent of the SPH variant, we evaluate our technique on weakly compressible SPH and on divergence-free SPH. Incorporating the corrected density into the correction process, the problem cannot be stated explicitly anymore. We propose an efficient and easy-to-implement algorithm to solve the implicit problem by applying the power method. Additionally, we demonstrate how our model can be applied to improve the density distribution on rigid bodies when using a well-known rigid-fluid coupling approach.	https://dl.acm.org/doi/abs/10.1145/3355089.3356503	Stefan Reinhardt, Tim Krake, Bernhard Eberhardt, Daniel Weiskopf
Contact/sense	When we feel and sense through machines, are we still ourselves? In a mixed reality where embodied actions and blinding visions are part woman/part machine, the tactile surface of plants is a portal that conjures augmented materialities into existence.	https://dl.acm.org/authorize?N690062	Rewa Wright, Simon Howden
Creating and understanding 3D annotated scene meshes	Deep learning requires availability of massive 3D data. How to acquire 3D scenes efficiently?	https://dl.acm.org/doi/abs/10.1145/3355047.3359426	Due Thanh Nguyen, Quang-Hieu Pham, Binh-Son Hua
Cubic stylization	We present a 3D stylization algorithm that can turn an input shape into the style of a cube while maintaining the of the original shape. The key insight is that cubic style sculptures can be captured by the energy with an ℓ -regularization on rotated surface normals. Minimizing this energy naturally leads to a detail-preserving, cubic geometry. Our optimization can be solved efficiently without any mesh surgery. Our method serves as a non-realistic modeling tool where one can incorporate many artistic controls to create stylized geometries.	https://dl.acm.org/doi/abs/10.1145/3355089.3356495	Hsueh-Ti Derek liu, Alec Jacobson
Curve-pleated structures	In this paper we study pleated structures generated by folding paper along curved creases. We discuss their properties and the special case of principal pleated structures. A discrete version of pleated structures is particularly interesting because of the rich geometric properties of the principal case, where we are able to establish a series of analogies between the smooth and discrete situations, as well as several equivalent characterizations of the principal property. These include being a conical mesh, and being flat-foldable. This structure-preserving discretization is the basis of computation and design. We propose a new method for designing pleated structures and reconstructing reference shapes as pleated structures: we first gain an overview of possible crease patterns by establishing a connection to pseudogeodesics, and then initialize and optimize a quad mesh so as to become a discrete pleated structure. We conclude by showing applications in design and reconstruction, including cases with combinatorial singularities. Our work is relevant to fabrication in so far as the offset properties of principal pleated structures allow us to construct curved sculptures of finite thickness.	https://dl.acm.org/doi/abs/10.1145/3355089.3356540	Caigui Jiang, Klara Mundilova, Florian Rist, Johannes Wallner, Helmut Pottmann
DReCon: data-driven responsive control of physics-based characters	Interactive control of self-balancing, physically simulated humanoids is a long standing problem in the field of real-time character animation. While physical simulation guarantees realistic interactions in the virtual world, simulated characters can appear unnatural if they perform unusual movements in order to maintain balance. Therefore, obtaining a high level of responsiveness to user control, runtime performance, and diversity has often been overlooked in exchange for motion quality. Recent work in the field of deep reinforcement learning has shown that training physically simulated characters to follow motion capture clips can yield high quality tracking results. We propose a two-step approach for building responsive simulated character controllers from unstructured motion capture data. First, meaningful features from the data such as movement direction, heading direction, speed, and locomotion style, are interactively specified and drive a kinematic character controller implemented using motion matching. Second, reinforcement learning is used to train a simulated character controller that is general enough to track the entire distribution of motion that can be generated by the kinematic controller. Our design emphasizes responsiveness to user input, visual quality, and low runtime cost for application in video-games.	https://dl.acm.org/doi/abs/10.1145/3355089.3356536	Kevin Bergamin, Simon Clavet, Daniel Holden, James Richard Forbes
Data-driven interior plan generation for residential buildings	We propose a novel data-driven technique for automatically and efficiently generating floor plans for residential buildings with given boundaries. Central to this method is a two-stage approach that imitates the human design process by locating rooms first and then walls while adapting to the input building boundary. Based on observations of the presence of the living room in almost all floor plans, our designed learning network begins with positioning a living room and continues by iteratively generating other rooms. Then, walls are first determined by an encoder-decoder network, and then they are refined to vector representations using dedicated rules. To effectively train our networks, we construct - a manually collected large-scale densely annotated dataset of floor plans from real residential buildings. Intensive experiments, including formative user studies and comparisons, are conducted to illustrate the feasibility and efficacy of our proposed approach. By comparing the plausibility of different floor plans, we have observed that our method substantially outperforms existing methods, and in many cases our floor plans are comparable to human-created ones.	https://dl.acm.org/doi/abs/10.1145/3355089.3356556	Wenming Wu, Xiao-Ming Fu, Rui Tang, Yuhan Wang, Yu-Hao Qi, Ligang Liu
Dataflow programming and processing for artists and beyond	We complement the last three editions of the course at SIGGRAPH Asia (2015, 2016, 2018) and SIGGRAPH (2017) to make it more of a hands-on nature and include OpenISS. We explore a rapid prototyping of interactive graphical applications for stage and beyond using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with various devices. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the (ISS) v2 and its OpenISS core framework for creative near-realtime broadcasting, and the use of AI and HCI techniques in art.	https://dl.acm.org/doi/abs/10.1145/3355047.3359423	Serguei A. Mokhov, Miao Song, Sudhir P. Mudur, Peter Grogono
Deep face normalization	From angling smiles to duck faces, all kinds of facial expressions can be seen in selfies, portraits, and Internet pictures. These photos are taken from various camera types, and under a vast range of angles and lighting conditions. We present a deep learning framework that can fully normalize unconstrained face images, i.e., remove perspective distortions, relight to an evenly lit environment, and predict a frontal and neutral face. Our method can produce a high resolution image while preserving important facial details and the likeness of the subject, along with the original background. We divide this ill-posed problem into three consecutive normalization steps, each using a different generative adversarial network that acts as an image generator. Perspective distortion removal is performed using a dense flow field predictor. A uniformly illuminated face is obtained using a lighting translation network, and the facial expression is neutralized using a generalized facial expression synthesis framework combined with a regression network based on deep features for facial recognition. We introduce new data representations for conditional inference, as well as training methods for supervised learning to ensure that different expressions of the same person can yield to not only a plausible but also a similar neutral face. We demonstrate our results on a wide range of challenging images collected in the wild. Key applications of our method range from robust image-based 3D avatar creation, portrait manipulation, to facial enhancement and reconstruction tasks for crime investigation. We also found through an extensive user study, that our normalization results can be hardly distinguished from ground truth ones if the person is not familiar.	https://dl.acm.org/doi/abs/10.1145/3355089.3356568	Koki Nagano, Huiwen Luo, Zejian Wang, Jaewoo Seo, Jun Xing, Liwen Hu, Lingyu Wei, Hao Li
Deep learning: a crash course	The topics for Part 1 of 4.	https://dl.acm.org/doi/abs/10.1145/3355047.3359415	Andrew Glassner
Deep point correlation design	Designing point patterns with desired properties can require substantial effort, both in hand-crafting coding and mathematical derivation. Retaining these properties in multiple dimensions or for a substantial number of points can be challenging and computationally expensive. Tackling those two issues, we suggest to automatically generate scalable point patterns from design goals using deep learning. We phrase pattern generation as a deep composition of weighted distance-based unstructured filters. Deep point pattern design means to optimize over the space of all such compositions according to a user-provided , a small program which measures a pattern's fidelity in respect to its spatial or spectral statistics, linear or non-linear (e. g., radial) projections, or any arbitrary combination thereof. Our analysis shows that we can emulate a large set of existing patterns (blue, green, step, projective, stair, etc.-noise), generalize them to countless new combinations in a systematic way and leverage existing error estimation formulations to generate novel point patterns for a user-provided class of integrand functions. Our point patterns scale favorably to multiple dimensions and numbers of points: we demonstrate nearly 10k points in 10-D produced in one second on one GPU. All the resources (source code and the pre-trained networks) can be found at https://sampling.mpi-inf.mpg.de/deepsampling.html.	https://dl.acm.org/doi/abs/10.1145/3355089.3356562	Thomas Leimkühler, Gurprit Singh, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel
DeepFovea: neural reconstruction for foveated rendering and video compression using learned statistics of natural videos	In order to provide an immersive visual experience, modern displays require head mounting, high image resolution, low latency, as well as high refresh rate. This poses a challenging computational problem. On the other hand, the human visual system can consume only a tiny fraction of this video stream due to the drastic acuity loss in the peripheral vision. Foveated rendering and compression can save computations by reducing the image quality in the peripheral vision. However, this can cause noticeable artifacts in the periphery, or, if done conservatively, would provide only modest savings. In this work, we explore a novel foveated reconstruction method that employs the recent advances in generative adversarial neural networks. We reconstruct a plausible peripheral video from a small fraction of pixels provided every frame. The reconstruction is done by finding the closest matching video to this sparse input stream of pixels on the learned manifold of natural videos. Our method is more efficient than the state-of-the-art foveated rendering, while providing the visual experience with no noticeable quality degradation. We conducted a user study to validate our reconstruction method and compare it against existing foveated rendering and video compression techniques. Our method is fast enough to drive gaze-contingent head-mounted displays in real time on modern hardware. We plan to publish the trained network to establish a new quality bar for foveated rendering and compression as well as encourage follow-up research.	https://dl.acm.org/doi/abs/10.1145/3355089.3356557	Anton S. Kaplanyan, Anton Sochenov, Thomas Leimkühler, Mikhail Okunev, Todd Goodall, Gizem Rufo
DeepRemaster: temporal source-reference attention networks for comprehensive video enhancement	The remastering of vintage film comprises of a diversity of sub-tasks including super-resolution, noise removal, and contrast enhancement which aim to restore the deteriorated film medium to its original state. Additionally, due to the technical limitations of the time, most vintage film is either recorded in black and white, or has low quality colors, for which colorization becomes necessary. In this work, we propose a single framework to tackle the entire remastering task semi-interactively. Our work is based on temporal convolutional neural networks with attention mechanisms trained on videos with data-driven deterioration simulation. Our proposed source-reference attention allows the model to handle an arbitrary number of reference color images to colorize long videos without the need for segmentation while maintaining temporal consistency. Quantitative analysis shows that our framework outperforms existing approaches, and that, in contrast to existing approaches, the performance of our framework increases with longer videos and more reference color images.	https://dl.acm.org/doi/abs/10.1145/3355089.3356570	Satoshi Iizuka, Edgar Simo-Serra
Deepness of the fry	What makes us special? What makes us stand out? What makes us unique? Nothing.	https://dl.acm.org/authorize?N690912	August Niclasen, Michelle Ann Nardone, Anja Perl
Deloitte: at the beginning of everything	World's largest consulting firm Deloitte has released the global campaign 'Make Your Impact' via Deloitte Digital. Deloitte invited Onesal to design and animate the brand film for the campaign. Our task was to illustrate new technologies and its impact in the world in an optimistic, inviting way, showing how decisions we make today, make tomorrow. Starting with a green dot, we did an abstract exploration of the evolution of the dot from different perspectives. Transforming, expanding, and creating new meaning, yet always returning the circular green dot to start over.	https://dl.acm.org/authorize?N690900	Nahuel Matias Salcedo
Design and structural optimization of topological interlocking assemblies	We study assemblies of convex rigid blocks regularly arranged to approximate a given freeform surface. Our designs rely solely on the geometric arrangement of blocks to form a stable assembly, neither requiring explicit connectors or complex joints, nor relying on friction between blocks. The convexity of the blocks simplifies fabrication, as they can be easily cut from different materials such as stone, wood, or foam. However, designing stable assemblies is challenging, since adjacent pairs of blocks are restricted in their relative motion only in the direction orthogonal to a single common planar interface surface. We show that despite this weak interaction, structurally stable, and in some cases, globally interlocking assemblies can be found for a variety of freeform designs. Our optimization algorithm is based on a theoretical link between static equilibrium conditions and a geometric, global interlocking property of the assembly---that an assembly is globally interlocking if and only if the equilibrium conditions are satisfied for arbitrary external forces and torques. Inspired by this connection, we define a measure of stability that spans from single-load equilibrium to global interlocking, motivated by tilt analysis experiments used in structural engineering. We use this measure to optimize the geometry of blocks to achieve a static equilibrium for a maximal cone of directions, as opposed to considering only self-load scenarios with a single gravity direction. In the limit, this optimization can achieve globally interlocking structures. We show how different geometric patterns give rise to a variety of design options and validate our results with physical prototypes.	https://dl.acm.org/doi/abs/10.1145/3355089.3356489	Ziqi Wang, Peng Song, Florin Isvoranu, Mark Pauly
Desire of rails: freedom	"""Desire of Rails"" is a film to demonstrate a concept of the conceptual art idea and the look of the art pieces in real world. In this movie, I set the pieces of art in the gallery space I designed. The handrails are awakened and have the desire to pass the limitation which is given to them to just existence for practical purpose. Through the AR devices, viewers can experience their childish dreams. First Dream - Roller Coaster Second Dream - Playing inside of a blanket in bedtime Third Dream - Revenge to the steps by jailing them Fourth Dream - Playing underwater Junho Kim added a feeling of dreamy liveliness to this film with his amazing sound work. Produced at Giantstep"	https://dl.acm.org/authorize?N690936	Ihsu Yoon, Junho Kim
DiCE: dichoptic contrast enhancement for VR and stereo displays	In stereoscopic displays, such as those used in VR/AR headsets, our eyes are presented with two different views. The disparity between the views is typically used to convey depth cues, but it could be also used to enhance image appearance. We devise a novel technique that takes advantage of binocular fusion to boost perceived local contrast and visual quality of images. Since the technique is based on fixed tone curves, it has negligible computational cost and it is well suited for real-time applications, such as VR rendering. To control the trade-off between contrast gain and binocular rivalry, we conduct a series of experiments to explain the factors that dominate rivalry perception in a dichoptic presentation where two images of different contrasts are displayed. With this new finding, we can effectively enhance contrast and control rivalry in mono- and stereoscopic images, and in VR rendering, as confirmed in validation experiments.	https://dl.acm.org/doi/abs/10.1145/3355089.3356552	Fangcheng Zhong, George Alex Koulieris, George Drettakis, Martin S. Banks, Mathieu Chambe, Frédo Durand, Rafał K. Mantiuk
Differentiable surface splatting for point-based geometry processing	We propose Differentiable Surface Splatting (DSS), a high-fidelity differentiable renderer for point clouds. Gradients for point locations and normals are carefully designed to handle discontinuities of the rendering function. Regularization terms are introduced to ensure uniform distribution of the points on the underlying surface. We demonstrate applications of DSS to inverse rendering for geometry synthesis and denoising, where large scale topological changes, as well as small scale detail modifications, are accurately and robustly handled without requiring explicit connectivity, outperforming state-of-the-art techniques. The data and code are at https://github.com/yifita/DSS.	https://dl.acm.org/doi/abs/10.1145/3355089.3356513	Wang Yifan, Felice Serena, Shihao Wu, Cengiz Öztireli, Olga Sorkine-Hornung
Discrete geodesic parallel coordinates	Geodesic parallel coordinates are orthogonal nets on surfaces where one of the two families of parameter lines are geodesic curves. We describe a discrete version of these special surface parameterizations and show that they are very useful for specific applications, most of which are related to the design and fabrication of surfaces in architecture. With the new discrete surface model, it is easy to control strip widths between neighboring geodesics. This facilitates tasks such as cladding a surface with strips of originally straight flat material or designing geodesic gridshells and timber rib shells. It is also possible to model nearly developable surfaces. These are characterized by geodesic strips with almost constant strip widths and are used for generating shapes that can be manufactured from materials which allow for some stretching or shrinking like felt, leather, or thin wooden boards. Most importantly, we show how to constrain the strip width parameters to model a class of intrinsically symmetric surfaces. These surfaces are isometric to surfaces of revolution and can be covered with doubly-curved panels that are produced with only a few molds when working with flexible materials like metal sheets.	https://dl.acm.org/doi/abs/10.1145/3355089.3356541	Hui Wang, Davide Pellis, Florian Rist, Helmut Pottmann, Christian Müller
Distortion-minimizing injective maps between surfaces	The problem of discrete surface parametrization, i.e. mapping a mesh to a planar domain, has been investigated extensively. We address the more general problem of mapping surfaces. In particular, we provide a formulation that yields a map between two disk-topology meshes, which is continuous and injective by construction and which locally minimizes intrinsic distortion. A common approach is to express such a map as the composition of two maps via a simple intermediate domain such as the plane, and to independently optimize the individual maps. However, even if both individual maps are of minimal distortion, there is potentially high distortion in the composed map. In contrast to many previous works, we minimize distortion in an end-to-end manner, directly optimizing the quality of the composed map. This setting poses additional challenges due to the discrete nature of both the source and the target domain. We propose a formulation that, despite the combinatorial aspects of the problem, allows for a purely continuous optimization. Further, our approach addresses the non-smooth nature of discrete distortion measures in this context which hinders straightforward application of off-the-shelf optimization techniques. We demonstrate that, despite the challenges inherent to the more involved setting, discrete surface-to-surface maps can be optimized effectively.	https://dl.acm.org/doi/abs/10.1145/3355089.3356519	Patrick Schmidt, Janis Born, Marcel Campen, Leif Kobbelt
Document rectification and illumination correction using a patch-based CNN	We propose a novel learning method to rectify document images with various distortion types from a single input image. As opposed to previous learning-based methods, our approach seeks to first learn the distortion flow on input image patches rather than the entire image. We then present a robust technique to stitch the patch results into the rectified document by processing in the gradient domain. Furthermore, we propose a second network to correct the uneven illumination, further improving the readability and OCR accuracy. Due to the less complex distortion present on the smaller image patches, our patch-based approach followed by stitching and illumination correction can significantly improve the overall accuracy in both the synthetic and real datasets.	https://dl.acm.org/doi/abs/10.1145/3355089.3356563	Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander
Dream clanger	'Dream Clanger' is a hybrid art/computer science project that re-imagines AFL Player GPS data and match video. Building on Baden Pailthorpe's 2017 major exhibition 'Clanger', this work pushes the envelope further by integrating machine learning.	https://dl.acm.org/authorize?N690063	Baden Pailthorpe
Dynamic hair modeling from monocular videos using deep neural networks	We introduce a deep learning based framework for modeling dynamic hairs from monocular videos, which could be captured by a commodity video camera or downloaded from Internet. The framework mainly consists of two neural networks, i.e., for inferring 3D spatial features of hair geometry from 2D image features, and for extracting temporal features of hair motions from video frames. The spatial features are represented as 3D occupancy fields depicting the hair volume shapes and 3D orientation fields indicating the hair growing directions. The temporal features are represented as bidirectional 3D warping fields, describing the forward and backward motions of hair strands cross adjacent frames. Both and are trained with synthetic hair data. The spatial and temporal features predicted by the networks are subsequently used for growing hair strands with both spatial and temporal consistency. Experiments demonstrate that our method is capable of constructing plausible dynamic hair models that closely resemble the input video, and compares favorably to previous single-view techniques.	https://dl.acm.org/doi/abs/10.1145/3355089.3356511	Lingchen Yang, Zefeng Shi, Youyi Zheng, Kun Zhou
Encounters: A Multiparticipant Audiovisual Art Experience with XR	"""Encounters"" provides a multiparticipant audiovisual art experience with a cross reality (XR) system which consists of HoloLens units, VIVE Tracker units, SteamVR system and so on. In this experience, participants fire virtual bullets or beams at physical objects which then create a physical sound and a corresponding virtual visual effect. This is done by placing a ""Kuroko"" unit, which consists of a VIVE Tracker unit, a Raspberry Pi unit and a solenoid, beside a physical object using the XR system. We prepared eight Kuroko units which participants can freely place anywhere in the physical space. Then, they would interact with physical objects in that space by making sounds using the XR system. We believe that through this multiparticipant experience, participants not only experience a new form of art expression using XR but also rethink their relationship with other participants or physical objects and the environment."	https://dl.acm.org/doi/abs/10.1145/3355355.3361886	Ryu Nakagawa, Ken Sonobe
Extended reality practice in art & design creative education	As Jerald (2018) states, though virtual reality (VR) has existed for over 50 years, its use as a creative medium is relatively new. In the last four years, as part of the 'second wave of VR', new affordability and accessibility of hardware and software for experiencing and creating VR has incited a surge of interest for the technology from creative industries. Meanwhile, interest and attempts to create VR projects has expanded into other forms of Extended Reality (XR) technologies, like Augmented Reality (AR) and Mixed Reality (MR). As a group of educators and practitioners from creative disciplines, our focus is to create a fundamentals of XR Education curriculum for undergraduates and/or postgraduates in schools of art and design who have no/less coding and software background. We believe in guiding students to approach VR as a creative medium is increasingly important. Furthermore, we also introduce the XR-ED Group (sponsored by ACM SIGGRAPH Educators Forum). This group is a collective of educators and practitioners interested in creating an XR curriculum, and to share the work of students. The group was first run as part of a co-located event with VRCAI 2018 / SIGGRAPH Asia 2018 in Tokyo, and this year will be running in Brisbane as a co-located event with VRCAI 2019 / SIGGRAPH Asia 2019.	https://dl.acm.org/doi/abs/10.1145/3355047.3359407	June Kim, Gregory Bennett, Snow Fu, Jan Kruse
Extraordinary accident: an immersive metaphor of Hong Kong	This paper presents Extraordinary Accident, an immersive experience exploring how different levels of abstraction can coexist and collaborate in the representation and recreation of urban space. Using Hong Kong as both inspiration and data source, the work attempts to liberate virtual reality compositions from their metaphorical ballast -that is, their recreational onus- and instead, with a temporal amalgamation of poetic representation at different scales, contribute to an alternative, potentially more intimate, understanding of the urban experience.	https://dl.acm.org/authorize?N690069	Tomas Laurenzo, Alejandro Rodríguez, Tatjana Kudinova
Extrusion-based ceramics printing with strictly-continuous deposition	We propose a method for integrated tool path planning and support structure generation tailored to the specific constraints of extrusion-based ceramics printing. Existing path generation methods for thermoplastic materials rely on transfer moves to navigate between different print paths in a given layer. However, when printing with clay, these transfer moves can lead to severe artifacts and failure. Our method eliminates transfer moves altogether by generating deposition paths that are continuous within and across layers. Our algorithm is implemented as a sequential top-down pass through the layer stack. In each layer, we detect points that require support, connect support points and model paths, and optimize the shape of the resulting continuous path with respect to length, smoothness, and distance to the model. For each of these subproblems, we propose dedicated solutions that take into account the fabrication constraints imposed by printable clay. We evaluate our method on a set of examples with multiple disconnected components and challenging support requirements. Comparisons to existing path generation methods designed for thermoplastic materials show that our method substantially improves print quality and often makes the difference between success and failure.	https://dl.acm.org/doi/abs/10.1145/3355089.3356509	Jean Hergel, Kevin Hinz, Sylvain Lefebvre, Bernhard Thomaszewski
Eye tracking and virtual reality	Virtual Reality has the potential to transform the way we work, rest and play. We are seeing use cases as diverse as education and pain management, with new applications being imagined every day. VR technology comes with new challenges, and many obstacles need to be overcome to ensure good user experience. Recently many new Virtual Reality systems with integrated eye tracking have become available. This course presents timely, relevant information on how Virtual Reality (VR) can leverage eye-tracking data to optimize the user experience and to alleviate usability issues surrounding many challenges in immersive VEs. The integration of eye tracking allows us to determine where the viewer is focusing their attention. If we, as the content creators and world builders, need the user to focus on another area of the VE we can use techniques to attract attention to these regions and also we can confirm we are doing so successful as we continually track the users gaze. Advancing these approaches could make the VR experience more comfortable, safe and effective for the user.	https://dl.acm.org/doi/abs/10.1145/3355047.3359424	Ann McNamara, Eakta Jain
FaceDrive: Facial Expression Driven Operation to Control Virtual Supernumerary Robotic Arms	Supernumerary Robotic Arms (SRAs) can make physical activities easier, but require cooperation with the operator. To improve cooperation, we predict the operator's intentions by using his/her Facial Expressions (FEs). We propose to map FEs to SRAs commands (e.g. grab, release). To measure FEs, we used a optical sensor-based approach (here inside a HMD), the sensors data are fed to a SVM classifying them in FEs. The SRAs can then carry out commands by predicting the operator's FEs (and arguably, the operator's intention). We made a Virtual reality Environment (VE) with SRAs and synchronizable avatar to investigate the most suitable mapping between FEs and SRAs. In SIGGRAPH Asia 2019, the user can manipulate virtual SRAs using his/her FEs.	https://dl.acm.org/doi/abs/10.1145/3355355.3361888	Masaaki Fukuoka, Adrien Verhulst, Fumihiko Nakamura, Ryo Takizawa, Katsutoshi Masai, Maki Sugimoto
Fluid carving: intelligent resizing for fluid simulation data	We present a method for intelligently resizing fluid simulation data using seam carving methods. While advances in post-processing techniques have allowed artists greater control over content late in the production process, this technology has largely remained confined to image processing. Our fluid carving system allows fluid simulation post-processing by performing content-aware non-uniform scaling on baked-out fluid simulation data. Specifically, we extend video seam carving techniques to 4-dimensional animated fluid volume data with a graph cut energy function based on mean curvature and kinetic energy. To reduce the complexity of performing graph cuts on 4D data, we provide a new graph construction formulation that greatly reduces the run-time and memory consumption, which are otherwise prohibitively expensive. We demonstrate that our system is useful for post-production fluid simulation changes and editable fluid FX libraries.	https://dl.acm.org/doi/abs/10.1145/3355089.3356572	Sean Flynn, Parris Egbert, Seth Holladay, Bryan Morse
Forglemmegei	When a lifelong friend departs, a stubborn old man has to face his inner fears in order to restore peace to his mind.	https://dl.acm.org/authorize?N690918	Katarina Lundquist, Michelle Ann Nardone, Anja Perl
FreeMo: Extending Hand Tracking Experiences Through Capture Volume and User Freedom	Camera-based hand tracking technologies can enable unprecedented interactive contents with gripping impressions of embodiment and immersion. However, current systems are often restricted by the tracking volume of their sensors which results in limited experiences and content creation possibilities. We present two demonstrations of FreeMo, a hand tracking device that extends the interaction space of a Leap Motion by augmenting it with self-actuating capabilities to chase after the user's palms. By doing so, we can offer content featuring significantly enhanced user freedom and immersion as showcased by our applications: an interactive room-scale VR experience with hand tracking from head to lap, and two-person competitive virtual air hockey game.	https://dl.acm.org/doi/abs/10.1145/3355355.3361882	Pascal Chiu, Isamu Endo, Kazuki Takashima, Kazuyuki Fujita, Yoshifumi Kitamura
GradNet: unsupervised deep screened poisson reconstruction for gradient-domain rendering	Monte Carlo (MC) methods for light transport simulation are flexible and general but typically suffer from high variance and slow convergence. Gradientdomain rendering alleviates this problem by additionally generating image gradients and reformulating rendering as a screened Poisson image reconstruction problem. To improve the quality and performance of the reconstruction, we propose a novel and practical deep learning based approach in this paper. The core of our approach is a multi-branch auto-encoder, termed GradNet, which end-to-end learns a mapping from a noisy input image and its corresponding image gradients to a high-quality image with low variance. Once trained, our network is fast to evaluate and does not require manual parameter tweaking. Due to the difficulty in preparing ground-truth images for training, we design and train our network in a completely unsupervised manner by learning directly from the input data. This is the first solution incorporating unsupervised deep learning into the gradient-domain rendering framework. The loss function is defined as an energy function including a data fidelity term and a gradient fidelity term. To further reduce the noise of the reconstructed image, the loss function is reinforced by adding a regularizer constructed from selected rendering-specific features. We demonstrate that our method improves the reconstruction quality for a diverse set of scenes, and reconstructing a high-resolution image takes far less than one second on a recent GPU.	https://dl.acm.org/doi/abs/10.1145/3355089.3356538	Jie Guo, Mengtian Li, Quewei Li, Yuting Qiang, Bingyang Hu, Yanwen Guo, Ling-Qi Yan
Gunpowder	It's teatime ! Unfortunately for Phileas all his teaboxes are empty ! He decides to go get some to the source, in China!	https://dl.acm.org/authorize?N690916	Romane Faure, Nathanael Perron, Léa Detrain, Benoît de Geyer d'Orth, Pei-Hsuan Lin, Anne-Lise Kubiak
Handheld mobile photography in very low light	"Taking photographs in low light using a mobile phone is challenging and rarely produces pleasing results. Aside from the physical limits imposed by read noise and photon shot noise, these cameras are typically handheld, have small apertures and sensors, use mass-produced analog electronics that cannot easily be cooled, and are commonly used to photograph subjects that move, like children and pets. In this paper we describe a system for capturing clean, sharp, colorful photographs in light as low as 0.3 lux, where human vision becomes monochromatic and indistinct. To permit handheld photography without flash illumination, we capture, align, and combine multiple frames. Our system employs ""motion metering"", which uses an estimate of motion magnitudes (whether due to handshake or moving objects) to identify the number of frames and the per-frame exposure times that together minimize both noise and motion blur in a captured burst. We combine these frames using robust alignment and merging techniques that are specialized for high-noise imagery. To ensure accurate colors in such low light, we employ a learning-based auto white balancing algorithm. To prevent the photographs from looking like they were shot in daylight, we use tone mapping techniques inspired by illusionistic painting: increasing contrast, crushing shadows to black, and surrounding the scene with darkness. All of these processes are performed using the limited computational resources of a mobile device. Our system can be used by novice photographers to produce shareable pictures in a few seconds based on a single shutter press, even in environments so dim that humans cannot see clearly."	https://dl.acm.org/doi/abs/10.1145/3355089.3356508	Orly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks, Tianfan Xue, Nikhil Karnad, Qiurui He, Jonathan T. Barron, Dillon Sharlet, Ryan Geiss, Samuel W. Hasinoff, Yael Pritch, Marc Levoy
Head Gaze Target Selection for Redirected Interaction	Haptic interaction in virtual reality poses an ongoing research challenge as to how touch sensations are delivered in applications. A simple solution is to have matching physical and virtual counterparts, i.e. a physical switch provides perfect haptic feedback for a virtual reality switch with matching geometry. Redirection illusions take this further allowing many virtual objects to be mapped to one physical object. In many systems that utilise redirection the interaction sequence is predetermined. This prevents users selecting their own target and a reset action is also required to provide an origin for the redirection. This paper overcomes these limitations with a novel application of head gaze to enable users to determine their own sequence of interactions with a remapped physical-virtual interface. We also introduce a technique for providing an optimal mapping between physical and virtual components using multiple physical target to remove the reset action.	https://dl.acm.org/doi/abs/10.1145/3355355.3361883	Brandon J. Matthews, Ross T. Smith
Hedgehog	A little boy speaks about hedgehogs all the time to everybody.	https://dl.acm.org/authorize?N690926	Vaibhav Keswani, Jeanne Laureau, Colombine Majou, Morgane Mattard, Kaisa Pirttinen, Jong-ha Yoon
Hierarchical and view-invariant light field segmentation by maximizing entropy rate on 4D ray graphs	Image segmentation is an important first step of many image processing, computer graphics, and computer vision pipelines. Unfortunately, it remains difficult to automatically and robustly segment cluttered scenes, or scenes in which multiple objects have similar color and texture. In these scenarios, light fields offer much richer cues that can be used efficiently to drastically improve the quality and robustness of segmentations. In this paper we introduce a new light field segmentation method that respects texture appearance, depth consistency, as well as occlusion, and creates well-shaped segments that are robust under view point changes. Furthermore, our segmentation is hierarchical, i.e. with a single optimization, a whole hierarchy of segmentations with different numbers of regions is available. All this is achieved with a submodular objective function that allows for efficient greedy optimization. Finally, we introduce a new tree-array type data structure, i.e. a disjoint tree, to efficiently perform submodular optimization on very large graphs. This approach is of interest beyond our specific application of light field segmentation. We demonstrate the efficacy of our method on a number of synthetic and real data sets, and show how the obtained segmentations can be used for applications in image processing and graphics.	https://dl.acm.org/doi/abs/10.1145/3355089.3356521	Rui Li, Wolfgang Heidrich
Holographic near-eye displays based on overlap-add stereograms	"Holographic near-eye displays are a key enabling technology for virtual and augmented reality (VR/AR) applications. Holographic stereograms (HS) are a method of encoding a light field into a hologram, which enables them to natively support view-dependent lighting effects. However, existing HS algorithms require the choice of a hogel size, forcing a tradeoff between spatial and angular resolution. Based on the fact that the short-time Fourier transform (STFT) connects a hologram to its observable light field, we develop the overlap-add stereogram (OLAS) as the correct method of ""inverting"" the light field into a hologram via the STFT. The OLAS makes more efficient use of the information contained within the light field than previous HS algorithms, exhibiting better image quality at a range of distances and hogel sizes. Most remarkably, the OLAS does not degrade spatial resolution with increasing hogel size, overcoming the spatio-angular resolution tradeoff that previous HS algorithms face. Importantly, the optimal hogel size of previous methods typically varies with the depth of every object in a scene, making the OLAS not only a hogel size-invariant method, but also nearly scene independent. We demonstrate the performance of the OLAS both in simulation and on a prototype near-eye display system, showing focusing capabilities and view-dependent effects."	https://dl.acm.org/doi/abs/10.1145/3355089.3356517	Nitish Padmanaban, Yifan Peng, Gordon Wetzstein
Hybrid animation production and the dream of flight	Through a detailed account of a recent practice-based research project - a short animation project called Jasper, this paper explores how a hybrid analogue/digital production approach can generate a unique and engaging visual style - one that sits between the tangible, handcrafted feel of miniatures and the cleanness, fluidity and flexibility of computer-generated animation. The author examines the new creative possibilities and challenges that a hybrid animation production approach presents and also outlines various technical platforms encountered during the production of Jasper, including motion-controlled camera systems, 3D printing, game engines, point cloud scans and augmented reality.	https://dl.acm.org/authorize?N690065	Simon Rippingale, Andrew Johnston, Andrew Bluff
HyperDrum: Interactive Synchronous Drumming in Virtual Reality using Everyday Objects	Hyperscanning is a method to detect if brain wave synchronicity exists between two or more individuals, which is usually due to behavioral or social interactions. It is usually limited to neuroscience studies and very rarely used as an interaction or visual feedback mechanic. In this work, we propose HyperDrum, which is about leveraging this cognitive synchronization to create a collaborative music production experience with immersive visualization in virtual reality. We let the participants wear electroencephalography (EEG) head-mounted displays to create music together using a physical drum. As the melody becomes in synced, we perform hyperscanning to evaluate the degree of synchronicity. The produced music and visualization reflects the synchronicity level while at the same time trains the participants to create music together, enriching the experience and performance. HyperDrum's main goal is twofold; to blend cognitive neuroscience with creativity in VR, and to encourage connectivity between humans using both art and science.	https://dl.acm.org/doi/abs/10.1145/3355355.3361894	Ryo Hajika, Kunal Gupta, Prasanth Sasikumar, Yun Suen Pai
Iam Twisq	"Hong Kong is known as a concrete jungle and we seek for a different angle to review Hong Kong's city landscape from the sight of a bird. The music video of ""Iam Twisq"" is re-creating an urban playground by animating shapes, forms and colours extracted from the original buildings."	https://dl.acm.org/authorize?N690922	SCM Films
Ikibanchi	The story takes place in a megastructure where Gaily, a young cyborg steals a mysterious and powerful artefact, kept by a giant Mecha.	https://dl.acm.org/authorize?N690939	Eloïse Bouvarel, Nicolas Cevrero, Jonathan Delefosse, Antoine Molénat, Kévin Poveda
Immersive analytics	Welcome and Overview Visualisation and Visual Analytics Introduction to Immersive Analytics Computing Beyond the Desktop Collaboration	https://dl.acm.org/doi/abs/10.1145/3355047.3359391	Ulrich Engelke, Maxime Cordeil, Andrew Cunningham, Barrett Ens
Instababy generator	This artwork is an installation that expresses the future in which users can manufacture designer's babies themselves. You can design, customize and manufacture your baby with your favorite gene on your laptop. A 3D printed child appears from the display, and the child's face created based on the visitor's face.	https://dl.acm.org/authorize?N690064	Emi Kusano, Junichi Yamaoka
Integral formulations of volumetric transmittance	"Computing the light attenuation between two given points is an essential yet expensive task in volumetric light transport simulation. Existing unbiased transmittance estimators are all based on ""null-scattering"" random walks enabled by augmenting the media with fictitious matter. This formulation prevents the use of traditional Monte Carlo estimator variance analysis, thus the efficiency of such methods is understood from a mostly empirical perspective. In this paper, we present several novel integral formulations of volumetric transmittance in which existing estimators arise as direct Monte Carlo estimators. Breaking from physical intuition, we show that the null-scattering concept is not strictly required for unbiased transmittance estimation, but is a form of control variates for effectively reducing variance. Our formulations bring new insight into the problem and the efficiency of existing estimators. They also provide a framework for devising new types of transmittance estimators with distinct and complementary performance tradeoffs, as well as a clear recipe for applying sample stratification."	https://dl.acm.org/doi/abs/10.1145/3355089.3356559	Iliyan Georgiev, Zackary Misso, Toshiya Hachisuka, Derek Nowrouzezahrai, Jaroslav Křivánek, Wojciech Jarosz
Introduction to the Vulkan® computer graphics API	Vulkan is better at keeping the GPU busy than OpenGL is. OpenGL drivers need to do a lot of CPU work before handing work off to the GPU. Vulkan lets you get more power from the GPU card you already have.	https://dl.acm.org/doi/abs/10.1145/3355047.3359405	Mike Bailey
I’m tired of demos: an adaptive MR remote collaborative platform	In AR/MR remote collaboration, a remote expert often has to demonstrate tasks for a local worker. To make this easier, we have developed a new adaptive MR remote collaborative architecture which enables a remote expert to guide a local user in physical assembly and training tasks. A remote user can activate the instructions through simple and intuitive interaction, then clear instructions are shown in both AR (local) and VR (remote) views, enabling a local worker to operate the tool following the instructions. In the demonstration we use a hammer operation as a physical task, showing the benefits of the adaptive MR remote collaborative platform.	https://dl.acm.org/doi/abs/10.1145/3355355.3361878	Peng Wang, Xiaoliang Bai, Mark Billinghurst, Dechuan Han, Shusheng Zhang, Weiping He, Xiangyu Zhang, Yuxiang Yan
JumpinVR: Enhancing Jump Experience in a Limited Physical Space	We introduce a short virtual reality experience highlighting a use-case scenario of distance relocation technique in Redirected Jumping to reduce the size requirements for tracked working space of spatial applications. In our demo, the player traverses a virtual factory by jumping between moving platforms with jump distance scaled by gain.	https://dl.acm.org/doi/abs/10.1145/3355355.3361895	Tomáš Havlík, Daigo Hayashi, Kazuyuki Fujita, Kazuki Takashima, Robert W. Lindeman, Yoshifumi Kitamura
Kids	KIDS is a game of crowds. The project consists of a short film, an interactive animation and an art installation. How do we define ourselves when we are all equal? Who is steering the crowd? What if it is heading in the wrong direction? Where does the individual end and the group begin? What is done by choice, and what under duress? KIDS was made using traditional 2D hand-drawn line animation in black and white. The animation was assembled, composited and choreographed using a game engine with a custom-made animation system in conjunction with physics simulations. The characters in a crowd behave much like matter: They attract and repel, lead and follow, grow and shrink, align and separate. They are purely defined by how they relate to one other -without showing any distinguishable features. KIDS is the second collaboration of filmmaker Michael Frei and game designer Mario von Rickenbach after their project PLUG & PLAY. The project is co-produced by Playables, SRG SSR and Arte. The app is published by Double Fine Presents for mobile devices and computers.	https://dl.acm.org/authorize?N690911	Wouter Jansen, Michael Frei
Kinky kitchen	Choosen kitchen utensils and groceries are leading a life of their own. In an intimate moment filled with lust they interact in an untypical way. The viewer experiences the pleasure of their sensuality through sounds that at first set them on the wrong track.	https://dl.acm.org/authorize?N690917	Bea Hoeller
LOGAN: unpaired shape transform in latent overcomplete space	We introduce LOGAN, a deep neural network aimed at learning shape transforms from domains. The network is trained on two sets of shapes, e.g., tables and chairs, while there is neither a pairing between shapes from the domains as supervision nor any point-wise correspondence between any shapes. Once trained, LOGAN takes a shape from one domain and transforms it into the other. Our network consists of an autoencoder to encode shapes from the two input domains into a , where the latent codes shape features, resulting in an representation. The translator is based on a generative adversarial network (GAN), operating in the latent space, where an adversarial loss enforces cross-domain translation while a ensures that the right shape features are preserved for a natural shape transform. We conduct ablation studies to validate each of our key network designs and demonstrate superior capabilities in unpaired shape transforms on a variety of examples over baselines and state-of-the-art approaches. We show that LOGAN is able to learn what shape features to preserve during shape translation, either local or non-local, whether content or style, depending solely on the input domains for training.	https://dl.acm.org/doi/abs/10.1145/3355089.3356494	Kangxue Yin, Zhiqin Chen, Hui Huang, Daniel Cohen-Or, Hao Zhang
Language-based colorization of scene sketches	Being natural, touchless, and fun-embracing, language-based inputs have been demonstrated effective for various tasks from image generation to literacy education for children. This paper for the first time presents a language-based system for interactive colorization of scene sketches, based on semantic comprehension. The proposed system is built upon deep neural networks trained on a large-scale repository of scene sketches and cartoonstyle color images with text descriptions. Given a scene sketch, our system allows users, via language-based instructions, to interactively localize and colorize specific foreground object instances to meet various colorization requirements in a progressive way. We demonstrate the effectiveness of our approach via comprehensive experimental results including alternative studies, comparison with the state-of-the-art methods, and generalization user studies. Given the unique characteristics of language-based inputs, we envision a combination of our interface with a traditional scribble-based interface for a practical multimodal colorization system, benefiting various applications. The dataset and source code can be found at https://github.com/SketchyScene/SketchySceneColorization.	https://dl.acm.org/doi/abs/10.1145/3355089.3356561	Changqing Zou, Haoran Mo, Chengying Gao, Ruofei Du, Hongbo Fu
Learned large field-of-view imaging with thin-plate optics	Typical camera optics consist of a system of individual elements that are designed to compensate for the aberrations of a single lens. Recent computational cameras shift some of this correction task from the optics to post-capture processing, reducing the imaging optics to only a few optical elements. However, these systems only achieve reasonable image quality by limiting the field of view (FOV) to a few degrees - effectively ignoring severe off-axis aberrations with blur sizes of multiple hundred pixels. In this paper, we propose a lens design and learned reconstruction architecture that lift this limitation and provide an order of magnitude increase in field of view using only a single thin-plate lens element. Specifically, we design a lens to produce spatially shift-invariant point spread functions, over the full FOV, that are tailored to the proposed reconstruction architecture. We achieve this with a mixture PSF, consisting of a peak and and a low-pass component, which provides residual contrast instead of a small spot size as in traditional lens designs. To perform the reconstruction, we train a deep network on captured data from a display lab setup, eliminating the need for manual acquisition of training data in the field. We assess the proposed method in simulation and experimentally with a prototype camera system. We compare our system against existing single-element designs, including an aspherical lens and a pinhole, and we compare against a complex multielement lens, validating high-quality large field-of-view (i.e. 53°) imaging performance using only a single thin-plate element.	https://dl.acm.org/doi/abs/10.1145/3355089.3356526	Yifan Peng, Qilin Sun, Xiong Dun, Gordon Wetzstein, Wolfgang Heidrich, Felix Heide
Learning adaptive hierarchical cuboid abstractions of 3D shape collections	Abstracting man-made 3D objects as assemblies of primitives, , shape abstraction, is an important task in 3D shape understanding and analysis. In this paper, we propose an unsupervised learning method for automatically constructing compact and expressive shape abstractions of 3D objects in a class. The key idea of our approach is an adaptive hierarchical cuboid representation that abstracts a 3D shape with a set of parametric cuboids adaptively selected from a hierarchical and multi-level cuboid representation shared by all objects in the class. The adaptive hierarchical cuboid abstraction offers a compact representation for modeling the variant shape structures and their coherence at different abstraction levels. Based on this representation, we design a convolutional neural network (CNN) for predicting the parameters of each cuboid in the hierarchical cuboid representation and the adaptive selection mask of cuboids for each input 3D shape. For training the CNN from an unlabeled 3D shape collection, we propose a set of novel loss functions to maximize the approximation quality and compactness of the adaptive hierarchical cuboid abstraction and present a progressive training scheme to refine the cuboid parameters and the cuboid selection mask effectively. We evaluate the effectiveness of our approach on various 3D shape collections and demonstrate its advantages over the existing cuboid abstraction approach. We also illustrate applications of the resulting adaptive cuboid representations in various shape analysis and manipulation tasks.	https://dl.acm.org/doi/abs/10.1145/3355089.3356529	Chun-Yu Sun, Qian-Fang Zou, Xin Tong, Yang Liu
Learning an intrinsic garment space for interactive authoring of garment animation	Authoring dynamic garment shapes for character animation on body motion is one of the fundamental steps in the CG industry. Established workflows are either time and labor consuming (i.e., manual editing on dense frames with controllers), or lack keyframe-level control (i.e., physically-based simulation). Not surprisingly, garment authoring remains a bottleneck in many production pipelines. Instead, we present a deep-learning-based approach for semi-automatic authoring of garment animation, wherein the user provides the desired garment shape in a selection of keyframes, while our system infers a latent representation for its motion-independent (e.g., gravity, cloth materials, etc.). Given new character motions, the latent representation allows to automatically generate a plausible garment animation at interactive rates. Having factored out character motion, the learned intrinsic garment space enables smooth transition between keyframes on a new motion sequence. Technically, we learn an intrinsic garment space with an motion-driven autoencoder network, where the encoder maps the garment shapes to the intrinsic space under the condition of body motions, while the decoder acts as a to generate garment shapes according to changes in character body motion and intrinsic parameters. We evaluate our approach qualitatively and quantitatively on common garment types. Experiments demonstrate our system can significantly improve current garment authoring workflows via an interactive user interface. Compared with the standard CG pipeline, our system significantly reduces the ratio of required keyframes from 20% to 1 -- 2%.	https://dl.acm.org/doi/abs/10.1145/3355089.3356512	Tuanfeng Y. Wang, Tianjia Shao, Kai Fu, Niloy J. Mitra
Learning body shape variation in physics-based characters	Recently, deep reinforcement learning (DRL) has attracted great attention in designing controllers for physics-based characters. Despite the recent success of DRL, the learned controller is viable for a single character. Changes in body size and proportions require learning controllers from scratch. In this paper, we present a new method of learning parametric controllers for body shape variation. A single parametric controller enables us to simulate and control various characters having different heights, weights, and body proportions. The users are allowed to create new characters through body shape parameters, and they can control the characters immediately. Our characters can also change their body shapes on the fly during simulation. The key to the success of our approach includes the adaptive sampling of body shapes that tackles the challenges in learning parametric controllers, which relies on the marginal value function that measures control capabilities of body shapes. We demonstrate parametric controllers for various physically simulated characters such as bipeds, quadrupeds, and underwater animals.	https://dl.acm.org/doi/abs/10.1145/3355089.3356499	Jungdam Won, Jehee Lee
Learning efficient illumination multiplexing for joint capture of reflectance and shape	We propose a novel framework that automatically learns the lighting patterns for efficient, joint acquisition of unknown reflectance and shape. The core of our framework is a deep neural network, with a shared linear encoder that directly corresponds to the lighting patterns used in physical acquisition, as well as non-linear decoders that output per-pixel normal and diffuse / specular information from photographs. We exploit the diffuse and normal information from multiple views to reconstruct a detailed 3D shape, and then fit BRDF parameters to the diffuse / specular information, producing texture maps as reflectance results. We demonstrate the effectiveness of the framework with physical objects that vary considerably in reflectance and shape, acquired with as few as 16 ~ 32 lighting patterns that correspond to 7 ~ 15 seconds of per-view acquisition time. Our framework is useful for optimizing the efficiency in both novel and existing setups, as it can automatically adapt to various factors, including the geometry / the lighting layout of the device and the properties of appearance.	https://dl.acm.org/doi/abs/10.1145/3355089.3356492	Kaizhang Kang, Cihui Xie, Chengan He, Mingqi Yi, Minyi Gu, Zimin Chen, Kun Zhou, Hongzhi Wu
Learning generative models for rendering specular microgeometry	Rendering specular material appearance is a core problem of computer graphics. While smooth analytical material models are widely used, the high-frequency structure of real specular highlights requires considering discrete, finite microgeometry. Instead of explicit modeling and simulation of the surface microstructure (which was explored in previous work), we propose a novel direction: learning the high-frequency directional patterns from synthetic or measured examples, by training a generative adversarial network (GAN). A key challenge in applying GAN synthesis to spatially varying BRDFs is evaluating the reflectance for a single location and direction without the cost of evaluating the whole hemisphere. We resolve this using a novel method for partial evaluation of the generator network. We are also able to control large-scale spatial texture using a conditional GAN approach. The benefits of our approach include the ability to synthesize spatially large results without repetition, support for learning from measured data, and evaluation performance independent of the complexity of the dataset synthesis or measurement.	https://dl.acm.org/doi/abs/10.1145/3355089.3356525	Alexandr Kuznetsov, Miloš Hašan, Zexiang Xu, Ling-Qi Yan, Bruce Walter, Nima Khademi Kalantari, Steve Marschner, Ravi Ramamoorthi
Learning predict-and-simulate policies from unorganized human motion data	The goal of this research is to create physically simulated biped characters equipped with a rich repertoire of motor skills. The user can control the characters interactively by modulating their control objectives. The characters can interact physically with each other and with the environment. We present a novel network-based algorithm that learns control policies from unorganized, minimally-labeled human motion data. The network architecture for interactive character animation incorporates an RNN-based motion generator into a DRL-based controller for physics simulation and control. The motion generator guides forward dynamics simulation by feeding a sequence of future motion frames to track. The rich future prediction facilitates policy learning from large training data sets. We will demonstrate the effectiveness of our approach with biped characters that learn a variety of dynamic motor skills from large, unorganized data and react to unexpected perturbation beyond the scope of the training data.	https://dl.acm.org/doi/abs/10.1145/3355089.3356501	Soohwan Park, Hoseok Ryu, Seyoung Lee, Sunmin Lee, Jehee Lee
Light Me Up: An Augmented-Reality Projection System	Real-time facial projection mapping is a challenging problem due to the low system latency and the high spatial augmentation accuracy requirements. We propose a new compact and inexpensive projector-camera system (ProCam) composed of off-the-self devices that achieves dynamic facial projection mapping. A mini projector and a depth sensor camera are coupled together to project content on a user's face. In one application, the camera tracks the facial landmarks of a person and simulated makeup is mapped on the person's face. The latter is created by defining different zones of interest on the face. Instead of using sophisticated hardware, we propose an affordable system that can be easily installed anywhere while it assures an immerse experience. No initialization phase is needed and the system can handle different face topologies. In addition, the users can keep their eyes open and enjoy the projetion in a mirror.	https://dl.acm.org/doi/abs/10.1145/3355355.3361885	Panagiotis-Alexandros Bokaris, Benjamin Askenazi, Michael Haddad
LightWing II	LightWing II creates a mysterious sensation of tactile data. In this interactive installation, a kinetic construction is augmented with stereoscopic 3D projections and spatial sound. A light touch sets the delicate wing-like structure into a rotational oscillation and enables the visitor to navigate through holographic spaces and responsive narratives.	https://dl.acm.org/authorize?N690075	Uwe Rieger, Yinan Liu
Like and follow	When a young boy starts spending too much time in the real world, it's up to his smartphone to help get his attention back where it belongs.	https://dl.acm.org/authorize?N690938	Brent Forrest, Tobias Schlage
Live 6DoF Video Production with Stereo Camera	We propose a light-weight 6DoF video production pipeline which uses only one stereo camera as input. The subject can move freely in any direction (lateral and depth) as the stereo camera follows to keep the subject within the frame. DeepKeying, our own proprietary keying method based on deep learning, helps with ease of live shooting with no need for green screen. The live video stream is processed in real time to provide 6DoF video experience. Our method enables a sense of immersion at production quality.	https://dl.acm.org/doi/abs/10.1145/3355355.3361880	Ayaka Nakatani, Takayuki Shinohara, Kazuhito Miyaki
Lost City of Mer	Lost City of Mer is a virtual reality (VR) game experience combined with a smartphone app that immerses players in a fantasy undersea civilization devastated by ecological disaster caused by global warning. The project aims to harness the immersive and empathetic potential of VR to address climate change and create a sense of urgency in the player with regard to their personal carbon footprint. Players are invited to help rebuild the lost world of Mer and its devastated ecosystem in VR by re-establishing its unique flora and fauna, and fighting ongoing dangers and threats, with the aim of bringing back to life its mysterious Mer-people inhabitants. Guided by a solitary seal spirit named Athina – the last of its kind in a dying ocean – players try to save the Mer population from extinction. They tend to secret gardens of coral threatened by pollution, create habitats for Mer-people, and explore the destroyed civilization, in the process learning how their real-world actions impact the world around them. The project was developed with the input of environmental scientists from Harvard University and Dartmouth College. The experience is based on real science, but told through fantasy, as it draws on the cross-cultural myth of the mermaid to appeal to people across the globe.	https://dl.acm.org/doi/abs/10.1145/3355355.3361897	Gregory W. Bennett, Liz Canner
M-Hair: Extended Reality by Stimulating the Body Hair	M-Hair is a novel method for providing tactile feedback by stimulating only body hair. By applying passive magnetic materials to the body hair, these ones responsive to external magnetic forces/fields, creating a new opportunity for interactions, such as enriching media experiences, emotional touch, or even relieving pain.	https://dl.acm.org/doi/abs/10.1145/3355355.3361881	Roger Boldu, Sambhav Jain, Juan Pablo Forero Cortes, Haimo Zhang, Suranga Nanayakkara
MIS compensation: optimizing sampling techniques in multiple importance sampling	Multiple importance sampling (MIS) has become an indispensable tool in Monte Carlo rendering, widely accepted as a near-optimal solution for combining different sampling techniques. But an MIS combination, using the common balance or power heuristics, often results in an overly defensive estimator, leading to high variance. We show that by generalizing the MIS framework, variance can be substantially reduced. Specifically, we optimize one of the combined sampling techniques so as to decrease the overall variance of the resulting MIS estimator. We apply the approach to the computation of direct illumination due to an HDR environment map and to the computation of global illumination using a path guiding algorithm. The implementation can be as simple as subtracting a constant value from the tabulated sampling density done entirely in a preprocessing step. This produces a consistent noise reduction in all our tests with no negative influence on run time, no artifacts or bias, and no failure cases.	https://dl.acm.org/doi/abs/10.1145/3355089.3356565	Ondřej Karlík, Martin Šik, Petr Vévoda, Tomáš Skřivan, Jaroslav Křivánek
Maestro	Deep into a forest, a gathering of wild animals start a nocturnal opera, conducted by a squirrel.	https://dl.acm.org/authorize?N690920	Illogic Collectif, Théophile Dufresne, Victor Caire, Bloom Pictures
Mandoline: robust cut-cell generation for arbitrary triangle meshes	"Although geometry arising ""in the wild"" most often comes in the form of a surface representation, a plethora of geometrical and physical applications require the construction of volumetric embeddings either of the geometry itself or the domain surrounding it. Cartesian cut-cell-based mesh generation provides an attractive solution in which volumetric elements are constructed from the intersection of the input surface geometry with a uniform or adaptive hexahedral grid. This choice, especially common in computational fluid dynamics, has the potential to efficiently generate accurate, surface-conforming cells; unfortunately, current solutions are often slow, fragile, or cannot handle many common topological situations. We therefore propose a novel, robust cut-cell construction technique for triangle surface meshes that explicitly computes the precise geometry of the intersection cells, even on meshes that are open or non-manifold. Its fundamental geometric primitive is the intersection of an arbitrary segment with an axis-aligned plane. Beginning from the set of intersection points between triangle mesh edges and grid planes, our bottom-up approach robustly determines cut-edges, cut-faces, and finally cut-cells, in a manner designed to guarantee topological correctness. We demonstrate its effectiveness and speed on a wide range of input meshes and grid resolutions, and make the code available as open source."	https://dl.acm.org/doi/abs/10.1145/3355089.3356543	Michael Tao, Christopher Batty, Eugene Fiume, David I. W. Levin
Manen	Sergio, an old and solitary fisherman, takes out the heavy artillery to avenge a pelican.	https://dl.acm.org/authorize?N690935	Thomas Anglade, Maxime Announ, Lucie Dessertine, Estelle Saint-Jours
Mascot	The fox who wants to be a mascot for the city goes to a mascot training academy. He lives in a very tiny house and works many part-time jobs. He is still getting many Mascot auditions with taking loans at a high-interest rates for undergoing plastic surgery.	https://dl.acm.org/authorize?N690901	Leeha Kim, Kihyuk Kwak
Material-adapted refinable basis functions for elasticity simulation	In this paper, we introduce a hierarchical construction of material-adapted refinable basis functions and associated wavelets to offer efficient coarse-graining of linear elastic objects. While spectral methods rely on global basis functions to restrict the number of degrees of freedom, our basis functions are locally supported; yet, unlike typical polynomial basis functions, they are adapted to the material inhomogeneity of the elastic object to better capture its physical properties and behavior. In particular, they share spectral approximation properties with eigenfunctions, offering a good compromise between computational complexity and accuracy. Their construction involves only linear algebra and follows a fine-to-coarse approach, leading to a block-diagonalization of the stiffness matrix where each block corresponds to an intermediate scale space of the elastic object. Once this hierarchy has been precomputed, we can simulate an object at runtime on very coarse resolution grids and still capture the correct physical behavior, with orders of magnitude speedup compared to a fine simulation. We show on a variety of heterogeneous materials that our approach outperforms all previous coarse-graining methods for elasticity.	https://dl.acm.org/doi/abs/10.1145/3355089.3356567	Jiong Chen, Max Budninskiy, Houman Owhadi, Hujun Bao, Jin Huang, Mathieu Desbrun
Mice, a small story	In a dark subway tunnel, a group of mice find a gold ring-pull that seems to have a mysterious effect on one of them. Not so far from them, an owl and his enslaved rats are watching. The owl sends his rats to get hold of this strange object...	https://dl.acm.org/authorize?N690932	Jade Baillargeault, Nazli Doale, Dimitri James, Quang Daniel La, Morgane Lau, Mélanie Pango, Manon Pringault
Mitsuba 2: a retargetable forward and inverse renderer	"Modern rendering systems are confronted with a dauntingly large and growing set of requirements: in their pursuit of realism, physically based techniques must increasingly account for intricate properties of light, such as its spectral composition or polarization. To reduce prohibitive rendering times, vectorized renderers exploit coherence via instruction-level parallelism on CPUs and GPUs. Differentiable rendering algorithms propagate derivatives through a simulation to optimize an objective function, e.g., to reconstruct a scene from reference images. Catering to such diverse use cases is challenging and has led to numerous purpose-built systems---partly, because retrofitting features of this complexity onto an existing renderer involves an error-prone and infeasibly intrusive transformation of elementary data structures, interfaces between components, and their implementations (in other words, everything). We propose Mitsuba 2, a versatile renderer that is intrinsically retargetable to various applications including the ones listed above. Mitsuba 2 is implemented in modern C++ and leverages template metaprogramming to replace types and instrument the control flow of components such as BSDFs, volumes, emitters, and rendering algorithms. At compile time, it automatically transforms arithmetic, data structures, and function dispatch, turning generic algorithms into a variety of efficient implementations without the tedium of manual redesign. Possible transformations include changing the representation of color, generating a ""wide"" renderer that operates on bundles of light paths, just-in-time compilation to create computational kernels that run on the GPU, and forward/reverse-mode automatic differentiation. Transformations can be chained, which further enriches the space of algorithms derived from a single generic implementation. We demonstrate the effectiveness and simplicity of our approach on several applications that would be very challenging to create without assistance: a rendering algorithm based on coherent MCMC exploration, a caustic design method for gradient-index optics, and a technique for reconstructing heterogeneous media in the presence of multiple scattering."	https://dl.acm.org/doi/abs/10.1145/3355089.3356498	Merlin Nimier-David, Delio Vicini, Tizian Zeltner, Wenzel Jakob
Modeling curved folding with freeform deformations	We present a computational framework for interactive design and exploration of curved folded surfaces. In current practice, such surfaces are typically created manually using physical paper, and hence our objective is to lay the foundations for the digitalization of curved folded surface design. Our main contribution is a discrete binary characterization for folds between discrete developable surfaces, accompanied by an algorithm to simultaneously fold creases and smoothly bend planar sheets. We complement our algorithm with essential building blocks for curved folding deformations: objectives to control dihedral angles and mountain-valley assignments. We apply our machinery to build the first interactive freeform editing tool capable of modeling bending and folding of complicated crease patterns.	https://dl.acm.org/doi/abs/10.1145/3355089.3356531	Michael Rabinovich, Tim Hoffmann, Olga Sorkine-Hornung
Modeling endpoint distribution of pointing selection tasks in virtual reality environments	Understanding the endpoint distribution of pointing selection tasks can reveal the underlying patterns on how users tend to acquire a target, which is one of the most essential and pervasive tasks in interactive systems. It could further aid designers to create new graphical user interfaces and interaction techniques that are optimized for accuracy, efficiency, and ease of use. Previous research has explored the modeling of endpoint distribution outside of virtual reality (VR) systems that have shown to be useful in predicting selection accuracy and guide the design of new interactive techniques. This work aims at developing an endpoint distribution of selection tasks for VR systems which has resulted in , a novel model that can be used to predict endpoint distribution of pointing selection tasks in VR environments. The development of EDModel is based on two users studies that have explored how factors such as target size, movement amplitude, and target depth affect the endpoint distribution. The model is built from the collected data and its generalizability is subsequently tested in complex scenarios with more relaxed conditions. Three applications of EDModel inspired by previous research are evaluated to show the broad applicability and usefulness of the model: correcting the bias in Fitts's law, predicting selection accuracy, and enhancing pointing selection techniques. Overall, EDModel can achieve high prediction accuracy and can be adapted to different types of applications in VR.	https://dl.acm.org/doi/abs/10.1145/3355089.3356544	Difeng Yu, Hai-Ning Liang, Xueshi Lu, Kaixuan Fan, Barrett Ens
Multi-theme generative adversarial terrain amplification	Achieving highly detailed terrain models spanning vast areas is crucial to modern computer graphics. The pipeline for obtaining such terrains is via amplification of a low-resolution terrain to refine the details given a desired theme, which is a time-consuming and labor-intensive process. Recently, data-driven methods, such as the sparse construction tree, have provided a promising direction to equip the artist with better control over the theme. These methods learn to amplify terrain details by using an exemplar of high-resolution detailed terrains to transfer the theme. In this paper, we propose Generative Adversarial Terrain Amplification (GATA) that achieves better local/global coherence compared to the existing data-driven methods while providing even more ways to control the theme. GATA is comprised of two key ingredients. Thefi rst one is a novel embedding of themes into vectors of real numbers to achieve a single tool for multi-theme amplification. The theme component can leverage existing LIDAR data to generate similar terrain features. It can also generate newfi ctional themes by tuning the embedding vector or even encoding a new example terrain into an embedding. The second one is an adversarially trained model that, conditioned on an embedding and a low-resolution terrain, generates a high-resolution terrain adhering to the desired theme. The proposed integral approach reduces the need for unnecessary manual adjustments, can speed up the development, and brings the model quality to a new level. Our implementation of the proposed method has proved successful in large-scale terrain authoring for an open-world game.	https://dl.acm.org/doi/abs/10.1145/3355089.3356553	Yiwei Zhao, Han Liu, Igor Borovikov, Ahmad Beirami, Maziar Sanjabi, Kazi Zaman
Multithreading in Pixar's animation tools	Why is multithreading important? Trends in modern hardware Stagnant clock rates Increasing core counts Special purpose hardware New paradigms and patterns: Some non-intuitive	https://dl.acm.org/doi/abs/10.1145/3355047.3359395	Florian Zitzelsberger, George ElKoura
NASA's black marble night lights used to examine disaster recovery in Puerto Rico	At night, Earth is lit up in bright strings of roads dotted with cities and towns as human-made artificial light takes center stage. During 2017's Hurricane Maria, Puerto Rico's lights went out. In the days, weeks and months that followed, NASA researchers developed neighborhood-scale maps of lighting in communities across Puerto Rico. To do this, they combined satellite data of Earth at night from the NASA/NOAA Suomi National Polar-orbiting Partnership satellite with USGS/NASA Landsat data and OpenStreetMap data. They monitored where and when the electricity grid was restored, and analyzed the demographics and physical attributes of neighborhoods longest affected by the power outages. Power failures across Puerto Rico's rural communities accounted for 61 percent of the estimated cost of 3.9 billion customer-interruption hours, six months after Hurricane Maria. These regions are primarily rural in the mountainous interior of the island where residents were without power for over 120 days. However, even more heavily populated areas had variable recovery rates between neighborhoods, with suburbs often lagging behind urban centers. The absence of electricity as seen in the night lights data offers a new way to visualize storm impacts to vulnerable communities across the entirety of Puerto Rico on a daily basis.	https://dl.acm.org/authorize?N690919	Kel Elkins
Narrative's impact on quality of experience in digital storytelling	Our ways of telling stories have evolved along with advances in technology. This has led to the emergence of digital storytelling. This project explores narrative influences on Quality of Experience of users in digital stories. This is done by creating and implementing a location driven digital story presented to the user by an augmented reality application on a mobile device. This narrative system has been evaluated by 30 people who have participated in a subjective evaluation. The results show that the narrative setup results in a richer, livelier and more engaging experience.	https://dl.acm.org/authorize?N690066	Øyvind Sørdal Klungre, Asim Hameed, Andrew Perkis
Neural state machine for character-scene interactions	We propose , a novel data-driven framework to guide characters to achieve goal-driven actions with precise scene interactions. Even a seemingly simple task such as sitting on a chair is notoriously hard to model with supervised learning. This difficulty is because such a task involves complex planning with periodic and non-periodic motions reacting to the scene geometry to precisely position and orient the character. Our proposed deep auto-regressive framework enables modeling of multi-modal scene interaction behaviors purely from data. Given high-level instructions such as the goal location and the action to be launched there, our system computes a series of movements and transitions to reach the goal in the desired state. To allow characters to adapt to a wide range of geometry such as different shapes of furniture and obstacles, we incorporate an efficient data augmentation scheme to randomly switch the 3D geometry while maintaining the context of the original motion. To increase the precision to reach the goal during runtime, we introduce a control scheme that combines egocentric inference and goal-centric inference. We demonstrate the versatility of our model with various scene interaction tasks such as sitting on a chair, avoiding obstacles, opening and entering through a door, and picking and carrying objects generated in real-time just from a single model.	https://dl.acm.org/doi/abs/10.1145/3355089.3356505	Sebastian Starke, He Zhang, Taku Komura, Jun Saito
Neural style-preserving visual dubbing	Dubbing is a technique for translating video content from one language to another. However, state-of-the-art visual dubbing techniques directly copy facial expressions from source to target actors without considering identity-specific idiosyncrasies such as a unique type of smile. We present a style-preserving visual dubbing approach from single video inputs, which maintains the signature style of target actors when modifying facial expressions, including mouth motions, to match foreign languages. At the heart of our approach is the concept of motion style, in particular for facial expressions, i.e., the person-specific expression change that is yet another essential factor beyond visual accuracy in face editing applications. Our method is based on a recurrent generative adversarial network that captures the spatiotemporal co-activation of facial expressions, and enables generating and modifying the facial expressions of the target actor while preserving their style. We train our model with unsynchronized source and target videos in an unsupervised manner using cycle-consistency and mouth expression losses, and synthesize photorealistic video frames using a layered neural face renderer. Our approach generates temporally coherent results, and handles dynamic backgrounds. Our results show that our dubbing approach maintains the idiosyncratic style of the target actor better than previous approaches, even for widely differing source and target actors.	https://dl.acm.org/doi/abs/10.1145/3355089.3356500	Hyeongwoo Kim, Mohamed Elgharib, Michael Zollhöfer, Hans-Peter Seidel, Thabo Beeler, Christian Richardt, Christian Theobalt
Non-linear sphere tracing for rendering deformed signed distance fields	Signed distance fields (SDFs) are a powerful representation for modeling solids, volumes and surfaces. Their infinite resolution, controllable continuity and robust constructive solid geometry operations, coupled with smooth blending, enable powerful and intuitive sculpting tools for creating complex SDF models. SDF metric properties also admit efficient surface rendering with sphere tracing. Unfortunately, SDFs remain incompatible with many popular direct deformation techniques which re-position a surface via its representation. Linear blend skinning used in character articulation, for example, directly displaces each vertex of a triangle mesh. To overcome this limitation, we propose a variant of sphere tracing for directly rendering deformed SDFs. We show that this problem reduces to integrating a non-linear ordinary differential equation. We propose an efficient numerical solution, with controllable error, which first automatically computes an initial value along each cast ray before walking conservatively along a curved ray in the undeformed space according to the signed distance. Importantly, our approach does not require knowledge, computation or even global existence of the inverse deformation, which allows us to readily apply many existing forward deformations. We demonstrate our method's effectiveness for interactive rendering of a variety of popular deformation techniques that were, to date, limited to explicit surfaces.	https://dl.acm.org/doi/abs/10.1145/3355089.3356502	Dario Seyb, Alec Jacobson, Derek Nowrouzezahrai, Wojciech Jarosz
Numb	Numb, shaped as an exaggerated eyeball, follows you and reflects the blinks of yours. It makes you become aware of your own blinking and sensitive to your own sensation. Numb illustrates how we build relationship with technology through senses, and how we become sensitive to ourselves by and with technology.	https://dl.acm.org/authorize?N690076	Taeil Lee
O28	In Lisbon, a german married couple is about to get aboard the legendary n°28 tramway, but how should you ract when the brakes let go and embark you on a vertiginous race... with a baby on board.	https://dl.acm.org/authorize?N690930	Otalia Caussé, Geoffroy Collin, Louise Grardel, Antoine Marchand, Robin Merle, Fabien Meyran
One pair coat	Jolanda and Hendrik are a couple. They have very close relationship. Hendrik is her favorite coat to put on. We see a day in their life.	https://dl.acm.org/authorize?N690924	Yi Luo
OpenSketch: a richly-annotated dataset of product design sketches	Product designers extensively use sketches to create and communicate 3D shapes and thus form an ideal audience for sketch-based modeling, non-photorealistic rendering and sketch filtering. However, sketching requires significant expertise and time, making design sketches a scarce resource for the research community. We introduce , a dataset of product design sketches aimed at offering a rich source of information for a variety of computer-aided design tasks. contains more than 400 sketches representing 12 man-made objects drawn by 7 to 15 product designers of varying expertise. We provided participants with front, side and top views of these objects, and instructed them to draw from two perspective viewpoints. This drawing task forces designers to from their mental vision rather than directly copy what they see. They achieve this task by employing a variety of sketching techniques and methods not observed in prior datasets. Together with industrial design teachers, we distilled a taxonomy of line types and used it to label each stroke of the 214 sketches drawn from one of the two viewpoints. While some of these lines have long been known in computer graphics, others remain to be reproduced algorithmically or exploited for shape inference. In addition, we also asked participants to produce clean presentation drawings from each of their sketches, resulting in aligned pairs of drawings of different styles. Finally, we registered each sketch to its reference 3D model by annotating sparse correspondences. We provide an analysis of our annotated sketches, which reveals systematic drawing strategies over time and shapes, as well as a positive correlation between presence of construction lines and accuracy. Our sketches, in combination with provided annotations, form challenging benchmarks for existing algorithms as well as a great source of inspiration for future developments. We illustrate the versatility of our data by using it to test a 3D reconstruction deep network trained on synthetic drawings, as well as to train a filtering network to convert concept sketches into presentation drawings. We distribute our dataset under the Creative Commons CC0 license: https://ns.inria.fr/d3/OpenSketch.	https://dl.acm.org/doi/abs/10.1145/3355089.3356533	Yulia Gryaditskaya, Mark Sypesteyn, Jan Willem Hoftijzer, Sylvia Pont, Frédo Durand, Adrien Bousseau
Orometry-based terrain analysis and synthesis	Mountainous digital terrains are an important element of many virtual environments and find application in games, film, simulation and training. Unfortunately, while existing synthesis methods produce locally plausible results they often fail to respect global structure. This is exacerbated by a dearth of automated metrics for assessing terrain properties at a macro level. We address these issues by building on techniques from orometry, a field that involves the measurement of mountains and other relief features. First, we construct a sparse metric computed on the peaks and saddles of a mountain range and show that, when used for classification, this is capable of robustly distinguishing between different mountain ranges. Second, we present a synthesis method that takes a coarse elevation map as input and builds a graph of peaks and saddles respecting a given orometric distribution. This is then expanded into a fully continuous elevation function by deriving a consistent river network and shaping the valley slopes. In terms of authoring, users provide various control maps and are also able to edit, reposition, insert and remove terrain features all while retaining the characteristics of a selected mountain range. The result is a terrain analysis and synthesis method that considers and incorporates orometric properties, and is, on the basis of our perceptual study, more visually plausible than existing terrain generation methods.	https://dl.acm.org/doi/abs/10.1145/3355089.3356535	Oscar Argudo, Eric Galin, Adrien Peytavie, Axel Paris, James Gain, Eric Guérin
PhantomTouch: Creating an Extended Reality by the Illusion of Touch using a Shape-Memory Alloy Matrix	With the rise of VR applications, the ability to experience physical touches becomes progressively important to increase immersion. In this paper, we propose PhantomTouch, a wearable forearm augmentation, that enables recreation of natural touch sensation by applying shear forces onto the skin. In contrast to commonly used vibration-based haptics, our approach consists of arranging light-weight and stretchable 3 × 3cm plasters in a matrix onto the skin. Individual plasters were embedded with lines of shape-memory alloy (SMA) wires to control shear forces. The matrix arrangement of the plasters enables the illusion of a phantom touch, for instance, feeling a wrist grab or an arm stroke.	https://dl.acm.org/doi/abs/10.1145/3355355.3361877	Sachith Muthukumarana, Don Samitha Elvitigala, Juan Pablo Forero Cortes, Denys J.C. Matthies, Suranga Nanayakkara
Physical e-Sports in VAIR Field system	In this study, we define physical e-sports that require physical training and show an example of such sport created using a mobile virtual reality system, the VAIR Field. Unlike conventional e-sports, physical e-sports involve physical activities, becoming the technological evolution of conventional sports. Our system uses extended reality technology without a head-mounted display and is safe for children to play, but it requires physical exercise and physical ability. It is a new kind of sport that uses multiple mobile devices and virtual weapons, providing more than just visual reality and allowing multiple players to play at the same time. By superimposing the physical world and the virtual world, physical e-sports allow the body to move with full force.	https://dl.acm.org/doi/abs/10.1145/3355355.3361889	Masasuke Yasumoto, Takehiro Teraoka
Pumpers paradise	How would a world work that's all about being fit and muscle-packed? Simple everyday situations become real challenges for the Pumper: If it does not seem appropriate or even impossible to train in a situation, the Pumpers still find a way to satisfy their training needs in a comical manner.	https://dl.acm.org/authorize?N690902	Eddy Hohf
Pumping Life: Embodied Virtual Companion for Enhancing Immersive Experience with Multisensory Feedback	With the advance of virtual reality (VR) head-mounted display, the appearance of the virtual companion can be more realistic and full of vitality, such as breathing and facial expression. However, the users cannot interact physically with the companions due to they do not have a physical body. In this work, our goal is to enable the virtual companion with multisensory feedback in the VR, which allows the users to play with the virtual companion physically in the immersive environment. We present Pumping Life, a dynamic flow system for enhancing the virtual companion with multisensory feedback, which utilizes water pumps and heater to provide shape deformation and thermal feedback. In this work, to show the interactive gameplay with our system, we deploy the system into a teddy bear and design a VR role-playing game. In this game, the player needs to collaborate with the teddy bear to complete the mission, which would perceive the vitality and expression of the teddy bear with multiple tactile sensations.	https://dl.acm.org/doi/abs/10.1145/3355355.3361887	Jing Yuan Huang, Wei Hsuan Hung, Tzu Yin Hsu, Yi Chun Liao, Ping Hsuan Han
Purpleboy	Oscar is a child who sprouts in his parents garden. Nobody knows his biological sex but he claims the masculine gender. One day Oscar lives an extraordinary but painful adventure in an authoritarian and oppressive world. Will he manage to have the identity recognition he desires so much?	https://dl.acm.org/authorize?N690933	Alexandre Siqueira
QuadMixer: layout preserving blending of quadrilateral meshes	We propose QuadMixer, a novel interactive technique to compose quad mesh components preserving the majority of the original layouts. Quad Layout is a crucial property for many applications since it conveys important information that would otherwise be destroyed by techniques that aim only at preserving shape. Our technique keeps untouched all the quads in the patches which are not involved in the blending. We first perform robust boolean operations on the corresponding triangle meshes. Then we use this result to identify and build new surface patches for small regions neighboring the intersection curves. These blending patches are carefully quadrangulated respecting boundary constraints and stitched back to the untouched parts of the original models. The resulting mesh preserves the designed edge flow that, by construction, is captured and incorporated to the new quads as much as possible. We present our technique in an interactive tool to show its usability and robustness.	https://dl.acm.org/doi/abs/10.1145/3355089.3356542	Stefano Nuvoli, Alex Hernandez, Claudio Esperança, Riccardo Scateni, Paolo Cignoni, Nico Pietroni
RPM-Net: recurrent prediction of motion and parts from point cloud	We introduce RPM-Net, a deep learning-based approach which simultaneously infers and hallucinates their from a single, un-segmented, and possibly partial, 3D point cloud shape. RPM-Net is a novel Recurrent Neural Network (RNN), composed of an encoder-decoder pair with interleaved Long Short-Term Memory (LSTM) components, which together predict a temporal sequence of for the input point cloud. At the same time, the displacements allow the network to learn movable parts, resulting in a motion-based shape segmentation. Recursive applications of RPM-Net on the obtained parts can predict finer-level part motions, resulting in a hierarchical object segmentation. Furthermore, we develop a separate network to estimate part mobilities, e.g., per-part motion parameters, from the segmented motion sequence. Both networks learn deep predictive models from a training set that exemplifies a variety of mobilities for diverse objects. We show results of simultaneous motion and part predictions from synthetic and real scans of 3D objects exhibiting a variety of part mobilities, possibly involving multiple movable parts.	https://dl.acm.org/doi/abs/10.1145/3355089.3356573	Zihao Yan, Ruizhen Hu, Xingguang Yan, Luanmin Chen, Oliver Van Kaick, Hao Zhang, Hui Huang
Ratatoskr	Ratatoskr is a three minutes full CG diploma project based on the norse mythology, which was a rich, wild and culturally relevant source of inspiration for our film. Our tale of the squirrel Ratatoskr, his friend Eikpyrnir and the tree of life Yggdrasil, offers everything that makes a good film: from engaging characters, varied story worlds over inspired artwork, mystical patterns and colors down to historically and economically relevant conflicts. Ratatoskr is a story about greed.	https://dl.acm.org/authorize?N690925	Meike Mueller
Real2Sim: visco-elastic parameter estimation from dynamic motion	This paper presents a method for optimizing visco-elastic material parameters of a finite element simulation to best approximate the dynamic motion of real-world soft objects. We compute the gradient with respect to the material parameters of a least-squares error objective function using either direct sensitivity analysis or an adjoint state method. We then optimize the material parameters such that the simulated motion matches real-world observations as closely as possible. In this way, we can directly build a useful simulation model that captures the visco-elastic behaviour of the specimen of interest. We demonstrate the effectiveness of our method on various examples such as numerical coarsening, custom-designed objective functions, and of course real-world flexible elastic objects made of foam or 3D printed lattice structures, including a demo application in soft robotics.	https://dl.acm.org/doi/abs/10.1145/3355089.3356548	David Hahn, Pol Banzet, James M. Bern, Stelian Coros
Reducing simulator sickness with perceptual camera control	Virtual-reality provides an immersive environment but can induce cybersickness due to the discrepancy between visual and vestibular cues. To avoid this problem, the movement of the virtual camera needs to match the motion of the user in the real world. Unfortunately, this is usually difficult due to the mismatch between the size of the virtual environments and the space available to the users in the physical domain. The resulting constraints on the camera movement significantly hamper the adoption of virtual-reality headsets in many scenarios and make the design of the virtual environments very challenging. In this work, we study how the characteristics of the virtual camera movement (e.g., translational acceleration and rotational velocity) and the composition of the virtual environment (e.g., scene depth) contribute to perceived discomfort. Based on the results from our user experiments, we devise a computational model for predicting the magnitude of the discomfort for a given scene and camera trajectory. We further apply our model to a new path planning method which optimizes the input motion trajectory to reduce perceptual sickness. We evaluate the effectiveness of our method in improving perceptual comfort in a series of user studies targeting different applications. The results indicate that our method can reduce the perceived discomfort while maintaining the fidelity of the original navigation, and perform better than simpler alternatives.	https://dl.acm.org/doi/abs/10.1145/3355089.3356490	Ping Hu, Qi Sun, Piotr Didyk, Li-Yi Wei, Arie E. Kaufman
Repairing man-made meshes via visual driven global optimization with minimum intrusion	3D mesh models created by human users and shared through online platforms and datasets flourish recently. While the creators generally have spent large efforts in modeling the visually appealing shapes with both large scale structures and intricate details, a majority of the meshes are unfortunately flawed in terms of having duplicate faces, mis-oriented regions, disconnected patches, etc., due to multiple factors involving both human errors and software inconsistencies. All these artifacts have severely limited the possible low-level and high-level processing tasks that can be applied to the rich datasets. In this work, we present a novel approach to fix these man-made meshes such that the outputs are guaranteed to be oriented manifold meshes that preserve the original structures, big and small, as much as possible. Our key observation is that the models all visually look meaningful, which leads to our strategy of repairing the flaws while always preserving the visual quality. We apply local refinements and removals only where necessary to achieve minimal intrusion of the original meshes, and global adjustments through robust optimization to ensure the outputs are valid manifold meshes with optimal connections. We test the approach on large-scale 3D datasets, and obtain quality meshes that are more readily usable for further geometry processing tasks.	https://dl.acm.org/doi/abs/10.1145/3355089.3356507	Lei Chu, Hao Pan, Yang Liu, Wenping Wang
Reparameterizing discontinuous integrands for differentiable rendering	Differentiable rendering has recently opened the door to a number of challenging inverse problems involving photorealistic images, such as computational material design and scattering-aware reconstruction of geometry and materials from photographs. Differentiable rendering algorithms strive to estimate partial derivatives of pixels in a rendered image with respect to scene parameters, which is difficult because visibility changes are inherently non-differentiable. We propose a new technique for differentiating path-traced images with respect to scene parameters that affect visibility, including the position of cameras, light sources, and vertices in triangle meshes. Our algorithm computes the gradients of illumination integrals by applying changes of variables that remove or strongly reduce the dependence of the position of discontinuities on differentiable scene parameters. The underlying parameterization is created on the fly for each integral and enables accurate gradient estimates using standard Monte Carlo sampling in conjunction with automatic differentiation. Importantly, our approach does not rely on sampling silhouette edges, which has been a bottleneck in previous work and tends to produce high-variance gradients when important edges are found with insufficient probability in scenes with complex visibility and high-resolution geometry. We show that our method only requires a few samples to produce gradients with low bias and variance for challenging cases such as glossy reflections and shadows. Finally, we use our differentiable path tracer to reconstruct the 3D geometry and materials of several real-world objects from a set of reference photographs.	https://dl.acm.org/doi/abs/10.1145/3355089.3356510	Guillaume Loubet, Nicolas Holzschuch, Wenzel Jakob
SDM-NET: deep generative network for structured deformable mesh	We introduce SDM-NET, a deep generative neural network which produces Specifically, the network is trained to generate a spatial arrangement of closed, deformable mesh parts, which respects the global part structure of a shape collection, e.g., chairs, airplanes, etc. Our key observation is that while the overall structure of a 3D shape can be complex, the shape can usually be decomposed into a set of parts, each homeomorphic to a box, and the finer-scale geometry of the part can be recovered by the box. The architecture of SDM-NET is that of a (VAE). At the part level, a PartVAE learns a deformable model of part geometries. At the structural level, we train a Structured Parts VAE (SP-VAE), which learns the part structure of a shape collection and the part geometries, ensuring the coherence between global shape structure and surface details. Through extensive experiments and comparisons with the state-of-the-art deep generative models of shapes, we demonstrate the superiority of SDM-NET in generating meshes with visual quality, flexible topology, and meaningful structures, benefiting shape interpolation and other subsequent modeling tasks.	https://dl.acm.org/doi/abs/10.1145/3355089.3356488	Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, Hao Zhang
Samsara	There was a boy who took care of a fig tree which did not bear any fruit for a long time. One day, he went to another world through his dream and got the fruit he wanted so much but he feel he could not get enough. There was a boy who took care of a fig tree which did not bear any fruit for a long time. One day, he went to another world through his dream and got the fruit he wanted so much but could not get. Before long, however, the boy got tired of the world where everything he wanted was possible. In the end, he came back to the real world and kept on taking care of the fig tree.	https://dl.acm.org/authorize?N690934	So-Young Jung
ScalarFlow: a large-scale volumetric data set of real-world scalar transport flows for computer animation and machine learning	In this paper, we present , a first large-scale data set of reconstructions of real-world smoke plumes. In addition, we propose a framework for accurate physics-based reconstructions from a small number of video streams. Central components of our framework are a novel estimation of unseen inflow regions and an efficient optimization scheme constrained by a simulation to capture real-world fluids. Our data set includes a large number of complex natural buoyancy-driven flows. The flows transition to turbulence and contain observable scalar transport processes. As such, the ScalarFlow data set is tailored towards computer graphics, vision, and learning applications. The published data set contains volumetric reconstructions of velocity and density as well as the corresponding input image sequences with calibration data, code, and instructions how to reproduce the commodity hardware capture setup. We further demonstrate one of the many potential applications: a first perceptual evaluation study, which reveals that the complexity of the reconstructed flows would require large simulation resolutions for regular solvers in order to recreate at least parts of the natural complexity contained in the captured data.	https://dl.acm.org/doi/abs/10.1145/3355089.3356545	Marie-Lena Eckert, Kiwon Um, Nils Thuerey
SceneCam: Using AR to improve Multi-Camera Remote Collaboration	During multi-camera remote collaboration on physical tasks, as the name implies, multiple cameras capture different areas and perspectives of a task space. It can be challenging for the remote user to obtain the right view of the local user and to understand the spatial relationship between the disjointed views of task space areas. We present SceneCam, a prototype with which we use AR to explore different techniques for improving multi-camera remote collaboration by making optimal camera selection easier and faster for the remote user and by making the spatial relationship between task space areas explicit. To this end, SceneCam implements two camera selection techniques: nudging the remote user to select an optimal camera view of the local user, and automatic selection of an optimal camera view of the local user. Furthermore, SceneCam provides the remote user with two focus-in-context views - exocentric and egocentric views - that visualize the spatial relationship between the multiple task space areas and the local user.	https://dl.acm.org/doi/abs/10.1145/3355355.3361892	Troels A. Rasmussen, Weidong Huang
SceneGit: a practical system for diffing and merging 3D environments	Version control systems are the foundation of collaborative workflows for text documents. For 3D environments though, version control is still an open problem due to the heterogeneous data of 3D scenes and their size. In this paper, we present a practical version control system for 3D scenes comprised of shapes, materials, textures, and animations, combined together in scene graphs. We version objects at their finest granularity, to make repositories smaller and to allow artists to work concurrently on the same object. Since, for some scene data, computing an optimal set of changes between versions is not computationally feasible, version control systems use heuristics. Compared to prior work, we propose heuristics that are efficient, robust, and independent of the application. We test our system on a variety of large scenes edited with different workflows, and show that our approach can handle all cases well while remaining efficient as scene size increases. Compared to prior work, we are significantly faster and more robust. A user study confirms that our system aids collaboration.	https://dl.acm.org/doi/abs/10.1145/3355089.3356550	Edoardo Carra, Fabio Pellacini
Selectively metropolised Monte Carlo light transport simulation	"Light transport is a complex problem with many solutions. Practitioners are now faced with the difficult task of choosing which rendering algorithm to use for any given scene. Simple Monte Carlo methods, such as path tracing, work well for the majority of lighting scenarios, but introduce excessive variance when they encounter transport they cannot sample (such as caustics). More sophisticated rendering algorithms, such as bidirectional path tracing, handle a larger class of light transport robustly, but have a high computational overhead that makes them inefficient for scenes that are not dominated by difficult transport. The underlying problem is that rendering algorithms can only be executed indiscriminately on all transport, even though they may only offer improvement for a subset of paths. In this paper, we introduce a new scheme for selectively combining different Monte Carlo rendering algorithms. We use a simple transport method (e.g. path tracing) as the base, and treat high variance ""fireflies"" as seeds for a Markov chain that locally uses a Metropolised version of a more sophisticated transport method for exploration, removing the firefly in an unbiased manner. We use a weighting scheme inspired by multiple importance sampling to partition the integrand into regions the base method can sample well and those it cannot, and only use Metropolis for the latter. This constrains the Markov chain to paths where it offers improvement, and keeps it away from regions already handled well by the base estimator. Combined with stratified initialization, short chain lengths and careful allocation of samples, this vastly reduces non-uniform noise and temporal flickering artifacts normally encountered with a global application of Metropolis methods. Through careful design choices, we ensure our algorithm never performs much worse than the base estimator alone, and usually performs significantly better, thereby reducing the need to experiment with different algorithms for each scene."	https://dl.acm.org/doi/abs/10.1145/3355089.3356578	Benedikt Bitterli, Wojciech Jarosz
Selfish	The film is about when human beings are consuming delicious seafood, at the same time, sea animals are suffering from the trash we made. In an izakaya, a chef notices a group of special guests. He decides to serve them with a variety of signature dishes. He picks up a detergent bottle and several straws, putting down on the cut board. He cuts the bottle in pieces and slices the straws as a kind of seasoning. Later, he picks styrofoam with plastic pieces to make sushi, and uses different materials such as cigarette butt, plastic bottle and so forth, to make a plastic rice bowl. After a period of time, he finishes all the dishes and serves all of them to the customers- a green turtle, albatross and seal. They are stunned because what they expect is the fresh and delicious seafood which the izakaya serves to the human customers.	https://dl.acm.org/authorize?N690928	Po Chien Chen
SmartSim: Combination of Vibro-Vestibular Wheelchair and Curved Pedestal of Self-Gravitational Acceleration for Road Property and Motion Feedback	We developed a vehicle ride simulation system for immersive virtual reality, consisting of a wheelchair for vibration and vestibular sensation, and a pedestal with a curved surface for the wheelchair to run on, utilizing a gravitational component. Vehicle motion feedback systems often use a six degrees of freedom motion platform to induce virtual vehicle acceleration on the user's body. However, because motion platforms are typically complex and expensive, their use is limited to relatively large-scale systems. The proposed system enables the presentation of variety of road property sensations as well as continuous acceleration of vehicle motion using high-bandwidth wheel torque produced by two direct-current motors. Our unique combination of a wheel and a pedestal can present vibration and vestibular sensations of vehicle acceleration with simple, light-weight, and low-cost equipment. In our demonstration experience, users can perceive the sensation of various road properties, such as uneven surfaces, and continuous acceleration of a car or roller coaster.	https://dl.acm.org/doi/abs/10.1145/3355355.3361893	Vibol Yem, Ryunosuke Yagi, Yasushi Ikei
Smile	Smile is a mixed-media installation consisting of a screen in a black box, mounted on the wall. When an interactor smiles, drone-footage of the ruins of Gaza fades in. If the interactor stops smiling, the video stops. It only plays when the interactor widely smiles at it.	https://dl.acm.org/authorize?N690077	Tomas Laurenzo
SoftCon: simulation and control of soft-bodied animals with biomimetic actuators	We present a novel and general framework for the design and control of underwater soft-bodied animals. The whole body of an animal consisting of soft tissues is modeled by tetrahedral and triangular FEM meshes. The contraction of muscles embedded in the soft tissues actuates the body and limbs to move. We present a novel muscle excitation model that mimics the anatomy of muscular hydrostats and their muscle excitation patterns. Our deep reinforcement learning algorithm equipped with the muscle excitation model successfully learned the control policy of soft-bodied animals, which can be physically simulated in real-time, controlled interactively, and resilient to external perturbations. We demonstrate the effectiveness of our approach with various simulated animals including octopuses, lampreys, starfishes, stingrays and cuttlefishes. They learn diverse behaviors such as swimming, grasping, and escaping from a bottle. We also implemented a simple user interface system that allows the user to easily create their creatures.	https://dl.acm.org/doi/abs/10.1145/3355089.3356497	Sehee Min, Jungdam Won, Seunghwan Lee, Jungnam Park, Jehee Lee
Splendor	"""Splendor"" is an algorithmic computer animation piece featuring geometrical patterns found in Japanese traditional crafts. This work specifically focused on Kiriko (Japanese cut glass) and Kumiko (Japanese wooden lattice work). Both are characterized by intricate geometrical patterns by expert craftsmanship. I attempted to simulate those traditional patterns using a method called L-system which is mainly used for simulating the growth of plants in the field of CGI."	https://dl.acm.org/authorize?N690904	Joe Takayama
Spring	Spring is the story of a shepherd girl and her dog, who face ancient spirits in order to continue the cycle of life. This poetic and visually stunning short film was written and directed by Andy Goralczyk, inspired by his childhood in the mountains of Germany.	https://dl.acm.org/authorize?N690929	Francesco Siddi, Andy Goralczyk
Staged metaprogramming for shader system development	The shader system for a modern game engine comprises much more than just compilation of source code to executable kernels. Shaders must also be exposed to art tools, interfaced with engine code, and specialized for performance. Engines typically address each of these tasks in an ad hoc fashion, without a unifying abstraction. The alternative of developing a more powerful compiler framework is prohibitive for most engines. In this paper, we identify as a unifying abstraction and implementation strategy to develop a powerful shader system with modest effort. By using a multi-stage language to perform metaprogramming at compile time, engine-specific code can consume, analyze, transform, and generate shader code that will execute at runtime. Staged metaprogramming reduces the effort required to implement a shader system that provides earlier error detection, avoids repeat declarations of shader parameters, and explores opportunities to improve performance. To demonstrate the value of this approach, we design and implement a shader system, called Selos, built using staged metaprogramming. In our system, shader and application code are written in the same language and can share types and functions. We implement a design space exploration framework for Selos that investigates static versus dynamic composition of shader features, exploring the impact of shader specialization in a deferred renderer. Staged metaprogramming allows Selos to provide compelling features with a simple implementation.	https://dl.acm.org/doi/abs/10.1145/3355089.3356554	Kerry A. Seitz, Tim Foley, Serban D. Porumbescu, John D. Owens
State of the art on stylized fabrication	Digital fabrication devices are powerful tools for creating tangible reproductions of 3D digital models. Most available printing technologies aim at producing an accurate copy of a tridimensional shape. However, fabrication technologies can also be used to create a stylistic representation of a digital shape. We refer to this class of methods as stylized fabrication methods. These methods abstract geometric and physical features of a given shape to create an unconventional representation, to produce an optical illusion, or to devise a particular interaction with the fabricated model. In this course, we classify and overview this broad and emerging class of approaches and also propose possible directions for future research.	https://dl.acm.org/doi/abs/10.1145/3355047.3359411	Nico Pietroni, Bernd Bickel, Luigi Malomo, Paolo Cignoni
Stress	In an oppressing environment, a child plays with a knife.	https://dl.acm.org/authorize?N690923	Simon Dauchy, Tom Delforge, Antoine Wilmot, Théo Mechref, Julien Delcroix
StructureNet: hierarchical graph networks for 3D shape generation	The ability to generate novel, diverse, and realistic 3D shapes along with associated part semantics and structure is central to many applications requiring high-quality 3D assets or large volumes of realistic training data. A key challenge towards this goal is how to accommodate diverse shape variations, including both continuous deformations of parts as well as structural or discrete alterations which add to, remove from, or modify the shape constituents and compositional structure. Such object structure can typically be organized into a hierarchy of constituent object parts and relationships, represented as a hierarchy of -ary graphs. We introduce StructureNet, a hierarchical graph network which (i) can directly encode shapes represented as such -ary graphs, (ii) can be robustly trained on large and complex shape families, and (iii) be used to generate a great diversity of realistic structured shape geometries. Technically, we accomplish this by drawing inspiration from recent advances in graph neural networks to propose an order-invariant encoding of -ary graphs, considering jointly both part geometry and inter-part relations during network training. We extensively evaluate the quality of the learned latent spaces for various shape families and show significant advantages over baseline and competing methods. The learned latent spaces enable several structure-aware geometry processing applications, including shape generation and interpolation, shape editing, or shape structure discovery directly from un-annotated images, point clouds, or partial scans.	https://dl.acm.org/doi/abs/10.1145/3355089.3356527	Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy J. Mitra, Leonidas J. Guibas
Super Size Hero	Supersizehero is an immerse VR game for HTC Vive which puts the player in the role of an overweight hero trying to save the day. A special crafted, tracked fat suit allowing the player to actively use his belly serves as the main gameplay mechanic. The game is highscore based - each round the player needs to prevent a prison breakout or bank robbing by bouncing fleeing prisoners back into the prison, interrupt bank robbers and bring money back to the bank in order to gain as much points as possible in the given round. At the start of every level the player can choose one of three suits - each grants special abilities and a unique playstyle.	https://dl.acm.org/doi/abs/10.1145/3355355.3361876	Jiayan Chen
Tactile microcosm of alife	Tactile Microcosm of ALife offers interaction with artificial organisms, whereby the user can enjoy playing with fish-like organisms through aerial imaging and haptic feedback. The holographic organisms float in water in a petri dish, and the user can feel a forcefield of the vital of the organisms via force feedback.	https://dl.acm.org/authorize?N690078	Toshikazu Ohshima
Taichi: a language for high-performance computation on spatially sparse data structures	3D visual computing data are often spatially sparse. To exploit such sparsity, people have developed hierarchical sparse data structures, such as multi-level sparse voxel grids, particles, and 3D hash tables. However, developing and using these high-performance sparse data structures is challenging, due to their intrinsic complexity and overhead. We propose , a new data-oriented programming language for efficiently authoring, accessing, and maintaining such data structures. The language offers a high-level, data structure-agnostic interface for writing computation code. The user independently specifies the data structure. We provide several elementary components with different sparsity properties that can be arbitrarily composed to create a wide range of multi-level sparse data structures. This of data structures from computation makes it easy to experiment with different data structures without changing computation code, and allows users to write computation as if they are working with a dense array. Our compiler then uses the semantics of the data structure and index analysis to automatically optimize for locality, remove redundant operations for coherent accesses, maintain sparsity and memory allocations, and generate efficient parallel and vectorized instructions for CPUs and GPUs. Our approach yields competitive performance on common computational kernels such as stencil applications, neighbor lookups, and particle scattering. We demonstrate our language by implementing simulation, rendering, and vision tasks including a material point method simulation, finite element analysis, a multigrid Poisson solver for pressure projection, volumetric path tracing, and 3D convolution on sparse grids. Our computation-data structure decoupling allows us to quickly experiment with different data arrangements, and to develop high-performance data structures tailored for specific computational tasks. With <u> </u> th as many lines of code, we achieve 4.55× higher performance on average, compared to hand-optimized reference implementations.	https://dl.acm.org/doi/abs/10.1145/3355089.3356506	Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, Frédo Durand
The camera offset space: real-time potentially visible set computations for streaming rendering	Potential visibility has historically always been of importance when rendering performance was insufficient. With the rise of virtual reality, rendering power may once again be insufficient, e.g., for integrated graphics of head-mounted displays. To tackle the issue of efficient potential visibility computations on modern graphics hardware, we introduce the camera offset space (COS). Opposite to how traditional visibility computations work---where one determines which pixels are covered by an object under all potential viewpoints---the COS describes under which camera movement a sample location is covered by a triangle. In this way, the COS opens up a new set of possibilities for visibility computations. By evaluating the pairwise relations of triangles in the COS, we show how to efficiently determine occluded triangles. Constructing the COS for all pixels of a rendered view leads to a complete potentially visible set (PVS) for complex scenes. By fusing triangles to larger occluders, including locations between pixel centers, and considering camera rotations, we describe an exact PVS algorithm that includes all viewing directions inside a view cell. Implementing the COS is a combination of real-time rendering and compute steps. We provide the first GPU PVS implementation that works without preprocessing, on-the-fly, on unconnected triangles. This opens the door to a new approach of rendering for virtual reality head-mounted displays and server-client settings for streaming 3D applications such as video games.	https://dl.acm.org/doi/abs/10.1145/3355089.3356530	Jozef Hladky, Hans-Peter Seidel, Markus Steinberger
The gift	"After the huge success of Manor's 2017 film ""The Drawing"", Manor & BETC Shopper once again trusted Passion Paris with the production of a second Christmas film, ""The Gift"". We are in the same enchanting world, but, in this season's film, again directed by AgainstAllOdds, one of Santa's witty little elves becomes our endearing hero! We follow the story of Elfred, as he desperately struggles to build up THE perfect plan to avoid ending up as one of the gifts under the tree! But in the end.... he learns that appearances can be misleading. A new Christmas tale, full of action, humor and most importantly, magic and tenderness."	https://dl.acm.org/authorize?N690914	Against AllOdds, Marc Bodin-Joyeux
The heretic (part 1)	In a future when humans and technology are intertwined, we have yet to discover the origins of our fairy tales.	https://dl.acm.org/authorize?N690927	Veselin Efremov, Silvia Rasheva
The lifeguard	An old man returns to his beach where he spent his childhood, the wind rises, a storm approaches, and submerge him little by little into his memories.	https://dl.acm.org/authorize?N690931	Cécile Mercier, Milena Blin, Clémentine Lecluse, Iris Stanley, Ming-Yang Zhao, Rohan Kotnis, Charlotte Humbert
The ostrich politic	Ostriches carry on their daily activities burying their heads, believing It's an instinctive behavior. However, one day a research by phylogeneticist Dr. Kays proves otherwise.	https://dl.acm.org/authorize?N690915	Mohammed Houhou
The reduced immersed method for real-time fluid-elastic solid interaction and contact simulation	We introduce the Reduced Immersed Method ( ) for the real-time simulation of two-way coupled incompressible fluids and elastic solids and the interaction of multiple deformables with (self-)collisions. Our framework is based on a novel discretization of the , which model fluid and deformables as a single incompressible medium and their interaction as a unified system on a fixed domain combining Eulerian and Lagrangian terms. One advantage for real-time simulations resulting from this modeling is that two-way coupling phenomena can be faithfully simulated while avoiding costly calculations such as tracking the deforming fluid-solid interfaces and the associated fluid boundary conditions. Our discretization enables the combination of a PIC/FLIP fluid solver with a reduced-order Lagrangian elasticity solver. Crucial for the performance of RIM is the efficient transfer of information between the elasticity and the fluid solver and the synchronization of the Lagrangian and Eulerian settings. We introduce the concept of that enables an efficient reduced-order modeling of the transfer. Our experiments demonstrate that RIM handles complex meshes and highly resolved fluids for large time steps at high framerates on off-the-shelf hardware, even in the presence of high velocities and rapid user interaction. Furthermore, it extends reduced-order elasticity solvers such as Hyper-Reduced Projective Dynamics with natural collision handling.	https://dl.acm.org/doi/abs/10.1145/3355089.3356496	Christopher Brandt, Leonardo Scandolo, Elmar Eisemann, Klaus Hildebrandt
The relightables: volumetric performance capture of humans with realistic relighting	"We present ""The Relightables"", a volumetric capture system for photorealistic and high quality relightable full-body performance capture. While significant progress has been made on volumetric capture systems, focusing on 3D geometric reconstruction with high resolution textures, much less work has been done to recover photometric properties needed for relighting. Results from such systems lack high-frequency details and the subject's shading is prebaked into the texture. In contrast, a large body of work has addressed relightable acquisition for image-based approaches, which photograph the subject under a set of basis lighting conditions and recombine the images to show the subject as they would appear in a target lighting environment. However, to date, these approaches have not been adapted for use in the context of a high-resolution volumetric capture system. Our method combines this ability to realistically relight humans for arbitrary environments, with the benefits of free-viewpoint volumetric capture and new levels of geometric accuracy for dynamic performances. Our subjects are recorded inside a custom geodesic sphere outfitted with 331 custom color LED lights, an array of high-resolution cameras, and a set of custom high-resolution depth sensors. Our system innovates in multiple areas: First, we designed a novel active depth sensor to capture 12.4 MP depth maps, which we describe in detail. Second, we show how to design a hybrid geometric and machine learning reconstruction pipeline to process the high resolution input and output a volumetric video. Third, we generate temporally consistent reflectance maps for dynamic performers by leveraging the information contained in two alternating color gradient illumination images acquired at 60Hz. Multiple experiments, comparisons, and applications show that The Relightables significantly improves upon the level of realism in placing volumetrically captured human performances into arbitrary CG scenes."	https://dl.acm.org/doi/abs/10.1145/3355089.3356571	Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-Escolano, Rohit Pandey, Jason Dourgarian, Danhang Tang, Anastasia Tkach, Adarsh Kowdle, Emily Cooper, Mingsong Dou, Sean Fanello, Graham Fyffe, Christoph Rhemann, Jonathan Taylor, Paul Debevec, Shahram Izadi
The way home	Ana crash lands on a dangerous planet and meets a curious creature while waiting for rescue. When it ruins her one-way ticket back, Ana must learn to trust her newfound friend to find the way home.	https://dl.acm.org/authorize?N690937	Kai Lun Pang, Qian Yong Joshua Ang, Yu Liang John Liew, May Ling Georgia Low, Jia Qi Karen Mei, Yan Yu Toh, Xin Mun Winona Leong, Geng Hieng Clement Tan, Lay Siong Toh, Jin Wei Chan, Muhammad Faiz Bin Mohamed
Tomographic projector: large scale volumetric display with uniform viewing experiences	Over the past century, as display evolved, people have demanded more realistic and immersive experiences in theaters. Here, we present a tomographic projector for a volumetric display system that accommodates large audiences while providing a uniform experience. The tomographic projector combines high-speed digital micromirror and three spatial light modulators to refresh projection images at 7200 Hz. With synchronization of the tomographic projector and wearable focus-tunable eyepieces, the presented system can reconstruct 60 focal planes for volumetric representation right in front of audiences. We demonstrate proof of concept of the proposed system by implementing a miniaturized theater environment. Experimentally, we show that this system has wide expressible depth range with focus cues from 25 cm to optical infinity with sufficient tolerance while preserving high resolution and contrast. We also confirm that the proposed system provides uniform experience in a wide range of viewing zone through simulation and experiment. Additionally, the tomographic projector has capability to equalize vergence state that varies in conventional stereoscopic 3D theater according to viewing position as well as interpupillary distance. This study is concluded with thorough discussion about tomographic projectors in terms of challenges and research issues.	https://dl.acm.org/doi/abs/10.1145/3355089.3356577	Youngjin Jo, Seungjae Lee, Dongheon Yoo, Suyeon Choi, Dongyeon Kim, Byoungho Lee
TouchVR: a Wearable Haptic Interface for VR Aimed at Delivering Multi-modal Stimuli at the User’s Palm	TouchVR is a novel wearable haptic interface which can deliver multimodal tactile stimuli on the palm by DeltaTouch haptic display and vibrotactile feedback on the fingertips by vibration motors for the Virtual Reality (VR) user. DeltaTouch display is capable of generating 3D force vector at the contact point and presenting multimodal tactile sensation of weight, slippage, encounter, softness, and texture. The VR system consists of HTC Vive Pro base stations, head-mounted display (HMD), and Leap Motion controller for tracking the user's hand motion. The MatrixTouch, BallFeel, RoboX, and AnimalFeel applications have been developed to demonstrate the capabilities of the proposed technology. A novel haptic interface can potentially bring a new level of immersion of the user in VR and make it more interactive and tangible.	https://dl.acm.org/doi/abs/10.1145/3355355.3361896	Daria Trinitatova, Dzmitry Tsetserukou
Transport-based neural style transfer for smoke simulations	Artistically controlling fluids has always been a challenging task. Optimization techniques rely on approximating simulation states towards target velocity or density field configurations, which are often handcrafted by artists to indirectly control smoke dynamics. Patch synthesis techniques transfer image textures or simulation features to a target flow field. However, these are either limited to adding structural patterns or augmenting coarse flows with turbulent structures, and hence cannot capture the full spectrum of different styles and semantically complex structures. In this paper, we propose the first Transport-based Neural Style Transfer (TNST) algorithm for volumetric smoke data. Our method is able to transfer features from natural images to smoke simulations, enabling general content-aware manipulations ranging from simple patterns to intricate motifs. The proposed algorithm is physically inspired, since it computes the density transport from a source input smoke to a desired target configuration. Our transport-based approach allows direct control over the divergence of the stylization velocity field by optimizing incompressible and irrotational potentials that transport smoke towards stylization. Temporal consistency is ensured by transporting and aligning subsequent stylized velocities, and 3D reconstructions are computed by seamlessly merging stylizations from different camera viewpoints.	https://dl.acm.org/doi/abs/10.1145/3355089.3356560	Byungsoo Kim, Vinicius C. Azevedo, Markus Gross, Barbara Solenthaler
Upload Not Complete	Created by Taiwanese artists Hu, Chin-Hsiang, Tsai Bing-Hua and Chang, Zhao-Qing, this piece attempts to use a hybrid reality, LED lights, wearable devices, and fans to build a installation that uploads the human mind to digital space. Imagine that upload process can see virtual object in real space. When you see the virtual object and feel the influence (wind and vibration), after passing through the upwardly extending tunnel, the screen enters the completely virtual space, but you don't know whether the upload is completed.	https://dl.acm.org/doi/abs/10.1145/3355355.3361884	Chin-Hsiang Hu, Bing-Hua Tsai, Zhao-Qing Chang
Variance-aware multiple importance sampling	Many existing Monte Carlo methods rely on multiple importance sampling (MIS) to achieve robustness and versatility. Typically, the balance or power heuristics are used, mostly thanks to the seemingly strong guarantees on their variance. We show that these MIS heuristics are oblivious to the effect of certain variance reduction techniques like stratification. This shortcoming is particularly pronounced when unstratified and stratified techniques are combined (e.g., in a bidirectional path tracer). We propose to enhance the balance heuristic by injecting variance estimates of individual techniques, to reduce the variance of the combined estimator in such cases. Our method is simple to implement and introduces little overhead.	https://dl.acm.org/doi/abs/10.1145/3355089.3356515	Pascal Grittmann, Iliyan Georgiev, Philipp Slusallek, Jaroslav Křivánek
Video-guided real-to-virtual parameter transfer for viscous fluids	In physically-based simulation, it is essential to choose appropriate material parameters to generate desirable simulation results. In many cases, however, choosing appropriate material parameters is very challenging, and often tedious trial-and-error parameter tuning steps are inevitable. In this paper, we propose a real-to-virtual parameter transfer framework that identifies material parameters of viscous fluids with example video data captured from real-world phenomena. Our method first extracts positional data of fluids and then uses the extracted data as a reference to identify the viscosity parameters, combining forward viscous fluid simulations and parameter optimization in an iterative process. We evaluate our method with a range of synthetic and real-world example data, and demonstrate that our method can identify the hidden physical variables and viscosity parameters. This set of recovered physical variables and parameters can then be effectively used in novel scenarios to generate viscous fluid behaviors visually consistent with the example videos.	https://dl.acm.org/doi/abs/10.1145/3355089.3356551	Tetsuya Takahashi, Ming C. Lin
Who You Are is What You Tell: Effects of Perspectives on Virtual Reality Story Experiences	Virtual reality (VR) stories provide an immersive and interactive medium to present narrative content. While it provides an immersive way to present the content, it is challenging to present the story in a way that matches the intention of the producer. There are several aspects of the VR environment that may affect the way the viewer experiences and perceives the content including the inhabited character by the viewer, the available interactive objects, areas of interest in the scene and their salience to receive attention. This presented demonstration is part of a project where we are investigating the ways a VR story can be presented—in other words, how the VR environment can be designed—to provide affordances that will help the viewer to perceive the story as intended by the producer, without explicitly guiding the viewer for particular interactions.	https://dl.acm.org/doi/abs/10.1145/3355355.3361879	Enrique Klein Garcia-Godos, Valerie Williams Eguiguren, Arindam Dey
Wild eyes	"With a classic look and experimental perspectives, ""Wild Eyes"" takes us into the dark world of the blind girl Kitana. Through her eyes, we can see again and understand how big the sacrifice was. A 2d animated short about friendship and guilt."	https://dl.acm.org/authorize?N690947	Nina Prange
Wirtinger holography for near-eye displays	Near-eye displays using holographic projection are emerging as an exciting display approach for virtual and augmented reality at high-resolution without complex optical setups --- shifting optical complexity to computation. While precise phase modulation hardware is becoming available, phase retrieval algorithms are still in their infancy, and holographic display approaches resort to heuristic encoding methods or iterative methods relying on various relaxations. In this work, we depart from such existing approximations and solve the phase retrieval problem for a hologram of a scene at a single depth at a given time by revisiting complex Wirtinger derivatives, also extending our framework to render 3D volumetric scenes. Using Wirtinger derivatives allows us to pose the phase retrieval problem as a quadratic problem which can be minimized with first-order optimization methods. The proposed Wirtinger Holography is flexible and facilitates the use of different loss functions, including learned perceptual losses parametrized by deep neural networks, as well as stochastic optimization methods. We validate this framework by demonstrating holographic reconstructions with an order of magnitude lower error, both in simulation and on an experimental hardware prototype.	https://dl.acm.org/doi/abs/10.1145/3355089.3356539	Praneeth Chakravarthula, Yifan Peng, Joel Kollin, Henry Fuchs, Felix Heide
Write-a-video: computational video montage from themed text	We present , a tool for the creation of video montage using mostly text-editing. Given an input themed text and a related video repository either from online websites or personal albums, the tool allows novice users to generate a video montage much more easily than current video editing tools. The resulting video illustrates the given narrative, provides diverse visual content, and follows cinematographic guidelines. The process involves three simple steps: (1) the user provides input, mostly in the form of editing the text, (2) the tool automatically searches for semantically matching candidate shots from the video repository, and (3) an optimization method assembles the video montage. Visual-semantic matching between segmented text and shots is performed by cascaded keyword matching and visual-semantic embedding, that have better accuracy than alternative solutions. The video assembly is formulated as a hybrid optimization problem over a graph of shots, considering temporal constraints, cinematography metrics such as camera movement and tone, and user-specified cinematography idioms. Using our system, users without video editing experience are able to generate appealing videos.	https://dl.acm.org/doi/abs/10.1145/3355089.3356520	Miao Wang, Guo-Wei Yang, Shi-Min Hu, Shing-Tung Yau, Ariel Shamir
X-CAD: optimizing CAD models with extended finite elements	We propose a novel generic shape optimization method for CAD models based on the eXtended Finite Element Method (XFEM). Our method works directly on the intersection between the model and a regular simulation grid, without the need to mesh or remesh, thus removing a bottleneck of classical shape optimization strategies. This is made possible by a novel hierarchical integration scheme that accurately integrates finite element quantities with sub-element precision. For optimization, we efficiently compute analytical shape derivatives of the entire framework, from model intersection to integration rule generation and XFEM simulation. Moreover, we describe a differentiable projection of shape parameters onto a constraint manifold spanned by user-specified shape preservation, consistency, and manufacturability constraints. We demonstrate the utility of our approach by optimizing mass distribution, strength-to-weight ratio, and inverse elastic shape design objectives directly on parameterized 3D CAD models.	https://dl.acm.org/doi/abs/10.1145/3355089.3356576	Christian Hafner, Christian Schumacher, Espen Knoop, Thomas Auzinger, Bernd Bickel, Moritz Bächer
ZoomOut: spectral upsampling for efficient shape correspondence	We present a simple and efficient method for refining maps or correspondences by iterative upsampling in the spectral domain that can be implemented in a few lines of code. Our main observation is that high quality maps can be obtained even if the input correspondences are noisy or are encoded by a small number of coefficients in a spectral basis. We show how this approach can be used in conjunction with existing initialization techniques across a range of application scenarios, including symmetry detection, map refinement across complete shapes, non-rigid partial shape matching and function transfer. In each application we demonstrate an improvement with respect to both the quality of the results and the computational speed compared to the best competing methods, with up to two orders of magnitude speed-up in some applications. We also demonstrate that our method is both robust to noisy input and is scalable with respect to shape complexity. Finally, we present a theoretical justification for our approach, shedding light on structural properties of functional maps.	https://dl.acm.org/doi/abs/10.1145/3355089.3356524	Simone Melzi, Jing Ren, Emanuele Rodolà, Abhishek Sharma, Peter Wonka, Maks Ovsjanikov
gravityZero: an installation work for virtual environment	This paper reports the exposition of an artistic installation, gravityZERO, and its ongoing technical development. It consists of virtual sound, VR and robotic technologies in order to simulate the state of zero gravity. Audience members can experience a floating sensation within this virtual environment. gravityZERO (zero gravity) is an installation that combines video, sound, and robotics. Translucent cubes are assembled at the venue. Images are quasi-holographically projected on the cube's surfaces, and speakers are placed in the cube's corners. A person is suspended from the ceiling and floats as if there is no gravity. Each rope can be freely moved in 3D space within the cube through motor operation. This project is based upon an idea that utilizes an interface closely related to a human body, and also pursues new possibilities of the performance of Augmented Reality and Projection Mapping.	https://dl.acm.org/authorize?N690060	Suguru Goto, Satoru Higa, johnsmith, Chihiro Suzuki
阿公 a gong	A journey of a 7-years-old boy's acceptance of his grandpa's death in a traditional Taiwanese funeral.	https://dl.acm.org/authorize?N690945	Zozo Jhen, Tena Galovic, Marine Varguy, Yen-Chen Liu, Ellis Kayin Chan
