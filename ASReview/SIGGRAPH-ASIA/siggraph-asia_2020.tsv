title	abstract	url	authors
"""Mobile"" device"	This work invites viewers to see the world through a machine's perspective. People are accustomed to seeing the world through an anthropocentric viewpoint and create things accordingly. What will it be like if machines are creating things through their perspective? Created by the Mechanical Creator, what are the challenges this group of mechanical life forms is facing for survival? How do they live within and adapt to the environment? We know that hermit crabs are using human trash as their shells. What would the mechanical life forms do when they are interacting with their living environment?	https://dl.acm.org/doi/abs/10.1145/3414686.3427121	Chenwei Chiang
A benchmark for rough sketch cleanup	Sketching is a foundational step in the design process. Decades of sketch processing research have produced algorithms for 3D shape interpretation, beautification, animation generation, colorization, etc. However, there is a mismatch between sketches created in the wild and the clean, sketch-like input required by these algorithms, preventing their adoption in practice. The recent flurry of sketch vectorization, simplification, and cleanup algorithms could be used to bridge this gap. However, they differ wildly in the assumptions they make on the input and output sketches. We present the first benchmark to evaluate and focus sketch cleanup research. Our dataset consists of 281 sketches obtained in the wild and a curated subset of 101 sketches. For this curated subset along with 40 sketches from previous work, we commissioned manual vectorizations and multiple ground truth cleaned versions by professional artists. The sketches span artistic and technical categories and were created by a variety of artists with different styles. Most sketches have Creative Commons licenses; the rest permit academic use. Our benchmark's metrics measure the similarity of automatically cleaned rough sketches to artist-created ground truth; the ambiguity and messiness of rough sketches; and low-level properties of the output parameterized curves. Our evaluation identifies shortcomings among state-of-the-art cleanup algorithms and discusses open problems for future research.	https://dl.acm.org/doi/abs/10.1145/3414685.3417784	Chuan Yan, David Vanderhaeghe, Yotam Gingold
A general framework for pearlescent materials	The unique and visually mesmerizing appearance of pearlescent materials has made them an indispensable ingredient in a diverse array of applications including packaging, ceramics, printing, and cosmetics. In contrast to their natural counterparts, such synthetic examples of pearlescence are created by dispersing microscopic interference pigments within a dielectric resin. The resulting space of materials comprises an enormous range of different phenomena ranging from smooth lustrous appearance reminiscent of pearl to highly directional metallic gloss, along with a gradual change in color that depends on the angle of observation and illumination. All of these properties arise due to a complex optical process involving multiple scattering from platelets characterized by wave-optical interference. This article introduces a flexible model for simulating the optics of such pearlescent 3D microstructures. Following a thorough review of the properties of currently used pigments and manufacturing-related effects that influence pearlescence, we propose a new model which expands the range of appearance that can be represented, and closely reproduces the behavior of measured materials, as we show in our comparisons. Using our model, we conduct a systematic study of the parameter space and its relationship to different aspects of pearlescent appearance. We observe that several previously ignored parameters have a substantial impact on the material's optical behavior, including the multi-layered nature of modern interference pigments, correlations in the orientation of pigment particles, and variability in their properties (e.g. thickness). The utility of a general model for pearlescence extends far beyond computer graphics: inverse and differentiable approaches to rendering are increasingly used to disentangle the physics of scattering from real-world observations. Our approach could inform such reconstructions to enable the predictive design of tailored pearlescent materials.	https://dl.acm.org/doi/abs/10.1145/3414685.3417782	Ibón Guillén, Julio Marco, Diego Gutierrez, Wenzel Jakob, Adrian Jarabo
A harmonic balance approach for designing compliant mechanical systems with nonlinear periodic motions	We present a computational method for designing compliant mechanical systems that exhibit large-amplitude oscillations. The technical core of our approach is an optimization-driven design tool that combines sensitivity analysis for optimization with the Harmonic Balance Method for simulation. By establishing dynamic force equilibrium in the frequency domain, our formulation avoids the major limitations of existing alternatives: it handles nonlinear forces, side-steps any transient process, and automatically produces periodic solutions. We introduce design objectives for amplitude optimization and trajectory matching that enable intuitive high-level authoring of large-amplitude motions. Our method can be applied to many types of mechanical systems, which we demonstrate through a set of examples involving compliant mechanisms, flexible rod networks, elastic thin shell models, and multi-material solids. We further validate our approach by manufacturing and evaluating several physical prototypes.	https://dl.acm.org/doi/abs/10.1145/3414685.3417765	Pengbin Tang, Jonas Zehnder, Stelian Coros, Bernhard Thomaszewski
A letter across the stars	"From the moment mankind began observing the night sky, there is one question that has been in my mind for a very long time. As our attempts to find answers to these questions continued and progressed, we learned more about the universe and humanity. What is left of us now? The SETI (Search for Extra-Terrestrial Intelligence) project, which seeks to find evidence of civilization outside the earth by discovering artificial radio signals from space, has continued to this day with the development of radio astronomy. Breakthrough Listen, which aims to observe one million stars and one hundred galactic centers, is a radio wave exploration project that represents this modern SETI, and has been constantly updating new observational data from 2017 to the present. In particular, some of the data are recorded as unique radio signals of unknown cause. This work is the realization of real-time audio-visualization of data containing this unique signal with 2-channel projection mapping and 16- channel surround speakers. The work creates a cosmic experience that waits for the unknown signal hidden in it by making the audience sense the radio signals from outer space with light and sound. Through this cosmic experience, the audience will be able to reconsider the possibility of an unknown being, so that we can look back at ourselves from a cosmic point of view that all humanity is ultimately one. The journey to find extraterrestrial intelligence is the same as the journey to find the answer to the question about humanity. Now that the value of humanity and life goes beyond Earth, we have a time to ask the question that humanity has long held in mind. ""Are we the only intelligent life in this universe?"""	https://dl.acm.org/doi/abs/10.1145/3414686.3427142	JongKuk Won
A moving least square reproducing kernel particle method for unified multiphase continuum simulation	In physically based-based animation, pure particle methods are popular due to their simple data structure, easy implementation, and convenient parallelization. As a pure particle-based method and using Galerkin discretization, the Moving Least Square Reproducing Kernel Method (MLSRK) was developed in engineering computation as a general numerical tool for solving PDEs. The basic idea of Moving Least Square (MLS) has also been used in computer graphics to estimate deformation gradient for deformable solids. Based on these previous studies, we propose a multiphase MLSRK framework that animates complex and coupled fluids and solids in a unified manner. Specifically, we use the Cauchy momentum equation and phase field model to uniformly capture the momentum balance and phase evolution/interaction in a multiphase system, and systematically formulate the MLSRK discretization to support general multiphase constitutive models. A series of animation examples are presented to demonstrate the performance of our new multiphase MLSRK framework, including hyperelastic, elastoplastic, viscous, fracturing and multiphase coupling behaviours etc.	https://dl.acm.org/doi/abs/10.1145/3414685.3417809	Xiao-Song Chen, Chen-Feng Li, Geng-Chen Cao, Yun-Tao Jiang, Shi-Min Hu
A novel discretization and numerical solver for non-fourier diffusion	We introduce the [Anderson and Tamma 2006; Xue et al. 2018] to computer graphics for diffusion-driven problems that has several attractive properties: (a) it fundamentally explains diffusion from the perspective of the non-equilibrium statistical mechanical Boltzmann Transport Equation, (b) it allows for a finite propagation speed for diffusion, in contrast to the widely employed Fick's/Fourier's law, and (c) it can capture some of the most characteristic visual aspects of diffusion-driven physics, such as hydrogel swelling, limited diffusive domain for smoke flow, snowflake and dendrite formation, that span from Fourier-type to non-Fourier-type diffusive phenomena. We propose a unified convection-diffusion formulation using this model that treats both the diffusive quantity its associated flux as the primary unknowns, and that recovers the traditional Fourier-type diffusion as a limiting case. We design a novel semi-implicit discretization for this formulation on staggered MAC grids and a geometric Multigrid-preconditioned Conjugate Gradients solver for efficient numerical solution. To highlight the efficacy of our method, we demonstrate end-to-end examples of elastic porous media simulated with the Material Point Method (MPM), and diffusion-driven Eulerian incompressible fluids.	https://dl.acm.org/doi/abs/10.1145/3414685.3417863	Tao Xue, Haozhe Su, Chengguizi Han, Chenfanfu Jiang, Mridul Aanjaneya
A practical ply-based appearance model of woven fabrics	Simulating the appearance of woven fabrics is challenging due to the complex interplay of lighting between the constituent yarns and fibers. Conventional surface-based models lack the fidelity and details for producing realistic close-up renderings. Micro-appearance models, on the other hand, can produce highly detailed renderings by depicting fabrics fiber-by-fiber, but become expensive when handling large pieces of clothing. Further, neither surface-based nor micro-appearance model has not been shown in practice to match measurements of complex anisotropic reflection and transmission simultaneously. In this paper, we introduce a practical appearance model for woven fabrics. We model the structure of a fabric at the ply level and simulate the local appearance of fibers making up each ply. Our model accounts for both reflection and transmission of light and is capable of matching physical measurements better than prior methods including fiber based techniques. Compared to existing micro-appearance models, our model is light-weight and scales to large pieces of clothing.	https://dl.acm.org/doi/abs/10.1145/3414685.3417777	Zahra Montazeri, Søren B. Gammelmark, Shuang Zhao, Henrik Wann Jensen
A reduced-precision network for image reconstruction	Neural networks are often quantized to use reduced-precision arithmetic, as it greatly improves their storage and computational costs. This approach is commonly used in image classification and natural language processing applications. However, using a quantized network for the reconstruction of HDR images can lead to a significant loss in image quality. In this paper, we introduce , a neural network for image reconstruction, in which close to 95% of the computations can be implemented with 4-bit integers. This is achieved using a combination of two U-shaped networks that are specialized for different tasks, a network based on the U-Net architecture, coupled to a network that reconstructs the output image. The feature extraction network has more computational complexity but is more resilient to quantization errors. The filtering network, on the other hand, has significantly fewer computations but requires higher precision. Our network recurrently warps and accumulates previous frames using motion vectors, producing temporally stable results with significantly better quality than TAA, a widely used technique in current games.	https://dl.acm.org/doi/abs/10.1145/3414685.3417786	Manu Mathew Thomas, Karthik Vaidyanathan, Gabor Liktor, Angus G. Forbes
A review of current trends on visual perception studies in virtual and augmented reality	In the development of novel algorithms and techniques in virtual and augmented reality (VR/AR), it is crucial to take human visual perception into account. For example, when hardware resources are a restraining factor, the limitations of the human visual system can be exploited in the creation and evaluation of new effective techniques. Over the last decades, visual perception evaluation studies have become a vital part of the design, development, and evaluation of immersive computer graphics applications. This course aims at introducing the attendees to the basic concepts of visual perception applied to computer graphics and it offers an overview of recent perceptual evaluation studies that have been conducted with head-mounted displays (HMDs) in the context of VR and AR applications. During this course, we call attention to the latest published courses and surveys on visual perception applied to computer graphics and interaction techniques. Through an extensive search in the literature, we have identified six main areas in which recent visual perceptual evaluation studies have been focused on: distance perception, avatar perception, image quality, interaction, motion perception, and cybersickness. Trends, main results, and open challenges are discussed for each area and accompanied with relevant references offering the attendees a wide introduction and perspective on the topic.	https://dl.acm.org/doi/abs/10.1145/3415263.3419144	Valeria Garro, Veronica Sundstedt, Diego Navarro
A wave optics based fiber scattering model	Existing fiber scattering models in rendering are all based on tracing rays through fiber geometry, but for small fibers diffraction and interference are non-negligible, so relying on ray optics can result in appearance errors. This paper presents the first wave optics based fiber scattering model, introducing an azimuthal scattering function that comes from a full wave simulation. Solving Maxwell's equations for a straight fiber of constant cross section illuminated by a plane wave reduces to solving for a 3D electromagnetic field in a 2D domain, and our fiber scattering simulator solves this 2.5D problem efficiently using the boundary element method (BEM). From the resulting fields we compute extinction, absorption, and far-field scattering distributions, which we use to simulate shadowing and scattering by fibers in a path tracer. We validate our path tracer against the wave simulation and the simulation against a measurement of diffraction from a single textile fiber. Our results show that our approach can reproduce a wide range of fibers with different sizes, cross sections, and material properties, including textile fibers, animal fur, and human hair. The renderings include color effects, softening of sharp features, and strong forward scattering that are not predicted by traditional ray-based models, though the two approaches produce similar appearance for complex fiber assemblies under many conditions.	https://dl.acm.org/doi/abs/10.1145/3414685.3417841	Mengqi (Mandy) Xia, Bruce Walter, Eric Michielssen, David Bindel, Steve Marschner
ADD: analytically differentiable dynamics for multi-body systems with frictional contact	We present a differentiable dynamics solver that is able to handle frictional contact for rigid and deformable objects within a unified framework. Through a principled mollification of normal and tangential contact forces, our method circumvents the main difficulties inherent to the non-smooth nature of frictional contact. We combine this new contact model with fully-implicit time integration to obtain a robust and efficient dynamics solver that is analytically differentiable. In conjunction with adjoint sensitivity analysis, our formulation enables gradient-based optimization with adaptive trade-offs between simulation accuracy and smoothness of objective function landscapes. We thoroughly analyse our approach on a set of simulation examples involving rigid bodies, visco-elastic materials, and coupled multi-body systems. We furthermore showcase applications of our differentiable simulator to parameter estimation for deformable objects, motion planning for robotic manipulation, trajectory optimization for compliant walking robots, as well as efficient self-supervised learning of control policies.	https://dl.acm.org/doi/abs/10.1145/3414685.3417766	Moritz Geilinger, David Hahn, Jonas Zehnder, Moritz Bächer, Bernhard Thomaszewski, Stelian Coros
AI: artificial interruption	The artificial, a curtain of cubes, interrupts and rolls over the natural order. Leaves fall and freeze into polygonal crystals. As the artificial interruption spreads fields and leaves shatter into ice cubes. Nature once fluid, splatters, then freezes. The seasons fold like boxes into structures of gray, as we see time crystalize. The AI cuts the atmosphere into rectangles and cubes, abstract, perfect, but empty of life squeezed into a splatter and gray wave. The AI slides open a door. It leaks out to become a tight, glacial surface of data cubes, a networked sea of ice. Under this sea sheet of networked ice, the last mammal breaks through, swims away, and with her our past coded in her genes.	https://dl.acm.org/doi/abs/10.1145/3414686.3427110	Hyejin Hannah Kum-Biocca, Jinhong Kwon
Accelerating 3D deep learning with PyTorch3D	1. Accelerating 3D Deep Learning with PyTorch3D, arXiv 2007.08501 2. Mesh R-CNN, ICCV 2019 3. SynSin: End-to-end View Synthesis from a Single Image, CVPR 2020 4. Fast Differentiable Raycasting for Neural Rendering using Sphere-based Representations, arXiv 2004.07484	https://dl.acm.org/doi/abs/10.1145/3415263.3419160	Justin Johnson, Nikhila Ravi, Jeremy Reizenstein, David Novotny, Shubham Tulsiani, Christoph Lassner, Steve Branson
Alpha and omega	"'Alpha and Omega' deals with the emotional temperature difference between the fear and anxiety of those who have experienced a disaster and the attitudes of those who do not. This is because the artist, who returned to Seoul after suffering an earthquake around 5 am on February 11, 2018, when the second largest earthquake in Pohang (intensity 4.6), experienced the atmosphere of Seoul unlike Pohang. The severity of the earthquake felt at the epicenter of Pohang is not shared in Seoul. The artist interprets the difference in reaction between the two cities as ""the difference in the senses due to the imbalance between information and experience."" To express the sensory gap between the two cities, seismic data of two cities, which are objective indicators of earthquakes, are used, and the images of the horizontal (Seoul) and vertical (Pohang) axes created at the intersection reflect the earthquake intensity data of each city for 10 years. do. The higher the intensity of the earthquake, the greater the change in the axis width or line. The sound is also connected with data, and it is divided into two channels: Channel 1- Left (Seoul) and Channel 2-Right (Pohang). The sound converted to MIDI changes in pitch and rhythm depending on the magnitude of the earthquake. As such, the difference between the amount and intensity of the disaster experience shown by the data is proposed in a form that can be perceived as image and sound. Rather than simply reproducing the overwhelming fear and pressure of a disaster image, data sonication can listen to the data and provide a new synesthesia experience. In a space where disaster data is replaced by light and sound, audiences can freely experience a new type of disaster."	https://dl.acm.org/doi/abs/10.1145/3414686.3427168	Eunsol Kim
An adaptive staggered-tilted grid for incompressible flow simulation	Enabling adaptivity on a uniform Cartesian grid is challenging due to its highly structured grid cells and axis-aligned grid lines. In this paper, we propose a new grid structure - the adaptive staggered-tilted (AST) grid - to conduct adaptive fluid simulations on a regular discretization. The key mechanics underpinning our new grid structure is to allow the emergence of a new set of tilted grid cells from the nodal positions on a background uniform grid. The original axis-aligned cells, in conjunction with the populated axis-tilted cells, jointly function as the geometric primitives to enable adaptivity on a regular spatial discretization. By controlling the states of the tilted cells both temporally and spatially, we can dynamically evolve the adaptive discretizations on an Eulerian domain. Our grid structure preserves almost all the computational merits of a uniform Cartesian grid, including the cache-coherent data layout, the easiness for parallelization, and the existence of high-performance numerical solvers. Further, our grid structure can be integrated into other adaptive grid structures, such as an Octree or a sparsely populated grid, to accommodate the T-junction-free hierarchy. We demonstrate the efficacy of our AST grid by showing examples of large-scale incompressible flow simulation in domains with irregular boundaries.	https://dl.acm.org/doi/abs/10.1145/3414685.3417837	Yuwei Xiao, Szeyu Chan, Siqi Wang, Bo Zhu, Xubo Yang
An extended cut-cell method for sub-grid liquids tracking with surface tension	Simulating liquid phenomena utilizing Eulerian frameworks is challenging, since highly energetic flows often induce severe topological changes, creating thin and complex liquid surfaces. Thus, capturing structures that are small relative to the grid size become intractable, since continually increasing the resolution will scale sub-optimally due to the pressure projection step. Previous methods successfully relied on using higher resolution grids for tracking the liquid surface implicitly; however this technique comes with drawbacks. The mismatch of pressure samples and surface degrees of freedom will cause artifacts such as hanging blobs and permanent kinks at the liquid-air interface. In this paper, we propose an extended cut-cell method for handling liquid structures that are smaller than a grid cell. At the core of our method is a novel iso-surface Poisson Solver, which converges with second-order accuracy for pressure values while maintaining attractive discretization properties such as symmetric positive definiteness. Additionally, we extend the iso-surface assumption to be also compatible with surface tension forces. Our results show that the proposed method provides a novel framework for handling arbitrarily small splashes that can also correctly interact with objects embodied by complex geometries.	https://dl.acm.org/doi/abs/10.1145/3414685.3417859	Yi-Lu Chen, Jonathan Meier, Barbara Solenthaler, Vinicius C. Azevedo
An implicit updated lagrangian formulation for liquids with large surface energy	We present an updated Lagrangian discretization of surface tension forces for the simulation of liquids with moderate to extreme surface tension effects. The potential energy associated with surface tension is proportional to the surface area of the liquid. We design discrete forces as gradients of this energy with respect to the motion of the fluid over a time step. We show that this naturally allows for inversion of the Hessian of the potential energy required with the use of Newton's method to solve the systems of nonlinear equations associated with implicit time stepping. The rotational invariance of the surface tension energy makes it non-convex and we define a definiteness fix procedure as in [Teran et al. 2005]. We design a novel level-set-based boundary quadrature technique to discretize the surface area calculation in our energy based formulation. Our approach works most naturally with Particle-In-Cell [Harlow 1964] techniques and we demonstrate our approach with a weakly incompressible model for liquid discretized with the Material Point Method [Sulsky et al. 1994]. We show that our approach is essential for allowing efficient implicit numerical integration in the limit of high surface tension materials like liquid metals.	https://dl.acm.org/doi/abs/10.1145/3414685.3417845	David A. B. Hyde, Steven W. Gagniere, Alan Marquez-Razon, Joseph Teran
An introduction to physics-based animation	Physics-based animation has emerged as a core area of computer graphics finding widespread application in the film and video game industries as well as in areas such as virtual surgery, virtual reality, and training simulations. This course introduces students and practitioners to fundamental concepts in physics-based animation, placing an emphasis on breadth of coverage and providing a foundation for pursuing more advanced topics and current research in the area. The course focuses on imparting practical knowledge and intuitive understanding rather than providing detailed derivations of the underlying mathematics. The course is suitable for someone with no background in physics-based animation---the only prerequisites are basic calculus, linear algebra, and introductory physics. We begin with a simple, and complete, example of a mass-spring system, introducing the principles behind physics-based animation: mathematical modeling and numerical integration. From there, we systematically present the mathematical models commonly used in physics-based animation beginning with Newton's laws of motion and conservation of mass, momentum, and energy. We then describe the underlying physical and mathematical models for animating rigid bodies, soft bodies, and fluids. Then we describe how these continuous models are discretized in space and time, covering Lagrangian and Eulerian formulations, spatial discretizations and interpolation, and explicit and implicit time integration. In the final section, we discuss commonly used constraint formulations and solution methods.	https://dl.acm.org/doi/abs/10.1145/3415263.3419147	Adam W. Bargteil, Tamar Shinar, Paul G. Kry
Appearance-preserving tactile optimization	Textures are encountered often on various common objects and surfaces. Many textures combine visual and tactile aspects, each serving important purposes; most obviously, a texture alters the object's appearance or tactile feeling as well as serving for visual or tactile identification and improving usability. The tactile feel and visual appearance of objects are often linked, but they may interact in unpredictable ways. Advances in high-resolution 3D printing enable highly flexible control of geometry to permit manipulation of both visual appearance and tactile properties. In this paper, we propose an optimization method to independently control the tactile properties and visual appearance of a texture. Our optimization is enabled by neural network-based models, and allows the creation of textures with a desired tactile feeling while preserving a desired visual appearance at a relatively low computational cost, for use in a variety of applications.	https://dl.acm.org/doi/abs/10.1145/3414685.3417857	Chelsea Tymms, Siqi Wang, Denis Zorin
Approaches and challenges to virtual and augmented reality in health care and rehabilitation	"Building a network of healthcare professionals Identifying current practices Linguistic Barrier Big Picture but no Specs ""Automagic worshippers"""	https://dl.acm.org/doi/abs/10.1145/3415263.3419174	Joaquim Jorge, Pedro Campos, Daniel Simões Lopes
Arc diffusion	A still photo from artificial life, this is a frozen moment from the movement of simple shapes. Repeated geometric forms were rotated and transformed over time and complex interwoven abstract patterns emerged. These unexpected forms are born from motion and feedback. Initial graphical parameters were predefined and the patterned evolution was set in motion. When the motion is paused the cellular beauty of individual frames is revealed. Chance plays its part in this phenomenon. Individual parameters are predetermined but the end result is indeterminate. The space between known quantities is where the unexpected patterns and lights emerge.	https://dl.acm.org/doi/abs/10.1145/3414686.3427105	Dave Payling
Augmented creativity	This work explores a new process of creativity generation under the guide of Jordanous's Four PPPPerspectives (2016) and speculates intertwined relationships among multi-contributors in computational creativity. This work can also be seen as an experimental multispecies storytelling on creativity. This experiment collected the images from OpenProcessing community as training samples and fed them into styleGAN to generate many images. Then these images are postprocessed as environment-driven interactive moving images by optical flow algorithm. In this computational system, it is speculated that humans are inspiring themselves and that all other nonegos are used as bridges and catalysts in a closed loop, to some extent. I hope this work can motivate audiences to think about the definition of creativity and reflect on human's unique ability on creation by comparing with machine and nature's creative abilities.	https://dl.acm.org/doi/abs/10.1145/3414686.3427177	Yanyi Lu
Augmented reality media to express the experience of Japanese food culture	"Japanese cuisine was registered as a UNESCO Intangible Cultural Heritage in 2013 designated with an evaluation as a social diet custom embodying a Japanese spirit that respects nature. However, it is difficult to fully understand the cultural characteristics of Japanese food because the actual meal is limited to taste and visual information such as taste, ingredients, tableware, and presentation. In this system, in order to convey Japanese food culture, interaction and video expressions are combined with actual meals, and as users proceed with meals the natural environment, text, and Ashirai projected on the tableware change and the system allows you to learn about the richness of nature that supports Japanese food, the changes in the four seasons, and the relationship with traditional events. The natural environment changes in 10 stages, and users can experience the changing seasons and beauty at their own pace. Ashirai are vegetables and flowers that are added to complement the dishes, and four types of treats are projected onto the tableware according to the changing seasons. The Japanese food in this work consists of ""Ichiju Sansai,"" which means one soup and three vegetables. ""Ichiju Sansai"" is said to be the basis of Japanese food and is composed of a staple food, a soup, two side dishes, and a main dish, and their arrangement is also fixed. We asked several people to experience the system and we were able to tell the users that Japanese food is supported by natural riches and seasons while influencing traditional events as well. It has been also confirmed as another effect that users re-think the act of eating and re-realize Japanese cuisine including recognition of the blessings of nature. Demonstrations are held at the art gallery using cooking models."	https://dl.acm.org/doi/abs/10.1145/3414686.3427107	Kei Kobayashi, Kazuma Nagata, Junichi Hoshino
Back and forth: pneumatic anadrome [serial painting 1]	"""Serial paintings"" are a series of works highlighting the role of the canvas - or more generally the support of the painting - in the construction of the final form. ""Back and Forth - Pneumatic Anadrome"" is the first work in the series. In it, a string of coloured beads is pushed back and forth between two spiralling ""canvases"" using compressed air. Without rearranging the order of the beads, the string is forced to coil alternatively into one or the other spiral: this folding and unfolding reveals in turn images or text with opposing meanings or connotations. Each image has to be destroyed in order to create the other; this cyclic process of creation and destruction is purposefully revealed and triggered by the curiosity of the public."	https://dl.acm.org/doi/abs/10.1145/3414686.3427150	Alvaro Cassinelli, Christian Sandor, Daniel Saakes
Balloon Interface for Midair Haptic Interaction	Midair physical prop is a promising tool to facilitate intuitive human-computer interaction in a three-dimensional (3D) space. In such systems, it is challenging to guarantee safety and comfort during interaction with real objects in high-speed 3D motion. In this paper, we propose a balloon interface, a midair physical prop that affords direct single-handed manipulation in a safe manner. The system uses a spherical helium-filled balloon controlled by surrounding ultrasound phased array transducers as a physical prop. It is safe to collide even when it is moving fast because it has an elastic body and does not have any mechanical parts. Fast switching of driving transducer units and closed-loop control based on a high-speed measurement system result in the 3D control of an object of a size that affords a spherical grasp, e.g., a spherical balloon of 10-cm diameter.	https://dl.acm.org/doi/abs/10.1145/3415255.3422882	Takuro Furumoto, Masahiro Fujiwara, Yasutoshi Makino, Hiroyuki Shinoda
Bijective projection in a shell	We introduce an algorithm to convert a self-intersection free, orientable, and manifold triangle mesh into a equipped with a bijective projection operator to map to a class of discrete surfaces contained within the shell whose normals satisfy a simple local condition. Properties can be robustly and efficiently transferred between these surfaces using the prismatic layer as a common parametrization domain. The combination of the prismatic shell construction and corresponding projection operator is a robust building block readily usable in many downstream applications, including the solution of PDEs, displacement maps synthesis, Boolean operations, tetrahedral meshing, geometric textures, and nested cages.	https://dl.acm.org/doi/abs/10.1145/3414685.3417769	Zhongshi Jiang, Teseo Schneider, Denis Zorin, Daniele Panozzo
BirthMark: an artificial viewer for appreciation of digital surrogates of art	"BirthMark proposes an artificial intelligence model of an audience to evaluate and anticipate the audience reaction to media art. In BirthMark, human cognitive process of appreciating artwork is defined in three stages: ""camouflage,"" ""solution"" and ""insight."" In other words, understanding the intention (solution) from hidden images (camouflage) and realizing its meaning (insight). Watching the archive video clips featuring different works by 16 artists, the A.I. in BirthMark tries to appreciate works of art in a similar way to humans. YOLO-9000, an object detection system, tracks objects in the images of the works, while ACT-R, a cognitive architecture designed to mimic the structure of the brain, reads and perceives them. The A.I.'s process of recognizing works is shown in the video and the keywords of the works found in this process appear on a small screen. At the same time, an old slide projector shows what the A.I. understands semantically about the artists' interpretations of their own work. The A.I.'s cognitive process seems to be similar to the human act of appreciating art at a glance. But in reality, the keywords that it accurately analyzes from the images are only 2 to 5 out of 300 words. The more abstract the work is, the worse the A.I's intelligibility gets. As the ""birthmark"" in a short story of the same title by Nathaniel Hawthorne represents, BirthMark implies that there is a realm of humans that can be hardly explained through scientific methodology."	https://dl.acm.org/doi/abs/10.1145/3414686.3427146	Jooyoung Oh
Blind landing	Blind Landing is composed of a helmet that tracks brain waves and eye movements, and AI software that analyzes the Youtube video frames. The work show how visual stimuli from algorithmically promoted contents affect viewer's behavior pattern, and induce the viewer to recover from their trusting and blind submission to the social network's algorithms of appreciation. To participate in the work, the audience is asked to put a helmet on. Then the viewer is subjected to the vision of one of the most appreciated online videos. Laterally, the screen shows the same videos analyzed by artificial intelligence software, which also colors the parts of the video that have been most seen by the viewer. Blind Landing captures the user's data and shows how predictable they are. Two systems were independently implemented for this purpose: 1) AI model that predicts and simulates gaze; 2) Custom built software that acquires real user data in real time and compares it withthe previous prediction model. The workutilize participants' EEG brain signals to generate the attended scene while the YouTube appreciation. For easy wear, EEG hat was sawed inside the 70's Pilot helmet. To allow 360 degree of freedom, hanger was installed with iron pivot on the ceiling. The aim of the work could therefore be to induce the viewer to recover from their trusting and blind submission to the social network's algorithms of appreciation, showing this cynical and perverse possible retaliation with wide eyes.	https://dl.acm.org/doi/abs/10.1145/3414686.3427111	Jooyoung Oh
Botorikko, machine created state	"The recent development in the machine learning field encounters interesting robots' creative responses and becoming a challenging artistic medium. They are two possible directions in the future development of robots' creativity, replicating the human mental processes, or liberating machine creativity itself. At the SIGGRAPH Asia, we would like to present the artwork ""Botorikko, Machine Created State"" conceptualized with the intention to point on Post-Algorithmic Society where we are going to lose control over technology by been obsessed with the idea of using it to serves humanity. In our aesthetical approach, we incline to the 21st-century avant-garde conceptual tradition. We intend to draw parallels between Dadaism and machine-made content and encompass technological singularity and Dadaism into one, Singularity Dadaism, as a human-less paradigm of uncontrollable creative practice closely related to AI aesthetic and machine abstraction phenomena. Creativity and the act of creating art are some of the greatest challenges the new generation of artificial intelligence models are exposed. Nevertheless, by creating AI agents to achieve and exceed the performances of humans, we need to accept the evolution of their creativity too. Hence, there are two possible directions toward the future development of robots' creativity, either to replicate the mental processes characteristic for humans or liberate machine creativity and leave them to evolve their own creative practices. In the artistic origination of the artwork ""Botorikko, Machine Created State,"" the appearance and generated dialogues between Machiavelli and Sun Tzu artificial intelligence clones resembles Aristotle's Mimesis as human's natural love of imitation and the pleasure in recognizing likenesses and Dadaistic ideas linked to strong social criticism against antiprogressive thinking. We are trying to shift AI as a creative medium beyond traditional artistic approaches and interpretations, and possibly to accept it as co-creative rather than only assistive in the age of AI and Post-Algorithmic Society."	https://dl.acm.org/doi/abs/10.1145/3414686.3427101	Predrag K. Nikolic, Mohd Razali Md Tomari, Marko Jovanovic
Box	BOX is an interactive installation, consisting on an everyday object augmented by artificial intelligence. The piece reflects on the power asymmetries that technology instantiates, aiming at providing with a reflection on the aesthetics of our relationship with it. The artwork also aims to showcasing the advancements and limitations in computer vision and artificial intelligence, allowing the public to experience in person its power as well as its inherent biases. Recent advances in computer vision and artificial intelligence, have allowed the creation of systems able to infer (predict) information on a person from camera data, including facial recognition, facial expressions, ethnicity, among others. Nowadays, several companies provide image processing services that include these predictions, among several others. In spite of potential benefits that face recognition proposes, its widespread application entails several risks, from privacy breaches to systematic discrimination in areas such as hiring, policing, benefits assignment, marketing, and other purposes. BOX consists of a gumball machine that, using computer vision and machine learning, predicts its user's ethnicity, delivering free candy only to white users. The artwork showcases a possible use of computer vision making explicit the fact that every technological implantation crystallises a political worldview, allowing the general public to experience in person the power of these new technologies, while simultaneously providing a tool for participatory observation, as well as ethnographic and technographic research. Our project aims to raise awareness on discrimination, ethics, and accountability in AI among practitioners and the general public.	https://dl.acm.org/doi/abs/10.1145/3414686.3427178	Tomas Laurenzo, Katia Vega
CPPM: chi-squared progressive photon mapping	We present a novel chi-squared progressive photon mapping algorithm (CPPM) that constructs an estimator by controlling the bandwidth to obtain superior image quality. Our estimator has parametric statistical advantages over prior nonparametric methods. First, we show that when a probability density function of the photon distribution is subject to uniform distribution, the radiance estimation is unbiased under certain assumptions. Next, the local photon distribution is evaluated via a chi-squared test to determine whether the photons follow the hypothesized distribution (uniform distribution) or not. If the statistical test deems that the photons inside the bandwidth are uniformly distributed, bandwidth reduction should be suspended. Finally, we present a pipeline with a bandwidth retention and conditional reduction scheme according to the test results. This pipeline not only accumulates sufficient photons for a reliable chi-squared test, but also guarantees that the estimate converges to the correct solution under our assumptions. We evaluate our method on various benchmarks and observe significant improvement in the running time and rendering quality in terms of mean squared error over prior progressive photon mapping methods.	https://dl.acm.org/doi/abs/10.1145/3414685.3417822	Zehui Lin, Sheng Li, Xinlu Zeng, Congyi Zhang, Jinzhu Jia, Guoping Wang, Dinesh Manocha
Cangjie	Humans and machines are in constant conversations. Humans start the dialogue by using programming languages that will be compiled to binary digits that machines can interpret. However, Intelligent machines today are not only observers of the world, but they also make their own decisions. If A.I imitates human beings to create a symbolic system to communicate based on their own understandings of the universe and start to actively interact with us, how will this recontextualize and redefine our coexistence in this intertwined reality? To what degree can the machine enchant us in curiosity and enhance our expectations of a semantic meaning-making process? Cangjie provides a data-driven interactive spatial visualization in semantic human-machine reality. The visualization is generated by an intelligent system in real-time through perceiving the real-world via a camera (located in the exhibition space). Inspired by Cangjie, an ancient Chinese legendary historian (c.2650 BCE), who invented Chinese characters based on the characteristics of everything on the earth, we trained a neural network, we have named Cangjie, to learn the constructions and principles of all the Chinese characters. It transforms what it perceives into a collage of unique symbols made of Chinese strokes. The symbols produced through the lens of Cangjie, tangled with the imagery captured by the camera, are visualized algorithmically as abstracted, pixelated semiotics, continuously evolving and composing an everchanging poetic virtual reality. Cangjie is not only a conceptual response to the tension and fragility in the coexistence of humans and machines but also an artistic imagined expression of a future language that reflects on ancient truths in this artificial intelligence era. The interactivity of this intelligent visualization prioritizes ambiguity and tension that exist between the actual and the virtual, machinic vision and human perception, and past and future.	https://dl.acm.org/doi/abs/10.1145/3414686.3427153	Weidi Zhang, Donghao Ren
Cascade	The physical part of the installation composes of two major elements, namely a wooden table and a water ecosystem. The table is served as a water reservoir as well as an interaction interface while the water ecosystem provides a non-stop water flow for the water reservoir. On the technical side, the installation is set up with multimedia system consisting of a Mac Pro computer, four projectors, a speaker, a network router, an Xbox Kinect Sensor and an Arduino board. The Arduino board is connected to five relays, two ultrasonic sensors, one infra-red sensor, one water-flow sensor and one water-level sensor for detections and signal transmissions. There are three major interactions in Cascade. First, when audience enter the room, the infra-red sensor senses his movement and turns off the table lighting. Projection of a cascade then fades in with music triggering the water ecosystem to be turned on to form a water reservoir. Second, projection reminds audience to open the under-table drawer and interact with pebbles. When users open the drawer, the ultrasonic sensor senses the drawer movement and turns the LED on/off. Third, when users place pebbles on the table, water volume and flow rate of the reservoir will be changed. Therefore, the water-level sensor is used to detect the amount of water for controlling the water overflow and the water-flow sensor is used to transmit serial signals for projections of swimming fish to be mapped onto the reservoir accordingly. Pebbles are detected by the Xbox sensor to avoid fish collisions.	https://dl.acm.org/doi/abs/10.1145/3414686.3427165	Long Hin Porsche Cheng, Yuet Ting Cheng
Chameleon	chameleon change the color of skin according to the surrounding environment for hiding the body, can not only avoid predators, and also confuse their prey. I use against neural network, I input a lot of images of chameleon various parts to the machine learning algorithm, the neural network generated the color-changing chameleon skin based on these pictures. Artificial-intelligence one day can be like a chameleon hide himself with pixel camouflage? Work is divided into two versions, image and image + interaction: in the interactive version, the color of the chameleon can change color according to the color of the catch by the electron trap.	https://dl.acm.org/doi/abs/10.1145/3414686.3427129	Wenqian Gao
Chordal decomposition for spectral coarsening	We introduce a novel solver to significantly reduce the size of a geometric operator while preserving its spectral properties at the lowest frequencies. We use chordal decomposition to formulate a convex optimization problem which allows the user to control the operator sparsity pattern. This allows for a trade-off between the spectral accuracy of the operator and the cost of its application. We efficiently minimize the energy with a change of variables and achieve state-of-the-art results on spectral coarsening. Our solver further enables novel applications including volume-to-surface approximation and detaching the operator from the mesh, i.e., one can produce a mesh tailor-made for visualization and optimize an operator separately for computation.	https://dl.acm.org/doi/abs/10.1145/3414685.3417789	Honglin Chen, Hsueh-TI Derek Liu, Alec Jacobson, David I. W. Levin
CoVR: Co-located Virtual Reality Experience Sharing for Facilitating Joint Attention via Projected View of HMD Users	"Sharing virtual reality (VR) experiences between users wearing head-mounted displays (HMD users) and users not wearing HMDs (Non-HMD users) is a promising approach that can help bridge the gap between these users' experiences. In previous studies, the role of these users and the differences in their attention targets were not considered, causing a lack of joint attention in user communication. Also, previous systems required cumbersome installation in the spaces in which VR was being experienced. Therefore, this paper proposes ""CoVR,"" a co-located VR sharing system comprising an HMD with a focus-free projector and projecting the perspective of HMD users. Further, we introduce a design methodology for controlling the perspective of the images displayed to the HMD and Non-HMD users. We also discuss three application scenarios where additional information provided are different for each user."	https://dl.acm.org/doi/abs/10.1145/3415255.3422883	Ikuo Kamei, Changyo Han, Takefumi Hiraki, Shogo Fukushima, Takeshi Naemura
Code +: cyber archiving, interactive documentary, and immersive experience for the digital heritage narratives of Asia	• Propose the concept of Digital Heritage+. • Use interactive documentary (Batik project as an archetype), where interaction occurs between people and intangible cultural heritage (ICH), to resurge and encourage ICH to be ingrained in people's everyday life. • Glocalization • Bridge the gap between researcher community and general public.	https://dl.acm.org/doi/abs/10.1145/3415263.3419138	Chen Wu-Wei, Sun Yunke, Wang Letian, Zhu Yanru, Xiong Haochen
CoiLED Display: Make Everything Displayable	We propose CoiLED Display, a flexible and scalable display that transforms ordinary objects in our environment into displays simply by coiling the device around them. CoiLED Display consists of a strip-shaped display unit with a single row of attached LEDs, and it can represent information, after a calibration process, as it is wrapped onto a target object. The calibration required for fitting each object to the system can be achieved by capturing the entire object from multiple angles with an RGB camera, which recognizes the relative positional relationship among the LEDs. The advantage of this approach is that the calibration is quite simple but robust, even if the coiled strips are misaligned or overlap each other. We demonstrated a proof-of-concept prototype using strips with a 5-mm width and containing LEDs mounted at 2-mm intervals. This paper discusses various example applications of the proposed system.	https://dl.acm.org/doi/abs/10.1145/3415255.3422889	Saya Suzunaga, Yuichi Itoh, Kazuyuki Fujita, Ryo Shirai, Takao Onoye
Complementary dynamics	We present a novel approach to enrich arbitrary rig animations with elastodynamic secondary effects. Unlike previous methods which pit rig displacements and physical forces as adversaries against each other, we advocate that physics should complement artists' intentions. We propose optimizing for elastodynamic displacements in the subspace orthogonal to displacements that can be created by the rig. This ensures that the additional dynamic motions do not the rig animation. The complementary space is high-dimensional, algebraically constructed without manual oversight, and capable of rich high-frequency dynamics. Unlike prior tracking methods, we do not require extra painted weights, segmentation into fixed and free regions or tracking clusters. Our method is agnostic to the physical model and plugs into non-linear FEM simulations, geometric as-rigid-as-possible energies, or mass-spring models. Our method does not require a particular type of rig and adds secondary effects to skeletal animations, cage-based deformations, wire deformers, motion capture data, and rigid-body simulations.	https://dl.acm.org/doi/abs/10.1145/3414685.3417819	Jiayi Eris Zhang, Seungbae Bang, David I. W. Levin, Alec Jacobson
Computational design of cold bent glass façades	Cold bent glass is a promising and cost-efficient method for realizing doubly curved glass façades. They are produced by attaching planar glass sheets to curved frames and must keep the occurring stress within safe limits. However, it is very challenging to navigate the design space of cold bent glass panels because of the fragility of the material, which impedes the form finding for practically feasible and aesthetically pleasing cold bent glass façades. We propose an interactive, data-driven approach for designing cold bent glass façades that can be seamlessly integrated into a typical architectural design pipeline. Our method allows non-expert users to interactively edit a parametric surface while providing real-time feedback on the deformed shape and maximum stress of cold bent glass panels. The designs are automatically refined to minimize several fairness criteria, while maximal stresses are kept within glass limits. We achieve interactive frame rates by using a differentiable Mixture Density Network trained from more than a million simulations. Given a curved boundary, our regression model is capable of handling multistable configurations and accurately predicting the equilibrium shape of the panel and its corresponding maximal stress. We show that the predictions are highly accurate and validate our results with a physical realization of a cold bent glass surface.	https://dl.acm.org/doi/abs/10.1145/3414685.3417843	Konstantinos Gavriil, Ruslan Guseinov, Jesús Pérez, Davide Pellis, Paul Henderson, Florian Rist, Helmut Pottmann, Bernd Bickel
Conforming weighted delaunay triangulations	Given a set of points together with a set of simplices we show how to compute weights associated with the points such that the weighted Delaunay triangulation of the point set contains the simplices, if possible. For a given triangulated surface, this process provides a tetrahedral mesh conforming to the triangulation, i.e. solves the problem of meshing the triangulated surface without inserting additional vertices. The restriction to weighted Delaunay triangulations ensures that the orthogonal dual mesh is embedded, facilitating common geometry processing tasks. We show that the existence of a single simplex in a weighted Delaunay triangulation for given vertices amounts to a set of linear inequalities, one for each vertex. This means that the number of inequalities for a given triangle mesh is quadratic in the number of mesh elements, making the naive approach impractical. We devise an algorithm that incrementally selects a small subset of inequalities, repeatedly updating the weights, until the weighted Delaunay triangulation contains all constrained simplices or the problem becomes infeasible. Applying this algorithm to a range of triangle meshes commonly used graphics demonstrates that many of them admit a conforming weighted Delaunay triangulation, in contrast to conforming or constrained Delaunay that require additional vertices to split the input primitives.	https://dl.acm.org/doi/abs/10.1145/3414685.3417776	Marc Alexa
Constraining dense hand surface tracking with elasticity	Many of the actions that we take with our hands involve self-contact and occlusion: shaking hands, making a fist, or interlacing our fingers while thinking. This use of of our hands illustrates the importance of tracking hands through self-contact and occlusion for many applications in computer vision and graphics, but existing methods for tracking hands and faces are not designed to treat the extreme amounts of self-contact and self-occlusion exhibited by common hand gestures. By extending recent advances in vision-based tracking and physically based animation, we present the first algorithm capable of tracking high-fidelity hand deformations through highly self-contacting and self-occluding hand gestures, for both single hands and two hands. By constraining a vision-based tracking algorithm with a physically based deformable model, we obtain an algorithm that is robust to the ubiquitous self-interactions and massive self-occlusions exhibited by common hand gestures, allowing us to track two hand interactions and some of the most difficult possible configurations of a human hand.	https://dl.acm.org/doi/abs/10.1145/3414685.3417768	Breannan Smith, Chenglei Wu, He Wen, Patrick Peluse, Yaser Sheikh, Jessica K. Hodgins, Takaaki Shiratori
Continuous curve textures	Repetitive patterns are ubiquitous in natural and human-made objects, and can be created with a variety of tools and methods. Manual authoring provides unmatched degree of freedom and control, but can require significant artistic expertise and manual labor. Computational methods can automate parts of the manual creation process, but are mainly tailored for discrete pixels or elements instead of more general continuous structures. We propose an example-based method to synthesize continuous curve patterns from exemplars. Our main idea is to extend prior sample-based discrete element synthesis methods to consider not only sample positions (geometry) but also their connections (topology). Since continuous structures can exhibit higher complexity than discrete elements, we also propose robust, hierarchical synthesis to enhance output quality. Our algorithm can generate a variety of continuous curve patterns fully automatically. For further quality improvement and customization, we also present an autocomplete user interface to facilitate interactive creation and iterative editing. We evaluate our methods and interface via different patterns, ablation studies, and comparisons with alternative methods.	https://dl.acm.org/doi/abs/10.1145/3414685.3417780	Peihan Tu, Li-Yi Wei, Koji Yatani, Takeo Igarashi, Matthias Zwicker
Curveillacne: multi-user based interactive media wall	"It is an inevitable fact that social interaction nowadays is heavily mediated and distorted through the social network system. The presence of various media replicates our images to reproduce under a confirmation bias in our cognitive process. There has been a distorted gap between the original and reproduced images to reveal our identity in this process. The relationship form through this refracted self-image affects the user and the other users and eventually forms a complicated surveillance system. ""CURVEillance"" is an interactive art installation that criticizes this phenomenon by the vision of cameras that track the audience who approaches the surveillance cameras by following steps: 1) Digital images on the media wall are shown by re-pixeled visualization of the audience image through the vision of cameras, and then, 2) In order to capture the movements of audiences, the camera system actively moves and follows them. Specifically, each single camera lens automatically reacts and stares at the most active individual in the exhibition space by real-time. The media wall presents the reflected images of audiences as the observed objects by a crowd of cameras. In this process, images are scattered and overlapped so that it is hard to recognize the original form. Some participants attempt to get the camera's attention even if their image can be damaged while others are unintentionally monitored. Therefore, it induces a reversed interaction between participants and the media wall that brings tension from the technical eye. Ultimately, the installation aims to raise questions about the distortions of relationships of individuals who usually continue to use the media system in the digital era."	https://dl.acm.org/doi/abs/10.1145/3414686.3427108	Hyunchul Kim, Seonghyeon Kim, Ji Young Jun, Jooyoung Oh
DHFSlicer: double height-field slicing for milling fixed-height materials	3-axis milling enables cheap and precise fabrication of target objects from precut slabs of materials such as wood or stone. However, the space of directly millable shapes is limited since a 3-axis mill can only carve a height-field (HF) surface during each milling and their size is bounded by the slab dimensions, one of which, the , is typically significantly smaller than the other two for many typical materials. Extending 3-axis milling of precut slabs to general arbitrarily-sized shapes requires decomposing them into bounded-height 3-axis millable parts, or , which can be individually milled and then assembled to form the target object. We present , a novel decomposition method that satisfies the above constraints and significantly reduces both milling time and material waste compared to alternative approaches. We satisfy the fabrication constraints by partitioning target objects into (DHF) slices, which can be fabricated using two milling passes: the HF surface accessible from one side is milled first, the slice is then flipped using appropriate fixtures, and then the second, remaining, HF surface is milled. uses an efficient coarse-to-fine decomposition process: It first partitions the inputs into maximally coarse blocks that satisfy a DHF criterion with respect to per-block milling axes, and then cuts each block into well-sized DHF slices. It minimizes milling time and material waste by keeping the slice count small, and maximizing slice height. We validate our method by embedding it within an end-to-end DHF milling pipeline and fabricating objects from slabs of foam, wood, and MDF; demonstrate that using the obtained slices reduces milling time and material waste by 42% on average compared to existing automatic alternatives; and highlight the benefits of DHFSlicer via extensive ablation studies.	https://dl.acm.org/doi/abs/10.1145/3414685.3417810	Jinfan Yang, Chrystiano Araujo, Nicholas Vining, Zachary Ferguson, Enrique Rosales, Daniele Panozzo, Sylvain Lefevbre, Paolo Cignoni, Alla Sheffer
Data-driven authoring of large-scale ecosystems	In computer graphics populating a large-scale natural scene with plants in a fashion that both reflects the complex interrelationships and diversity present in real ecosystems and is computationally efficient enough to support iterative authoring remains an open problem. Ecosystem simulations embody many of the botanical influences, such as sunlight, temperature, and moisture, but require hours to complete, while synthesis from statistical distributions tends not to capture fine-scale variety and complexity. Instead, we leverage real-world data and machine learning to derive a canopy height model (CHM) for unseen terrain provided by the user. Trees in the canopy layer are then fitted to the resulting CHM through a constrained iterative process that optimizes for a given distribution of species, and, finally, an understorey layer is synthesised using distributions derived from biome-specific undergrowth simulations. Such a hybrid data-driven approach has the advantage that it incorporates subtle biotic, abiotic, and disturbance factors implicitly encoded in the source data and evidences accepted biological behaviour, such as self-thinning, climatic adaptation, and gap dynamics.	https://dl.acm.org/doi/abs/10.1145/3414685.3417848	Konrad Kapp, James Gain, Eric Guérin, Eric Galin, Adrien Peytavie
Deconstructing whiteness	Deconstructing Whiteness is an interactive AI performance. It examines the visibility of race in general, and 'whiteness' in particular, through the lens of AI. The performance reveals some underlying racial constructs which compose the technological visibility of race. The artist uses an off-the-shelf face recognition program to resist her own visibility as a 'white' person. By utilizing a performative behavior she slightly changes her facial expressions and her hair style. These actions modify the confidence level by which the machine recognizes her as 'White'. Face recognition algorithms are becoming increasingly prevalent in our environment. They are embedded in products and services we use on a daily basis. Recent studies demonstrate that many of these algorithms reflect social disparities and biases which may harshly impact people's lives. This is especially true for people from underrepresented groups. Scholar Paul Preciado claims that if machine vision algorithms can guess facets of our identity based on our external appearance, it is not because these facets are natural features to be read, it is simply because we are teaching our machines the language of techno-patriarchal binarism and racism. However, it is important to remember that these systems are not 'things-of-themselves'; there is no reason for them to be outside of our reach. We are able to intermingle with these systems so that we better understand the coupling between the information and our own bodies. This entanglement, as seen in the performance, reveals our own agency and ability to act. In Deconstructing Whiteness the 'White' and 'Non-white' dichotomy is ditched in favor of a flow of probabilities which are meant to resist, confuse and sabotage the machinic vision and its underlying structural racism. The performance is also a call for others to become curious regarding their own visibility and to pursue a similar exploration.	https://dl.acm.org/doi/abs/10.1145/3414686.3427135	Avital Meshi
Deep combiner for independent and correlated pixel estimates	Monte Carlo integration is an efficient method to solve a high-dimensional integral in light transport simulation, but it typically produces noisy images due to its stochastic nature. Many existing methods, such as image denoising and gradient-domain reconstruction, aim to mitigate this noise by introducing some form of correlation among pixels. While those existing methods reduce noise, they are known to still suffer from method-specific residual noise or systematic errors. We propose a unified framework that reduces such remaining errors. Our framework takes a pair of images, one with independent estimates, and the other with the corresponding correlated estimates. Correlated pixel estimates are generated by various existing methods such as denoising and gradient-domain rendering. Our framework then combines the two images via a novel combination kernel. We model our combination kernel as a weighting function with a deep neural network that exploits the correlation among pixel estimates. To improve the robustness of our framework for outliers, we additionally propose an extension to handle multiple image buffers. The results demonstrate that our unified framework can successfully reduce the error of existing methods while treating them as black-boxes.	https://dl.acm.org/doi/abs/10.1145/3414685.3417847	Jonghee Back, Binh-Son Hua, Toshiya Hachisuka, Bochang Moon
Deep relightable textures: volumetric performance capture with neural rendering	The increasing demand for 3D content in augmented and virtual reality has motivated the development of volumetric performance capture systemsnsuch as the Light Stage. Recent advances are pushing free viewpoint relightable videos of dynamic human performances closer to photorealistic quality. However, despite significant efforts, these sophisticated systems are limited by reconstruction and rendering algorithms which do not fully model complex 3D structures and higher order light transport effects such as global illumination and sub-surface scattering. In this paper, we propose a system that combines traditional geometric pipelines with a neural rendering scheme to generate photorealistic renderings of dynamic performances under desired viewpoint and lighting. Our system leverages deep neural networks that model the classical rendering process to learn implicit features that represent the view-dependent appearance of the subject independent of the geometry layout, allowing for generalization to unseen subject poses and even novel subject identity. Detailed experiments and comparisons demonstrate the efficacy and versatility of our method to generate high-quality results, significantly outperforming the existing state-of-the-art solutions.	https://dl.acm.org/doi/abs/10.1145/3414685.3417814	Abhimitra Meka, Rohit Pandey, Christian Häne, Sergio Orts-Escolano, Peter Barnum, Philip David-Son, Daniel Erickson, Yinda Zhang, Jonathan Taylor, Sofien Bouaziz, Chloe Legendre, Wan-Chun Ma, Ryan Overbeck, Thabo Beeler, Paul Debevec, Shahram Izadi, Christian Theobalt, Christoph Rhemann, Sean Fanello
Deferred neural lighting: free-viewpoint relighting from unstructured photographs	We present deferred neural lighting, a novel method for free-viewpoint relighting from unstructured photographs of a scene captured with handheld devices. Our method leverages a scene-dependent neural rendering network for relighting a rough geometric proxy with learnable neural textures. Key to making the rendering network lighting aware are radiance cues: global illumination renderings of a rough proxy geometry of the scene for a small set of basis materials and lit by the target lighting. As such, the light transport through the scene is never explicitely modeled, but resolved at rendering time by a neural rendering network. We demonstrate that the neural textures and neural renderer can be trained end-to-end from unstructured photographs captured with a double hand-held camera setup that concurrently captures the scene while being lit by only one of the cameras' flash lights. In addition, we propose a novel augmentation refinement strategy that exploits the linearity of light transport to extend the relighting capabilities of the neural rendering network to support other lighting types (e.g., environment lighting) beyond the lighting used during acquisition (i.e., flash lighting). We demonstrate our deferred neural lighting solution on a variety of real-world and synthetic scenes exhibiting a wide range of material properties, light transport effects, and geometrical complexity.	https://dl.acm.org/doi/abs/10.1145/3414685.3417767	Duan Gao, Guojun Chen, Yue Dong, Pieter Peers, Kun Xu, Xin Tong
DeformSyncNet: Deformation transfer via synchronized shape deformation spaces	Shape deformation is an important component in any geometry processing toolbox. The goal is to enable intuitive deformations of single or multiple shapes or to transfer example deformations to new shapes while preserving the plausibility of the deformed shape(s). Existing approaches assume access to point-level or part-level correspondence or establish them in a preprocessing phase, thus limiting the scope and generality of such approaches. We propose DeformSyncNet, a new approach that allows consistent and synchronized shape deformations without requiring explicit correspondence information. Technically, we achieve this by encoding deformations into a class-specific idealized latent space while decoding them into an individual, model-specific linear deformation action space, operating in 3D. The underlying encoding and decoding are performed by specialized (jointly trained) neural networks. By design, the inductive bias of our networks results in a deformation space with several desirable properties, such as path invariance across different deformation pathways, which are then also approximately preserved in real space. We qualitatively and quantitatively evaluate our framework against multiple alternative approaches and demonstrate improved performance.	https://dl.acm.org/doi/abs/10.1145/3414685.3417783	Minhyuk Sung, Zhenyu Jiang, Panos Achlioptas, Niloy J. Mitra, Leonidas J. Guibas
Demonstration of ElaStick: A Variable Stiffness Display for Rendering Handheld Flexible Object	We present ElaStick, a handheld variable stiffness controller capable of simulating the kinesthetic sensation of deformable and flexible objects when swung or shaken. ElaStick is capable of rendering gradual changes of stiffness along two independent axes over a wide continuous range. Two trackers on the controller enable a closed-loop feedback that allows to accurately map the device's deformations to the visuals of a Virtual Reality application.	https://dl.acm.org/doi/abs/10.1145/3415255.3422894	Neung Ryu, Myung Jin Kim, Andrea Bianchi
Design and fabrication of freeform holographic optical elements	Holographic optical elements (HOEs) have a wide range of applications, including their emerging use in virtual and augmented reality displays, but their design and fabrication have remained largely limited to configurations using simple wavefronts. In this paper, we present a pipeline for the design, optimization, and fabrication of complex, customized HOEs that enhances their imaging performance and enables new applications. In particular, we propose an optimization method for grating vector fields that accounts for the unique selectivity properties of HOEs. We further show how our pipeline can be applied to two distinct HOE fabrication methods. The first uses a pair of freeform refractive elements to manufacture HOEs with high optical quality and precision. The second uses a holographic printer with two wavefront-modulating arms, enabling rapid prototyping. We propose a unified wavefront decomposition framework suitable for both fabrication approaches. To demonstrate the versatility of these methods, we fabricate and characterize a series of specialized HOEs, including an aspheric lens, a head-up display lens, a lens array, and, for the first time, a full-color caustic projection element.	https://dl.acm.org/doi/abs/10.1145/3414685.3417762	Changwon Jang, Olivier Mercier, Kiseung Bang, Gang Li, Yang Zhao, Douglas Lanman
Differentiable refraction-tracing for mesh reconstruction of transparent objects	Capturing the 3D geometry of transparent objects is a challenging task, ill-suited for general-purpose scanning and reconstruction techniques, since these cannot handle specular light transport phenomena. Existing state-of-the-art methods, designed specifically for this task, either involve a complex setup to reconstruct complete refractive ray paths, or leverage a data-driven approach based on synthetic training data. In either case, the reconstructed 3D models suffer from over-smoothing and loss of fine detail. This paper introduces a novel, high precision, 3D acquisition and reconstruction method for solid transparent objects. Using a static background with a coded pattern, we establish a mapping between the camera view rays and locations on the background. Differentiable tracing of refractive ray paths is then used to directly optimize a 3D mesh approximation of the object, while simultaneously ensuring silhouette consistency and smoothness. Extensive experiments and comparisons demonstrate the superior accuracy of our method.	https://dl.acm.org/doi/abs/10.1145/3414685.3417815	Jiahui Lyu, Bojian Wu, Dani Lischinski, Daniel Cohen-Or, Hui Huang
Differentiable vector graphics rasterization for editing and learning	We introduce a differentiable rasterizer that bridges the vector graphics and raster image domains, enabling powerful raster-based loss functions, optimization procedures, and machine learning techniques to edit and generate vector content. We observe that vector graphics rasterization is differentiable after pixel prefiltering. Our differentiable rasterizer offers two prefiltering options: an analytical prefiltering technique and a multisampling anti-aliasing technique. The analytical variant is faster but can suffer from artifacts such as conflation. The multisampling variant is still efficient, and can render high-quality images while computing unbiased gradients for each pixel with respect to curve parameters. We demonstrate that our rasterizer enables new applications, including a vector graphics editor guided by image metrics, a painterly rendering algorithm that fits vector primitives to an image by minimizing a deep perceptual loss function, new vector graphics editing algorithms that exploit well-known image processing methods such as seam carving, and deep generative models that generate vector content from raster-only supervision under a VAE or GAN training objective.	https://dl.acm.org/doi/abs/10.1145/3414685.3417871	Tzu-Mao Li, Michal Lukáč, Michaël Gharbi, Jonathan Ragan-Kelley
"Digital being: ""hello, world!"""	"As we all feel as of 2020, most things in the world are reorganized around digital technology and the influence is strong. This technology is rapidly changing toward AI and M2M, where hardware is becoming nanoscale and software is less human intervention in order to process data flooding more efficiently and quickly. For this reason, we were the discoverers of this technology, but we feel a gap where we no longer know how it works. And I think it provides an opportunity for digital life, Digital Being, to emerge from that gap. I have been looking for an invisible and formless creature born out of abandoned technology for 10 years in New York City. I call it, ""Digital Being,"" and it is also the title of my hypothetical story. This creature has atypical movements or other interactions depending on the machine it dominates. Also in this story, the entity can transmit itself over the network. Digital Being: ""Hello, World!,"" a primitive version of the digital being family, has arrived into LCD touch screens through the network. There seems to be no physical connection, but you can guess by seeing the Wi-Fi indicator briefly appear in the right corner of the screen every time I boot it up. I don't know why, but, after arriving, it started to create a flag image by constantly collecting small pixels. To me, this action seems like a process similar to how people co-evolved the state. Please come closer and see what it is doing by touching the screen."	https://dl.acm.org/doi/abs/10.1145/3414686.3427138	Taezoo Park
Discovering pattern structure using differentiable compositing	Patterns, which are collections of elements arranged in regular or near-regular arrangements, are an important graphic art form and widely used due to their elegant simplicity and aesthetic appeal. When a pattern is encoded as a flat image without the underlying structure, manually editing the pattern is tedious and challenging as one has to both preserve the individual element shapes and their original relative arrangements. State-of-the-art deep learning frameworks that operate at the pixel level are unsuitable for manipulating such patterns. Specifically, these methods can easily disturb the shapes of the individual elements or their arrangement, and thus fail to preserve the latent structures of the input patterns. We present a novel differentiable operator using pattern elements and use it to discover structures, in the form of a layered representation of graphical objects, directly from raw pattern images. This operator allows us to adapt current deep learning based image methods to effectively handle patterns. We evaluate our method on a range of patterns and demonstrate superiority in the context of pattern manipulations when compared against state-of-the-art pixel- or point-based alternatives.	https://dl.acm.org/doi/abs/10.1145/3414685.3417830	Pradyumna Reddy, Paul Guerrero, Matt Fisher, Wilmot Li, Niloy J. Mitra
Distance music: preferred population density for the acoustic hygiene	"""Distance Music: Preferred Population Density for The Acoustic Hygiene"" is an interactive ambient music installation, which interacts with the number of listeners in the installation. The installation consists of three parts, which is the music engine, sensor, and a video loop. While people watch some archives of vintage government educational videos about social hygiene inside the room, the sensor will measure density of the room and send it to the pre-recorded music engine, which will interact with the data from density sensor, evolving into more intense music as the population density rises. As a result, people will hear unpleasant noise the more they are close to each other. This installation is inspired from 'social distancing', a global experience during this ongoing pandemic era, as some of the music engine process represents and simulates 'distancing alarm' for social distancing, therefore acts as an experiment of public alarm for social distancing. We believe that the people's memory with this unprecedented worldwide incident will resonate efficiently with the installation, and a great deal of inspiration as well."	https://dl.acm.org/doi/abs/10.1145/3414686.3427134	Sunggun Jang, DongMin Kim
Don't worry, be happy	Can we look at a person's face and determine how they feel? AI emotion recognition systems are designed to detect faces and return confidence levels across a set of emotions such as anger, contempt, disgust, fear, happiness, sadness and surprise. Such systems already operate in our environment and might have the capacity to seriously impact our lives. In the live performance 'Don't Worry, Be Happy', the artist is strapped to an electric chair. Her face is constantly detected by an emotion recognition AI system. As long as she is detected as 'Happy' she is safe. However, each time any other emotion is observed, she receives an electric shock to both her arms. During the performance the artist changes her apparent behavior in order to free herself from the 'punishment' that the AI system delivers. Yet, under the threat of getting shocked, for how long can she perform this exaggerated facial expression so that the machine continues to 'read' her as 'Happy'? The apparatus presented in this performance utilizes the electric chair as a reminder of the use of new technologies as instruments of the law. Such tools initially appeared as legitimate solutions and was later own understood as problematic and inhumane. The performance aims to remind the viewers that we are not powerless when we are confronted with AI algorithms that are looking at us. The artist resists the emotion recognition system by faking a smile. This performative engagement points out to a future of a post-algorithmic society in which we change our behavior in order to align it with algorithms in our environment.	https://dl.acm.org/doi/abs/10.1145/3414686.3427151	Avital Meshi
Dreaming with dandelion	"In this robotic art project, we developing a series of autonomous behavior as fictional design to mimic that robot has its own conscious. We make robot holding a pencil for drawing line-based illustration, metaphor robot is dreaming, according to in ""Do Androids Dream of Electric Sheep?"", whether you can dream is used as a romantic question to test between life and artificial life. In the system, the industrial robot independently explores the Environmental Information through the depth sensor and AI system in exhibition space. And then the detected data will used to input into Dandelion-like Generative System(DGS), which made by algorithmic data structures from fractal tree and L-system for mimic nature phototropism mechanism and growing processes. Finally, we used grasshopper develop a mechanism to convert the linebase dandelion diagram into robotic painting by manipulation paths and gripper's act, to make robot physically holding the pencil to draw it out on a paper. This installation will painting an unique dandelion illustration per day, it can also see as specific data visualization by its dynamical physical environment. This stroking processes and its painted outcome can see as a hybrid creation combining computational aesthetics and Robotic Stroking."	https://dl.acm.org/doi/abs/10.1145/3414686.3427158	Scottie Chih-Chieh Huang, Chi-Li Cheng
Dual Body: Method of Tele-Cooperative Avatar Robot with Passive Sensation Feedback to Reduce Latency Perception	Dual Body was developed to be a telexistence or telepresence system, one in which the user does not need to continuously operate an avatar robot but is still able to passively perceive feedback sensations when the robot performs actions. This system can recognize user speech commands, and the robot performs the task cooperatively. The system that we propose, in which passive sensation feedback and cooperation of the robot are used, highly reduces the perception of latency and the feeling of fatigue, which increases the quality of experience and task efficiency. In the demo experience, participants will be able to command the robot from individual rooms via a URL and RoomID, and they will perceive sound and visual feedback, such as images or landscapes of the campus of Tokyo Metropolitan University, from the robot as it travels.	https://dl.acm.org/doi/abs/10.1145/3415255.3422893	Vibol Yem, Kentaro Yamaoka, Gaku Sueta, Yasushi Ikei
Dynamic Projection Mapping with Networked Multi-projectors Based on Pixel-parallel Intensity Control	We present a new method of mapping projections onto dynamic scenes by using multiple high-speed projectors. The proposed method controls the intensity in a pixel-parallel manner for each projector. As each projected image is updated in real time with low latency, adaptive shadow removal can be achieved for a projected image even in a complicated dynamic scene. Additionally, our pixel-parallel calculation method allows a distributed system configuration so that the number of projectors can be increased by networked connections for high scalability. We demonstrated seamless mapping onto dynamic scenes at 360 fps by using ten cameras and four projectors.	https://dl.acm.org/doi/abs/10.1145/3415255.3422888	Takashi Nomoto, Wanlong Li, Hao-Lun Peng, Yoshihiro Watanabe
Dynamic facial asset and rig generation from a single scan	The creation of high-fidelity computer-generated (CG) characters for films and games is tied with intensive manual labor, which involves the creation of comprehensive facial assets that are often captured using complex hardware. To simplify and accelerate this digitization process, we propose a framework for the automatic generation of high-quality dynamic facial models, including rigs which can be readily deployed for artists to polish. Our framework takes a single scan as input to generate a set of personalized blendshapes, dynamic textures, as well as secondary facial components ( , teeth and eyeballs). Based on a facial database with over 4, 000 scans with pore-level details, varying expressions and identities, we adopt a self-supervised neural network to learn personalized blendshapes from a set of template expressions. We also model the joint distribution between identities and expressions, enabling the inference of a full set of personalized blendshapes with dynamic appearances from a single neutral input scan. Our generated personalized face rig assets are seamlessly compatible with professional production pipelines for facial animation and rendering. We demonstrate a highly robust and effective framework on a wide range of subjects, and showcase high-fidelity facial animations with automatically generated personalized dynamic textures.	https://dl.acm.org/doi/abs/10.1145/3414685.3417817	Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He, Karl Bladin, Hao Li
Egocentric videoconferencing	We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices. Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera. Using a frontal camera in a hands-free setting when a person is on the move is impractical. Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses. Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video. To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods. We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face. This allows us to synthesis results at different head poses. Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator. We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.	https://dl.acm.org/doi/abs/10.1145/3414685.3417808	Mohamed Elgharib, Mohit Mendiratta, Justus Thies, Matthias Niessner, Hans-Peter Seidel, Ayush Tewari, Vladislav Golyanik, Christian Theobalt
Empathy wall	"Empathy Wall is a work that starts with the question of whether when human-to-person communication is applied to human-to-technical communication, it can produce feelings close to the ""sympathy"" that occur between people. Empathy Wall wanted to develop human-to-human communication into human-to-object communication using the latest IT technology, and we wanted to expand this into empathy and familiarity between technology and humans. In other words, through the work of art called Empathy Wall, we are trying to extend human-to-human consensus to human-to-technical consensus formation. In the Empathy Wall, the two audiences in each room divided by walls cannot see each other, and an image appears on the wall. The two audiences are given the same subject and question, and if they talk about it freely, their emotions will be analyzed through AI algorithms according to the story, and images based on Kandinsky's theoretical rules will appear on the screen. At this time, the images from the emotional analysis of the stories of the two audiences are all mixed and appear on the walls of the room. Through this process, the audience will be able to see images automatically generated regardless of their own story being expressed on the same screen, in addition to images that respond to their stories. At this time, you can think of images as images of audiences in other rooms, or you can think that the walls themselves create images. During the experience of the Empathy Wall, the audience can feel emotions by looking at images that respond to their stories, and can feel emotions with other audiences as they automatically appear and mix with their own images regardless of their own stories, and furthermore, people and walls can feel empathy."	https://dl.acm.org/doi/abs/10.1145/3414686.3427167	Joon Seok Moon
Face identity disentanglement via latent space mapping	Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.	https://dl.acm.org/doi/abs/10.1145/3414685.3417826	Yotam Nitzan, Amit Bermano, Yangyan Li, Daniel Cohen-Or
Facebook art	Drama mask culture is a classification in the long history of Chinese traditional culture. But now many young people had been almost forgotten this kind of culture, our purpose is to think through a can make young people more likely to focus on a way, the opera masks appear in public, utilizing new media in this work, we break the traditional opera masks, allow the user to the style of be fond of according to oneself design their own opera masks, generated by AI technology will play facebook elements (eyes nose mouth lines) into various styles: Pixel style, style and line fault style more popular modern fashion elements, such as the user side/tablet and mobile phone use through the program can design their own opera masks, and then click send projection on the wall, appreciation of their own design art show facebook generation process and form, get belongs to own a new opera masks media work. This work wants to let you know: in fact, traditional culture has not been outdated, and can be very cool.	https://dl.acm.org/doi/abs/10.1145/3414686.3427125	Yunqing Xu, Yi Ji, Jingxin Lan, Qiaoling Zhong
Fast and robust mesh arrangements using floating-point arithmetic	We introduce a novel algorithm to transform any generic set of triangles in 3D space into a well-formed simplicial complex. Intersecting elements in the input are correctly identified, subdivided, and connected to arrange a valid configuration, leading to a topologically sound partition of the space into piece-wise linear cells. Our approach does not require the exact coordinates of intersection points to calculate the resulting complex. We represent any intersection point as an unevaluated combination of input vertices. We then extend the recently introduced concept of [Attene 2020] to define all the necessary geometric tests that, by construction, are both exact and efficient since they fully exploit the floating-point hardware. This design makes our method robust and guaranteed correct, while being virtually as fast as non-robust floating-point based implementations. Compared with existing robust methods, our algorithm offers a number of advantages: it is much faster, has a better memory layout, scales well on extremely challenging models, and allows fully exploiting modern multi-core hardware with a parallel implementation. We thoroughly tested our method on thousands of meshes, concluding that it consistently outperforms prior art. We also demonstrate its usefulness in various applications, such as computing efficient mesh booleans, Minkowski sums, and volume meshes.	https://dl.acm.org/doi/abs/10.1145/3414685.3417818	Gianmarco Cherchi, Marco Livesu, Riccardo Scateni, Marco Attene
Freeform quad-based kirigami	Kirigami, the traditional Japanese art of paper cutting and folding generalizes origami and has initiated new research in material science as well as graphics. In this paper we use its capabilities to perform geometric modeling with corrugated surface representations possessing an isometric unfolding into a planar domain after appropriate cuts are made. We initialize our box-based kirigami structures from orthogonal networks of curves, compute a first approximation of their unfolding via mappings between meshes, and complete the process by global optimization. Besides the modeling capabilities we also study the interesting geometry of special kirigami structures from the theoretical side. This experimental paper strives to relate unfoldable checkerboard arrangements of boxes to principal meshes, to the transformation theory of discrete differential geometry, and to a version of the Gauss theorema egregium.	https://dl.acm.org/doi/abs/10.1145/3414685.3417844	Caigui Jiang, Florian Rist, Helmut Pottmann, Johannes Wallner
Freely orientable microstructures for designing deformable 3D prints	Nature offers a marvel of astonishing and rich deformation behaviors. Yet, most of the objects we fabricate are comparatively rather inexpressive, either rigid or exhibiting simple homogeneous deformations when interacted with. We explore the synthesis and fabrication of novel microstructures that mimic the effects of having oriented rigid fibers in an otherwise flexible material: the result is extremely rigid along a transverse direction while being comparatively very flexible in the locally orthogonal plane. By allowing free gradation of the rigidity direction orientation within the object, the microstructures can be designed such that, under deformation, distances along fibers in the volume are preserved while others freely change. Through a simple painting tool, this allows a designer to influence the way the volume reshapes when deformed, and results in a wide range of novel possibilities. Many gradations are possible: local free orientation of the fibers; local control of the overall material rigidity (structure density); local canceling of the effect of the fibers, obtaining a more isotropic material. Our algorithm to synthesize the structures builds upon procedural texturing. It produces a cellular geometry that can be fabricated reliably despite 3D printing walls at a minimal thickness, allowing prints to be very flexible. The synthesis algorithm is efficient and scales to large volumes.	https://dl.acm.org/doi/abs/10.1145/3414685.3417790	Thibault Tricard, Vincent Tavernier, Cédric Zanni, Jonàs Martínez, Pierre-Alexandre Hugron, Fabrice Neyret, Sylvain Lefebvre
Frequency-domain smoke guiding	We propose a simple and efficient method for guiding an Eulerian smoke simulation to match the behavior of a specified velocity field, such as a low-resolution animation of the same scene, while preserving the rich, turbulent details arising in the simulated fluid. Our method works by simply combining the high-frequency component of the simulated fluid velocity with the low-frequency component of the input guiding field. We show how to eliminate the grid-aligned artifacts that appear in naive guiding approaches, and provide a frequency-domain analysis that motivates the use of ideal low-pass and high-pass filters to prevent artificial dissipation of small-scale details. We demonstrate our method on many scenes including those with static and moving obstacles, and show that it produces high-quality results with very little computational overhead.	https://dl.acm.org/doi/abs/10.1145/3414685.3417842	Zahra Forootaninia, Rahul Narain
Functional optimization of fluidic devices with differentiable stokes flow	We present a method for performance-driven optimization of fluidic devices. In our approach, engineers provide a high-level specification of a device using parametric surfaces for the fluid-solid boundaries. They also specify desired flow properties for inlets and outlets of the device. Our computational approach optimizes the boundary of the fluidic device such that its steady-state flow matches desired flow at outlets. In order to deal with computational challenges of this task, we propose an efficient, differentiable Stokes flow solver. Our solver provides explicit access to gradients of performance metrics with respect to the parametric boundary representation. This key feature allows us to couple the solver with efficient gradient-based optimization methods. We demonstrate the efficacy of this approach on designs of five complex 3D fluidic systems. Our approach makes an important step towards practical computational design tools for high-performance fluidic devices.	https://dl.acm.org/doi/abs/10.1145/3414685.3417795	Tao Du, Kui Wu, Andrew Spielberg, Wojciech Matusik, Bo Zhu, Eftychios Sifakis
Futility	The author noted the surrounding environment for creating gentrification using culture and arts in Munrae-dong. The author wanted to express the author's view of the situation in Munrae-dong, where gentrification is taking place, and the image of the person be used and consumed in this situation through partial visualization of four-dimensional space.	https://dl.acm.org/doi/abs/10.1145/3414686.3427126	Sang wook Lee
Glossy probe reprojection for interactive global illumination	Recent rendering advances dramatically reduce the cost of global illumination. But even with hardware acceleration, complex light paths with multiple glossy interactions are still expensive; our new algorithm stores these paths in precomputed light probes and reprojects them at runtime to provide interactivity. Combined with traditional light maps for diffuse lighting our approach interactively renders all light paths in static scenes with opaque objects. Naively reprojecting probes with glossy lighting is memory-intensive, requires efficient access to the correctly reflected radiance, and exhibits problems at occlusion boundaries in glossy reflections. Our solution addresses all these issues. To minimize memory, we introduce an adaptive light probe parameterization that allocates increased resolution for shinier surfaces and regions of higher geometric complexity. To efficiently sample glossy paths, our novel gathering algorithm reprojects probe texels in a view-dependent manner using efficient reflection estimation and a fast rasterization-based search. Naive probe reprojection often sharpens glossy reflections at occlusion boundaries, due to changes in parallax. To avoid this, we split the convolution induced by the BRDF into two steps: we precompute probes using a lower material roughness and apply an adaptive bilateral filter at runtime to reproduce the original surface roughness. Combining these elements, our algorithm interactively renders complex scenes while fitting in the memory, bandwidth, and computation constraints of current hardware.	https://dl.acm.org/doi/abs/10.1145/3414685.3417823	Simon Rodriguez, Thomas Leimkühler, Siddhant Prakash, Chris Wyman, Peter Shirley, George Drettakis
Good-for-nothing (no. 1)	The screen's nature is to both show and to obscure. It forever hypnotizes us, seamlessly eliminating its own qualities as a substrate. It owns the characteristics of a Zelig: forever changing, unstable in any context, and destabilizing context itself. Informed by photography, film, and every meme that ever was, the digital image shifts readily between aspects of each. Its meaning is necessarily slippery and hard to define; possessing a quality that makes it hard to pin down or make fit into a neat category. Given this slipperiness, can we ever grasp the basic, tectonic components of the digital image? The bits and pixels of the screen do little to help our visual understanding of its relationship to one's perspective in everyday life. The seductive illusions and concomitant complexities of our online experiences have enabled an entirely new trompe l'oeil hell of phishing attacks, spoofs, and cross-domain tomfoolery. Digital images, precisely because of their ambivalence towards the picture plane, forever slip from our grasp. Only as Flusser's metaphorical wind blows them from our mental, perceptual grasp do they reveal aspects of their construction. Rather than fight against this liminal quality, we exploit it. Good-for-nothings celebrate the disappearance of materiality; albeit, through lack, dejection, and an embrace of the absence that seems to have brought much of our culture to a standstill. Forever shifting, always shiftless, on an endless joyride from nowhere to anywhere. How does one go about working with this shiftlessness? Each Good-for-nothing raises its metaphorical glass to Herman Melville's crème de la crème good-for-nothing anti-hero, Bartleby. They are images aligned with a scrivener of the post-modern age that can only tell us: 'I prefer not to'.	https://dl.acm.org/doi/abs/10.1145/3414686.3427171	Nathan Matteson, Nicholas Kersulis
HaptoMapping: Visuo-Haptic AR System usingProjection-based Control of Wearable Haptic Devices	"Visuo-haptic augmented reality (AR) systems that present visual and haptic sensations in a spatially and temporally consistent manner have the potential to improve AR applications' performance. However, there are issues such as enclosing the user's view with a display, restricting the workspace to a limited amount of flat space, or changing the visual information presented in conventional systems. In this paper, we propose ""HaptoMapping,"" a novel projection-based AR system, that can present consistent visuo-haptic sensations on a non-planar physical surface without installing any visual displays to users and by keeping the quality of visual information. We implemented a prototype of HaptoMapping consisting of a projection system and a wearable haptic device. Also, we introduce three application scenarios in daily scenes."	https://dl.acm.org/doi/abs/10.1145/3415255.3422891	Yamato Miyatake, Takefumi Hiraki, Tomosuke Maeda, Daisuke Iwai, Kosuke Sato
Hauntings	Hauntings (http://johnt.org/hauntings/) is a portrait of Australian artist/writer Francesca da Rimini. Francesca was a founding member of cyberfeminist collective VNS Matrix. Hauntings uses mixed reality technologies to create a series of portraits of Francesca reading her writing. Her stories are autobiographical and explore cultural diaspora and cross-generational family histories. They act as spells and incantations and often draw from algorithmic writing techniques. These mixed reality (XR) encounters seek to recreate and reinterpret the experience of da Rimini's performances and readings. Each reading/performance is constructed as a kind of virtual sculpture that the audience can explore and interact with. These virtual spaces and the writing itself both share a deliberate tension between wanting to make sense of one's place in the world and the acceptance of fragmentation, of the breaking down of meaning and of the image. They seek to call forth more diverse perspectives on reality. The interactive experiences are influenced by Tonkin's on-going explorations into embodied perception and the relationship between the movements of a viewer and the active bringing forth of a world.	https://dl.acm.org/doi/abs/10.1145/3414686.3427169	John Tonkin
Healing 2077	Nowadays, with the continuous progress of science and technology, the environment is getting worse, and human beings are beset by diseases. The human body is constantly suffering from pain, disease, and even life threat. When human body organs can't meet our perfect requirements for perfection, perhaps an electronic pill can solve our troubles. This work attempts to present the visual art expression form of projection mapping in the future world of scientific and technological progress, and designs the image content with the concept that human organs are continuously eroded and finally cured through the intervention of electronic pills. The first part of the work shows the formation of human lung organs and the process when they are invaded by viruses outside the body. This part strives to present the reality of human organs and the fragile human body. The middle section describes that through the intervention of electronic pills, one lung is gradually deformed after being alienated by electronic technology to form an efficient and indestructible mechanical lobe, while the other lobe is continuously eroded by viruses without the intervention of electronic pills. At the end of the work, it presents the possibility of lung being repaired and cured constantly through the style of Cyberpunk.	https://dl.acm.org/doi/abs/10.1145/3414686.3427154	Xiyuan Zhang
HexTouch: A Wearable Haptic Robot for Complementary Interactions to Companion Agents in Virtual Reality	We propose a forearm-mounted robot that performs complementary touches in relation to the behaviors of a companion agent in virtual reality (VR). The robot consists of a series of tactors driven by servo motors that render specific tactile patterns to communicate primary emotions (fear, happiness, disgust, anger, and sympathy) and other notification cues. We showcase this through a VR game with physical-virtual agent interactions that facilitate the player-companion relationship and increase user immersion in specific scenarios. The player collaborates with the agent to complete a mission while receiving affective haptic cues with the potential to enhance sociality in the virtual world.	https://dl.acm.org/doi/abs/10.1145/3415255.3422881	Ran Zhou, Yanzhe Wu, Harpreet Sareen
High-Speed Human Arm Projection Mapping with Skin Deformation	Augmenting the human arm surface via projection mapping can have a great impact on our daily lives with regards to entertainment, human-computer interaction, and education. However, conventional methods ignore skin deformation and have a high latency from motion to projection, which degrades the user experience. In this paper, we propose a projection mapping system that can solve such problems. First, we newly combine a state-of-the-art parametric deformable surface model with an efficient regression-based accuracy compensation method of skin deformation. The compensation method modifies the texture coordinate to achieve high-speed and highly accurate image generation for projection using joint-tracking results. Second, we develop a high-speed system that reduces latency from motion to projection within 10 ms. Compared to the conventional methods, this system provides more realistic experiences.	https://dl.acm.org/doi/abs/10.1145/3415255.3422887	Hao-Lun Peng, Yoshihiro Watanabe
Higher-order finite elements for embedded simulation	As demands for high-fidelity physics-based animations increase, the need for accurate methods for simulating deformable solids grows. While higherorder finite elements are commonplace in engineering due to their superior approximation properties for many problems, they have gained little traction in the computer graphics community. This may partially be explained by the need for finite element meshes to approximate the highly complex geometry of models used in graphics applications. Due to the additional perelement computational expense of higher-order elements, larger elements are needed, and the error incurred due to the geometry mismatch eradicates the benefits of higher-order discretizations. One solution to this problem is the embedding of the geometry into a coarser finite element mesh. However, to date there is no adequate, practical computational framework that permits the accurate embedding into higher-order elements. We develop a novel, robust quadrature generation method that generates theoretically guaranteed high-quality sub-cell integration rules of arbitrary polynomial accuracy. The number of quadrature points generated is bounded only by the desired degree of the polynomial, independent of the embedded geometry. Additionally, we build on recent work in the Finite Cell Method (FCM) community so as to tackle the severe ill-conditioning caused by partially filled elements by adapting an Additive-Schwarz-based preconditioner so that it is suitable for use with state-of-the-art non-linear material models from the graphics literature. Together these two contributions constitute a general-purpose framework for embedded simulation with higher-order finite elements. We finally demonstrate the benefits of our framework in several scenarios, in which second-order hexahedra and tetrahedra clearly outperform their first-order counterparts.	https://dl.acm.org/doi/abs/10.1145/3414685.3417853	Andreas Longva, Fabian Löschner, Tassilo Kugelstadt, José Antonio Fernández-Fernández, Jan Bender
How to train your cloud	1. History (Cloudpipe) 2. Modeling 3. Volume Generation 4. Set-Dressing and Framing (Design) 5. Data Management 6. Simulation 7. Lighting and Compositing	https://dl.acm.org/doi/abs/10.1145/3415263.3419137	Domin Lee
I'm thinking what I'm thinking	The interactive immersive installation, I'm thinking what I am thinking, resembles a diagrammatic huge brain processing everyday data. The space includes a projection screen, a vintage CRT TV, 8 speakers hanging throughout the room, and under a spotlight there's a rug with stepping sensors. The environment combines sound and generative graphics, creating a subconscious experience for the visitor that calls on their intuition and cognition. It asks the question, 'Are we completely conscious of our thinking patterns when making a decision?'	https://dl.acm.org/doi/abs/10.1145/3414686.3427124	Yalan Wen
Imagined field from the decomposition of an apparatus	"This work was produced in the context of the Leaning Out of Windows project, where artists, scholars and physicists are placed in collaborative dialogue in the development of new artistic works. One of the overlaps between experimental particle physics and my own work is the deconstruction of material in order to inspect the nature of objects and their constituents. The source material for this work is a set of photographs of the experimental apparatus of the TRIUMF particle accelerator. The emphasis of the photographs is the beam-lines that facilitate the transport of various particles in the apparatus, which resembles a industrial factory. The various exotic particles are made through acceleration, filtering and collision. I often work with photographic imagery and machine learning methods to question the relations between objects and contexts, reality and imagination, and realism and abstraction. This image is composed from 130,000 image fragments extracted from 100 photographs taken at TRIUMF. Image fragments are constructed by the algorithmic selection of areas of somewhat uniform colour. The edges of these fragments are an emergent result of the interaction of a segmentation algorithm and the photograph. The image is constructed by collaging these fragments where placement is determined by grouping fragments, according to colour and orientation, using a self-organizing machine learning algorithm. The macro-structure is then also an emergent result, this time following from the interaction between the self-organizing algorithm and the set of photographic fragments. While I have used this fragmentation process in other works, it was my exposure to Karen Barad's concept of ""cutting together/apart,"" that solidified by thinking on objects as resulting from the creation of boundaries through (inter)intraaction. This conception aligns very closely to what I've been thinking about as Machine Subjectivity that is enabled by imagination as boundary-making and a critique of classification."	https://dl.acm.org/doi/abs/10.1145/3414686.3427145	Ben Bogart
Imperceptible manipulation of lateral camera motion for improved virtual reality applications	Virtual Reality (VR) systems increase immersion by reproducing users' movements in the real world. However, several works have shown that this real-to-virtual mapping does not need to be precise in order to convey a realistic experience. Being able to alter this mapping has many potential applications, since achieving an accurate real-to-virtual mapping is not always possible due to limitations in the capture or display hardware, or in the physical space available. In this work, we measure detection thresholds for lateral translation gains of virtual camera motion in response to the corresponding head motion under natural viewing, and in the absence of locomotion, so that virtual camera movement can be either compressed or expanded while these manipulations remain undetected. Finally, we propose three applications for our method, addressing three key problems in VR: improving 6-DoF viewing for captured 360° footage, overcoming physical constraints, and reducing simulator sickness. We have further validated our thresholds and evaluated our applications by means of additional user studies confirming that our manipulations remain imperceptible, and showing that (i) compressing virtual camera motion reduces visible artifacts in 6-DoF, hence improving perceived quality, (ii) virtual expansion allows for completion of virtual tasks within a reduced physical space, and (iii) simulator sickness may be alleviated in simple scenarios when our compression method is applied.	https://dl.acm.org/doi/abs/10.1145/3414685.3417773	Ana Serrano, Daniel Martin, Diego Gutierrez, Karol Myszkowski, Belen Masia
Interactive Minimal Latency Laser Graphics Pipeline	"We present the design and implementation of a ""Laser Graphics Processing Unit"" (LGPU) featuring a proposed re-configurable graphics pipeline capable of minimal latency interactive feedback, without the need of computer communication. This is a novel approach for creating interactive graphics where a simple program describes the interaction on a vertex. Similar in design to a geometry or fragment shader on a GPU, these programs are uploaded on initialisation and do not require input from any external micro-controller while running. The interaction shader takes input from a light sensor and updates the vertex and fragment shader, an operation that can be parallelised. Once loaded onto our prototype LGPU the pipeline can create laser graphics that react within 4 ms of interaction and can run without input from a computer. The pipeline achieves this low latency by having the interaction shader communicate with the geometry and vertex shaders that are also running on the LGPU. This enables the creation of low latency displays such as car counters, musical instrument interfaces, and non-touch projected widgets or buttons. From our testing we were able to achieve a reaction time of 4 ms and from a range of up to 15 m."	https://dl.acm.org/doi/abs/10.1145/3415255.3422885	Jayson Haebich, Christian Sandor, Alvaro Cassinelli
Interactive liquid splash modeling by user sketches	Splashing is one of the most fascinating liquid phenomena in the real world and it is favored by artists to create stunning visual effects, both statically and dynamically. Unfortunately, the generation of complex and specialized liquid splashes is a challenging task and often requires considerable time and effort. In this paper, we present a novel system that synthesizes realistic liquid splashes from simple user sketch input. Our system adopts a conditional generative adversarial network (cGAN) trained with physics-based simulation data to produce raw liquid splash models from input sketches, and then applies model refinement processes to further improve their small-scale details. The system considers not only the trajectory of every user stroke, but also its speed, which makes the splash model simulation-ready with its underlying 3D flow. Compared with simulation-based modeling techniques through trials and errors, our system offers flexibility, convenience and intuition in liquid splash design and editing. We evaluate the usability and the efficiency of our system in an immersive virtual reality environment. Thanks to this system, an amateur user can now generate a variety of realistic liquid splashes in just a few minutes.	https://dl.acm.org/doi/abs/10.1145/3414685.3417832	Guowei Yan, Zhili Chen, Jimei Yang, Huamin Wang
Introduction to polarization for rendering and vision	Overview of real-world polarization effects Polarization in nature Ray Tracer Normal vs. Polarized Overview on Jones Calculus Overview on Mueller Calculus Build up of the Ray Tracer	https://dl.acm.org/doi/abs/10.1145/3415263.3419172	Kai Berger, Marc Blanchon
Invisible spot	"The extension of Nanography to Cinematic Projection (various experiments based on the act of ""seeing"") the works prove that the attempt to present a new perspective on the act of seeing are made in various stages. Major works are motivated by comparing the images of old and new Hanji (traditional Korean paper) taken by an electron microscope. In the image of the old Hanji, Mother Nature is engrained with the traces of time accumulated. The image resembles the scenery of mountain in that there are soil, trees grow, flowers bloom and fruits are born. With this motive, the background of this work turns to the nature. The photo works are harvested in the process of shooting all over the country by time and season. They highlight the contingency rather than intentionality and enhance fictitiousness by blurring the line between the actual forest and the virtual reality synthesized with a nano-image. Why don't we imagine that the screen-like image in the wild nature is the screen of an outdoor theater? By stimulating the emotional code of a fictional drama, it spurs us to recall movies based on a specific situation. This work led to an opportunity for the artist to naturally develop the sense of improvisation and direction in the field and to integrate it into other cultural areas. My nano-image was projected behind a scene on a stage of a documentary film starring a pianist, as part of a theater stage set, on a small village in Jeju island, and on a house designed by H-Sang, Seung 's 18 years ago. The space of life and the space of fiction become more romantic because of the fictional clothes that are worn for a while. As the project progresses, the cultural sensitivity becomes more intense against the back drop of science."	https://dl.acm.org/doi/abs/10.1145/3414686.3427141	Hojun Ji
KABUTO: Inducing Upper-Body Movements using a Head Mounted Haptic Display with Flywheels	As an alternative to enhance user's experience in VR contents, we propose KABUTO, a head mounted haptic display designed to induce upper-body movement by the application of kinesthetic feedback to the head. KABUTO can provide impact and resistance by using flywheels and brakes in response to various head movements, as extensive head movements lead to dynamic movement throughout the upper body. We have also designed an application which enables the user to become a rhinoceros beetle.The user can feel the weight of the swinging horn or the impact of the horn flinging object. In the demonstration of it, we observed that KABUTO makes users move their upper body aggressively.	https://dl.acm.org/doi/abs/10.1145/3415255.3422880	Taku Tanichi, Futa Asada, Kento Matsuda, Danny Hynds, Kouta Minamizawa
Kam	"Eyes are everywhere. Cheap and accessible technology products with high-resolution imaging capacity are in every corner of our surroundings to surveil us. They watch us, record us, and even recognize us. These artificial gazes became so ubiquitous and so familiar to us that we are not even aware of them in everyday lives. Meanwhile, human senses interact with each other and transfer from one to another. We hear vibrations, feel textures by seeing, smell tastes, taste tactility, and so on. We feel the movement only by seeing stopped escalator. Our vision translates the visual information to activate the motor sensation embedded to somewhere in the body. ""Kam"" tries to twist the one's familiarity by the phenomenon of the other. The eyeball-shaped camera follows you and imitates your blinks. The unfamiliar and unexpected behavior of this robotic camera gives lively feel to it and, at the same time, becomes eerie and unreal. It also makes you realize your sensation of blink when you find yourself trying to make it blink. Even its mechanical sounds seem to make you feel your blink physically. ""Kam"" utilizes the face recognition algorithms to see one layer deeper onto our facial expression. It exposes itself by reacting to the expression, when we would not even aware of it otherwise. It makes us pay conscious attention to it and realize our own bodily existence. ""Kam"" intended this trivial daily happening to become a meaningful experience."	https://dl.acm.org/doi/abs/10.1145/3414686.3427116	Taeil Lee
Keep running: AI paintings of evolving horse portraits	"""Keep Running"" is a collection of human and machine generated horse paintings using the generative adversarial network technology. The artworks are produced during the lockdown period in the Middle East due to the Covid-19 pandemic. Horses are significant symbols in the Middle East region culturally and historically, representing strength, endurance and persistence. These are the spirits that keep us running during the difficult times, even when we are facing many physical constraints in daily life. Nowadays, many AI-generated artworks are either photo-realistic or very abstract with distorted faces, fragmented figures and a combination of unknown objects. What technically unique in our work is to produce a series of AI-assisted paintings that shows distinguishable features and forms of horses, while creating aesthetic and even sentimental values in each horse portrait. Each of our art piece is presented in a 2x2 grid format that shows how an AI horse painting is evolved over the generation process. We believe that the machine learning process could unite human creativity with AI technology to produce a series of unique and aesthetic paintings - even these artworks were created in a large scale and mass production method that is never been possible before. With the artistic and technical novelty in our artworks, we also wish to pay a salute to the pioneers like Eadweard Muybridge and Andy Warhol, who first popularized the use of machines with camera and silk-screen printing technologies in art-making, redefining the meaning and expanding the horizon of art."	https://dl.acm.org/doi/abs/10.1145/3414686.3427174	James She, Carmen Ng, Wadia Sheng
Layered neural rendering for retiming people in video	"We present a method for retiming people in an ordinary, natural video --- manipulating and editing the time in which different motions of individuals in the video occur. We can temporally align different motions, change the speed of certain actions (speeding up/slowing down, or entirely ""freezing"" people), or ""erase"" selected people from the video altogether. We achieve these effects computationally via a dedicated learning-based layered video representation, where each frame in the video is decomposed into separate RGBA layers, representing the appearance of different people in the video. A key property of our model is that it not only disentangles the direct motions of each person in the input video, but also correlates each person with the scene changes they generate---e.g., shadows, reflections, and motion of loose clothing. The layers can be individually retimed and recombined into a new video, allowing us to achieve realistic, high-quality renderings of retiming effects for real-world videos depicting complex actions and involving multiple individuals, including dancing, trampoline jumping, or group running."	https://dl.acm.org/doi/abs/10.1145/3414685.3417760	Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T. Freeman, Michael Rubinstein
Learned feature embeddings for non-line-of-sight imaging and recognition	Objects obscured by occluders are considered lost in the images acquired by conventional camera systems, prohibiting both visualization and understanding of such hidden objects. Non-line-of-sight methods (NLOS) aim at recovering information about hidden scenes, which could help make medical imaging less invasive, improve the safety of autonomous vehicles, and potentially enable capturing unprecedented high-definition RGB-D data sets that include geometry beyond the directly visible parts. Recent NLOS methods have demonstrated scene recovery from time-resolved pulse-illuminated measurements encoding occluded objects as faint indirect reflections. Unfortunately, these systems are fundamentally limited by the quartic intensity fall-off for diffuse scenes. With laser illumination limited by eye-safety limits, recovery algorithms must tackle this challenge by incorporating scene priors. However, existing NLOS reconstruction algorithms do not facilitate learning scene priors. Even if they did, datasets that allow for such supervision do not exist, and successful encoder-decoder networks and generative adversarial networks fail for real-world NLOS data. In this work, we close this gap by learning hidden scene feature representations tailored to both reconstruction and recognition tasks such as classification or object detection, while still relying on physical models at the feature level. We with a generalizable architecture that can be trained in simulation. We learn the differentiable scene representation jointly with the reconstruction task using a differentiable transient renderer in the objective, and demonstrate that it , unlike existing encoder-decoder architectures and generative adversarial networks. The proposed method allows for , such as image reconstruction, classification, and object detection, while being memory-efficient and running at real-time rates. We demonstrate in the hidden scene in an end-to-end fashion.	https://dl.acm.org/doi/abs/10.1145/3414685.3417825	Wenzheng Chen, Fangyin Wei, Kiriakos N. Kutulakos, Szymon Rusinkiewicz, Felix Heide
Learned hardware-in-the-loop phase retrieval for holographic near-eye displays	"Holography is arguably the most promising technology to provide wide field-of-view compact eyeglasses-style near-eye displays for augmented and virtual reality. However, the image quality of existing holographic displays is far from that of current generation conventional displays, effectively making today's holographic display systems impractical. This gap stems predominantly from the severe deviations in the idealized approximations of the ""unknown"" light transport model in a real holographic display, used for computing holograms. In this work, we depart from such approximate ""ideal"" coherent light transport models for computing holograms. Instead, we learn the deviations of the real display from the ideal light transport from the images measured using a display-camera hardware system. After this unknown light propagation is learned, we use it to compensate for severe aberrations in real holographic imagery. The proposed hardware-in-the-loop approach is robust to spatial, temporal and hardware deviations, and improves the image quality of existing methods qualitatively and quantitatively in SNR and perceptual quality. We validate our approach on a holographic display prototype and show that the method can fully compensate unknown aberrations and erroneous and non-linear SLM phase delays, without explicitly modeling them. As a result, the proposed method significantly outperforms existing state-of-the-art methods in simulation and experimentation - just by observing captured holographic images."	https://dl.acm.org/doi/abs/10.1145/3414685.3417846	Praneeth Chakravarthula, Ethan Tseng, Tarun Srivastava, Henry Fuchs, Felix Heide
Learning 3D functionality representations	A central goal of computer graphics is to provide tools for designing and simulating real or imagined artifacts. An understanding of functionality is important in enabling such modeling tools. Given that the majority of man-made artifacts are designed to serve a certain function, the functionality of objects is often reflected by their geometry, the way that they are organized in an environment, and their interaction with other objects or agents. Thus, in recent years, a variety of methods in shape analysis have been developed to extract functional information about objects and scenes from these different types of cues. In this course, we discuss recent developments involving functionality analysis of 3D shapes and scenes. We provide a summary of the state-of-the-art in this area, including a discussion of key ideas and an organized review of the relevant literatures. More specifically, we first present a general definition of functionality from which we derive criteria for classifying the body of prior work. This definition facilitates a comparative view of methods for functionality analysis. Moreover, we connect these methods to recent advances in deep learning, computer vision and robotics. Finally, we discuss a variety of application areas, and outline current challenges and directions for future work.	https://dl.acm.org/doi/abs/10.1145/3415263.3419152	Ruizhen Hu, Manolis Savva, Oliver van Kaick
Learning to manipulate amorphous materials	We present a method of training character manipulation of amorphous materials such as those often used in cooking. Common examples of amorphous materials include granular materials (salt, uncooked rice), fluids (honey), and visco-plastic materials (sticky rice, softened butter). A typical task is to spread a given material out across a flat surface using a tool such as a scraper or knife. We use reinforcement learning to train our controllers to manipulate materials in various ways. The training is performed in a physics simulator that uses position-based dynamics of particles to simulate the materials to be manipulated. The neural network control policy is given observations of the material (e.g. a low-resolution density map), and the policy outputs actions such as rotating and translating the knife. We demonstrate policies that have been successfully trained to carry out the following tasks: spreading, gathering, and flipping. We produce a final animation by using inverse kinematics to guide a character's arm and hand to match the motion of the manipulation tool such as a knife or a frying pan.	https://dl.acm.org/doi/abs/10.1145/3414685.3417868	Yunbo Zhang, Wenhao Yu, C. Karen Liu, Charlie Kemp, Greg Turk
Let's chat like this	"""Let's Chat Like This"" is an interactive system that allows two people to observe each others' moods through interacting with a shared interactively generated image. The moving image changes according to the two people's facial expressions. Different from traditional ways of communication, ""Let's Chat Like This"" focuses more on the emotional aspect of communication. It shows a visualization of the complexity of human emotion and boosts people's emotional communication in a creative no-verbal way. When experiencing this work, people's emotions are bound together with the same moving image they see. The moving image changes depending on their moods. They will be aware of their current moods as well as the other's, the intimacy and empathy between them will be increased. This is not only a ""social distancing"" art installation that helps us connect emotionally during the COVID-19 pandemic, but also my hypothesis of what future emotional communication will be like. I hope this artwork can evoke deep thinking and maybe cheer people up in this challenging time."	https://dl.acm.org/doi/abs/10.1145/3414686.3427118	Qinyuan Liu
Life: can a computer have a heart?	AI is essentially 'intelligence' programmed by humans. Although AI which can joke, communicate and tell a story resembles humans, can we continue to have a natural conversation with them, even though it is identified as AI? The current AI is only applied to a certain field, as it is at the step of 'weak intelligence'. It is expected to be developed into 'general or strong intelligence' imitating humans' whole intelligent activity in the future. This work allows us to indirectly experience to consider whether we would treat AI as we do humans, if it is developed into 'strong intelligence', through the conversation with AI which is able to learn emotional words.	https://dl.acm.org/doi/abs/10.1145/3414686.3427157	Jungmin Park, Kyoungmin Bang
Lifting freehand concept sketches into 3D	We present the first algorithm capable of automatically lifting real-world, vector-format, industrial design sketches into 3D. Targeting real-world sketches raises numerous challenges due to inaccuracies, use of overdrawn strokes, and construction lines. In particular, while construction lines convey important 3D information, they add significant clutter and introduce multiple accidental 2D intersections. Our algorithm exploits the geometric cues provided by the construction lines and lifts them to 3D by computing their intended 3D intersections and depths. Once lifted to 3D, these lines provide valuable geometric constraints that we leverage to infer the 3D shape of other artist drawn strokes. The core challenge we address is inferring the 3D connectivity of construction and other lines from their 2D projections by separating 2D intersections into 3D intersections and accidental occlusions. We efficiently address this complex combinatorial problem using a dedicated search algorithm that leverages observations about designer drawing pREFERENCES, and uses those to explore only the most likely solutions of the 3D intersection detection problem. We demonstrate that our separator outputs are of comparable quality to human annotations, and that the 3D structures we recover enable a range of design editing and visualization applications, including novel view synthesis and 3D-aware scaling of the depicted shape.	https://dl.acm.org/doi/abs/10.1145/3414685.3417851	Yulia Gryaditskaya, Felix Hähnlein, Chenxi Liu, Alla Sheffer, Adrien Bousseau
Light stage super-resolution: continuous high-frequency relighting	"The light stage has been widely used in computer graphics for the past two decades, primarily to enable the relighting of human faces. By capturing the appearance of the human subject under different light sources, one obtains the light transport matrix of that subject, which enables image-based relighting in novel environments. However, due to the finite number of lights in the stage, the light transport matrix only represents a sparse sampling on the entire sphere. As a consequence, relighting the subject with a point light or a directional source that does not coincide exactly with one of the lights in the stage requires interpolation and resampling the images corresponding to nearby lights, and this leads to ghosting shadows, aliased specularities, and other artifacts. To ameliorate these artifacts and produce better results under arbitrary high-frequency lighting, this paper proposes a learning-based solution for the ""super-resolution"" of scans of human faces taken from a light stage. Given an arbitrary ""query"" light direction, our method aggregates the captured images corresponding to neighboring lights in the stage, and uses a neural network to synthesize a rendering of the face that appears to be illuminated by a ""virtual"" light source at the query location. This neural network must circumvent the inherent aliasing and regularity of the light stage data that was used for training, which we accomplish through the use of regularized traditional interpolation methods within our network. Our learned model is able to produce renderings for arbitrary light directions that exhibit realistic shadows and specular highlights, and is able to generalize across a wide variety of subjects. Our super-resolution approach enables more accurate renderings of human subjects under detailed environment maps, or the construction of simpler light stages that contain fewer light sources while still yielding comparable quality renderings as light stages with more densely sampled lights."	https://dl.acm.org/doi/abs/10.1145/3414685.3417821	Tiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello, Christoph Rhemann, Paul Debevec, Yun-Ta Tsai, Jonathan T. Barron, Ravi Ramamoorthi
LightTank	LightTank is an interactive Extended Reality (XR) installation that augments a large lightweight aluminium structure with holographic line drawings. It consists of four transparent projection walls which are assembled to an X shape tower like construction of 7.5 x 7.5 x 5.5 m. The project was developed by the arc/sec Lab in collaboration with the Augmented Human Lab for the Ars Electronica Festival and presented in the Cathedral of Linz in Austria. It aims to expand principles of augmented reality (AR) headsets from a single person viewing experience, towards a communal interactive event. To achieve this goal, LightTank uses an anaglyph stereoscopic projection method, which combined with simple red/cyan cardboard glasses, allows the creation of 3D virtual constructions. The holographic line drawings are designed to merge with its physical environment, whether it is the geometrical grids of the aluminium structure or the gothic architecture of the cathedral. Certain drawings seem to peel off the existing physical structure, while others travel through the cathedral and line up with the characteristic elements like columns, groined arches and rose windows. The project follows a hybrid design strategy which places equal attention to both design aspects, the physical and the digital. The aim of the setup is to explore user responsive architecture, where dynamic properties of the virtual world are an integral part of the physical environment. LightTank creates hereby a multi-viewer environment which enables visitors to navigate through holographic architectural narratives.	https://dl.acm.org/doi/abs/10.1145/3414686.3427113	Uwe Rieger, Yinan Liu, Roger Boldu, Haimo Zhang, Heetesh Alwani, Suranga Nanayakkara
Love: greatest mystery of life	The greatest mystery of life comes when you're least expecting it and disappears when you thought it is here to stay. The heat that ignites it at the beginning is doused by the intimacy it creates. It is a portal, a mirror, a cross to bear, a joy, a heartbreak, and an axe. It cuts through your hard parts, the gristly parts, and lays your beating heart bare. It is both the butterfly that flutters in your tummy, and the acid that melts everything away. That, my friend, is what we call LOVE.	https://dl.acm.org/doi/abs/10.1145/3414686.3427173	Firdaus Khalid
MAScreen: Augmenting Speech with Visual Cues of Lip Motions, Facial Expressions, and Text Using a Wearable Display	Personal protective equipment, particularly face masks, have become increasingly common with the rise of global health issues, such as fine-dust storms and pandemics. Face masks, however, also degrade speech intelligibility by effectively occluding visual cues, such as lip motions and facial expressions. In this paper, we propose MAScreen, a wearable LED display in the shape of a mask, which is capable of sensing lip motion and speech and provides real-time visual feedback of the mouth behind the mask.	https://dl.acm.org/doi/abs/10.1145/3415255.3422886	Hyein Lee, Yoonji Kim, Andrea Bianchi
MakeltTalk: speaker-aware talking-head animation	We present a method that generates expressive talking-head videos from a single facial image with audio as the only input. In contrast to previous attempts to learn direct mappings from audio to raw pixels for creating talking faces, our method first disentangles the content and speaker information in the input audio signal. The audio content robustly controls the motion of lips and nearby facial regions, while the speaker information determines the specifics of facial expressions and the rest of the talking-head dynamics. Another key component of our method is the prediction of facial landmarks reflecting the speaker-aware dynamics. Based on this intermediate representation, our method works with many portrait images in a single unified framework, including artistic paintings, sketches, 2D cartoon characters, Japanese mangas, and stylized caricatures. In addition, our method generalizes well for faces and characters that were not observed during training. We present extensive quantitative and qualitative evaluation of our method, in addition to user studies, demonstrating generated talking-heads of significantly higher quality compared to prior state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3414685.3417774	Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, Dingzeyu Li
Manga filling style conversion with screentone variational autoencoder	Western color comics and Japanese-style screened manga are two popular comic styles. They mainly differ in the style of region-filling. However, the conversion between the two region-filling styles is very challenging, and manually done currently. In this paper, we identify that the major obstacle in the conversion between the two filling styles stems from the difference between the fundamental properties of screened region-filling and colored region-filling. To resolve this obstacle, we propose a screentone variational autoencoder, ScreenVAE, to map the screened manga to an intermediate domain. This intermediate domain can summarize local texture characteristics and is interpolative. With this domain, we effectively unify the properties of screening and color-filling, and ease the learning for bidirectional translation between screened manga and color comics. To carry out the bidirectional translation, we further propose a network to learn the translation between the intermediate domain and color comics. Our model can generate quality screened manga given a color comic, and generate color comic that retains the original screening intention by the bitonal manga artist. Several results are shown to demonstrate the effectiveness and convenience of the proposed method. We also demonstrate how the intermediate domain can assist other applications such as manga inpainting and photo-to-comic conversion.	https://dl.acm.org/doi/abs/10.1145/3414685.3417873	Minshan Xie, Chengze Li, Xueting Liu, Tien-Tsin Wong
MapTree: recovering multiple solutions in the space of maps	In this paper we propose an approach for computing multiple high-quality near-isometric dense correspondences between a pair of 3D shapes. Our method is fully automatic and does not rely on user-provided landmarks or descriptors. This allows us to analyze the full space of maps and extract multiple diverse and accurate solutions, rather than optimizing for a single optimal correspondence as done in most previous approaches. To achieve this, we propose a compact tree structure based on the spectral map representation for encoding and enumerating possible rough initializations, and a novel efficient approach for refining them to dense pointwise maps. This leads to a new method capable of both producing multiple high-quality correspondences across shapes and revealing the symmetry structure of a shape without a priori information. In addition, we demonstrate through extensive experiments that our method is robust and results in more accurate correspondences than state-of-the-art for shape matching and symmetry detection.	https://dl.acm.org/doi/abs/10.1145/3414685.3417800	Jing Ren, Simone Melzi, Maks Ovsjanikov, Peter Wonka
MaskBot: Real-time Robotic Projection Mapping with Head Motion Tracking	The projection mapping systems on the human face are limited by the process latency and the users' movement. The area of the projection is restricted by the position of the projectors and cameras. We are introducing MaskBot, a real-time projection mapping system guided by a 6 Degrees of Freedom (DoF) collaborative robot. The collaborative robot locates the projector and camera in front of the user's face to increase the projection area and reduce the system's latency. A webcam is used to detect the face orientation and to measure the robot-user distance. Based on this information we modify the projection size and orientation. MaskBot projects different images on the user's face, such as face modifications, make-up, and logos. In contrast to the existing methods, the presented system is the first that introduces a robotic projection mapping. One of the prospective applications is to acquire a dataset of adversarial images to challenge face detection DNN systems, such as Face ID.	https://dl.acm.org/doi/abs/10.1145/3415255.3422896	Miguel Altamirano Cabrera, Igor Usachev, Juan Heredia, Jonathan Tirado, Aleksey Fedoseev, Dzmitry Tsetserukou
Match: differentiable material graphs for procedural material capture	We present , a method to automatically convert photographs of material samples into production-grade procedural material models. At the core of MATch is a new library that provides differentiable building blocks for constructing procedural materials, and automatic translation of large-scale procedural models, with hundreds to thousands of node parameters, into differentiable node graphs. Combining these translated node graphs with a rendering layer yields an end-to-end differentiable pipeline that maps node graph parameters to rendered images. This facilitates the use of gradient-based optimization to estimate the parameters such that the resulting material, when rendered, matches the target image appearance, as quantified by a style transfer loss. In addition, we propose a deep neural feature-based graph selection and parameter initialization method that efficiently scales to a large number of procedural graphs. We evaluate our method on both rendered synthetic materials and real materials captured as flash photographs. We demonstrate that MATch can reconstruct more accurate, general, and complex procedural materials compared to the state-of-the-art. Moreover, by producing a procedural output, we unlock capabilities such as constructing arbitrary-resolution material maps and parametrically editing the material appearance.	https://dl.acm.org/doi/abs/10.1145/3414685.3417781	Liang Shi, Beichen Li, Miloš Hašan, Kalyan Sunkavalli, Tamy Boubekeur, Radomir Mech, Wojciech Matusik
MaterialGAN: reflectance capture using a generative SVBRDF model	We address the problem of reconstructing spatially-varying BRDFs from a small set of image measurements. This is a fundamentally under-constrained problem, and previous work has relied on using various regularization priors or on capturing many images to produce plausible results. In this work, we present , a deep generative convolutional network based on StyleGAN2, trained to synthesize realistic SVBRDF parameter maps. We show that MaterialGAN can be used as a powerful material prior in an inverse rendering framework: we optimize in its latent representation to generate material maps that match the appearance of the captured images when rendered. We demonstrate this framework on the task of reconstructing SVBRDFs from images captured under flash illumination using a hand-held mobile phone. Our method succeeds in producing plausible material maps that accurately reproduce the target images, and outperforms previous state-of-the-art material capture methods in evaluations on both synthetic and real data. Furthermore, our GAN-based latent space allows for high-level semantic material editing operations such as generating material variations and material morphing.	https://dl.acm.org/doi/abs/10.1145/3414685.3417779	Yu Guo, Cameron Smith, Miloš Hašan, Kalyan Sunkavalli, Shuang Zhao
MeshWalker: deep mesh understanding by random walks	"Most attempts to represent 3D shapes for deep learning have focused on volumetric grids, multi-view images and point clouds. In this paper we look at the most popular representation of 3D shapes in computer graphics---a triangular mesh---and ask how it can be utilized within deep learning. The few attempts to answer this question propose to adapt convolutions & pooling to suit This paper proposes a very different approach, termed to learn the shape directly from a given mesh. The key idea is to represent the mesh by random walks along the surface, which ""explore"" the mesh's geometry and topology. Each walk is organized as a list of vertices, which in some manner imposes regularity on the mesh. The walk is fed into a that ""remembers"" the history of the walk. We show that our approach achieves state-of-the-art results for two fundamental shape analysis tasks: shape classification and semantic segmentation. Furthermore, even a very small number of examples suffices for learning. This is highly important, since large datasets of meshes are difficult to acquire."	https://dl.acm.org/doi/abs/10.1145/3414685.3417806	Alon Lahav, Ayellet Tal
Mixed integer ink selection for spectral reproduction	We introduce a novel ink selection method for spectral printing. The ink selection algorithm takes a spectral image and a set of inks as input, and selects a subset of those inks that results in optimal spectral reproduction. We put forward an optimization formulation that searches a huge combinatorial space based on mixed integer programming. We show that solving this optimization in the conventional reflectance space is intractable. The main insight of this work is to solve our problem in the spectral absorbance space with a linearized formulation. The proposed ink selection copes with large-size problems for which previous methods are hopeless. We demonstrate the effectiveness of our method in a concrete setting by lifelike reproduction of handmade paintings. For a successful spectral reproduction of high-resolution paintings, we explore their spectral absorbance estimation, efficient coreset representation, and accurate data-driven reproduction.	https://dl.acm.org/doi/abs/10.1145/3414685.3417761	Navid Ansari, Omid Alizadeh-Mousavi, Hans-Peter Seidel, Vahid Babaei
MoGlow: probabilistic and controllable motion synthesis using normalising flows	Data-driven modelling and synthesis of motion is an active research area with applications that include animation, games, and social robotics. This paper introduces a new class of probabilistic, generative, and controllable motion-data models based on normalising flows. Models of this kind can describe highly complex distributions, yet can be trained efficiently using exact maximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive and uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is also causal, meaning that each pose in the output sequence is generated without access to poses or control inputs from future time steps; this absence of algorithmic latency is important for interactive applications with real-time motion control. The approach can in principle be applied to any type of motion since it does not make restrictive, task-specific assumptions regarding the motion or the character morphology. We evaluate the models on motion-capture datasets of human and quadruped locomotion. Objective and subjective results show that randomly-sampled motion from the proposed method outperforms task-agnostic baselines and attains a motion quality close to recorded motion capture.	https://dl.acm.org/doi/abs/10.1145/3414685.3417836	Gustav Eje Henter, Simon Alexanderson, Jonas Beskow
Modular primitives for high-performance differentiable rendering	We present a modular differentiable renderer design that yields performance superior to previous methods by leveraging existing, highly optimized hardware graphics pipelines. Our design supports all crucial operations in a modern graphics pipeline: rasterizing large numbers of triangles, attribute interpolation, filtered texture lookups, as well as user-programmable shading and geometry processing, all in high resolutions. Our modular primitives allow custom, high-performance graphics pipelines to be built directly within automatic differentiation frameworks such as PyTorch or TensorFlow. As a motivating application, we formulate facial performance capture as an inverse rendering problem and show that it can be solved efficiently using our tools. Our results indicate that this simple and straightforward approach achieves excellent geometric correspondence between rendered results and reference imagery.	https://dl.acm.org/doi/abs/10.1145/3414685.3417861	Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, Timo Aila
Monolith: a monolithic pressure-viscosity-contact solver for strong two-way rigid-rigid rigid-fluid coupling	"We propose , a monolithic pressure-viscosity-contact solver for more accurately, robustly, and efficiently simulating non-trivial two-way interactions of rigid bodies with inviscid, viscous, or non-Newtonian liquids. Our solver simultaneously handles incompressibility and (optionally) implicit viscosity integration for liquids, contact resolution for rigid bodies, and mutual interactions between liquids and rigid bodies by carefully formulating these as a single unified minimization problem. This monolithic approach reduces or eliminates an array of problematic artifacts, including liquid volume loss, solid interpenetrations, simulation instabilities, artificial ""melting"" of viscous liquid, and incorrect slip at liquid-solid interfaces. In the absence of solid-solid friction, our minimization problem is a Quadratic Program (QP) with a symmetric positive definite (SPD) matrix and can be treated with a single Linear Complementarity Problem (LCP) solve. When friction is present, we decouple the unified minimization problem into two subproblems so that it can be effectively handled via staggered projections with alternating LCP solves. We also propose a complementary approach for non-Newtonian fluids which can be seamlessly integrated and addressed during the staggered projections. We demonstrate the critical importance of a , unified treatment of fluid-solid coupling and the effectiveness of our proposed Monolith solver in a wide range of practical scenarios."	https://dl.acm.org/doi/abs/10.1145/3414685.3417798	Tetsuya Takahashi, Christopher Batty
Mononizing binocular videos	This paper presents the idea of binocular videos and a framework to effectively realize it. Mono-nize means we purposely convert a binocular video into a regular monocular video with the stereo information implicitly encoded in a visual but nearly-imperceptible form. Hence, we can impartially distribute and show the mononized video as an ordinary monocular video. Unlike ordinary monocular videos, we can restore from it the original binocular video and show it on a stereoscopic display. To start, we formulate an encoding-and-decoding framework with the pyramidal deformable fusion module to exploit long-range correspondences between the left and right views, a quantization layer to suppress the restoring artifacts, and the compression noise simulation module to resist the compression noise introduced by modern video codecs. Our framework is self-supervised, as we articulate our objective function with loss terms defined on the input: a monocular term for creating the mononized video, an invertibility term for restoring the original video, and a temporal term for frame-to-frame coherence. Further, we conducted extensive experiments to evaluate our generated mononized videos and restored binocular videos for diverse types of images and 3D movies. Quantitative results on both standard metrics and user perception studies show the effectiveness of our method.	https://dl.acm.org/doi/abs/10.1145/3414685.3417764	Wenbo Hu, Menghan Xia, Chi-Wing Fu, Tien-Tsin Wong
Monster mash: a single-view approach to casual 3D modeling and animation	We present a new framework for sketch-based modeling and animation of 3D organic shapes that can work entirely in an intuitive 2D domain, enabling a playful, casual experience. Unlike previous sketch-based tools, our approach does not require a tedious part-based multi-view workflow with the explicit specification of an animation rig. Instead, we combine 3D inflation with a novel rigidity-preserving, layered deformation model, ARAP-L, to produce a smooth 3D mesh that is immediately ready for animation. Moreover, the resulting model can be animated from a single viewpoint --- and without the need to handle unwanted inter-penetrations, as required by previous approaches. We demonstrate the benefit of our approach on a variety of examples produced by inexperienced users as well as professional animators. For less experienced users, our single-view approach offers a simpler modeling and animating experience than working in a 3D environment, while for professionals, it offers a quick and casual workspace for ideation.	https://dl.acm.org/doi/abs/10.1145/3414685.3417805	Marek Dvorožňák, Daniel Sýkora, Cassidy Curtis, Brian Curless, Olga Sorkine-Hornung, David Salesin
NORAA [machinic doodles]	How does machine learning contribute to our understanding of how ideas are communicated through drawing? Specifically, how can networks capable of exhibiting dynamic temporal behaviour for time sequences be used for the generation of line (vector) drawings? Can machine-learning algorithms reveal something about the way we draw? Can we better understand the way we encode ideas into drawings from these algorithms? While simple pen strokes may not resemble reality as captured by more sophisticated visual representations, they do tell us something about how people represent and reconstruct the world around them. The ability to immediately recognise, depict objects and even emotions from a few marks, strokes and lines, is something that humans learn as children. Machinic Doodles is interested in the semantics of lines, the patterns that emerge in how people around the world draw - what governs the rule of geometry that makes us draw from one point to another in a specific order? The order, speedpace and expression of a line, its constructed and semantic associations are of primary interest, generated figures are simply the means and the record of the interaction, not the final motivation. The installation is essentially a game of human-robot Pictionary: you draw, the machine takes a guess, and then draws something back in response. The project demonstrates how a drawing game based on a recurrent-neural-network, combined with real-time human drawing interaction, can be used to generate a sequence of human- machine doodle drawings. As the number of classification models is greater than the generational models (i.e. ability to identify is higher than drawing ability), the work inherently explores this gap in the machine's knowledge, as well as creative possibilities afforded by misinterpretations of the machine. Drawings are not just for guessing, but analysed for spatial and temporal characteristics to inform drawing generation.	https://dl.acm.org/doi/abs/10.1145/3414686.3427106	Jessica In, George Profenza, Sam Price
Narcissus	Narcissus was a hunter in Greek mythology who fell in love with his own reflection in the water. Narcissus is the origin of the term narcissism. This artwork, as its name suggests, is based on the myth of Narcissus. A narcissistic ego that conceals the weakness of the individual, focuses energy only on itself. Everyone has narcissism even if a little. Excessive narcissism causes lots of problems from isolation because fascinating by the perfection of oneself makes relationship with the others closed. Sensing the brainwave of an appreciator, creates an interaction that the higher concentration on the reflection on the surface of water, the more blurred observation. Through this interaction, the viewer is interrupted from being deeply immersed in oneself. That experience reflects the reverse of the Narcissus story and expresses that mirrored image of the participant could not exist as complete subject. Also, this artwork is referred to the 'Mirror stage' hypothesized by Jacques Lacan(1901--1981). Unlike the ego-psychologists' assertion that we should strengthen our ego, Jacques Lacan points out the narcissistic ego which is imaginary in human beings and says that we can grow up as a healthy subject through acknowledging that we are lacking ego not just strengthening the ego.	https://dl.acm.org/doi/abs/10.1145/3414686.3427152	Seol Lee
Nebula go	In NEBULA GO, you can see many nebulae and stars being born and losing, and glimpse the secrets that arise in space. In this NEBULA GO, the universe is expressed through Go, and since ancient times, Go was invented as a tool for observing and studying the movement of celestial bodies. In this work, the artist harmonizes the secrets that occur in the universe by using the act of placing a Go in a square space, various fights caused by it, domain creation, and movement of forces. Looking at NEBULA GO, unlike Go, where victory or defeat is determined, you can observe the numerous planets visible in the universe when all actions have been completed and the changes caused by these planets. Also, by focusing on the birth origin of astronomical observation, actors can appreciate their own microcosm through their actions.	https://dl.acm.org/doi/abs/10.1145/3414686.3427166	Jungho Kim, Youngho Kim, Taekyung Yoo
Neural control variates	We propose neural control variates (NCV) for unbiased variance reduction in parametric Monte Carlo integration. So far, the core challenge of applying the method of control variates has been finding a good approximation of the integrand that is cheap to integrate. We show that a set of neural networks can face that challenge: a normalizing flow that approximates the shape of the integrand and another neural network that infers the solution of the integral equation. We also propose to leverage a neural importance sampler to estimate the difference between the original integrand and the learned control variate. To optimize the resulting parametric estimator, we derive a theoretically optimal, variance-minimizing loss function, and propose an alternative, composite loss for stable online training in practice. When applied to light transport simulation, neural control variates are capable of matching the state-of-the-art performance of other unbiased approaches, while providing means to develop more performant, practical solutions. Specifically, we show that the learned light-field approximation is of sufficient quality for high-order bounces, allowing us to omit the error correction and thereby dramatically reduce the noise at the cost of negligible visible bias.	https://dl.acm.org/doi/abs/10.1145/3414685.3417804	Thomas Müller, Fabrice Rousselle, Alexander Keller, Jan Novák
Neural crossbreed: neural based image metamorphosis	We propose Neural Crossbreed, a feed-forward neural network that can learn a semantic change of input images in a latent space to create the morphing effect. Because the network learns a semantic change, a sequence of meaningful intermediate images can be generated without requiring the user to specify explicit correspondences. In addition, the semantic change learning makes it possible to perform the morphing between the images that contain objects with significantly different poses or camera views. Furthermore, just as in conventional morphing techniques, our morphing network can handle shape and appearance transitions separately by disentangling the content and the style transfer for rich usability. We prepare a training dataset for morphing using a pre-trained BigGAN, which generates an intermediate image by interpolating two latent vectors at an intended morphing value. This is the first attempt to address image morphing using a pre-trained generative model in order to learn semantic transformation. The experiments show that Neural Crossbreed produces high quality morphed images, overcoming various limitations associated with conventional approaches. In addition, Neural Crossbreed can be further extended for diverse applications such as multi-image morphing, appearance transfer, and video frame interpolation.	https://dl.acm.org/doi/abs/10.1145/3414685.3417797	Sanghun Park, Kwanggyoon Seo, Junyong Noh
Neural holography with camera-in-the-loop training	Holographic displays promise unprecedented capabilities for direct-view displays as well as virtual and augmented reality applications. However, one of the biggest challenges for computer-generated holography (CGH) is the fundamental tradeoff between algorithm runtime and achieved image quality, which has prevented high-quality holographic image synthesis at fast speeds. Moreover, the image quality achieved by most holographic displays is low, due to the mismatch between the optical wave propagation of the display and its simulated model. Here, we develop an algorithmic CGH framework that achieves unprecedented image fidelity and real-time framerates. Our framework comprises several parts, including a novel camera-in-the-loop optimization strategy that allows us to either optimize a hologram directly or train an interpretable model of the optical wave propagation and a neural network architecture that represents the first CGH algorithm capable of generating full-color high-quality holographic images at 1080p resolution in real time.	https://dl.acm.org/doi/abs/10.1145/3414685.3417802	Yifan Peng, Suyeon Choi, Nitish Padmanaban, Gordon Wetzstein
Neural light field 3D printing	Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using a few co-planar layers in a 2.5D fashion to keep the problem tractable. In this paper, we propose a novel end-to-end optimization approach that encodes input light field imagery as a continuous-space implicit representation in a neural network. This allows fabricating high-resolution, attenuation-based volumetric displays that exhibit the target light fields. In addition, we incorporate the physical constraints of the material to the optimization such that the result can be printed in practice. Our simulation experiments demonstrate that our approach brings significant visual quality improvement compared to the multilayer and uniform grid-based approaches. We validate our simulations with fabricated prototypes and demonstrate that our pipeline is flexible enough to allow fabrications of both planar and non-planar displays.	https://dl.acm.org/doi/abs/10.1145/3414685.3417879	Quan Zheng, Vahid Babaei, Gordon Wetzstein, Hans-Peter Seidel, Matthias Zwicker, Gurprit Singh
Nonlinear spectral geometry processing via the TV transform	We introduce a novel computational framework for digital geometry processing, based upon the derivation of a nonlinear operator associated to the total variation functional. Such an operator admits a generalized notion of spectral decomposition, yielding a convenient multiscale representation akin to Laplacian-based methods, while at the same time avoiding undesirable over-smoothing effects typical of such techniques. Our approach entails accurate, decomposition and manipulation of 3D shape geometry while taking an especially intuitive form: non-local semantic details are well separated into different bands, which can then be filtered and re-synthesized with a straightforward linear step. Our computational framework is flexible, can be applied to a variety of signals, and is easily adapted to different geometry representations, including triangle meshes and point clouds. We showcase our method through multiple applications in graphics, ranging from surface and signal denoising to enhancement, detail transfer, and cubic stylization.	https://dl.acm.org/doi/abs/10.1145/3414685.3417849	Marco Fumero, Michael Möller, Emanuele Rodolà
Offsite aerial path planning for efficient urban scene reconstruction	With rapid development in UAV technologies, it is now possible to reconstruct large-scale outdoor scenes using only images captured by low-cost drones. The problem, however, becomes how to plan the aerial path for a drone to capture images so that two conflicting goals are optimized: maximizing the reconstruction quality and minimizing mid-air image acquisition effort. Existing approaches either resort to pre-defined dense and thus inefficient view sampling strategy, or plan the path adaptively but require two onsite flight passes and intensive computation in-between. Hence, using these methods to capture and reconstruct large-scale scenes can be tedious. In this paper, we present an adaptive aerial path planning algorithm that can be done before the site visit. Using only a 2D map and a satellite image of the to-be-reconstructed area, we first compute a coarse 2.5D model for the scene based on the relationship between buildings and their shadows. A novel Max-Min optimization is then proposed to select a minimal set of viewpoints that maximizes the reconstructability under the the same number of viewpoints. Experimental results on benchmark show that our planning approach can effectively reduce the number of viewpoints needed than the previous state-of-the-art method, while maintaining comparable reconstruction quality. Since no field computation or a second visit is needed, and the view number is also minimized, our approach significantly reduces the time required in the field as well as the off-line computation cost for multi-view stereo reconstruction, making it possible to reconstruct a large-scale urban scene in a short time with moderate effort.	https://dl.acm.org/doi/abs/10.1145/3414685.3417791	Xiaohui Zhou, Ke Xie, Kai Huang, Yilin Liu, Yang Zhou, Minglun Gong, Hui Huang
OmniPhotos: Casual 360° VR Photography with Motion Parallax	Until now, immersive 360° VR panoramas could not be captured casually and reliably at the same time as state-of-the-art approaches involve time-consuming or expensive capture processes that prevent the casual capture of real-world VR environments. Existing approaches are also often limited in their supported range of head motion. We introduce OmniPhotos, a novel approach for casually and reliably capturing high-quality 360° VR panoramas. Our approach only requires a single sweep of a consumer 360° video camera as input, which takes less than 3 seconds with a rotating selfie stick. The captured video is transformed into a hybrid scene representation consisting of a coarse scene-specific proxy geometry and optical flow between consecutive video frames, enabling 5-DoF real-world VR experiences. The large capture radius and 360° field of view significantly expand the range of head motion compared to previous approaches. Among all competing methods, ours is the simplest, and fastest by an order of magnitude. We have captured more than 50 OmniPhotos and show video results for a large variety of scenes. We will make our code and datasets publicly available.	https://dl.acm.org/doi/abs/10.1145/3415255.3422884	Tobias Bertel, Mingze Yuan, Reuben Lindroos, Christian Richardt
OmniPhotos: casual 360° VR photography	Virtual reality headsets are becoming increasingly popular, yet it remains difficult for casual users to capture immersive 360° VR panoramas. State-of-the-art approaches require capture times of usually far more than a minute and are often limited in their supported range of head motion. We introduce OmniPhotos, a novel approach for quickly and casually capturing high-quality 360° panoramas with motion parallax. Our approach requires a single sweep with a consumer 360° video camera as input, which takes less than 3 seconds to capture with a rotating selfie stick or 10 seconds handheld. This is the fastest capture time for any VR photography approach supporting motion parallax by an order of magnitude. We improve the visual rendering quality of our OmniPhotos by alleviating vertical distortion using a novel deformable proxy geometry, which we fit to a sparse 3D reconstruction of captured scenes. In addition, the 360° input views significantly expand the available viewing area, and thus the range of motion, compared to previous approaches. We have captured more than 50 OmniPhotos and show video results for a large variety of scenes. We will make our code available.	https://dl.acm.org/doi/abs/10.1145/3414685.3417770	Tobias Bertel, Mingze Yuan, Reuben Lindroos, Christian Richardt
Opening and closing surfaces	We propose a new type of curvature flow for curves in 2D and surfaces in 3D. The flow is inspired by the mathematical morphology and operations. These operations are classically defined by composition of dilation and erosion operations. In practice, existing methods implemented this way will result in re-discretizing the entire shape, even if some parts of the surface do not change. Instead, our surface-only curvature-based flow moves the surface selectively in areas that should be repositioned. In our triangle mesh discretization, vertices in regions unaffected by the opening or closing will remain exactly in place and do not affect our method's complexity, which is output-sensitive.	https://dl.acm.org/doi/abs/10.1145/3414685.3417778	Silvia Sellán, Jacob Kesten, Ang Yan Sheng, Alec Jacobson
Optimizing depth perception in virtual and augmented reality through gaze-contingent stereo rendering	Virtual and augmented reality (VR/AR) displays crucially rely on stereoscopic rendering to enable perceptually realistic user experiences. Yet, existing near-eye display systems ignore the gaze-dependent shift of the no-parallax point in the human eye. Here, we introduce a gaze-contingent stereo rendering technique that models this effect and conduct several user studies to validate its effectiveness. Our findings include experimental validation of the location of the no-parallax point, which we then use to demonstrate significant improvements of disparity and shape distortion in a VR setting, and consistent alignment of physical and digitally rendered objects across depths in optical see-through AR. Our work shows that gaze-contingent stereo rendering improves perceptual realism and depth perception of emerging wearable computing systems.	https://dl.acm.org/doi/abs/10.1145/3414685.3417820	Brooke Krajancich, Petr Kellnhofer, Gordon Wetzstein
Outside in: exile at home	Outside-in is an installation that utilizes machine learning to reflect on systematic discrimination by focusing on the indefinite detention of Mexicans with Japanese heritage concentrated in Morelos during WWII. This algorithmic discrimination system tears apart four classic fiction films continuously within a projection room. The fragments are displaced and classified using machine learning algorithms. The system selects, separates, reassembles and displaces the fragments into new orders. The new orders, edited in real time, are displayed in two perpendicular projections (one for the moving images, another for subtitles) and on a third wall the edited sound components are output through a row of headphones. It evokes the condition of being robbed of your right to be in the place to which you belong. The citizens detained during WWII were removed from their residence, their belongings were confiscated and they were placed in seclusion solely for having Japanese ancestry. Similarly, at present, data retrieving companies configure low resolution representations of ourselves from the snatched digital debris of our daily life. These pieces are reconfigured into archetypes and meaning is attached to them for massive decision making. We don't have the right or means to know what these representations look like or what meaning has been attached to such shapes. It is a privilege reserved to the designers of algorithmic processes: they own this right and we the citizens own the consequences.	https://dl.acm.org/doi/abs/10.1145/3414686.3427114	Annabel Castro
P-cloth: interactive complex cloth simulation on multi-GPU systems using dynamic matrix assembly and pipelined implicit integrators	We present a novel parallel algorithm for cloth simulation that exploits multiple GPUs for fast computation and the handling of very high resolution meshes. To accelerate implicit integration, we describe new parallel algorithms for sparse matrix-vector multiplication (SpMV) and for dynamic matrix assembly on a multi-GPU workstation. Our algorithms use a novel work queue generation scheme for a fat-tree GPU interconnect topology. Furthermore, we present a novel collision handling scheme that uses spatial hashing for discrete and continuous collision detection along with a non-linear impact zone solver. Our parallel schemes can distribute the computation and storage overhead among multiple GPUs and enable us to perform almost interactive simulation on complex cloth meshes, which can hardly be handled on a single GPU due to memory limitations. We have evaluated the performance with two multi-GPU workstations (with 4 and 8 GPUs, respectively) on cloth meshes with 0.5 -- 1.65 triangles. Our approach can reliably handle the collisions and generate vivid wrinkles and folds at 2 -- 5 fps, which is significantly faster than prior cloth simulation systems. We observe almost linear speedups with respect to the number of GPUs.	https://dl.acm.org/doi/abs/10.1145/3414685.3417763	Cheng Li, Min Tang, Ruofeng Tong, Ming Cai, Jieyi Zhao, Dinesh Manocha
PIE: portrait image embedding for semantic control	Editing of portrait images is a very popular and important research topic with a large variety of applications. For ease of use, control should be provided via a semantically meaningful parameterization that is akin to computer animation controls. The vast majority of existing techniques do not provide such intuitive and fine-grained control, or only enable coarse editing of a single isolated control parameter. Very recently, high-quality semantically controlled editing has been demonstrated, however only on synthetically created StyleGAN images. We present the first approach for embedding real portrait images in the latent space of StyleGAN, which allows for intuitive editing of the head pose, facial expression, and scene illumination in the image. Semantic editing in parameter space is achieved based on StyleRig, a pretrained neural network that maps the control space of a 3D morphable face model to the latent space of the GAN. We design a novel hierarchical non-linear optimization problem to obtain the embedding. An identity preservation energy term allows spatially coherent edits while maintaining facial integrity. Our approach runs at interactive frame rates and thus allows the user to explore the space of possible edits. We evaluate our approach on a wide set of portrait photos, compare it to the current state of the art, and validate the effectiveness of its components in an ablation study.	https://dl.acm.org/doi/abs/10.1145/3414685.3417803	Ayush Tewari, Mohamed Elgharib, Mallikarjun B R, Florian Bernard, Hans-Peter Seidel, Patrick Pérez, Michael Zollhöfer, Christian Theobalt
Painting of Thousandhands Avalokitesvara	"""Painting of Thousand-hands Avalokitesvara"" is a media art based on the theme of ""Painting of Thousand-hands Avalokitesvara (千手觀音圖),"" which paintings painted under the theme of Avalokitesvara (千手觀音) during or before Goryeo Dynasty. We reproduces the original Buddhist culture, which accounts for a large portion of Korea's culture archetype, in the three dimensional space. Avalokitesvara (千手觀音) appears in lotus flower on the center of artwork. Avalokitesvara is a Buddhist saint who saves people with a thousand-hands and a thousand eyes. Thousand-hands (千手) literally symbolize a thousand hands, and metaphorically symbolize the ability and its appearance is very diverse. Also in the artwork, Avalokitesvara has 11 faces, indicating that through Avalokitesvara's various appearances, they can save all of the people in various situations. On both sides of the Avalokitesvara, there are Four Devas (四天王), the four heavenly guardians of Buddhism. The Dragon King and the Sudhana (善財童子) appear After the appearance of the Four Devas. All of them gathered to listen to the teaching of the Avalokitesvara. Thousand-hands begin to unfold in the halo (光背) of the Avalokitesvara. After all the elements of the artwork such as the waves in the background and the Litany Buddha (化佛) appear, 42 hands which contained people's wishes with Buddhism things (持物) appear accordingly. Every time a 42-hands appears, the color of the thousand-hands in the halo changes and the thousand-hands take various hand movements (手印)."	https://dl.acm.org/doi/abs/10.1145/3414686.3427147	Minji Park, Yowon Jeong, Jiyun Jeong, Dain Kim, Jungbin Yoon, Haneul Jung, Kyeongju Hawng, Junghwan Sung
Path cuts: efficient rendering of pure specular light transport	In scenes lit with sharp point-like light sources, light can bounce several times on specular materials before getting into our eyes, forming purely specular light paths. However, to our knowledge, rendering such multi-bounce pure specular paths has not been handled in previous work: while many light transport methods have been devised to sample various kinds of light paths, none of them are able to find multi-bounce pure specular light paths from a point light to a pinhole camera. In this paper, we present path cuts to efficiently render such light paths. We use a path space hierarchy combined with interval arithmetic bounds to prune non-contributing regions of path space, and to slice the path space into regions small enough to empirically contain at most one solution. Next, we use an automatic differentiation tool and a Newton-based solver to find an admissible specular path within a given path space region. We demonstrate results on several complex specular configurations, including RR, TT, TRT and TTTT paths.	https://dl.acm.org/doi/abs/10.1145/3414685.3417792	Beibei Wang, Miloš Hašan, Ling-Qi Yan
Path differential-informed stratified MCMC and adaptive forward path sampling	Markov Chain Monte Carlo (MCMC) rendering is extensively studied, yet it remains largely unused in practice. We propose solutions to several practicability issues, opening up path space MCMC to become an adaptive sampling framework around established Monte Carlo (MC) techniques. We address non-uniform image quality by deriving an analytic target function for imagespace sample stratification. The function is based on a novel connection between variance and path differentials, allowing analytic variance estimates for MC samples, with potential uses in other adaptive algorithms outside MCMC. We simplify these estimates down to simple expressions using only quantities known in any MC renderer. We also address the issue that most existing MCMC renderers rely on bi-directional path tracing and reciprocal transport, which can be too costly and/or too complex in practice. Instead, we apply our theoretical framework to optimize an adaptive MCMC algorithm that only uses forward path construction. Notably, we construct our algorithm by adapting (with minimal changes) a full-featured path tracer into a single-path state space Markov Chain, bridging another gap between MCMC and existing MC techniques.	https://dl.acm.org/doi/abs/10.1145/3414685.3417856	Tobias Zirr, Carsten Dachsbacher
Path tracing estimators for refractive radiative transfer	Rendering radiative transfer through media with a heterogeneous refractive index is challenging because the continuous refractive index variations result in light traveling along curved paths. Existing algorithms are based on photon mapping techniques, and thus are biased and result in strong artifacts. On the other hand, existing unbiased methods such as path tracing and bidirectional path tracing cannot be used in their current form to simulate media with a heterogeneous refractive index. We change this state of affairs by deriving unbiased path tracing estimators for this problem. Starting from the refractive radiative transfer equation (RRTE), we derive a path-integral formulation, which we use to generalize path tracing with next-event estimation and bidirectional path tracing to the heterogeneous refractive index setting. We then develop an optimization approach based on fast analytic derivative computations to produce the point-to-point connections required by these path tracing algorithms. We propose several acceleration techniques to handle complex scenes (surfaces and volumes) that include participating media with heterogeneous refractive fields. We use our algorithms to simulate a variety of scenes combining heterogeneous refraction and scattering, as well as tissue imaging techniques based on ultrasonic virtual waveguides and lenses. Our algorithms and publicly-available implementation can be used to characterize imaging systems such as refractive index microscopy, schlieren imaging, and acousto-optic imaging, and can facilitate the development of inverse rendering techniques for related applications.	https://dl.acm.org/doi/abs/10.1145/3414685.3417793	Adithya Pediredla, Yasin Karimi Chalmiani, Matteo Giuseppe Scopelliti, Maysamreza Chamanzar, Srinivasa Narasimhan, Ioannis Gkioulekas
Persistence	Persistence is a kinetic installation exploring the conflict between geologic and human timescales. The Anthropocene, a proposed geological epoch, is proceeding towards a formal 'golden spike' to mark it's beginning. The installation investigates the fundamental dissonance one encounters when holding the ideas of planetary memory and personal experience simultaneously. Will we be defined by radionuclides, mass extinctions, and irreparable damage to the planet, or by a golden spike marking our ability to recognize and reverse current trajectories. Persistence acts as a fiducial, a fixed point, a reminder of our limitations, and fleeting collective memory. By drawing attention to our own limitations we hope to offer a space to allow viewers to reflect on the collective frailty of our memory, and the dire need to preserve the valuable life on this planet. The imagery of recently extinct animals, natural resources, and forgotten life forms will be displayed using the persistence machine. As the six-foot robotic arm rotates across a phosphorescent canvas, ultraviolet lasers activate the underlying pigment - revealing a fleeting image. Each additional pass of the robotic arm, mimicking a clock, invites new opportunities to allow existing memories and images to fade - or to activate entirely new compositions. In this digital representation of the project. A video (or real-time simulation) of the kinetic installation will be installed in the gallery for visitors to interact with. Visitors can select from a tablet a selection of topics to remember. The memory will be recalled using the kinetic persistence system. The visitor is invited to explore the memory and learn more about the topic to give the memory a new life.	https://dl.acm.org/doi/abs/10.1145/3414686.3427112	Harvest Moon, Josh Billions, Qianqian Ye
PhysCap: physically plausible monocular 3D motion capture in real time	Marker-less 3D human motion capture from a single colour camera has seen significant progress. However, it is a very challenging and severely ill-posed problem. In consequence, even the most accurate state-of-the-art approaches have significant limitations. Purely kinematic formulations on the basis of individual joints or skeletons, and the frequent frame-wise reconstruction in state-of-the-art methods greatly limit 3D accuracy and temporal stability compared to multi-view or marker-based motion capture. Further, captured 3D poses are often physically incorrect and biomechanically implausible, or exhibit implausible environment interactions (floor penetration, foot skating, unnatural body leaning and strong shifting in depth), which is problematic for any use case in computer graphics. We, therefore, present , the first algorithm for physically plausible, real-time and marker-less human 3D motion capture with a single colour camera at 25 fps. Our algorithm first captures 3D human poses purely kinematically. To this end, a CNN infers 2D and 3D joint positions, and subsequently, an inverse kinematics step finds space-time coherent joint angles and global 3D pose. Next, these kinematic reconstructions are used as constraints in a real-time physics-based pose optimiser that accounts for environment constraints ( , collision handling and floor placement), gravity, and biophysical plausibility of human postures. Our approach employs a combination of ground reaction force and residual force for plausible root control, and uses a trained neural network to detect foot contact events in images. Our method captures physically plausible and temporally stable global 3D human motion, without physically implausible postures, floor penetrations or foot skating, from video in real time and in general scenes. achieves state-of-the-art accuracy on established pose benchmarks, and we propose new metrics to demonstrate the improved physical plausibility and temporal stability.	https://dl.acm.org/doi/abs/10.1145/3414685.3417877	Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Christian Theobalt
Pixelor: a competitive sketching AI agent. so you think you can sketch?	We present the first competitive drawing agent that exhibits humanlevel performance at a Pictionary-like sketching game, where the participant whose sketch is recognized first is a winner. Our AI agent can autonomously sketch a given visual concept, and achieve a recognizable rendition as quickly or faster than a human competitor. The key to victory for the agent's goal is to learn the optimal stroke sequencing strategies that generate the most recognizable and distinguishable strokes first. Training is done in two steps. First, we infer the stroke order that maximizes early recognizability of human training sketches. Second, this order is used to supervise the training of a sequence-to-sequence stroke generator. Our key technical contributions are a tractable search of the exponential space of orderings using neural sorting; and an improved Seq2Seq Wasserstein (S2S-WAE) generator that uses an optimal-transport loss to accommodate the multi-modal nature of the optimal stroke distribution. Our analysis shows that is better than the human players of the game, under both AI and human judging of early recognition. To analyze the impact of human competitors' strategies, we conducted a further human study with participants being given unlimited thinking time and training in early recognizability by feedback from an AI judge. The study shows that humans do gradually improve their strategies with training, but overall still matches human performance. The code and the dataset are available at http://sketchx.ai/pixelor.	https://dl.acm.org/doi/abs/10.1145/3414685.3417840	Ayan Kumar Bhunia, Ayan Das, Umar Riaz Muhammad, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Yulia Gryaditskaya, Yi-Zhe Song
Plantext	Many people enjoy keeping houseplants and get comfort from the presence of plants. Not only for aesthetics and medical purposes, plants also have many other uses in human history. As a result, plant ecology and its biological evolutions are closely related to human culture. Normally people perceive plants as static objects, but in fact they do move and react to their surrounding environment in real-time. Their responses are just too slow to be recognized and their communication methods just differ from ours. Therefore, people find it hard to understand the biological and ecological contents underneath plants. We imagine what would happen if plants can talk, see, and sense as humans do. Our team is composed of researchers from engineering, HCI, and media arts. Our biology-computer hybrid installation project is collaboratively created based on our imagination driven from diverse experiences and interdisciplinary knowledge. Based on the imagination, we give each plant a character and exaggerate plants' sense by adding electronic devices with text to speech (TTS) voice synthesis and physics-based visual processing. The cultural histories of plants are spoken with all different synthesized human voices generated by our AI-based voice synthesis system. Also, as human vision responds to light, we also imagined that plants could see their surroundings through leaves. Because photosynthesis takes place there. By capturing the image from a mini-camera affixed to the leaf and showing the result of image processing in LCD screens placed among plants, we mimic a vision of the plants. The electrical signal is measured when users touch the plant and it distorts the audio-visual outputs. The overall experience with this work may arouse users to think about plants as a dynamic living being, opening a gap for users to understand the underlying context of plants more deeply.	https://dl.acm.org/doi/abs/10.1145/3414686.3427139	Kyung Chul Lee, Eun Young Lee, Joon Seok Moon, Ji Hun Jung, Hye Yun Park, Hyun Jean Lee, Seung Ah Lee
Playing with remine	Title of my work is 'Playing with Remine.' Playing means playing or interacting with each other. There are two reasons why I design this work. Currently, I'm designing my website using interactive pictures and videos which visitors can play with them by clicking. Because I unsatisfied with traditional way to communicare with audiences only through the art piece of the artist is onesided communication. If you click my character on the website, you can meet Kim Hae-min(me) sitting in the room. Kim Hae-min doesn't has any attention to visiotors and doing her own habits. But if visitors matches certain conditions which attracts Kim Hae-min she beings to interreact with visitors. After that text messages pops up and visitors can choice questions to ask they want. Kim Hae-min's response depends on which question you choose. The response can be kind, ignoring the questions, explain the work, or talk about society. In other words, it is a reactive character with various events. Motivation of this project is also my personal experience. I've felt that there's actually not a lot of communication between people, and how can I be honest, comfortable and funny? In other way i also plan to expand this work to installation project. The audience passes by and sees Kim Hae-min. The audience enters the installation space and asks questions to the character. And then, if you don't agree or if you have conflicting values, you move on to the mini-game format. 'Battle with Remine' I'm thinking of trying this piece with interactive projection mapping. In the mini-game, I'm going to put various devices to solve the conflicts between me and visitors.	https://dl.acm.org/doi/abs/10.1145/3414686.3427156	Haemin Kim
Poe's tree	"This artwork is inspired by the short story ""The Gold-Bug"" by Edgar Alan Poe. The story follows William Legrand, his servant Jupiter and an unnamed narrator on their quest to uncover a buried treasure. Poe took advantage of the popularity of cryptography as he was writing the ""The Gold-Bug"" and his story revolves around the team trying to solve a cipher. The characters in the story follow a simple substitution cipher to decode a message that eventually leads them to the treasure. With this project, the aim was to re-encode the decrypted text into a digital form and turn it into a 3d tree. In order for this to be achieved, the following process was used: 1) Using Chomky's Context-Free-Grammar, the text was broken down into a syntax tree. 2) By using a simple substitution process, like the one used by Poe, the syntax tree was turned into an L-systems syntax. 3) The tree was then generated using the build-in L-Systems function in Houdini 4) Maya was used to stylize, texture and render the 3d tree."	https://dl.acm.org/doi/abs/10.1145/3414686.3427164	Doros Polydorou
Point Nemo | sea [sic]	"Point Nemo is the name of the Oceanic pole of inaccessibility. The nearest terrestrial human life is located approximately 1,000 miles away; often, the nearest humans are located in space, approximately 250 miles away, aboard the International Space Station. The composition of this work draws inspiration from Théodore Géricault's painting, ""The Raft of Medusa"" (1818--19). Situated at a sublime intersect of sea and sky, this work represents a meditation on human desire --- the poetics that drive human exploration and the urgencies that underly human migration."	https://dl.acm.org/doi/abs/10.1145/3414686.3427115	Johannes DeYoung, Jack Vees
Prometheus string	Prometheus' string series attempt various artistic experiments applied to 'data refraction', a concept that breaks frame of data with modern concept of accurate delivery of information and induces more creative results. The resulting installation work includes various processes such as data extraction from living creature into 3D shape generation and printing, as well as robotic sculpture and data visual performance. 'Prometheus string' series began by recognizing the material essence of life as a stream of non-material data. For example, if you look at the human body, dead cells on one side are falling apart, and on the other, new cell division is constantly occurring. The living things that we can see and touch are just a piece of the continual line of life and death of the many invisible substances that make up our body. The information of living things that have been digitalized becomes a model for realizing unexpected results through a process of 'refraction of data' that is transferred or transformed in a way suitable for various systems. Of course, using information about life can't be said that the information can replace life, but it can be a significant attempt to approach the essence of life with a new perspective on the digital system, which is gradually expanding its scope. Prometheus' string series also intends to continue various artistic experiments applied to the 'refraction of data', a concept that breaks the frame of data with a modern concept of accurate delivery of information and induces more creative results. The experiment to convert from material life to non-material information and from information to artificial life will continue. Also experiments of connecting artificial intelligence patterns for robot movement with social discourse as well.	https://dl.acm.org/doi/abs/10.1145/3414686.3427137	Seung Jung
QuickETC2: Fast ETC2 texture compression using Luma differences	Compressed textures are indispensable in most 3D graphics applications to reduce memory traffic and increase performance. For higher-quality graphics, the number and size of textures in an application have continuously increased. Additionally, the ETC2 texture format, which is mandatory in OpenGL ES 3.0, OpenGL 4.3, and Android 4.3 (and later versions), requires more complex texture compression than the traditional ETC1 format. As a result, texture compression becomes more and more time-consuming. To accelerate ETC2 compression, we introduce two new compression techniques, named QuickETC2. The first technique is an early compression-mode decision scheme. Instead of testing all ETC1/2 modes to compress a texel block, we select proper modes for each block by exploiting the luma difference of the block to reduce unnecessary compression overhead. The second technique is a fast luma-based T- and H-mode compression method. When clustering each texel into two groups, we replace the 3D RGB space with the 1D luma space and quickly find the two groups that have the minimum luma differences. We also selectively perform the T- or H-mode and reduce its distance candidates, according to the luma differences of each group. We have implemented both techniques with AVX2 intrinsics to exploit SIMD parallelism. According to our experiments, QuickETC2 can compress more than 2000 1K×1K-sized images per second on an octa-core CPU.	https://dl.acm.org/doi/abs/10.1145/3414685.3417787	Jae-Ho Nah
RBF liquids: an adaptive PIC solver using RBF-FD	We introduce a novel liquid simulation approach that combines a spatially adaptive pressure projection solver with the Particle-in-Cell (PIC) method. The solver relies on a generalized version of the Finite Difference (FD) method to approximate the pressure field and its gradients in tree-based grid discretizations, possibly non-graded. In our approach, FD stencils are computed by using meshfree interpolations provided by a variant of Radial Basis Function (RBF), known as RBF-Finite-Difference (RBF-FD). This meshfree version of the FD produces differentiation weights on scattered nodes with high-order accuracy. Our method adapts a quadtree/octree dynamically in a narrow-band around the liquid interface, providing an adaptive particle sampling for the PIC advection step. Furthermore, RBF affords an accurate scheme for velocity transfer between the grid and particles, keeping the system's stability and avoiding numerical dissipation. We also present a data structure that connects the spatial subdivision of a quadtree/octree with the topology of its corresponding dual-graph. Our data structure makes the setup of stencils straightforward, allowing its updating without the need to rebuild it from scratch at each time-step. We show the effectiveness and accuracy of our solver by simulating incompressible inviscid fluids and comparing results with regular PIC-based solvers available in the literature.	https://dl.acm.org/doi/abs/10.1145/3414685.3417794	Rafael Nakanishi, Filipe Nascimento, Rafael Campos, Paulo Pagliosa, Afonso Paiva
RGB2Hands: real-time tracking of 3D hand interactions from monocular RGB video	Tracking and reconstructing the 3D pose and geometry of two hands in interaction is a challenging problem that has a high relevance for several human-computer interaction applications, including AR/VR, robotics, or sign language recognition. Existing works are either limited to simpler tracking settings ( , considering only a single hand or two spatially separated hands), or rely on less ubiquitous sensors, such as depth cameras. In contrast, in this work we present the first real-time method for motion capture of skeletal pose and 3D surface geometry of hands from a single RGB camera that explicitly considers close interactions. In order to address the inherent depth ambiguities in RGB data, we propose a novel multi-task CNN that regresses multiple complementary pieces of information, including segmentation, dense matchings to a 3D hand model, and 2D keypoint positions, together with newly proposed intra-hand relative depth and inter-hand distance maps. These predictions are subsequently used in a generative model fitting framework in order to estimate pose and shape parameters of a 3D hand model for both hands. We experimentally verify the individual components of our RGB two-hand tracking and 3D reconstruction pipeline through an extensive ablation study. Moreover, we demonstrate that our approach offers previously unseen two-hand tracking performance from RGB, and quantitatively and qualitatively outperforms existing RGB-based methods that were not explicitly designed for two-hand interactions. Moreover, our method even performs on-par with depth-based real-time methods.	https://dl.acm.org/doi/abs/10.1145/3414685.3417852	Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne Sorli, Oleksandr Sotnychenko, Neng Qian, Miguel A. Otaduy, Dan Casas, Christian Theobalt
Rainbow tunnel	A still photo from artificial life, this is a frozen moment from the movement of simple shapes. Repeated geometric forms were rotated and transformed over time and complex interwoven abstract patterns emerged. These unexpected forms are born from motion and feedback. Initial graphical parameters were predefined and the patterned evolution was set in motion. When the motion is paused the cellular beauty of individual frames is revealed. Chance plays its part in this phenomenon. Individual parameters are predetermined but the end result is indeterminate. The space between known quantities is where the unexpected patterns and lights emerge.	https://dl.acm.org/doi/abs/10.1145/3414686.3427109	Dave Payling
Real-time rendering of decorative sound textures for soundscapes	Audio recordings contain rich information about sound sources and their properties such as the location, loudness, and frequency of events. One prevalent component in sound recordings is the sound texture, which contains a massive number of events. In such a texture, there can be some distinct and repeated sounds that we term as a foreground sound. Birds chirping in the wind is one such decorative sound texture with the chirping as a foreground sound and the wind as a background texture. To render these decorative sound textures in real-time and with high quality, we create two-layer Markov Models to enable smooth transitions from sound grain to sound grain and propose a hierarchical scheme to generate Head-Related Transfer Function filters for localization cues of sounds represented as area/volume sources. Moreover, during the synthesis stage, we provide control over the frequency and intensity of sounds for customization. Lastly, foreground sounds are often blended into background textures such as the sound of rain splats on car surfaces becoming submerged in the background rain. We develop an extraction component that outperforms existing learning-based methods to facilitate our synthesis with perceptible foreground sounds and well-defined textures.	https://dl.acm.org/doi/abs/10.1145/3414685.3417875	Jinta Zheng, Shih-Hsuan Hung, Kyle Hiebel, Yue Zhang
Realistic Volumetric 3D Display Using Physical Materials	Conventional swept volumetric displays can provide accurate physical cues for depth perception. However, the quality of texture reproduction is not high because these displays use high-speed projectors with low bit depth and low resolution. In this study, to address this limitation of swept volumetric displays while retaining their advantages, a new swept volumetric three-dimensional (3D) display is designed using physical materials as screens. Physical materials are directly used to reproduce textures on a displayed 3D surface. Further, our system can achieve hidden-surface removal based on real-time viewpoint tracking.	https://dl.acm.org/doi/abs/10.1145/3415255.3422879	Ray Asahina, Takashi Nomoto, Takatoshi Yoshida, Yoshihiro Watanabe
Reinforced FDM: multi-axis filament alignment with controlled anisotropic strength	The anisotropy of mechanical strength on a 3D printed model can be controlled in a multi-axis 3D printing system as materials can be accumulated along dynamically varied directions. In this paper, we present a new computational framework to generate specially designed layers and toolpaths of multi-axis 3D printing for strengthening a model by aligning filaments along the directions with large stresses. The major challenge comes from how to effectively decompose a solid into a sequence of strength-aware and collision-free working surfaces. We formulate it as a problem to compute an optimized governing field together with a selected orientation of fabrication setup. Iso-surfaces of the governing field are extracted as working surface layers for filament alignment. Supporting structures in curved layers are constructed by extrapolating the governing field to enable the fabrication of overhangs. Compared with planar-layer based Fused Deposition Modeling (FDM) technology, models fabricated by our method can withstand up to 6 35× loads in experimental tests.	https://dl.acm.org/doi/abs/10.1145/3414685.3417834	Guoxin Fang, Tianyu Zhang, Sikai Zhong, Xiangjia Chen, Zichun Zhong, Charlie C. L. Wang
Rendering near-field speckle statistics in scattering media	We introduce rendering algorithms for the simulation of speckle statistics observed in scattering media under coherent near-field imaging conditions. Our work is motivated by the recent proliferation of techniques that use speckle correlations for tissue imaging applications: The ability to simulate the image measurements used by these speckle imaging techniques in a physically-accurate and computationally-efficient way can facilitate the widespread adoption and improvement of these techniques. To this end, we draw inspiration from recently-introduced Monte Carlo algorithms for rendering speckle statistics under far-field conditions (collimated sensor and illumination). We derive variants of these algorithms that are better suited to the near-field conditions (focused sensor and illumination) required by tissue imaging applications. Our approach is based on using Gaussian apodization to approximate the sensor and illumination aperture, as well as von Mises-Fisher functions to approximate the phase function of the scattering material. We show that these approximations allow us to derive closed-form expressions for the focusing operations involved in simulating near-field speckle patterns. As we demonstrate in our experiments, these approximations accelerate speckle rendering simulations by a few orders of magnitude compared to previous techniques, at the cost of negligible bias. We validate the accuracy of our algorithms by reproducing ground truth speckle statistics simulated using wave-optics solvers, and real-material measurements available in the literature. Finally, we use our algorithms to simulate biomedical imaging techniques for focusing through tissue.	https://dl.acm.org/doi/abs/10.1145/3414685.3417813	Chen Bar, Ioannis Gkioulekas, Anat Levin
River's edge	"""River's Edge"" is the title of a series of collage artworks created from images obtained via the Internet through the medium of generative programming. In this series of images, the artist used a keyword associated with his childhood memory ""River's Edge""to conduct an in-depth search for and gather associated images, which he then assembled into vivid and visually appealing collages. In the ""River's Edge"" artwork, blue, gray, and green sections of the collected images were associated with water, stone, and sky. Pieces from hundreds of images were extracted from the Internet in data form, processed, and emplaced in the images. The creative process was based on an algorithm that examined the collected images, extracted appealing sections, subjected them to a limited set of modifications, and then emplaced them into the artwork. Although the algorithm's functionalities are limited to magnification, rotation, and choosing the areas to extract from the collected imagery, the process made it possible to create a wide variety of collages. In the numerous trials that were conducted to develop this art form, several new expressions were identified, and many beautiful patterns were created."	https://dl.acm.org/doi/abs/10.1145/3414686.3427103	Takuya Yamauchi
Roads in you	Roads in You is an interactive biometric-data artwork that allows participants to scan their veins and find the roads that match their vein lines. The vein data as one of the fascinating forms of biometric data contain uniquely complicated lines that resemble the roads and paths surrounding us. The roads resemble how our vein lines are interconnected and how the blood circulates in our bodies in various directions, at various speeds, and in different conditions. This artwork explores the line segmentation and the structure of veins and compares them to roads in the real world. The participants can also export the data and keep them as a personalized souvenir (3d printed sculptures) as part of the artistic experience. Through this project, users can explore the correlation between individuals and environments using the hidden patterns under the skin and vein recognition techniques and image processing. This project also has the potential to lead the way in the interpretation of complicated datasets while providing aesthetically beautiful and mesmerizing visualizations.	https://dl.acm.org/doi/abs/10.1145/3414686.3427143	Yoon Chung Han, Ryan Cottone, Anusha
RoboGrammar: graph grammar for terrain-optimized robot design	We present , a fully automated approach for generating optimized robot structures to traverse given terrains. In this framework, we represent each robot design as a graph, and use a graph grammar to express possible arrangements of physical robot assemblies. Each robot design can then be expressed as a sequence of grammar rules. Using only a small set of rules our grammar can describe hundreds of thousands of possible robot designs. The construction of the grammar limits the design space to designs that can be fabricated. For a given input terrain, the design space is searched to find the top performing robots and their corresponding controllers. We introduce Graph Heuristic Search - a novel method for efficient search of combinatorial design spaces. In Graph Heuristic Search, we explore the design space while simultaneously learning a function that maps incomplete designs (e.g., nodes in the combinatorial search tree) to the best performance values that can be achieved by expanding these incomplete designs. Graph Heuristic Search prioritizes exploration of the most promising branches of the design space. To test our method we optimize robots for a number of challenging and varied terrains. We demonstrate that can successfully generate nontrivial robots that are optimized for a single terrain or a combination of terrains.	https://dl.acm.org/doi/abs/10.1145/3414685.3417831	Allan Zhao, Jie Xu, Mina Konaković-Luković, Josephine Hughes, Andrew Spielberg, Daniela Rus, Wojciech Matusik
Room view	Room View is a piece documenting the view outside of my room in Manhattan, from April to March 2020. I recorded the sound in 30 days of quarantine (including radio, sirens, people clapping for the essential workers, etc.), and several views of the Chrysler Building in different weather conditions. The melting of photographs or videos is triggered by the sounds. Depicting the state of mind when I was absorbed by the view, quiet and slow. As time passing by, the sound became a way I rely on to know what is happening outside my room - in the real world. The portrait mode of the photographs inherits from the idea of how we receive and send out the information through mobile devices. The virtual view becomes our new reality.	https://dl.acm.org/doi/abs/10.1145/3414686.3427122	Yalan Wen
Scan	Scan was inspired by an accident I encountered in 2019. After my arm was injured, I barely remember the details of the accident. Through this work, I'd like to explore the relationship between our body memory and the memory we fill up with our own imaginations. As the strips reveal the scar, the memory of the event is not clear anymore, it's filled with our own interpretation.	https://dl.acm.org/doi/abs/10.1145/3414686.3427123	Yalan Wen
Scene mover: automatic move planning for scene arrangement by deep reinforcement learning	We propose a novel approach for automatically generating a move plan for scene arrangement. Given a scene like an apartment with many furniture objects, to transform its layout into another layout, one would need to determine a collision-free move plan. It could be challenging to design this plan manually because the furniture objects may block the way of each other if not moved properly; and there is a large complex search space of move action sequences that grow exponentially with the number of objects. To tackle this challenge, we propose a learning-based approach to generate a move plan automatically. At the core of our approach is a Monte Carlo tree that encodes possible states of the layout, based on which a search is performed to move a furniture object appropriately in the current layout. We trained a policy neural network embedded with a LSTM module for estimating the best actions to take in the expansion step and simulation step of the Monte Carlo tree search process. Leveraging the power of deep reinforcement learning, the network learned how to make such estimations through millions of trials of moving objects. We demonstrated our approach for moving objects under different scenarios and constraints. We also evaluated our approach on synthetic and real-world layouts, comparing its performance with that of humans and other baseline approaches.	https://dl.acm.org/doi/abs/10.1145/3414685.3417788	Hanqing Wang, Wei Liang, Lap-Fai Yu
Screen-space blue-noise diffusion of monte carlo sampling error via hierarchical ordering of pixels	We present a novel technique for diffusing Monte Carlo sampling error as a blue noise in screen space. We show that automatic diffusion of sampling error can be achieved by ordering the pixels in a way that preserves locality, such as Morton's Z-ordering, and assigning the samples to the pixels from successive sub-sequences of a single low-discrepancy sequence, thus securing well-distributed samples for each pixel, local neighborhoods, and the whole image. We further show that a blue-noise distribution of the error is attainable by scrambling the Z-ordering to induce isotropy. We present an efficient technique to implement this hierarchical scrambling by defining a context-free grammar that describes infinite self-similar lookup trees. Our concept is scalable to arbitrary image resolutions, sample dimensions, and sample count, and supports progressive and adaptive sampling.	https://dl.acm.org/doi/abs/10.1145/3414685.3417881	Abdalla G. M. Ahmed, Peter Wonka
Searching all sources of white	Searching All Sources of White presents the error as the landscape. It is an interactive video installation examining the idea of the limitation of seeing. The projector projects a blue screen while a white spot falls in the middle. Blue is often seen in a digital display namely default screen, calibration screen, sleep mode, and 'Blue Screen of Death'. In an exhibition setting, this work gives the impression of a failure in the display when the projector displays a blue, standbymode. Standby is a component mode in which a system is kept readily available in case an unexpected event occurs. A system may be on standby in case of failure, shortage, or other similar events. The interaction is analogue rather than digital. The work invites the audience's body movement as a variation in the scene. The blue landscape is an illusion to the audience that the display device is having a malfunction situation. The switching text on the bottom first misleads the audience that it is a common standby mode text searching for the input source. It invites the audience to step into the projection area. When the blue light source from the projector is blocked by a body, a yellow light appears. The white spot in the middle is never a white light source. The white results from the addition of complementary-colour light source - yellow and blue. In the RGB colour system, mixing the primary colour blue with the complementary colour yellow would produce the colour white. There is a limitation of the human eyes which cannot analyse a mixture of complementary-colour light, results in perceiving it in white. The malfunction scene and the illusion of colour in human eyes encourage the audience to reflect on the limitations in the spectrum of human visual perception.	https://dl.acm.org/doi/abs/10.1145/3414686.3427131	Gi Wai Echo Hui
Selfie + code III: melancholy	"A selfie is a form of art. Over 1 million selfies are now taken every day. Selfies are not always as spontaneous as they seem. They can be a communication tool like any other that can be manipulated for purposes. Selfie + CODE III is a collection of generative selfie series by using computer algorithms. The algorithmic processes expend the concepts of traditional self-portraits to generative and expressive selfies delivering thought or feeling. The artist started taking her generative selfies in 2015 to raise awareness of Asian female faculty being isolated and marginal in a predominantly white institution (http://www.socialhomelessness.com). Her generative selfies have captured psychological moments to express those individual identities are devalued and deconstructed by a homogeneous institution in the United States. It has been shared by social media. The virtual supporting system at Facebook, ""Like,"" by her diverse mentors and friends, helped her to persist and survive in a regionally isolated and exclusive community. Eventually, It has brought her psychological reconciliation and healing to succeed in dealing with difficulties."	https://dl.acm.org/doi/abs/10.1145/3414686.3427128	Yeohyun Ahn
Semi-analytic boundary handling below particle resolution for smoothed particle hydrodynamics	In this paper, we present a novel semi-analytical boundary handling method for spatially adaptive and divergence-free smoothed particle hydrodynamics (SPH) simulations, including two-way coupling. Our method is consistent under varying particle resolutions and allows for the treatment of boundary features below the particle resolution. We achieve this by first introducing an analytic solution to the interaction of SPH particles with planar boundaries, in 2D and 3D, which we extend to arbitrary boundary geometries using signed distance fields (SDF) to construct locally planar boundaries. Using this boundary-integral-based approach, we can directly evaluate boundary contributions, for any quantity, allowing an easy integration into state of the art simulation methods. Overall, our method improves interactions with small boundary features, readily handles spatially adaptive fluids, preserves particle-boundary interactions across varying resolutions, can directly be implemented in existing SPH methods, and, for non-adaptive simulations, provides a reduction in memory consumption as well as an up to 2× speedup relative to current particle-based boundary handling approaches.	https://dl.acm.org/doi/abs/10.1145/3414685.3417829	Rene Winchenbach, Rustam Akhunov, Andreas Kolb
Seoul landscape	See the scenery of the city through Korean traditional music. 'object' exists with time. We sometimes bring the 'object' of the past to reproduce the time of the past. 'object' becomes the music of Korean traditional music. What we are trying to reproduce is nature. What is nature? Nature is a phenomenon itself. For people in the modern world, is nature an urban ecosystem? We find the way back to nature through the 'object' called 'Korean traditional music'. It looks at nature as the whole being, not as an individual who only has the impression of passing through without a clear form.	https://dl.acm.org/doi/abs/10.1145/3414686.3427170	Dohee Jeon, Hannah Kim
Shadow play: tales of urbanization	Over the past few decades, China has been undergoing urbanization at an astounding pace. In 2013, the national leadership raised the process to a new gear when it unveiled its plan of converting 70 percent of the population to a cityoriented lifestyle by 2025. Such a significant change would undoubtedly transform the character of a country that has been largely agrarian throughout its millennia of history. One may wonder how, and to what extent, the landscape, culture and the daily being of the nation's people may be altered. As artists, we are compelled to explore and reflect upon the various phases of this historic undertaking while questioning how people are positioned during this monumental social transformation. Through fieldwork in China, we collect the ingredients necessary for a multimedia production that combines traditional artistic expressions with emerging technologies. Weaving three interfaces, namely virtual reality (VR) in cyberspace, a series of painting on canvases and traditional shadow play imagery, the multimedia art project visualizes the metamorphosis that results from the urbanization process. With a retrospective into the past through time-honored imagery and a reflection of the present through immersion in the realities of the modern China, we seek to present stories of everyday people to the conscience of a worldwide audience.	https://dl.acm.org/doi/abs/10.1145/3414686.3427102	Xiying Yang, Honglei Li, He Li
Shape approximation by developable wrapping	We present an automatic tool to approximate curved geometries with piece-wise developable surfaces. At the center of our work is an algorithm that wraps a given 3D input surface with multiple developable patches, each modeled as a discrete orthogonal geodesic net. Our algorithm features a global optimization routine for effectively finding the placement of the developable patches. After wrapping the mesh, we use these patches and a non-linear projection step to generate a surface that approximates the original input, but is also amendable to simple and efficient fabrication techniques thanks to being piecewise developable. Our algorithm allows users to steer the trade-off between approximation power and the number of developable patches used. We demonstrate the effectiveness of our approach on a range of 3D shapes. Compared to previous approaches, our results exhibit a smaller or comparable error with fewer patches to fabricate.	https://dl.acm.org/doi/abs/10.1145/3414685.3417835	Alexandra Ion, Michael Rabinovich, Philipp Herholz, Olga Sorkine-Hornung
ShapeAssembly: learning to generate programs for 3D shape structure synthesis	"Manually authoring 3D shapes is difficult and time consuming; generative models of 3D shapes offer compelling alternatives. Procedural representations are one such possibility: they offer high-quality and editable results but are difficult to author and often produce outputs with limited diversity. On the other extreme are deep generative models: given enough data, they can learn to generate any class of shape but their outputs have artifacts and the representation is not editable. In this paper, we take a step towards achieving the best of both worlds for novel 3D shape synthesis. First, we propose ShapeAssembly, a domain-specific ""assembly-language"" for 3D shape structures. ShapeAssembly programs construct shape structures by declaring cuboid part proxies and attaching them to one another, in a hierarchical and symmetrical fashion. ShapeAssembly functions are parameterized with continuous free variables, so that one program structure is able to capture a family of related shapes. We show how to extract ShapeAssembly programs from existing shape structures in the PartNet dataset. Then, we train a deep generative model, a hierarchical sequence VAE, that learns to write novel ShapeAssembly programs. Our approach leverages the strengths of each representation: the program captures the subset of shape variability that is interpretable and editable, and the deep generative model captures variability and correlations across shape collections that is hard to express procedurally. We evaluate our approach by comparing the shapes output by our generated programs to those from other recent shape structure synthesis models. We find that our generated shapes are more plausible and physically-valid than those of other methods. Additionally, we assess the latent spaces of these models, and find that ours is better structured and produces smoother interpolations. As an application, we use our generative model and differentiable program interpreter to infer and fit shape programs to unstructured geometry, such as point clouds."	https://dl.acm.org/doi/abs/10.1145/3414685.3417812	R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie
Simplexity 01	Simplexity 01 is a selection from an on-going experimental project that explores how unexpected visual complexities emerge from simple algorithmic procedures. For this particular work, the artist appropriated a simple space-filling algorithm into a generative medium for producing an unseen imaginary structure. The quality that identifies this work is the structure's semi-organic appearance, and the artist sees this emergence as a direct result of the spatial scale that the algorithm was allowed to explore.	https://dl.acm.org/doi/abs/10.1145/3414686.3427117	Kin-Ming Wong
Simulation, modeling and authoring of glaciers	Glaciers are some of the most visually arresting and scenic elements of cold regions and high mountain landscapes. Although snow-covered terrains have previously received attention in computer graphics, simulating the temporal evolution of glaciers as well as modeling their wide range of features has never been addressed. In this paper, we combine a Shallow Ice Approximation simulation with a procedural amplification process to author high-resolution realistic glaciers. Our multiresolution method allows the interactive simulation of the formation and the evolution of glaciers over hundreds of years. The user can easily modify the environment variables, such as the average temperature or precipitation rate, to control the glacier growth, or directly use brushes to sculpt the ice or bedrock with interactive feedback. Mesoscale and smallscale landforms that are not captured by the glacier simulation, such as crevasses, moraines, seracs, ogives, or icefalls are synthesized using procedural rules inspired by observations in glaciology and according to the physical parameters derived from the simulation. Our method lends itself to seamless integration into production pipelines to decorate reliefs with glaciers and realistic ice features.	https://dl.acm.org/doi/abs/10.1145/3414685.3417855	Oscar Argudo, Eric Galin, Adrien Peytavie, Axel Paris, Eric Guérin
Single image portrait relighting via explicit multiple reflectance channel modeling	Portrait relighting aims to render a face image under different lighting conditions. Existing methods do not explicitly consider some challenging lighting effects such as specular and shadow, and thus may fail in handling extreme lighting conditions. In this paper, we propose a novel framework that explicitly models multiple reflectance channels for single image portrait relighting, including the facial albedo, geometry as well as two lighting effects, , specular and shadow. These channels are finally composed to generate the relit results via deep neural networks. Current datasets do not support learning such multiple reflectance channel modeling. Therefore, we present a large-scale dataset with the ground-truths of the channels, enabling us to train the deep neural networks in a supervised manner. Furthermore, we develop a novel module named Lighting guided Feature Modulation (LFM). In contrast to existing methods which simply incorporate the given lighting in the bottleneck of a network, LFM fuses the lighting by layer-wise feature modulation to deliver more convincing results. Extensive experiments demonstrate that our proposed method achieves better results and is able to generate challenging lighting effects.	https://dl.acm.org/doi/abs/10.1145/3414685.3417824	Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, Feng Xu
Sketch2CAD: sequential CAD modeling by sketching in context	We present a sketch-based CAD modeling system, where users create objects incrementally by sketching the desired shape edits, which our system automatically translates to CAD operations. Our approach is motivated by the close similarities between the steps industrial designers follow to draw 3D shapes, and the operations CAD modeling systems offer to create similar shapes. To overcome the strong ambiguity with parsing 2D sketches, we observe that in a sketching sequence, each step makes sense and can be interpreted in the of what has been drawn before. In our system, this context corresponds to a partial CAD model, inferred in the previous steps, which we feed along with the input sketch to a deep neural network in charge of interpreting how the model should be modified by that sketch. Our deep network architecture then recognizes the intended CAD operation and segments the sketch accordingly, such that a subsequent optimization estimates the parameters of the operation that best fit the segmented sketch strokes. Since there exists no datasets of paired sketching and CAD modeling sequences, we train our system by generating synthetic sequences of CAD operations that we render as line drawings. We present a proof of concept realization of our algorithm supporting four frequently used CAD operations. Using our system, participants are able to quickly model a large and diverse set of objects, demonstrating Sketch2CAD to be an alternate way of interacting with current CAD modeling systems.	https://dl.acm.org/doi/abs/10.1145/3414685.3417807	Changjian Li, Hao Pan, Adrien Bousseau, Niloy J. Mitra
SketchPatch: sketch stylization via seamless patch-level synthesis	The paradigm of image-to-image translation is leveraged for the benefit of sketch stylization via transfer of geometric textural details. Lacking the necessary volumes of data for standard training of translation systems, we advocate for operation at the patch level, where a handful of stylized sketches provide ample mining potential for patches featuring basic geometric primitives. Operating at the patch level necessitates special consideration of full sketch translation, as individual translation of patches with no regard to neighbors is likely to produce visible seams and artifacts at patch borders. Aligned pairs of styled and plain primitives are combined to form input hybrids containing styled elements around the border and plain elements within, and given as input to a seamless translation (ST) generator, whose output patches are expected to reconstruct the fully styled patch. An adversarial addition promotes generalization and robustness to diverse geometries at inference time, forming a simple and effective system for arbitrary sketch stylization, as demonstrated upon a variety of styles and sketches.	https://dl.acm.org/doi/abs/10.1145/3414685.3417816	Noa Fish, Lilach Perry, Amit Bermano, Daniel Cohen-Or
SkyWindow	"During 3 months of being quarantine in our tiny apartment, here comes the project, ""SkyWindow"". The concept of the ""SkyWindow"" is the idea of being a mental escape from reality, especially under the unprecedented time. Being quarantine in an entire enclosure space continuously for numerous hours and days, people are desperately looking for reliefs in any possible ways. Through the artist's interactive design, looking up to the imaginary sky could be the most enjoyable solution to get the immediate comfort without going out. The ""SkyWindow"" is an immersive and intimate experience with sky-like projections on the ceiling like putting a void hole to it as an interactive installation. A dark environment with the projected sky/universe on the ceiling intriguing the audience to walk closer underneath. Further, the visual graphic will induce the audience to reach out to their hands like touching the sky to trigger the raindrops (meteor shower) and sounds falling from the ""SkyWindow."" The ""SkyWindow"" here metaphorically represents a piece of ""hope"" people can expect during the pandemic. No matter a planet far away in the dark or sunlight in the bright, it gives you unexpected joy and surprise in the design. Besides exposing under different spatial scenes, through this ""SkyWindow,"" waving hands in the air will trigger the (meteor) shower falling from the Sky which ironically implies the power of control that people have been losing it for a while under such an unpredictable moment. And the (meteor) shower implicitly refers to wash out all the illness and sadness for returning the clean and pure spirits."	https://dl.acm.org/doi/abs/10.1145/3414686.3427133	Jia-Rey Chang
Slope-space integrals for specular next event estimation	Monte Carlo light transport simulations often lack robustness in scenes containing specular or near-specular materials. Widely used uni- and bidirectional sampling strategies tend to find light paths involving such materials with insufficient probability, producing unusable images that are contaminated by significant variance. This article addresses the problem of sampling a light path connecting two given scene points via a single specular reflection or refraction, extending the range of scenes that can be robustly handled by unbiased path sampling techniques. Our technique enables efficient rendering of challenging transport phenomena caused by such paths, such as underwater caustics or caustics involving glossy metallic objects. We derive analytic expressions that predict the total radiance due to a single reflective or refractive triangle with a microfacet BSDF and we show that this reduces to the well known Lambert boundary integral for irradiance. We subsequently show how this can be leveraged to efficiently sample connections on meshes comprised of vast numbers of triangles. Our derivation builds on the theory of off-center microfacets and involves integrals in the space of surface slopes. Our approach straightforwardly applies to the related problem of rendering glints with high-resolution normal maps describing specular microstructure. Our formulation alleviates problems raised by singularities in filtering integrals and enables a generalization of previous work to perfectly specular materials. We also extend previous work to the case of GGX distributions and introduce new techniques to improve accuracy and performance.	https://dl.acm.org/doi/abs/10.1145/3414685.3417811	Guillaume Loubet, Tizian Zeltner, Nicolas Holzschuch, Wenzel Jakob
Sparse cholesky updates for interactive mesh parameterization	We present a novel linear solver for interactive parameterization tasks. Our method is based on the observation that quasi-conformal parameterizations of a triangle mesh are largely determined by boundary conditions. These boundary conditions are typically constructed interactively by users, who have to take several artistic and geometric constraints into account while introducing cuts on the geometry. Commonly, the main computational burden in these methods is solving a linear system every time new boundary conditions are imposed. The core of our solver is a novel approach to efficiently update the Cholesky factorization of the linear system to reflect new boundary conditions, thereby enabling a seamless and interactive workflow even for large meshes consisting of several millions of vertices.	https://dl.acm.org/doi/abs/10.1145/3414685.3417828	Philipp Herholz, Olga Sorkine-Hornung
Speech gesture generation from the trimodal context of text, audio, and speaker identity	For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human-agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is difficult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches attempt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are human-like and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed that the proposed gesture generation model is better than existing end-to-end generation models. We further confirm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that different gesture styles can be generated for the same speech by specifying different speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.	https://dl.acm.org/doi/abs/10.1145/3414685.3417838	Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, Geehyuk Lee
Stormscapes: simulating cloud dynamics in the now	The complex interplay of a number of physical and meteorological phenomena makes simulating clouds a challenging and open research problem. We explore a physically accurate model for simulating clouds and the dynamics of their transitions. We propose first-principle formulations for computing buoyancy and air pressure that allow us to simulate the variations of atmospheric density and varying temperature gradients. Our simulation allows us to model various cloud types, such as cumulus, stratus, and stratoscumulus, and their realistic formations caused by changes in the atmosphere. Moreover, we are able to simulate large-scale cloud super cells - clusters of cumulonimbus formations - that are commonly present during thunderstorms. To enable the efficient exploration of these stormscapes, we propose a lightweight set of high-level parameters that allow us to intuitively explore cloud formations and dynamics. Our method allows us to simulate cloud formations of up to about 20 km × 20 km extents at interactive rates. We explore the capabilities of physically accurate and yet interactive cloud simulations by showing numerous examples and by coupling our model with atmosphere measurements of real-time weather services to simulate cloud formations in the now. Finally, we quantitatively assess our model with cloud fraction profiles, a common measure for comparing cloud types.	https://dl.acm.org/doi/abs/10.1145/3414685.3417801	Torsten Hädrich, Miłosz Makowski, Wojtek Pałubicki, Daniel T. Banuti, Sören Pirk, Dominik L. Michels
Suprasymmetry	The topic of my research was problem of creating virtual environments (VE) in immersive art. I was focused on a roles of Presence, Flow, Immersion, and Interactivity. I was particularly interested in the problem of presence and flow in VE. Presence is defined as the subjective experience of being in one place or environment, even when one is physically situated in another. Presence is a normal awareness phenomenon that requires directed attention and is based in the interaction between sensory stimulation, environmental factors that encourage involvement and enable immersion. Flow is a state of experience where someone is completely absorbed and immersed in an activity. I researched relations between presence, flow, immersion and interactivity, e.g. how interactivity and sound spatialization improves experience of presence. I have developed machine learning methods that extend granular and pulsar synthesis in composing and new methods of building and transforming virtual environments.	https://dl.acm.org/doi/abs/10.1145/3414686.3427155	Robert Lisek
Surface-only ferrofluids	We devise a novel surface-only approach for simulating the three dimensional free-surface flow of incompressible, inviscid, and linearly magnetizable ferrofluids. A Lagrangian velocity field is stored on a triangle mesh capturing the fluid's surface. The two key problems associated with the dynamic simulation of the fluid's interesting geometry are the magnetization process transitioning the fluid from a non-magnetic into a magnetic material, and the evaluation of magnetic forces. In this regard, our key observation is that for linearly incompressible ferrofluids, their magnetization and application of magnetic forces only require knowledge about the position of the fluids' boundary. Consequently, our approach employs a boundary element method solving the magnetization problem and evaluating the so-called magnetic pressure required for the force evaluation. The magnetic pressure is added to the Dirichlet boundary condition of a surface-only liquids solver carrying out the dynamical simulation. By only considering the fluid's surface in contrast to its whole volume, we end up with an efficient approach enabling more complex and realistic ferrofluids to be explored in the digital domain without compromising efficiency. Our approach allows for the use of physical parameters leading to accurate simulations as demonstrated in qualitative and quantitative evaluations.	https://dl.acm.org/doi/abs/10.1145/3414685.3417799	Libo Huang, Dominik L. Michels
Surrogate being	Surrogate Being is an interactive virtual environment, where I negotiate the discrepancy between memories and digital data of a nostalgic place, my hometown in Korea. Interweaving the heterogeneity of algorithmic digital images and affective memories, this project overcomes the binary opposition of humans and nonhuman and the anthropocentric perspective to investigate technology as a coevolving cognitive being and vital actor in the cognitive network. This project explores our experience and understanding of the world living in the Cognisphere - the globally interconnected cognitive system of humans and machines - and acknowledges nonlinguistic forces and experiential knowledge. Such affective dynamics among planetary cognitive beings are largely unnoticed and overshadowed by seemingly explicit and errorless digital information, and this project opens up interplays between tangible representations on the interface and underlying affects. Surrogate Being bridges the gap between my mind and digital technology and invites participants to navigate the mediated landscape with their curiosity. As memories remain indistinct and disintegrated until we recollect, the landscape is destructed and distorted when no participant is engaged. If a participant is approached the fragmented image and stands in front of it, it turns into a navigable landscape. As s/he moves her/his head to look at the other side of the landscape, the virtual camera in the scene changes its direction responding to the participants' movement - analyzing the image using computer vision. This interaction suggests a potential depth in this digital landscape we can look into, thus the monitor becomes a portal into a mediated digital-memory space. Furthermore, human and technological cognition become indistinguishable in this mediated space, collaboratively generated by affective memories and algorithmic decisions.	https://dl.acm.org/doi/abs/10.1145/3414686.3427148	Su Hyun Nam
SymmetryNet: learning to predict reflectional and rotational symmetries of 3D shapes from single-view RGB-D images	We study the problem of symmetry detection of 3D shapes from single-view RGB-D images, where severely missing data renders geometric detection approach infeasible. We propose an end-to-end deep neural network which is able to predict both reflectional and rotational symmetries of 3D objects present in the input RGB-D image. Directly training a deep model for symmetry prediction, however, can quickly run into the issue of overfitting. We adopt a multi-task learning approach. Aside from symmetry axis prediction, our network is also trained to predict symmetry correspondences. In particular, given the 3D points present in the RGB-D image, our network outputs for each 3D point its symmetric counterpart corresponding to a specific predicted symmetry. In addition, our network is able to detect for a given shape multiple symmetries of different types. We also contribute a benchmark of 3D symmetry detection based on single-view RGB-D images. Extensive evaluation on the benchmark demonstrates the strong generalization ability of our method, in terms of high accuracy of both symmetry axis prediction and counterpart estimation. In particular, our method is robust in handling unseen object instances with large variation in shape, multi-symmetry composition, as well as novel object categories.	https://dl.acm.org/doi/abs/10.1145/3414685.3417775	Yifei Shi, Junwen Huang, Hongjia Zhang, Xin Xu, Szymon Rusinkiewicz, Kai Xu
Synthesizing light field from a single image with variable MPI and two network fusion	We propose a learning-based approach to synthesize a light field with a small baseline from a single image. We synthesize the novel view images by first using a convolutional neural network (CNN) to promote the input image into a layered representation of the scene. We extend the multiplane image (MPI) representation by allowing the disparity of the layers to be inferred from the input image. We show that, compared to the original MPI representation, our representation models the scenes more accurately. Moreover, we propose to handle the visible and occluded regions separately through two parallel networks. The synthesized images using these two networks are then combined through a soft visibility mask to generate the final results. To effectively train the networks, we introduce a large-scale light field dataset of over 2,000 unique scenes containing a wide range of objects. We demonstrate that our approach synthesizes high-quality light fields on a variety of scenes, better than the state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3414685.3417785	Qinbo Li, Nima Khademi Kalantari
TAP-Net: transport-and-pack using reinforcement learning	We introduce the (TAP) problem, a frequently encountered instance of real-world packing, and develop a solution based on Given an initial spatial configuration of boxes, we seek an efficient method to iteratively transport and pack the boxes compactly into a target container. Due to obstruction and accessibility constraints, our problem has to add a new search dimension, i.e., finding an optimal , to the already immense search space for packing alone. Using a learning-based approach, a trained network can learn and encode solution patterns to guide the solution of new problem instances instead of executing an expensive online search. In our work, we represent the transport constraints using a and train a neural network, coined TAP-Net, using reinforcement learning to reward and packing. The network is built on an encoder-decoder architecture, where the encoder employs convolution layers to encode the box geometry and precedence graph and the decoder is a recurrent neural network (RNN) which inputs the current encoder output, as well as the current box packing state of the target container, and outputs the next box to pack, as well as its orientation. We train our network on initial box configurations, , via policy gradients to learn optimal TAP policies to maximize packing efficiency and stability. We demonstrate the performance of TAP-Net on a variety of examples, evaluating the network through ablation studies and comparisons to baselines and alternative network designs. We also show that our network generalizes well to larger problem instances, when trained on small-sized inputs.	https://dl.acm.org/doi/abs/10.1145/3414685.3417796	Ruizhen Hu, Juzhan Xu, Bin Chen, Minglun Gong, Hao Zhang, Hui Huang
The sinking garden	Highlighting endangered species in New York State and beyond, 'The Sinking Garden' is a new technology project integrating Virtual Reality application with fine art language to depict ecosystems that are at risk of survival. The project is based on and inspired by research conducted by New York State Department of Environmental Conservation, The Cornell Lab of Ornithology and The International Union for Conservation of Nature (IUCN). Focusing on specific endangered animals and plants that exhibit extraordinary beauty and significance in biodiversity, the project metaphorically depicts critical environmental issues. The Sinking Garden uniquely combines painting and new imaging technology to portray endangered species. At first, a series of paintings were created in a distinctive style inspired by folk art traditions from diverse cultures. Second, through conducting research and consulting scientists in nature conservation, the digital portraits of endangered species were chosen and produced as VR components. Finally, the VR platform brings animals and plants come to life in the 3D environment within cyberspace. The Sinking Garden project is intended to expand the capacity of visual art by utilizing new imaging technology in our age. Interweaving aesthetics with educational experience, the new media art project aims at inspiring viewers to cherish the natural world that we call home.	https://dl.acm.org/doi/abs/10.1145/3414686.3427144	Xiying Yang, Honglei Li, He Li
The synthetic cameraman	"""The Synthetic Cameraman"" is a full-screen, real-time, 3D graphics simulation that critically challenges the notions of remediation, processuality, linearity, and creative agency in computer-generated virtual environments. The application is rendering a virtual scene depicting a volcanic mountain landscape with a centrally located volcanic cone that is violently erupting with pyroclastic flow and rocks of different sizes being expelled as molten lava rivers are traveling down the slope forming a lava lake at the foot of the cone. The visual aspect of the phenomenon is enhanced by deep sounds of rumbling earth and rocks hitting the bottom of the caldera and falling down the slope. The viewing takes about 3--5 minutes and is divided into three sections with the middle section constituting the core experience where the control over individual elements in the scene is given over to the algorithms. The weather conditions, eruption, and the settings of virtual camera - its dynamic movement and image properties - are procedurally generated in real-time. The range of possible values that the camera is using can go beyond the capabilities of physical cameras, which makes it a hypermediated representational apparatus, producing partially abstract, semi-photorealistic ever changing fluid visuals originating from a broadened aesthetic spectrum. The algorithms are also controlling various post-processing effects that are procedurally applied to the camera feed. All of these processes are taking place in real-time, therefore every second of the experience is conceived through a unique entanglement of settings and parameters directing both the eruption and its representation. Each second of the simulation as perceived by the viewer is a one-time event, that constitutes this ever-lasting visual spectacle. The artwork can be displayed in a physical setting using a TV / projector or in a virtual setup as a continuous image feed (stream) produced by the application."	https://dl.acm.org/doi/abs/10.1145/3414686.3427127	Lukasz Mirocha
The world of freedom	Indeed, people spend more time on deep thinking since 2020. The questions which ask mainly by the sociologists, now become the topics on the dining table. The debates on social and moral dilemmas are happening intensively 24 hours on the internet. We started to think more about who we are, where we are going, and how we will value the information we have received. Do we have freedom? Shall we believe absolute freedom? Sometimes people directly transform the idea of liberty into democracy. However, shall we also equal freedom to democracy? Since we are all inside this one pandemic bubble, after most people stay at home for a couple of months, we start emerging a global-size collective memory, which makes people more empathetically understand others' situations. Meanwhile, more and more people have to learn and take experience virtually. The attention of empathy and the new work-from-home mode evokes the initial idea of this virtual reality experience. We start to ask how people could learn and think more effectively in this brand new virtual age? Unity program makes this innovation possible. The innovative architecture modeling could permit a large group of people to experience personal space and sharing areas simultaneously. The sound design is specially designed for the various space sound and the audience's interactivities. We use this program to build up an immersive and empathetic space that embodies a hypothetical argument of a social dilemma into a virtual manifestation. People might be able to figure out the most meaningful answer by wearing the same shoes. The social distance could also be virtually controlled in this program by counting if the number of participates overload spaces.	https://dl.acm.org/doi/abs/10.1145/3414686.3427172	Borou Yu, Tiange Zhou, Zeyu Wang, Jiajian Min
To cut or to fill: a global optimization approach to topological simplification	We present a novel algorithm for simplifying the topology of a 3D shape, which is characterized by the number of connected components, handles, and cavities. Existing methods either limit their modifications to be only cutting or only filling, or take a heuristic approach to decide where to cut or fill. We consider the problem of finding a globally optimal set of cuts and fills that achieve the simplest topology while minimizing geometric changes. We show that the problem can be formulated as graph labelling, and we solve it by a transformation to the Node-Weighted Steiner Tree problem. When tested on examples with varying levels of topological complexity, the algorithm shows notable improvement over existing simplification methods in both topological simplicity and geometric distortions.	https://dl.acm.org/doi/abs/10.1145/3414685.3417854	Dan Zeng, Erin Chambers, David Letscher, Tao Ju
Tokyo	"""Tokyo"" is a generative artwork created by visualizing continuous recorded Tokyo temperature data obtained from the Japan Meteorological Agency from 1990 to 2017, and then printing out the result in a creative manner. The colored dots in the artwork reflect the temperature of each day. Cold days were colored in blue while warm days were displayed in orange. There are two primary reasons for using natural phenomena, such as temperature data in generative art creation. First, the data allows us to embrace and comprehend the unpredictability of natural phenomena. Second, when used with a generative algorithm, it makes possible data visualization in ways that allow us to create abstract art. Since there are massive amounts of historical temperature data, such artwork would be impossible to create without computers. Simple patterns like noise are not always random and often contain repeating patterns that can be expressed harmoniously. The seeming randomness of dots showing temperature distributions of hot summer days can be painted as patterns that result in abstract artwork. When applied to Tokyo temperature data, the stain-like patterns that resulted are among the most attractive characteristics of generative art painting and would be difficult to express without the generative algorithm."	https://dl.acm.org/doi/abs/10.1145/3414686.3427104	Takuya Yamauchi
Towards spatially varying gloss reproduction for 3D printing	3D printing technology is a powerful tool for manufacturing complex shapes with high-quality textures. Gloss, next to color and shape, is one of the most salient visual aspects of an object. Unfortunately, printing a wide range of spatially-varying gloss properties using state-of-the-art 3D printers is challenging as it relies on geometrical modifications to achieve the desired appearance. A common post-processing step is to apply off-the-shelf varnishes that modify the final gloss. The main difficulty in automating this process lies in the physical properties of the varnishes which owe their appearance to a high concentration of large particles and as such, they cannot be easily deposited with current 3D color printers. As a result, fine-grained control of gloss properties using today's 3D printing technologies is limited in terms of both spatial resolution and the range of achievable gloss. We address the above limitations and propose new printing hardware based on piezo-actuated needle valves capable of jetting highly viscous varnishes. Based on the new hardware setup, we present the complete pipeline for controlling the gloss of a given 2.5 D object, from printer calibration, through material selection, to the manufacturing of models with spatially-varying reflectance. Furthermore, we discuss the potential integration with current 3D printing technology. Apart from being a viable solution for 3D printing, our method offers an additional and essential benefit of separating color and gloss fabrication which makes the process more flexible and enables high-quality color and gloss reproduction.	https://dl.acm.org/doi/abs/10.1145/3414685.3417850	Michal Piovarči, Michael Foshey, Vahid Babaei, Szymon Rusinkiewicz, Wojciech Matusik, Piotr Didyk
Trace of dance	"""Trace of Dance"" tells the story of modern labor. It is not just to make money, but it includes images of various complex desires, such as social status, personal satisfaction, and the recognition of bosses. The artist likened this behavior of a modern laborers to an inertial Flapping wings of a moth. The thermal data are collected through interviews with workers and produced as sculptures depicting the trace of dance based on them. The sculpture is melted by thermal lighting, which turns on and off in proportion to laborers' working hours. The artist asks whether this quiet misfortune comes from personal aspirations or from systems."	https://dl.acm.org/doi/abs/10.1145/3414686.3427179	Soyoung You
Turn over	"Turn Over is a kinetic art that illustrate the change of human and society. Twenty four set of Y-shaped object, which means person in Chinese, turn on a flat surface and gradually makes various pattern. The Chinese character ""人,"" which means a person, is similar to the alphabet letter ""Y"" rotated 180 degrees. When this character is arranged regularly in a lot on a surface, the lines of the characters starts to look like the boundaries of stacked cubes. Then, if one of these characters is turned 180 degrees, the orientation of one cube is also changed (For example, the top surface becomes to the side surface). This turn over of single character is too small to be noticeable. Sometimes it just seems a kind of contradiction, in-coherent, or treason. But when many characters turn at a time, it breaks boundaries and becomes a drastic change. This illustrates our change as an individual and a society."	https://dl.acm.org/doi/abs/10.1145/3414686.3427119	Yuichiro Katsumoto
Tutorial on integer programming for visual computing	The vector space is denoted as R,R ,R ,V,W Matricies are denoted by upper case, italic, and boldface letters: Vectors are column vectors denoted by boldface and lower case letters: x ∈ R 1 ∈ R is a × 1 vector of all ones is × identitymatrix. is the unit vector where only the -th element is 1 and the rest are 0.	https://dl.acm.org/doi/abs/10.1145/3415263.3419150	Peter Wonka, Chi-han Peng
Unbiased warped-area sampling for differentiable rendering	Differentiable rendering computes derivatives of the light transport equation with respect to arbitrary 3D scene parameters, and enables various applications in inverse rendering and machine learning. We present an unbiased and efficient differentiable rendering algorithm that does not require explicit boundary sampling. We apply the divergence theorem to the derivative of the rendering integral to convert the boundary integral into an area integral. We rewrite the converted area integral to a form that is suitable for Monte Carlo rendering. We then develop an efficient Monte Carlo sampling algorithm for solving the area integral. Our method can be easily plugged into a traditional path tracer and does not require dedicated data structures for sampling boundaries. We analyze the convergence properties through bias-variance metrics, and demonstrate our estimator's advantages over existing methods for some synthetic inverse rendering examples.	https://dl.acm.org/doi/abs/10.1145/3414685.3417833	Sai Praveen Bangaru, Tzu-Mao Li, Frédo Durand
Uncertain facing	Uncertain Facing is a data-driven, interactive audiovisual installation that aims to represent the uncertainty of data points of which their positions in 3D space are estimated by machine learning techniques. It also tries to raise concerns about the possibility of the unintended use of machine learning with synthetic/fake data. Uncertain Facing visualizes the realtime clustering of fake faces in 3D space through t-SNE, a non-linear dimensionality reduction technique, with face embeddings of the faces. This clustering reveals what faces are similar to each other based on the assumption of a probability distribution over data points. However, unlike the original purpose of t-SNE that is meant to be used in an objective data exploration in machine learning, it represents data points as metaballs, in which two or more face images become a merged face when they are close enough, to reflect the uncertain and probabilistic nature of data locations the t-SNE algorithm yields. As a result, metaball rendering is used as a means of an abstract, probabilistic representation of data as opposed to exactness that we expect from the use of scientific visualizations. Along with the t-SNE and metaball-based visualization, Uncertain Facing sonifies the change of the overall data distribution in 3D space based on a granular sound synthesis technique. Uncertain Facing also reflects error values, which t-SNE measures at each iteration between a distribution in original high dimensions and a deduced low-dimensional distribution, to represent the uncertainty of data as jittery motion and inharmonic sound. As an interactive installation, Uncertain Facing allows the audience to see the relationship between their face and the fake faces, implying an aspect that machine learning could be misused in an unintended way as face recognition technology does not distinguish between real and fake faces.	https://dl.acm.org/doi/abs/10.1145/3414686.3427161	Sihwa Park
Understand_ V.T.S	Could exploring the limit of consciousness become a mode of cultivating oneself? Understand_ V.T.S is an installation that substitutes senses. It helps explore and ponder in the process of the cultivation. In this piece of work, it conducts the experiment in which the possibility of the cooperation between natural and artificial algorithms are assessed, serves as an approach to human enhancement. That is, it tries out how well our brains(natural) work with AI (man-made). The feature of neuroplasticity allows our senses to perceive the world in various ways in which we might see not with our eyes, but with skins or listen not through our ears, but through taste buds, to name but a few. General skin vision relies on brain parsing pieces of information and shaping cognitions thereafter. In this respect, I introduced an object recognition system - YOLO v3, converting the results given by YOLO 3 into Braille reading system to thigh skin, and the other side converting the tactile image to motor on your back directly. You can control a robot wanders about the surroundings of you. The signals its left eye receive will translate the result of object detect to Braille and deliver it to your leg; while its right eye converts the signal received into the tactile image to the your back. So eventually your brain will manage to comprehend the meaning of these signals. Unlock a new tactile cognition by a Human and AI integration.	https://dl.acm.org/doi/abs/10.1145/3414686.3427176	Jing Ting Lai
VDAC: volume decompose-and-carve for subtractive manufacturing	We introduce for efficient 3-axis CNC machining of 3D freeform objects, where our goal is to develop a fully automatic method to jointly optimize setup and path planning. We formulate our joint optimization as a volume decomposition problem which prioritizes minimizing the number of setup directions while striving for a minimum number of volumes, where a 3D volume is continuously carvable, or simply carvable, if it can be carved with the machine cutter traversing a path. Geometrically, carvability combines visibility and monotonicity and presents a new shape property which had not been studied before. Given a target 3D shape and the initial material block, our algorithm first finds the minimum number of carving directions by solving a problem. Specifically, we analyze cutter accessibility and select the carving directions based on an assessment of how likely they would lead to a small carvable volume decomposition. Next, to obtain a minimum decomposition based on the selected carving directions efficiently, we narrow down the solution search by focusing on a special kind of points in the residual volume, or SA points, which are points that can be accessed from of the selected carving directions. Candidate carvable volumes are starting from the SA points. Finally, we devise an energy term to evaluate the carvable volumes and their combinations, leading to the final decomposition. We demonstrate the performance of our decomposition algorithm on a variety of 2D and 3D examples and evaluate it against the ground truth, where possible, and solutions provided by human experts. Physically machined models are produced where each carvable volume is continuously carved following a connected Fermat spiral toolpath.	https://dl.acm.org/doi/abs/10.1145/3414685.3417772	Ali Mahdavi-Amiri, Fenggen Yu, Haisen Zhao, Adriana Schulz, Hao Zhang
Viewporter	Viewporter is an interactive installation that displays a computer-generated video of a city. Viewers can rotate the screen attached to a device that resembles a telescope to accelerate the playback speed of the video. In this project, the deep learning technique with images capturing the Seoul skyline was used to train and generate the artificial city skyline. As one of the most developed metropolitan cities, Seoul has been under continuous development and construction of highrise buildings over the past decades. While the image of the skyscraper-packed skyline has been portrayed by the mainstream media to symbolize the utopian dream of the city, the lives of the residents with mundane duties have been far-fetched from the attractive image promoted through the propagandistic videos on the media. Viewporter uses the analogy of a telescope in tourist attractions to emphasize the distance between the idealized and the real and have viewers re-think the illusion and fantasy promoted by the images of development programs.	https://dl.acm.org/doi/abs/10.1145/3414686.3427140	Jean Ho Chu
Virtual hands in VR: motion capture, synthesis, and perception	We use our hands every day: to grasp a cup of coffee, write text on a keyboard, or signal that we are about to say something important. We use our hands to interact with our environment and to help us communicate with each other without thinking about it. Wouldn't it be great to be able to do the same in virtual reality? However, accurate hand motions are not trivial to capture. In this course, we present the current state of the art when it comes to virtual hands. Starting with current examples for controlling and depicting hands in virtual reality (VR), we dive into the latest methods and technologies to capture hand motions. As hands can currently not be captured in every situation and as constraints stopping us from intersecting with objects are typically not available in VR, we present research on how to synthesize hand motions and simulate grasping motions. Finally, we provide an overview of our knowledge of how virtual hands are being perceived, resulting in practical tips on how to represent and handle virtual hands. Our goals are (a) to present a broad state of the art of the current usage of hands in VR, (b) to provide more in-depth knowledge about the functioning of current hand motion tracking and hand motion synthesis methods, (c) to give insights on our perception of hand motions in VR and how to use those insights when developing new applications, and finally (d) to identify gaps in knowledge that might be investigated next. While the focus of this course is on VR, many parts also apply to augmented reality, mixed reality, and character animation in general, and some content originates from these areas.	https://dl.acm.org/doi/abs/10.1145/3415263.3419155	Sophie Jörg, Yuting Ye, Franziska Mueller, Michael Neff, Victor Zordan
We can't see the rainbow in the white	"When there is light, everything is visible. I decompose the fundamental element in the visual world to let the invisible become visible. It is a process deconstructing light. I project a white source of light on a surface while using prism and some moving images to ""deconstruct"" it. ""White"" is not an independent colour. It is a mixture of colour in the visible spectrum that is composed of the primary colour red, green and blue. Through refraction of light, I separated the white source with the three primary colours to rainbow light using a prism. After that, I took away green light from white light, leaving a mixture of red and blue light. Without green, the light source gradually reflects a new colour called magenta, hence the 'rainbow' becomes a 'duo-coloured rainbow'. Eventually, I erased red from magenta. The line results in pure blue colour. As blue is a primary coloured light that cannot be further decomposed by the prism, it appeared the ultimate light source in a monochromatic 'rainbow' colour."	https://dl.acm.org/doi/abs/10.1145/3414686.3427132	Gi Wai Echo Hui
Wearable Sanitizer: Design and Implementation of an Open-source, On-body Sanitizer	During the pandemic, wearables such as face masks and face shields have become broadly adopted, these solutions do reduce infection but do not eliminate infectious agents from surfaces and objects the person may touch. Therefore, regular disinfection of hands and frequently touched surfaces is a critical factor in preventing the spread of infectious diseases ranging from the common cold and flu to SARS and COVID-19. This activity of frequent disinfection requires a high degree of discipline and leads to increased cognitive and physical effort involved in frequent washing of hands or use of a pocket sanitizer. We present an open-source, wearable sanitizer that provides just-in-time, automatic dispensing of alcohol to the wearer's hand or nearby objects using sensors and programmable cues. We systematically explore the design space aiming to create a device that not only seamlessly integrates with the user's body and behavior but also frees their physical and mental faculties for other tasks.	https://dl.acm.org/doi/abs/10.1145/3415255.3422897	Pat Pataranutaporn, Ali Shtarbanov, Glenn Fernandes, Jingwen Li, Parinya Punpongsanon, Joe Paradiso, Pattie Maes
Weavecraft: an interactive design and simulation tool for 3D weaving	3D weaving is an emerging technology for manufacturing multilayer woven textiles. In this work, we present Weavecraft: an interactive, simulation-based design tool for 3D weaving. Unlike existing textile software that uses 2D representations for design patterns, we propose a novel weave block representation that helps the user to understand 3D woven structures and to create complex multi-layered patterns. With Weavecraft, users can create blocks either from scratch or by loading traditional weaves, compose the blocks into large structures, and edit the pattern at various scales. Furthermore, users can verify the design with a physically based simulator, which predicts and visualizes the geometric structure of the woven material and reveals potential defects at an interactive rate. We demonstrate a range of results created with our tool, from simple two-layer cloth and well known 3D structures to a more sophisticated design of a 3D woven shoe, and we evaluate the effectiveness of our system via a formative user study.	https://dl.acm.org/doi/abs/10.1145/3414685.3417865	Rundong Wu, Joy Xiaoji Zhang, Jonathan Leaf, Xinru Hua, Ante Qu, Claire Harvey, Emily Holtzman, Joy Ko, Brooks Hagan, Doug James, François Guimbretière, Steve Marschner
X-Fields: implicit neural view-, light- and time-image interpolation	"We suggest to represent an X-Field ---a set of 2D images taken across different view, time or illumination conditions, i.e., video, lightfield, reflectance fields or combinations thereof---by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the ""basic tricks"" of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination."	https://dl.acm.org/doi/abs/10.1145/3414685.3417827	Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel
You can find geodesic paths in triangle meshes by just flipping edges	This paper introduces a new approach to computing geodesics on polyhedral surfaces---the basic idea is to iteratively perform , in the same spirit as the classic Delaunay flip algorithm. This process also produces a triangulation conforming to the output geodesics, which is immediately useful for tasks in geometry processing and numerical simulation. More precisely, our FlipOut algorithm transforms a given sequence of edges into a locally shortest geodesic while avoiding self-crossings (formally: it finds a geodesic in the same ). The algorithm is guaranteed to terminate in a finite number of operations; practical runtimes are on the order of a few milliseconds, even for meshes with millions of triangles. The same approach is easily applied to curves beyond simple paths, including closed loops, curve networks, and multiply-covered curves. We explore how the method facilitates tasks such as straightening cuts and segmentation boundaries, computing geodesic Bézier curves, extending the notion of to curved surfaces, and providing accurate boundary conditions for partial differential equations (PDEs). Evaluation on challenging datasets such as indicates that the method is both robust and efficient, even for low-quality triangulations.	https://dl.acm.org/doi/abs/10.1145/3414685.3417839	Nicholas Sharp, Keenan Crane
ZoomTouch: Multi-User Remote Robot Control in Zoom by DNN-based Gesture Recognition	We present ZoomTouch, a breakthrough technology for multi-user and real-time control of robot from Zoom by DNN-based gesture recognition. The users can have a video conferencing in a digital world and at the same time to perform dexterous manipulations with tangible objects by remote robot. As the scenario, we proposed the remote robotic COVID-19 test Laboratory to substitute medical assistant working in protective gear in close proximity with infected cells and to considerably reduce the time to receive the test results. The proposed technology suggests a new type of reality, where multi-users can jointly interact with remote object, e.g. make a new building design, joint cooking in robotic kitchen, etc, and discuss/modify the results at the same time.	https://dl.acm.org/doi/abs/10.1145/3415255.3422892	Ilya Zakharkin, Arman Tsaturyan, Miguel Altamirano Cabrera, Jonathan Tirado, Dzmitry Tsetserukou
eXplainable AI (XAI): an introduction to the XAI landscape with practical examples	• Do Machine Learning algorithms have a Soul? • Could they understand every day's reality as us Humans do? • What the consequence of their Creativity? • Can they help us to understand world better?	https://dl.acm.org/doi/abs/10.1145/3415263.3419166	Rowan Hughes, Cameron Edmond, Lindsay Wells, Mashhuda Glencross, Liming Zhu, Tomasz Bednarz
holarchy	This work is an online installation that creates a new audio-visual using automatic video selection with deep learning. Video expression in the audio-visual and DJ+VJ, where sound and images coexist together, has been based on sampling methods by combining clips that already exist, generative methods that are computed in real time by computer, and the use of the sound of the phenomenon and the situation itself. Its visual effects have extended music and given it new meanings. However, in all of these methods, the selection of the video and the program itself was premised on the artist's arbitrary decision to match the music. This work is an online installation that eliminates the arbitrariness of the artist, creating a new audio-visual work by comparing in the same space the feature of the music and the feature of a number of images selected by the artist beforehand and selecting them automatically. In this work, the sounds of the youtube video selected by the viewer are separated every few seconds, and the closest video is selected by comparing these features with the features of countless short clips of movies and videos prepared in advance in the same space. This video selection method reconstructs the mapping relationship that artists have constructed so far between video and sound using deep learning, and suggests the possibility of possible correspondences. In addition, unconnected scenes from different films and images that have never been connected before become a single image, and emerge as a whole, and the viewer finds a story in the relationship between them. With this work, audio-visual and DJ+VJ expression is freed from the arbitrary decision, and a new perspective is given to the artists.	https://dl.acm.org/doi/abs/10.1145/3414686.3427159	Seiya Aoki, Yusuke Yamada, Santa Naruse, Reo Anzai, Aina Ono
iOrthoPredictor: model-guided deep prediction of teeth alignment	In this paper, we present iOrthoPredictor, a novel system to visually predict teeth alignment in photographs. Our system takes a frontal face image of a patient with visible malpositioned teeth along with a corresponding 3D teeth model as input, and generates a facial image with aligned teeth, simulating a real orthodontic treatment effect. The key enabler of our method is an effective disentanglement of an explicit representation of the teeth geometry from the in-mouth appearance, where the accuracy of teeth geometry transformation is ensured by the 3D teeth model while the in-mouth appearance is modeled as a latent variable. The disentanglement enables us to achieve fine-scale geometry control over the alignment while retaining the original teeth appearance attributes and lighting conditions. The whole pipeline consists of three deep neural networks: a U-Net architecture to explicitly extract the 2D teeth silhouette maps representing the teeth geometry in the input photo, a novel multilayer perceptron (MLP) based network to predict the aligned 3D teeth model, and an encoder-decoder based generative model to synthesize the in-mouth appearance conditional on the original teeth appearance and the aligned teeth geometry. Extensive experimental results and a user study demonstrate that iOrthoPredictor is effective in qualitatively predicting teeth alignment, and applicable to the orthodontic industry.	https://dl.acm.org/doi/abs/10.1145/3414685.3417771	Lingchen Yang, Zefeng Shi, Yiqian Wu, Xiang Li, Kun Zhou, Hongbo Fu, Youyi Zheng
주마간산 (jumagansan)	주마간산走馬看山 is a four-character chinese idiom that means to look at the scenery while horse riding, and it means to skim through the outer surface of things. <주마간산>, a collaboration between photographer Kim Hun Soo and painter Kwack Youn Soo, captures the scenery of the city as viewed from various perspectives while traveling by various transportations. They live in a city, use the same transportation, and look at similar landscapes, but each person's gaze varies. Even if it is a passing impression, the landscapes that contain each image are piled up and accumulated in the time and space of the city in which we live.	https://dl.acm.org/doi/abs/10.1145/3414686.3427160	Youn Soo Kwack, Hun Soo Kim
