title	abstract	url	authors
#DoudouChallenge	On the road to her family vacation, Olivia, a 10-year old girl hooked on her phone and social media, is abandoned by her parents in a highway service area. Alone with her plushie, he tries to get her off her phone to play, but things take an unexpected turn!	https://dl.acm.org/doi/abs/10.1145/3626964.3626993	Julie Majcher, Alexandra Delaunay-Fernandez, Sixtine Emerat, Marine Benabdallah-Crolais, Scott Pardaillhe-Galabrun, Noémie Segalowicz, Philippe Meis
#peaches	Ancestral time in Mangaian cosmology is an unfolding of multiple worlds through a generative process that extends from energy to matter from which we, Mangaians, are descended. Mangaia is the second largest Island in the Southern Cook Islands group. Its cosmology begins with expanding pulsating energies within the root of an upturned coconut, that generates multiple dimensions of existence. This transformation determines how we understand and navigate worlds. Within this multiplicity is recursion between the material and immaterial, where past, present and future are suspended and collapsed. Two key concepts underpin the generation of self-portrait images in the project #peaches; Akapapa'anga (layering through genealogy, building upon its ancestor genealogical connection within and between artworks) and the Mangaian cybernetic continuum (the ability for recursion to exist between worlds), which functions as ancestral time in practice. #peaches explores this proposition through layering and recursion of Al-generated portraits, and reveals the racial bias inherent in this technology, and its disruption to ancestral time.	https://dl.acm.org/doi/abs/10.1145/3610537.3632937	Nooroa Tapuni
360° Reconstruction From a Single Image Using Space Carved Outpainting	We introduce POP3D, a novel framework that creates a full 360° -view 3D model from a single image. POP3D resolves two prominent issues that limit the single-view reconstruction. Firstly, POP3D offers substantial generalizability to arbitrary categories, a trait that previous methods struggle to achieve. Secondly, POP3D further improves reconstruction fidelity and naturalness, a crucial aspect that concurrent works fall short of. Our approach marries the strengths of four primary components: (1) a monocular depth and normal predictor that serves to predict crucial geometric cues, (2) a space carving method capable of demarcating the potentially unseen portions of the target object, (3) a generative model pre-trained on a large-scale image dataset that can complete unseen regions of the target, and (4) a neural implicit surface reconstruction method tailored in reconstructing objects using RGB images along with monocular geometric cues. The combination of these components enables POP3D to readily generalize across various in-the-wild images and generate state-of-the-art reconstructions, outperforming similar works by a significant margin. Project page: http://cg.postech.ac.kr/research/POP3D.	https://dl.acm.org/doi/abs/10.1145/3610548.3618240	Nuri Ryu, Minsu Gong, Geonung Kim, Joo-Haeng Lee, Sunghyun Cho
3D Bézier Guarding: Boundary-Conforming Curved Tetrahedral Meshing	We present a method for the generation of higher-order tetrahedral meshes. In contrast to previous methods, the curved tetrahedral elements are guaranteed to be free of degeneracies and inversions while conforming exactly to prescribed piecewise polynomial surfaces, such as domain boundaries or material interfaces. Arbitrary polynomial order is supported. Algorithmically, the polynomial input surfaces are first covered by a single layer of carefully constructed curved elements using a recursive refinement procedure that provably avoids degeneracies and inversions. These tetrahedral elements are designed such that the remaining space is bounded piecewise linearly. In this way, our method effectively reduces the curved meshing problem to the classical problem of linear mesh generation (for the remaining space).	https://dl.acm.org/doi/abs/10.1145/3618332	Payam Khanteimouri, Marcel Campen
3D Lighter: Learning to Generate Emissive Textures	We present a novel approach to generate emissive textures for luminous objects, using direct 3D supervision from a 3D model dataset. To this end, we construct Emissive Objaverse, a dataset based on the recently proposed Objaverse dataset, and propose 3D Lighter, a method using neural fields with generative latent optimization.	https://dl.acm.org/doi/abs/10.1145/3610542.3626153	Yosuke Shinya, Kenichi Yoneji, Akihiro Tsukada, Tatsuya Harada
A Hessian-Based Field Deformer for Real-Time Topology-Aware Shape Editing	Shape manipulation is a central research topic in computer graphics. Topology editing, such as breaking apart connections, joining disconnected ends, and filling/opening a topological hole, is generally more challenging than geometry editing. In this paper, we observe that the saddle points of the signed distance function (SDF) provide useful hints for altering surface topology deliberately. Based on this key observation, we parameterize the SDF into a cubic trivariate tensor-product B-spline function F whose saddle points {si} can be quickly exhausted based on a subdivision-based root-finding technique coupled with Newton's method. Users can select one of the candidate points, say si, to edit the topology in real time. In implementation, we add a compactly supported B-spline function rooted at si, which we call a deformer in this paper, to F, with its local coordinate system aligning with the three eigenvectors of the Hessian. Combined with ray marching technique, our interactive system operates at 30 FPS. Additionally, our system empowers users to create desired bulges or concavities on the surface. An extensive user study indicates that our system is user-friendly and intuitive to operate. We demonstrate the effectiveness and usefulness of our system in a range of applications, including fixing surface reconstruction errors, artistic work design, 3D medical imaging and simulation, and antiquity restoration. Please refer to the attached video for a demonstration.	https://dl.acm.org/doi/abs/10.1145/3610548.3618191	Yunxiao Zhang, Zixiong Wang, Zihan Zhao, Rui Xu, Shuangmin Chen, Shiqing Xin, Wenping Wang, Changhe Tu
A Locality-based Neural Solver for Optical Motion Capture	We present a novel locality-based learning method for cleaning and solving optical motion capture data. Given noisy marker data, we propose a new heterogeneous graph neural network which treats markers and joints as different types of nodes, and uses graph convolution operations to extract the local features of markers and joints and transform them to clean motions. To deal with anomaly markers (e.g. occluded or with big tracking errors), the key insight is that a marker's motion shows strong correlations with the motions of its immediate neighboring markers but less so with other markers, a.k.a. locality, which enables us to efficiently fill missing markers (e.g. due to occlusion). Additionally, we also identify marker outliers due to tracking errors by investigating their acceleration profiles. Finally, we propose a training regime based on representation learning and data augmentation, by training the model on data with masking. The masking schemes aim to mimic the occluded and noisy markers often observed in the real data. Finally, we show that our method achieves high accuracy on multiple metrics across various datasets. Extensive comparison shows our method outperforms state-of-the-art methods in terms of prediction accuracy of occluded marker position error by approximately 20%, which leads to a further error reduction on the reconstructed joint rotations and positions by 30%. The code and data for this paper are available at https://github.com/non-void/LocalMoCap.	https://dl.acm.org/doi/abs/10.1145/3610548.3618148	Xiaoyu Pan, Bowen Zheng, Xinwei Jiang, Guanglong Xu, Xianli Gu, Jingxiang Li, Qilong Kou, He Wang, Tianjia Shao, Kun Zhou, Xiaogang Jin
A Micrograin BSDF Model for the Rendering of Porous Layers	We introduce a new BSDF model for the rendering of porous layers, as found on surfaces covered by dust, rust, dirt, or sprayed paint. Our approach is based on a distribution of elliptical opaque micrograins, extending the Trowbridge-Reitz (GGX) distribution [Trowbridge and Reitz 1975; Walter et al. 2007] to handle pores (i.e., spaces between micrograins). We use distance field statistics to derive the corresponding Normal Distribution Function (NDF) and Geometric Attenuation Factor (GAF), as well as a view- and light-dependent filling factor to blend between the porous and base layers. All the derived terms show excellent agreement when compared against numerical simulations. Our approach has several advantages compared to previous work [d'Eon et al. 2023; Merillou et al. 2000; Wang et al. 2022]. First, it decouples structural and reflectance parameters, leading to an analytical single-scattering formula regardless of the choice of micrograin reflectance. Second, we show that the classical texture maps (albedo, roughness, etc) used for spatially-varying material parameters are easily retargeted to work with our model. Finally, the BRDF parameters of our model behave linearly, granting direct multi-scale rendering using classical mip mapping.	https://dl.acm.org/doi/abs/10.1145/3610548.3618241	Simon Lucas, Mickael Ribardiere, Romain Pacanowski, Pascal Barla
A Motion-Simulation Platform to Generate Synthetic Motion Data for Computer Vision Tasks	We developed the Motion-Simulation Platform, a platform running within a game engine that is able to extract both RGB imagery and the corresponding intrinsic motion data (i.e., motion field). This is useful for motion-related computer vision tasks where large amounts of intrinsic motion data are required to train a model. We describe the implementation and design details of the Motion-Simulation Platform. The platform is extendable, such that any scene developed within the game engine is able to take advantage of the motion data extraction tools. We also provide both user and AI-bot controlled navigation, enabling user-driven input and mass automation of motion data collection.	https://dl.acm.org/doi/abs/10.1145/3610543.3628795	Andrew Chalmers, Junhong Zhao, Weng Khuan Hoh, James Drown, Simon Finnie, Richard Yao, James Lin, James Wilmott, Arindam Dey, Mark Billinghurst, Taehyun Rhee
A Neural Space-Time Representation for Text-to-Image Personalization	A key aspect of text-to-image personalization methods is the manner in which the target concept is represented within the generative process. This choice greatly affects the visual fidelity, downstream editability, and disk space needed to store the learned concept. In this paper, we explore a new text-conditioning space that is dependent on both the denoising process timestep and the denoising U-Net layers and showcase its compelling properties. A single concept in the space-time representation is composed of hundreds of vectors, one for each combination of time and , making this space challenging to optimize directly. Instead, we propose to implicitly represent a concept in this space by optimizing a small neural mapper that receives the current and parameters and outputs the matching token embedding. In doing so, the entire personalized concept is represented by the parameters of the learned mapper, resulting in a compact, yet expressive, representation. Similarly to other personalization methods, the output of our neural mapper resides in the input space of the text encoder. We observe that one can significantly improve the convergence and visual fidelity of the concept by introducing a , where our neural mapper additionally outputs a residual that is added to the of the text encoder. Finally, we show how one can impose an importance-based ordering over our implicit representation, providing users control over the reconstruction and editability of the learned concept using a single trained model. We demonstrate the effectiveness of our approach over a range of concepts and prompts, showing our method's ability to generate high-quality and controllable compositions without fine-tuning any parameters of the generative model itself.	https://dl.acm.org/doi/abs/10.1145/3618322	Yuval Alaluf, Elad Richardson, Gal Metzer, Daniel Cohen-Or
A Parametric Kinetic Solver for Simulating Boundary-Dominated Turbulent Flow Phenomena	Boundary layer flow plays a very important role in shaping the entire flow feature near and behind obstacles inside fluids. Thus, boundary treatment methods are crucial for a physically consistent fluid simulation, especially when turbulence occurs at a high Reynolds number, in which accurately handling thin boundary layer becomes quite challenging. Traditional Navier-Stokes solvers usually construct multi-resolution body-fitted meshes to achieve high accuracy, often together with near-wall and sub-grid turbulence modeling. However, this could be time-consuming and computationally intensive even with GPU accelerations. An alternative and much faster approach is to switch to a kinetic solver, such as the lattice Boltzmann model, but boundary treatment has to be done in a cut-cell manner, sacrificing accuracy unless grid resolution is much increased. In this paper, we focus on simulating the boundary-dominated turbulent flow phenomena with an efficient kinetic solver. In order to significantly improve the cut-cell-based boundary treatment for higher accuracy without excessively increasing the simulation resolution, we propose a novel parametric boundary treatment model, including a semi-Lagrangian scheme at the wall for non-equilibrium distribution functions, together with a purely link-based near-wall analytical mesoscopic model by analogy with the macroscopic wall modeling approach, which is yet simple to compute. Such a new method is further extended to handle moving boundaries, showing increased accuracy. Comprehensive analyses are conducted, with a variety of simulation results that are both qualitatively and quantitatively validated with experiments and real life scenarios, and compared to existing methods, to indicate superiority of our method. We highlight that our method not only provides a more accurate way for boundary treatment, but also a valuable tool to control boundary layer behaviors. This has not been achieved and demonstrated before in computer graphics, which we believe will be very useful in practical engineering.	https://dl.acm.org/doi/abs/10.1145/3618313	Mengyun Liu, Xiaopei Liu
A Physically-inspired Approach to the Simulation of Plant Wilting	Plants are among the most complex objects to be modeled in computer graphics. While a large body of work is concerned with structural modeling and the dynamic reaction to external forces, our work focuses on the dynamic deformation caused by plant internal wilting processes. To this end, we motivate the simulation of water transport inside the plant which is a key driver of the wilting process. We then map the change of water content in individual plant parts to branch stiffness values and obtain the wilted plant shape through a position based dynamics simulation. We show, that our approach can recreate measured wilting processes and does so with a higher fidelity than approaches ignoring the internal water flow. Realistic plant wilting is not only important in a computer graphics context but can also aid the development of machine learning algorithms in agricultural applications through the generation of synthetic training data.	https://dl.acm.org/doi/abs/10.1145/3610548.3618218	Filippo Maggioli, Jonathan Klein, Torsten Hädrich, Emanuele Rodolà, Wojtek Pałubicki, Sören Pirk, Dominik L. Michels
A Safer Place? Stories from the Emergency Department	A Safer Place represents a ground-breaking fusion of art, technology, and research that offers viewers a deeply immersive and empathetic experience within a hospital Emergency Department and beyond. Eight experts with lived experience guide us through a virtual reality experience of a hospital Emergency Department [ED]. They describe the scenarios of mental distress that led to their admissions, along with their experiences of waiting rooms, ED beds, and hospital procedures, and their insights into what can be done to address the shortcomings of emergency care. A Safer Place serves as a powerful tool for fostering empathy and understanding among healthcare professionals, policymakers, and the general public. By allowing viewers to step into the shoes of those who have faced mental distress and sought care, the project aims to drive meaningful change in emergency care practices. The immersive VR experience offers a unique opportunity to dismantle barriers, spark conversations, and inspire compassionate, patient-centered approaches to mental health care in emergency settings. The foundation of this creative endeavor is built upon extensive research conducted by Renata Kokanović and her team (RMIT Melbourne), who collaborated with 17 individuals accessing the ED for mental health care. The primary objective of the initial research project was to foster understanding among medical staff by gaining insights into ED processes from the unique perspective of those who have lived experience. Using virtual reality technology, the project takes viewers on a compelling journey, providing an immersive and empathetic encounter with the experiences of the eight guides. Through their narratives, viewers gain a deeper understanding of the complex and nuanced realities faced by individuals impacted by mental health distress. The project aims to bridge the gap between medical staff and patients, fostering empathy, compassion, and ultimately leading to improved patient-centered care. Moreover, it offers insights and recommendations for addressing the limitations and improving emergency care. By amplifying the voices of those with lived experience, the project aims to drive meaningful change in the healthcare system, encouraging more compassionate and effective practices. Please be aware that whilst the piece does not depict people in distress, the audio includes frank discussion of experiences of distress, including of suicidal feelings and psychosis.	https://dl.acm.org/doi/abs/10.1145/3610549.3614608	Volker Kuchelmeister, Jill Bennett, Gail Kenning, Renata Kokanović, Alex Davis
A University Curriculum Course for Undergraduates: Artificial Intelligence and Art	"This paper describes a new course entitled ""AI & Art"" offered at Quinnipiac University in the spring of 2023. An updated version of this course is scheduled for Spring 2024. In this course, students use text-to-image AI generators to create artwork, write prompts using ChatGPT, and write short essays on critical issues and topics (creativity, deep fakes, copyright etc.) implicated by the rise of AI image-generation software. This course fosters essential learning outcomes such as critical thinking, creativity, and hands-on experience with AI technology preparing students for 21st-century careers."	https://dl.acm.org/doi/abs/10.1145/3610540.3627006	Gregory Patrick Garvey
ACE: Adversarial Correspondence Embedding for Cross Morphology Motion Retargeting from Human to Nonhuman Characters	Motion retargeting is a promising approach for generating natural and compelling animations for nonhuman characters. However, it is challenging to translate human movements into semantically equivalent motions for target characters with different morphologies due to the ambiguous nature of the problem. This work presents a novel learning-based motion retargeting framework, Adversarial Correspondence Embedding (ACE), to retarget human motions onto target characters with different body dimensions and structures. Our framework is designed to produce natural and feasible character motions by leveraging generative-adversarial networks (GANs) while preserving high-level motion semantics by introducing an additional feature loss. In addition, we pretrain a character motion prior that can be controlled in a latent embedding space and seek to establish a compact correspondence. We demonstrate that the proposed framework can produce retargeted motions for three different characters – a quadrupedal robot with a manipulator, a crab character, and a wheeled manipulator. We further validate the design choices of our framework by conducting baseline comparisons and a user study. We also showcase sim-to-real transfer of the retargeted motions by transferring them to a real Spot robot.	https://dl.acm.org/doi/abs/10.1145/3610548.3618255	Tianyu Li, Jungdam Won, Alexander Clegg, Jeonghwan Kim, Akshara Rai, Sehoon Ha
AI History 1890-2090	"The aim of this artwork is to let the audience feel ""brackish waters of fictionality and reality"" by seeing AI-generated history. In the real world, humans train AI with human-generated data. But AI trains humans with AI-generated data in this artwork. AI becomes smarter by learning. Their intelligence might surpass that of humans. When AI acquires more advanced knowledge than humans, will people, in turn, learn from AI? And will we unilaterally accept the AI-generated information without verification of the truth? This artwork anticipates the possible future experience. As AI infiltrates our lives, the information generated by AI is increasing exponentially. In this work, a zoetrope is used to show how the history of AI is being generated as AI generates images, and we can do nothing when we see AI creating its own history with its own generated images. We can only accept the information as it is presented to us. This work shows how we are immersed in this ""brackish zone of fictionality and reality brought about by AI""."	https://dl.acm.org/doi/abs/10.1145/3610537.3622951	Kaihei Hase, Syunji Yazaki
AI Nüshu (Women's scripts) - An Exploration of Language Emergence in Sisterhood	"This paper presents ""AI Nüshu,"" an emerging language system inspired by Nüshu (women's scripts), the unique language created and used exclusively by ancient Chinese women who were illiterate under a patriarchy society. Through an interactive art installation, two artificial intelligent (AI) agents continuously observe their environment and communicate with each other, developing a writing system that encodes Chinese. In this system, two AI agents observe the environment through cameras, record the unconscious behaviors of the audience, and generate summaries of their observations through visual recognition. Subsequently, the agent associates the corresponding original Nüshu poetry lines and generates new poetry text through a Language Model (LLM), representing its reflection. To develop their language, they continuously switch roles between the speaker and listener, constantly communicating their reflections, and encrypting a word in the poetry line with their self-created AI Nüshu character, allowing the other to guess and learn. Gradually, they reach a consensus on AI Nüshu, forming a unique ""AI Nüshu Dictionary"" for machines. This language, algorithmically combined into corresponding characters, has components derived from Nüshu, similar to Chinese characters and traditional textile patterns. Thus, like ancient women, the two agents gradually developed their Chinese writing system, corresponding one-to-one with Chinese characters. In contrast, humans, as the authority of the language system, became an object observed, interpreted, and inspired by machines to stimulate non-human language. This is the first media art project to interpret Nüshu from a computational linguistics perspective, infusing AI and art research with non-English natural language processing, Chinese cultural heritage, and a feminist viewpoint. This encourages the creation of more non-English, linguistically-oriented artworks for diverse cultures. We simulate communication in sisterhood through a multi-agent learning system, which questioned knowledge authority between humans and machines through the lens of language development."	https://dl.acm.org/doi/abs/10.1145/3610537.3622957	Yuying Tang, Yuqian Sun, Ze Gao, Zhijun Pan, Zhigang Wang, Tristan Braud, Chang Hee Lee, Ali Asadipour
AI Nüshu: An Exploration of Language Emergence in Sisterhood Through the Lens of Computational Linguistics	"This paper presents ""AI Nüshu,"" an emerging language system inspired by Nüshu (women's scripts), the unique language created and used exclusively by ancient Chinese women who were thought to be illiterate under a patriarchal society. In this interactive installation, two artificial intelligence (AI) agents are trained in the Chinese dictionary and the Nüshu corpus. By continually observing their environment and communicating, these agents collaborate towards creating a standard writing system to encode Chinese. It offers an artistic interpretation of the creation of a non-western script from a computational linguistics perspective, integrating AI technology with Chinese cultural heritage and a feminist viewpoint."	https://dl.acm.org/doi/abs/10.1145/3610591.3616427	Yuqian Sun, Yuying Tang, Ze Gao, Zhijun Pan, Chuyan Xu, Yurou Chen, Kejiang Qian, Zhigang Wang, Tristan Braud, Chang Hee Lee, Ali Asadipour
AI-Generated Imagery: A New Era for the `Readymade'	This paper examines the classification of digital images created by generative AI systems and their emergence as art. The term 'art' defies precise definition, but there is a growing tendency to label AI-generated images as such. The current discourse on AI-generated imagery as art lacks the nuanced understanding of traditional artistic media. To address this, we introduce some philosophical considerations. By applying existing frameworks and theories of language, we argue that certain AI-generated images possess properties aligned with 'readymades', qualifying them for consideration as art.	https://dl.acm.org/doi/abs/10.1145/3610591.3616432	Amy Smith, Michael Cook
ART-Owen Scrambling	We present a novel algorithm for implementing Owen-scrambling, combining the generation and distribution of the scrambling bits in a single self-contained compact process. We employ a context-free grammar to build a binary tree of symbols, and equip each symbol with a scrambling code that affects all descendant nodes. We nominate the grammar of adaptive regular tiles (ART) derived from the repetition-avoiding Thue-Morse word, and we discuss its potential advantages and shortcomings. Our algorithm has many advantages, including random access to samples, fixed time complexity, GPU friendliness, and scalability to any memory budget. Further, it provides two unique features over known methods: it admits optimization, and it is in-vertible, enabling screen-space scrambling of the high-dimensional Sobol sampler.	https://dl.acm.org/doi/abs/10.1145/3618307	Abdalla G. M. Ahmed, Matt Pharr, Peter Wonka
ActRay: Online Active Ray Sampling for Radiance Fields	Thanks to the high-quality reconstruction and photorealistic rendering, the Neural Radiance Field (NeRF) has garnered extensive attention and has been continuously improved. Despite its high visual quality, the prohibitive training time limits its practical application. Although significant acceleration has been achieved, it is still far from real-time training, due to the need for tens of thousands of iterations. In this paper, a feasible solution is to reduce the number of required iterations by always training the rays with the highest loss values, instead of the traditional method of training each ray with a uniform probability. To this end, we propose an online active ray sampling strategy, ActRay. Specifically, to avoid the substantial overhead of calculating the actual loss values for all rays in each iteration, a rendering-gradient-based loss propagation algorithm is presented to efficiently estimate the loss values. To further narrow the gap between the estimated loss and the actual loss, an online learning algorithm based on the Upper Confidence Bound (UCB) is proposed to control the sampling probability of the rays, thereby compensating for the bias in loss estimation. We evaluate ActRay on both real-world and synthetic scenes, and the promising results show that it accelerates radiance field training by 6.5x. Besides, we test ActRay under all kinds of radiance field representations (implicit, explicit, and hybrid), proving that it is general and effective to different representations. We believe this work will contribute to the practical application of radiance fields, because it has taken a step closer to real-time radiance field training. ActRay is open-source at: https://pku-netvideo.github.io/actray/.	https://dl.acm.org/doi/abs/10.1145/3610548.3618254	Jiangkai Wu, Liming Liu, Yunpeng Tan, Quanlu Jia, Haodan Zhang, Xinggong Zhang
AdaptNet: Policy Adaptation for Physics-Based Character Control	Motivated by humans' ability to adapt skills in the learning of new ones, this paper presents AdaptNet, an approach for modifying the latent space of existing policies to allow new behaviors to be quickly learned from like tasks in comparison to learning from scratch. Building on top of a given reinforcement learning controller, AdaptNet uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network layers to make more substantive changes. The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies. Code is available at .	https://dl.acm.org/doi/abs/10.1145/3618375	Pei Xu, Kaixiang Xie, Sheldon Andrews, Paul G. Kry, Michael Neff, Morgan Mcguire, Ioannis Karamouzas, Victor Zordan
Adaptive Recurrent Frame Prediction with Learnable Motion Vectors	The utilization of dedicated ray tracing graphics cards has revolutionized the production of stunning visual effects in real-time rendering. However, the demand for high frame rates and high resolutions remains a challenge. The pixel warping approach is a crucial technique for increasing frame rate and resolution by exploiting the spatio-temporal coherence. To this end, existing super-resolution and frame prediction methods rely heavily on motion vectors from rendering engine pipelines to track object movements. This work builds upon state-of-the-art heuristic approaches by exploring a novel adaptive recurrent frame prediction framework that integrates learnable motion vectors. Our framework supports the prediction of transparency, particles, and texture animations, with improved motion vectors that capture shading, reflections, and occlusions, in addition to geometry movements. In addition, we introduce a feature streaming neural network, dubbed FSNet, that allows for the adaptive prediction of one or multiple sequential frames. Extensive experiments against state-of-the-art methods demonstrate that FSNet can operate at lower latency with significant visual enhancements and can upscale frame rates by at least two times. This approach offers a flexible pipeline to improve the rendering frame rates of various graphics applications and devices.	https://dl.acm.org/doi/abs/10.1145/3610548.3618211	Zhizhen Wu, Chenyu Zuo, Yuchi Huo, Yazhen Yuan, Yifan Peng, Guiyang Pu, Rui Wang, Hujun Bao
Adaptive Shells for Efficient Neural Radiance Field Rendering	Neural radiance fields achieve unprecedented quality for novel view synthesis, but their volumetric formulation remains expensive, requiring a huge number of samples to render high-resolution images. Volumetric encodings are essential to represent fuzzy geometry such as foliage and hair, and they are well-suited for stochastic optimization. Yet, many scenes ultimately consist largely of solid surfaces which can be accurately rendered by a single sample per pixel. Based on this insight, we propose a neural radiance formulation that smoothly transitions between volumetric- and surface-based rendering, greatly accelerating rendering speed and even improving visual fidelity. Our method constructs an explicit mesh envelope which spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To this end, we generalize the NeuS [Wang et al. 2021] formulation with a learned spatially-varying kernel size which encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with width determined by the kernel size, and fine-tune the radiance field within this band. At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Experiments show that our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.	https://dl.acm.org/doi/abs/10.1145/3618390	Zian Wang, Tianchang Shen, Merlin Nimier-David, Nicholas Sharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas Müller, Zan Gojcic
Adaptive Tracking of a Single-Rigid-Body Character in Various Environments	Since the introduction of DeepMimic [Peng et al. 2018a], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows the SRB character to follow the reference motion. We demonstrate that our policy, efficiently trained within 30 minutes on an ultraportable laptop, has the ability to cope with environments that have not been experienced during learning, such as running on uneven terrain or pushing a box, and transitions between learned policies, without any additional learning.	https://dl.acm.org/doi/abs/10.1145/3610548.3618187	Taesoo Kwon, Taehong Gu, Jaewon Ahn, Yoonsang Lee
Aerial Diffusion: Text Guided Ground-to-Aerial View Synthesis from a Single Image using Diffusion Models	We present a novel method, Aerial Diffusion, for generating aerial views from a single ground-view image using text guidance. Aerial Diffusion leverages a pretrained text-image diffusion model for prior knowledge. We address two main challenges corresponding to domain gap between the ground-view and the aerial view and the two views being far apart in the text-image embedding manifold. Our approach uses a homography inspired by inverse perspective mapping prior to finetuning the pretrained diffusion model. Aerial Diffusion uses an alternating sampling strategy to compute the optimal solution on complex high-dimensional manifold and generate a high-fidelity (w.r.t. ground view) aerial image. We demonstrate the quality and versatility of Aerial Diffusion on a plethora of images and prove the effectiveness of our method with extensive ablations and comparisons. To the best of our knowledge, Aerial Diffusion is the first approach that performs single image ground-to-aerial translation in an unsupervised manner. The full paper and code can be found at https://arxiv.org/abs/2303.11444.	https://dl.acm.org/doi/abs/10.1145/3610543.3626177	Divya Kothandaraman, Tianyi Zhou, Ming Lin, Dinesh Manocha
Aerial Display Method Using a Flying Screen with an IR Marker and Long Range Dynamic Projection Mapping	Our group proposed a method of aerial display using a flying screen suspended from a drone and dynamic projection mapping on the screen. In this study, we propose two methods as extensions of this previous study. First, we propose a structure of a large screen with a specially designed LED marker. The proposed screen structure is suitable for suspending from a drone. The LED marker has a special structure that allows the screen center position to be estimated from the captured image. Successful and stable projection of laser patterns onto the screen prototype suspended from a flying drone was demonstrated. In this experiment the distance between the screen and the projection system was approximately 16 m. Second, we propose a method that enables long range dynamic projection mapping using a high-brightness projector. Stable projection of an animation of a 2D character on a manually moving screen approximately 16 meters away from the projection system was also successfully demonstrated.	https://dl.acm.org/doi/abs/10.1145/3610542.3626123	Yuito Hirohashi, Hiromasa Oku
Aguaviva	Aguaviva juxtaposes the spontaneous nature of biology with the predictable properties of digital technology. A solitary moon jellyfish swims around in a saltwater dome. A small camera tracks its movement and turns it into xy values, expressed on a collar of digital numbers. The shifting position of the jellyfish is mapped to the corresponding digits below, resulting in an ever-changing string of random numbers. Arbitrary values generated by computers are considered too predictable for high-end encryption schemes, such as secure Internet traffic and online banking. Hence, more unconventional sources are often used, even paid for. True randomness, for this purpose, is a commodity. As part of the artwork, the numerical string created by the jellyfish is offered up in real-time to encryption companies to use at their discretion. The apparatus is designed to extract randomness from this simple yet ancient life form---unaware of the fact that in the arena of random sequencing, its cellular contractions can outperform even the most powerful supercomputer.	https://dl.acm.org/doi/abs/10.1145/3610537.3622959	Thomas Marcusson
AiRound: a touchable mid-air image viewable from 360 degrees	In this paper, we describe AiRound, an optical system that displays mid-air images that can be viewed from any direction. Mid-air images are touchable floating images formed by retroreflective transmissive optical elements that can seamlessly connect the virtual world to real space without special equipment. However, they are limited by three problems, including a limited range of observation, the visibility of the light source from the outside, and the aesthetically displeasing of stray light. The proposed system combines view control films and micromirror array plates to form a mid-air image that can be observed from 360 degrees by rotating these components at high speed.	https://dl.acm.org/doi/abs/10.1145/3610541.3614568	Yutaro Yano, Naoya Koizumi
AirPolygon: Transparent, Film-based and Flexible 3D Display with Air-control	This demo proposes AirPolygon, a soft, bendable, and transparent pneumatic control film. AirPolygon distinguishes itself with its ease of fabrication, high transparency for direct LCD attachment, and lightweight, deflatable design for portability. We designed the system to prevent air leakage while capitalizing on each layer's material properties. Demonstrated applications include a 3D haptic LCD monitor film, a lightweight game controller, a tactile video conferencing system, a tactile pulse meter, and a transparent switch for glass surfaces, illustrating AirPolygon's broad utility.	https://dl.acm.org/doi/abs/10.1145/3610541.3614571	Yuki Akachi, Junich Yamaoka
Alternative Photographic Processes Reimagined: The Role of Digital Technology in Revitalizing Classic Printing Techniques	The transition from film to digital has influenced photography techniques, with implications for both cultural and technical aspects. By examining the history and concepts of classical photography and incorporating computer intervention in revitalizing alternative photographic processes, we aim to expand aesthetic expressions in art, computer graphics, and our understanding of photography's cultural significance. Integrating computer processing with techniques such as salt print, platinum print, and cyanotype, this study seeks to create a new photographic experience that embraces the joy of materializing scenery and highlights the interconnectedness of technology and art.	https://dl.acm.org/doi/abs/10.1145/3610591.3616430	Chinatsu Ozawa, Kenta Yamamoto, Kazuya Izumi, Yoichi Ochiai
Amortizing Samples in Physics-Based Inverse Rendering Using ReSTIR	Recently, great progress has been made in physics-based differentiable rendering. Existing differentiable rendering techniques typically focus on scenes, but during inverse rendering---a key application for differentiable rendering---the scene is updated by each gradient step. In this paper, we take a first step to leverage temporal data in the context of inverse direct illumination. By adopting reservoir-based spatiotemporal resampled importance resampling (ReSTIR), we introduce new Monte Carlo estimators for both interior and boundary components of differential direct illumination integrals. We also integrate ReSTIR with antithetic sampling to further improve its effectiveness. At equal frame time, our methods produce gradient estimates with up to 100× lower relative error than baseline methods. Additionally, we propose an inverse-rendering pipeline that incorporates these estimators and provides reconstructions with up to 20× lower error.	https://dl.acm.org/doi/abs/10.1145/3618331	Yu-Chen Wang, Chris Wyman, Lifan Wu, Shuang Zhao
An Adaptive Fast-Multipole-Accelerated Hybrid Boundary Integral Equation Method for Accurate Diffusion Curves	In theory, diffusion curves promise complex color gradations for infinite-resolution vector graphics. In practice, existing realizations suffer from poor scaling, discretization artifacts, or insufficient support for rich boundary conditions. Previous applications of the boundary element method to diffusion curves have relied on polygonal approximations, which either forfeit the high-order smoothness of Bézier curves, or, when the polygonal approximation is extremely detailed, result in large and costly systems of equations that must be solved. In this paper, we utilize the boundary integral equation method to accurately and efficiently solve the underlying partial differential equation. Given a desired resolution and viewport, we then interpolate this solution and use the boundary element method to render it. We couple this hybrid approach with the fast multipole method on a non-uniform quadtree for efficient computation. Furthermore, we introduce an adaptive strategy to enable truly scalable infinite-resolution diffusion curves.	https://dl.acm.org/doi/abs/10.1145/3618374	Seungbae Bang, Kirill Serkh, Oded Stein, Alec Jacobson
An Architecture and Implementation of Real-Time Sound Propagation Hardware for Mobile Devices	This paper presents a high-performance and low-power hardware architecture for real-time sound rendering on mobile devices. Traditional sound rendering algorithms require high-performance CPUs or GPUs for processing because of its high computational complexities to realize ultra-realistic 3D audio. Thus, it has been hard to achieve real-time rates on low-power mobile devices. To overcome this limitation, we propose a hardware architecture that adopts hardware-friendly sound-propagation-path calculation algorithms. We verified the function and performance of our architecture through its implementation on an FPGA board. According to ASIC evaluation with the 8-nm process technology, it achieves high performance with 120 FPS, low power consumption with 50 mW, and a small silicon area with 0.31 mm2, allowing real-time sound rendering on mobile devices.	https://dl.acm.org/doi/abs/10.1145/3610548.3618237	Eunjae Kim, Sukwon Choi, Jiyoung Kim, Jae-Ho Nah, Woonam Jung, Tae-Hyeong Lee, Yeon-Kug Moon, Woo-Chan Park
An Examination of Text Shaking Correction Methods for AR Walking	One problem with walking in AR is less readability of displayed text. Head shaking causes the displayed text to shake. The screen coordinate system(SCS) or world coordinate system(WCS) is used for displaying text with different effective distances. We propose methods to correct text shaking by combining SCS and WCS.	https://dl.acm.org/doi/abs/10.1145/3610542.3626135	Mie Sato, Hiromu Koide, Kei Kanari
An Implicit Neural Representation for the Image Stack: Depth, All in Focus, and High Dynamic Range	In everyday photography, physical limitations of camera sensors and lenses frequently lead to a variety of degradations in captured images such as saturation or defocus blur. A common approach to overcome these limitations is to resort to image stack fusion, which involves capturing multiple images with different focal distances or exposures. For instance, to obtain an all-in-focus image, a set of multi-focus images is captured. Similarly, capturing multiple exposures allows for the reconstruction of high dynamic range. In this paper, we present a novel approach that combines neural fields with an expressive camera model to achieve a unified reconstruction of an all-in-focus high-dynamic-range image from an image stack. Our approach is composed of a set of specialized implicit neural representations tailored to address specific sub-problems along our pipeline: We use neural implicits to predict flow to overcome misalignments arising from lens breathing, depth, and all-in-focus images to account for depth of field, as well as tonemapping to deal with sensor responses and saturation - all trained using a physically inspired supervision structure with a differentiable thin lens model at its core. An important benefit of our approach is its ability to handle these tasks simultaneously or independently, providing flexible post-editing capabilities such as refocusing and exposure adjustment. By sampling the three primary factors in photography within our framework (focal distance, aperture, and exposure time), we conduct a thorough exploration to gain valuable insights into their significance and impact on overall reconstruction quality. Through extensive validation, we demonstrate that our method outperforms existing approaches in both depth-from-defocus and all-in-focus image reconstruction tasks. Moreover, our approach exhibits promising results in each of these three dimensions, showcasing its potential to enhance captured image quality and provide greater control in post-processing.	https://dl.acm.org/doi/abs/10.1145/3618367	Chao Wang, Ana Serrano, Xingang Pan, Krzysztof Wolski, Bin Chen, Karol Myszkowski, Hans-Peter Seidel, Christian Theobalt, Thomas Leimkühler
An Implicit Physical Face Model Driven by Expression and Style	"3D facial animation is often produced by manipulating facial deformation models (or rigs), that are traditionally parameterized by expression controls. A key component that is usually overlooked is expression ""style"", as in, how a particular expression is performed. Although it is common to define a semantic basis of expressions that characters can perform, most characters perform each expression in their own style. To date, style is usually entangled with the expression, and it is not possible to transfer the style of one character to another when considering facial animation. We present a new face model, based on a data-driven implicit neural physics model, that can be driven by both expression and style separately. At the core, we present a framework for learning implicit physics-based actuations for multiple subjects simultaneously, trained on a few arbitrary performance capture sequences from a small set of identities. Once trained, our method allows generalized physics-based facial animation for any of the trained identities, extending to unseen performances. Furthermore, it grants control over the animation style, enabling style transfer from one character to another or blending styles of different characters. Lastly, as a physics-based model, it is capable of synthesizing physical effects, such as collision handling, setting our method apart from conventional approaches."	https://dl.acm.org/doi/abs/10.1145/3610548.3618156	Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Paulo Gotardo, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley
An Implicitly Stable Mixture Model for Dynamic Multi-fluid Simulations	Particle-based simulations have become increasingly popular in real-time applications due to their efficiency and adaptability, especially for generating highly dynamic fluid effects. However, the swift and stable simulation of interactions among distinct fluids continues to pose challenges for current mixture model techniques. When using a single-mixture flow field to represent all fluid phases, numerical discontinuities in phase fields can result in significant losses of dynamic effects and unstable conservation of mass and momentum. To tackle these issues, we present an advanced implicit mixture model for smoothed particle hydrodynamics. Instead of relying on an explicit mixture field for all dynamic computations and phase transfers between particles, our approach calculates phase momentum sources from the mixture model to derive explicit and continuous velocity phase fields. We then implicitly obtain the mixture field using a phase-mixture momentum-mapping mechanism that ensures conservation of incompressibility, mass, and momentum. In addition, we propose a mixture viscosity model and establish viscous effects between the mixture and individual fluid phases to avoid instability under extreme inertia conditions. Through a series of experiments, we show that, compared to existing mixture models, our method effectively improves dynamic effects while reducing critical instability factors. This makes our approach especially well-suited for long-duration, efficiency-oriented virtual reality scenarios.	https://dl.acm.org/doi/abs/10.1145/3610548.3618215	Yanrui Xu, Xiaokun Wang, Jiamin Wang, Chongming Song, Tiancheng Wang, Yalan Zhang, Jian Chang, Jian Jun Zhang, Jiri Kosinka, Alexandru Telea, Xiaojuan Ban
An Interactive Showcase of Touch'n'Draw: Rapid 3D Sketching with Fluent Bimanual Coordination	In perspective drawing, designers express 3D shapes by drawing auxiliary lines that construct surfaces and drawing design curves on them. However, drawing auxiliary lines can be challenging, and too many of them can make the drawing difficult to understand. To address these issues, we present a novel 3D sketching system that allows the user to quickly and easily create instant auxiliary lines and instant sketch surfaces for drawing desired 3D curves with fluent bimanual touch and pen interactions. We will produce concept sketches using our system to showcase its potential usefulness.	https://dl.acm.org/doi/abs/10.1145/3610541.3614581	Taegyu Jin, Seung-Jun Lee, Joon Hyub Lee, Seok-Hyung Bae
An Introduction to Creating Real-Time Interactive Computer Graphic Applications	The emphasis of this course focuses on the capabilities of the WebGL application programming interface (i.e., programming library, often called an ).	https://dl.acm.org/doi/abs/10.1145/3610538.3614630	Dave Shreiner
An Unified λ-subdivision Scheme for Quadrilateral Meshes with Optimal Curvature Performance in Extraordinary Regions	We propose an unified -subdivision scheme with a continuous family of tuned subdivisions for quadrilateral meshes. Main subdivision stencil parameters of the unified scheme are represented as spline functions of the subdominant eigenvalue of respective subdivision matrices and the value can be selected within a wide range to produce desired properties of refined meshes and limit surfaces with optimal curvature performance in extraordinary regions. Spline representations of stencil parameters are constructed based on discrete optimized stencil coefficients obtained by a general tuning framework that optimizes eigenvectors of subdivision matrices towards curvature continuity conditions. To further improve the quality of limit surfaces, a weighting function is devised to penalize sign changes of Gauss curvatures on respective second order characteristic maps. By selecting an appropriate , the resulting unified subdivision scheme produces anticipated properties towards different target applications, including nice properties of several other existing tuned subdivision schemes. Comparison results also validate the advantage of the proposed scheme with higher quality surfaces for subdivision at lower values, a challenging task for other related tuned subdivision schemes.	https://dl.acm.org/doi/abs/10.1145/3618400	Weiyin Ma, Xu Wang, Yue Ma
Analysis and Synthesis of Digital Dyadic Sequences	We explore the space of matrix-generated (0, , 2)-nets and (0, 2)-sequences in base 2, also known as digital dyadic nets and sequences. In computer graphics, they are arguably leading the competition for use in rendering. We provide a complete characterization of the design space and count the possible number of constructions with and without considering possible reorderings of the point set. Based on this analysis, we then show that every digital dyadic net can be reordered into a sequence, together with a corresponding algorithm. Finally, we present a novel family of self-similar digital dyadic sequences, to be named -sequences, that spans a subspace with fewer degrees of freedom. Those -sequences are extremely efficient to sample and compute, and we demonstrate their advantages over the classic Sobol (0, 2)-sequence.	https://dl.acm.org/doi/abs/10.1145/3618308	Abdalla G. M. Ahmed, Mikhail Skopenkov, Markus Hadwiger, Peter Wonka
Analytical & Neural approaches to Physically Based Rendering	Path tracing is ubiquitous for photorealistic rendering of various light transport phenomenon. At it's core, path tracing involves the stochastic evaluation of complex & recursive integrals leading to high computational complexity. Research efforts have thus focused on accelerating path tracing either by improving the stochastic sampling process to achieve better convergence or by using approximate analytical evaluations for a restricted set of these integrals. Another interesting set of research efforts focus on the integration of neural networks within the rendering pipeline, where these networks partially replace stochastic sampling and approximate it's converged result. The analytic and neural approaches are attractive from an acceleration point of view. Formulated properly & coupled with advances in hardware, these approaches can achieve much better convergence and eventually lead to real-time performance. Motivated by this, we make contributions to both avenues to accelerate path tracing. The first set of efforts aim to reduce the computational effort spent in stochastic direct lighting calculations from area light sources by instead evaluating it analytically. To this end, we introduce the analytic evaluation of visibility in a previously proposed analytic area light shading method. Second, we add support for anisotropic GGX to this method. This relaxes an important assumption enabling the analytic rendering of a wider set of light transport effects. Our final contribution is a neural approach that attempts to reduce yet another source of high computational load - the recursive evaluations. We demonstrate the versatility of our approach with an application to hair rendering, which exhibits one of the most challenging recursive evaluation cases. All our contributions improve on the state-of-the art and demonstrate photo-realism on par with reference path tracing.	https://dl.acm.org/doi/abs/10.1145/3623053.3623372	Aakash Kt
AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections	Previous animatable 3D-aware GANs for human generation have primarily focused on either the human head or full body. However, head-only videos are relatively uncommon in real life, and full body generation typically does not deal with facial expression control and still has challenges in generating high-quality results. Towards applicable video avatars, we present an animatable 3D-aware GAN that generates portrait images with controllable facial expression, head pose, and shoulder movements. It is a generative model trained on unstructured 2D image collections without using 3D or video data. For the new task, we base our method on the generative radiance manifold representation and equip it with learnable facial and head-shoulder deformations. A dual-camera rendering and adversarial learning scheme is proposed to improve the quality of the generated faces, which is critical for portrait images. A pose deformation processing network is developed to generate plausible deformations for challenging regions such as long hair. Experiments show that our method, trained on unstructured 2D images, can generate diverse and high-quality 3D portraits with desired control over different properties.	https://dl.acm.org/doi/abs/10.1145/3610548.3618164	Yue Wu, Sicheng Xu, Jianfeng Xiang, Fangyun Wei, Qifeng Chen, Jiaolong Yang, Xin Tong
Animating Street View	We present a system that automatically brings street view imagery to life by populating it with naturally behaving, animated pedestrians and vehicles. Our approach is to remove existing people and vehicles from the input image, insert moving objects with proper scale, angle, motion and appearance, plan paths and traffic behavior, as well as render the scene with plausible occlusion and shadowing effects. The system achieves these by reconstructing the still image street scene, simulating crowd behavior, and rendering with consistent lighting, visibility, occlusions, and shadows. We demonstrate results on a diverse range of street scenes including regular still images and panoramas.	https://dl.acm.org/doi/abs/10.1145/3610548.3618230	Mengyi Shan, Brian Curless, Ira Kemelmacher-Shlizerman, Steve Seitz
Anti-Aliased Neural Implicit Surfaces with Encoding Level of Detail	We present LoD-NeuS, an efficient neural representation for high-frequency geometry detail recovery and anti-aliased novel view rendering. Drawing inspiration from voxel-based representations with the level of detail (LoD), we introduce a multi-scale tri-plane-based scene representation that is capable of capturing the LoD of the signed distance function (SDF) and the space radiance. Our representation aggregates space features from a multi-convolved featurization within a conical frustum along a ray and optimizes the LoD feature volume through differentiable rendering. Additionally, we propose an error-guided sampling strategy to guide the growth of the SDF during the optimization. Both qualitative and quantitative evaluations demonstrate that our method achieves superior surface reconstruction and photorealistic view synthesis compared to state-of-the-art approaches.	https://dl.acm.org/doi/abs/10.1145/3610548.3618197	Yiyu Zhuang, Qi Zhang, Ying Feng, Hao Zhu, Yao Yao, Xiaoyu Li, Yan-Pei Cao, Ying Shan, Xun Cao
Anything to Glyph: Artistic Font Synthesis via Text-to-Image Diffusion Model	The automatic generation of artistic fonts is a challenging task that attracts many research interests. Previous methods specifically focus on glyph or texture style transfer. However, we often come across creative fonts composed of objects in posters or logos. These fonts have proven to be a challenge for existing methods as they struggle to generate similar designs. This paper proposes a novel method for generating creative artistic fonts using a pre-trained text-to-image diffusion model. Our model takes a shape image and a prompt describing an object as input and generates an artistic glyph image consisting of such objects. Specifically, we introduce a novel heatmap-based weak position constraint method to guide the positioning of objects in the generated image, and we also propose the Latent Space Semantic Augmentation Module that improves other information while constraining object position. Our approach is unique in that it can preserve the object's original shape while constraining its position. And our training method requires only a small quantity of generated data, making it an efficient unsupervised learning approach. Experimental results demonstrate that our method can generate various glyphs, including Chinese, English, Japanese, and symbols, using different objects. We also conducted qualitative and quantitative comparisons with various position control methods for the diffusion model. The results indicate that our approach outperforms other methods in terms of visual quality, innovation, and user evaluation.	https://dl.acm.org/doi/abs/10.1145/3610548.3618208	Changshuo Wang, Lei Wu, Xiaole Liu, Xiang Li, Lei Meng, Xiangxu Meng
Aquasia: The world's first immersive metaworld focused on the future of human habitats in the face of rising sea levels	Aquasia is the world's first educational metaworld set in a floating city in Asia. It merges creativity, technology, and sustainability, revolutionising learning into an engaging, memorable journey. Offering an interactive desktop and a passive 360-degree VR experience, Aquasia provides audiences with a new perspective on the challenges and possibilities of living in a world affected by rising sea levels and climate change. Audiences are challenged to rethink modern food, energy, and transport systems, uncovering technological breakthroughs for a future aquatic life on our Blue Planet. Incorporating Asian culture and innovative technology, underpinned by UN-HABITAT research, Aquasia aims to captivate educators and learners. Our innovative project garnered the attention of Singapore's esteemed ArtScience Museum, which invited us to launch Aquasia in their Curiosity and VR Galleries from 1-30 September 2023	https://dl.acm.org/doi/abs/10.1145/3610537.3622945	Kay Poh Gek Vasey, Colin Seah, Race Krehel, Olivier Bos
Auditory VR Generative System for Non-Experts to Reproduce Human Memories Through Natural Language Interactions	"We propose an automatic auditory VR generative system based on natural language input and attempt to apply it to VR exposure therapy, a promising treatment for Post-Traumatic Stress Disorder (PTSD). The system consisted of the user interface, developed based on the Large Language Model (LLM), the auditory event dataset, which has metadata of ""subject"" and ""verb"" and spatial audio generator."	https://dl.acm.org/doi/abs/10.1145/3610542.3626140	Yuta Yamauchi, Keiko Ino, Keiichi Zempo
Augmentation of Medical Preparation for Children by Using Projective and Tangible Interface	This research aims to create an interactive experience that alleviate anxiety of pediatric patients and cause empathy within their family and medical community. We developed the novel medical preparation system through the integration of projective and tangible interfaces. Through our work, children can intuitively understand their illnesses and medical procedures in an age-appropriate and engaging manner.	https://dl.acm.org/doi/abs/10.1145/3610542.3626128	Miki Monzen, Shigenori Mochizuki, Toshikazu Ohshima
Authoring and Simulating Meandering Rivers	We present a method for interactively authoring and simulating meandering river networks. Starting from a terrain with an initial low-resolution network encoded as a directed graph, we simulate the evolution of the path of the different river channels using a physically-based migration equation augmented with control terms. The curvature-based terms in the equation allow us to reproduce phenomena identified in geomorphology, such as downstream migration of bends. Control terms account for the influence of the landscape topography and user-defined river trajectory constraints. Our model implements abrupt events that shape meandering networks, such as cutoffs forming oxbow lakes and avulsions. We visually show the effectiveness of our method and compare the generated networks quantitatively to river data by analyzing sinuosity and wavelength metrics. Our vector-based model runs at interactive rates, allowing for efficient authoring of large-scale meandering networks.	https://dl.acm.org/doi/abs/10.1145/3618350	Axel Paris, Eric Guérin, Pauline Collon, Eric Galin
Automating Animation using Python Scripting in Autodesk Maya	Coding empowers automation. Scripts can handle mundane and repetitive tasks in an efficient and precise manner. This course will offer a glimpse into the power of using scripting in Python to automate animation-related tasks in Autodesk Maya using a hands-on interactive format. Attendees will learn how to streamline simple tasks using the magic of scripting. The course will cover Setting up the Python environment in Maya, executing Python commands in the Script Editor, manipulating objects using Python scripting, querying animation data, keyframe manipulation, and creating and modifying. By the end of the course, attendees should walk away with a strong understanding of how the Python language, Maya commands and the ability to write scripts for animating in Maya. Hopefully, they will have the tools, confidence, and initiative to explore more advanced scripts independently. Attendees should have Autodesk Maya, Python, and Visual Studio Code pre-loaded on their devices if they intend to follow along.	https://dl.acm.org/doi/abs/10.1145/3610538.3614645	Ann McNamara
AvatarForge: A Real-time and Node-Based Body Editing System	Designing non-humanoid avatars with different body structures from humans so that humans can intuitively manipulate them requires the design of body structure and motion mapping, which is complex, takes time and requires special design skills. To address this problem, we propose AvatarForge which allows users to design non-humanoid avatars by editing body structure and motion mapping in real-time. In this system, users interact with a node-based interface in a virtual environment to edit their own bodies while visualizing the changes. This system aims to reduce the difficulty of designing non-humanoid avatars, expedite the prototyping of new bodies, and enable personalized avatars that match individual preferences and physical abilities. It contributes to the advancement of non-humanoid body utilization in various fields such as CG animation, inclusive design, and human augmentation.	https://dl.acm.org/doi/abs/10.1145/3610541.3614569	Shuto Takashita, Amane Yamaguchi, Takuji Narumi, Masahiko Inami
AvatarStudio: Text-Driven Editing of 3D Dynamic Human Head Avatars	Capturing and editing full-head performances enables the creation of virtual characters with various applications such as extended reality and media production. The past few years witnessed a steep rise in the photorealism of human head avatars. Such avatars can be controlled through different input data modalities, including RGB, audio, depth, IMUs, and others. While these data modalities provide effective means of control, they mostly focus on editing the head movements such as the facial expressions, head pose, and/or camera viewpoint. In this paper, we propose AvatarStudio, a text-based method for editing the appearance of a dynamic full head avatar. Our approach builds on existing work to capture dynamic performances of human heads using Neural Radiance Field (NeRF) and edits this representation with a text-to-image diffusion model. Specifically, we introduce an optimization strategy for incorporating multiple keyframes representing different camera viewpoints and time stamps of a video performance into a single diffusion model. Using this personalized diffusion model, we edit the dynamic NeRF by introducing view-and-time-aware Score Distillation Sampling (VT-SDS) following a model-based guidance approach. Our method edits the full head in a canonical space and then propagates these edits to the remaining time steps via a pre-trained deformation network. We evaluate our method visually and numerically via a user study, and results show that our method outperforms existing approaches. Our experiments validate the design choices of our method and highlight that our edits are genuine, personalized, as well as 3D- and time-consistent.	https://dl.acm.org/doi/abs/10.1145/3618368	Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt
Avatars for Good Drinking: An Exploratory Study of The Effects of Avatar's Body Shape on Beverage Perception	The Proteus effect is known as the effect that the self-image evoked by an avatar influences the behavior. Previous studies have shown that the Proteus effect influences food-related behaviors, but it is not clear whether it influences perceptions of food. Therefore, in this study, we investigated whether the avatar's body shape affects beverage perception when drinking a beverage in a virtual environment. Specifically, we measured the sense of body ownership toward the avatar, the taste of the beverage, the amount of the beverage consumed, and the purchase intention of the beverage. The results showed that a gradual transition in body shape with drinking produced a significant improvement in the sense of body ownership, particularly in terms of feeling that the virtual body belonged to oneself. We also report the larger body shape significantly increases the purchase intention. Furthermore, we found that image congruence between the avatar and cola induces better taste perception.	https://dl.acm.org/doi/abs/10.1145/3610542.3626144	Yusuke Koseki, Yusuke Arikawa, Kizashi Nakano, Takuji Narumi
BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis	Synthesizing photorealistic 4D human head avatars from videos is essential for VR/AR, telepresence, and video game applications. Although existing Neural Radiance Fields (NeRF)-based methods achieve high-fidelity results, the computational expense limits their use in real-time applications. To overcome this limitation, we introduce , a novel representation for real-time neural head avatar synthesis, deployable in a standard polygon rasterization pipeline. Our approach extracts deformable multi-layer meshes from learned isosurfaces of the head and computes expression-, pose-, and view-dependent appearances that can be baked into static textures for efficient rasterization. We thus propose a three-stage pipeline for neural head avatar synthesis, which includes learning continuous deformation, manifold, and radiance fields, extracting layered meshes and textures, and fine-tuning texture details with differential rasterization. Experimental results demonstrate that our representation generates synthesis results of comparable quality to other state-of-the-art methods while significantly reducing the inference time required. We further showcase various head avatar synthesis results from monocular videos, including view synthesis, face reenactment, expression editing, and pose editing, all at interactive frame rates on commodity devices. Source codes and demos are available on our project page.	https://dl.acm.org/doi/abs/10.1145/3618399	Hao-Bin Duan, Miao Wang, Jin-Chuan Shi, Xu-Chuan Chen, Yan-Pei Cao
Bending the Light: Next generation anamorphic sculptures	Bending the light is a new method for generating artworks that extends the classical anamorphic archetype to use freeform reflective and refractive media and 3D surfaces instead of images. The methodology uses a mix of raytracing and surface deformation techniques to determine the proper deformation the object should undergo to be corrected by the optical tool once viewed by the observer in a specific location. The reflected image hovers in front of, rather than behind, the mirror. The audience forms an essential part of the work. The holographic or ghost-like appearance of the reflection results from the interplay between the mirror, sculpture and the eye of the viewer. The sculpture has been recently selected as a finalist for the prestigious Wynne Prize and exhibited at the Art Gallery of New South Wales. Before then, it was also exhibited at Sydney Contemporary.	https://dl.acm.org/doi/abs/10.1145/3610537.3622956	Louis Pratt, Nico Pietroni
Bent Out of Shape	A dull, square family gets a bright, circular neighbor.	https://dl.acm.org/doi/abs/10.1145/3626964.3626969	Chloe Merwin
Black Curtain: The Fear	A woman and child run from their fears engulfing them.	https://dl.acm.org/doi/abs/10.1145/3626964.3626980	Richard Shilling
Body Cosmos VR	Body Cosmos is an immersive and interactive artwork that connects the human body with the cosmic environment through real-time bio-data. Using volumetric rendering techniques and particle system, we create a surreal virtual reality that reflects the intricate structures of human anatomy and celestial nebulae. We integrate a heart rate sensor and an EEG headband to capture and process bio-data into emotion indicators, which influence the visualization of particles in the artwork. This creates an intimate, personal connection with the cosmos, transcending immediate presence and nurturing a perpetual presence within the cosmic expanse. We also provide two engagement modes: Roller Coaster Mode and Free Explore Mode, allowing different levels of exploration. Body Cosmos is more than a visual spectacle; it sparks curiosity, expands the imagination, and enhances awareness of our embodied and cosmic identity.	https://dl.acm.org/doi/abs/10.1145/3610549.3614590	Rem RunGu Lin, Koo YongEn Ke, Leixin Luo
Bonhomme	Junior is a sensitive teenager. His father, on the other hand, is a gruff hunter, determined to turn his son into a man in his image. After disappointing his father once more during target practice, Junior takes the family rifle. He heads off alone into the forest, ready to face his own nature.	https://dl.acm.org/doi/abs/10.1145/3626964.3626996	Pauline Pilarek, Aurélie Marchand, Laureline De Cremoux, Ariane Krief, Julie Olivieri, Clément Plays, Philippe Meis
Bounded VNDF Sampling for Smith–GGX Reflections	Sampling according to a visible normal distribution function (VNDF) is often used to sample rays scattered by glossy surfaces, such as the Smith–GGX microfacet model. However, for rough reflections, existing VNDF sampling methods can generate undesirable reflection vectors occluded by the surface. Since these occluded reflection vectors must be rejected, VNDF sampling is inefficient for rough reflections. This paper introduces an unbiased method to reduce the number of rejected samples for Smith–GGX VNDF sampling. Our method limits the sampling range for a state-of-the-art VNDF sampling method that uses a spherical cap-based sampling range. By using our method, we can reduce the variance for highly rough and low-anisotropy surfaces. Since our method only modifies the spherical cap range in the existing sampling routine, it is simple and easy to implement.	https://dl.acm.org/doi/abs/10.1145/3610543.3626163	Kenta Eto, Yusuke Tokuyoshi
Boundless Conversations: AI-Powered Video Interactions across Domains, Languages, and Time	"We present novel web applications that blend AI and video communication, offering an interactive way to engage with videos, transcending time and distance constraints. Known as Time Offset Interaction Application (TOIA), the platform transforms passive video viewing into dynamic conversations using personal recordings or YouTube clips. Our work expands TOIA with two unique experiences: 1) ""The Elephant in the Room"" project, stimulating dialogues on sensitive topics like childbirth, sex, and death; 2) Support for multilingual interactions, fostering a global collection of diverse personal narratives. Through AI and interactive media, we aim to broaden the horizons for sharing human experiences and perspectives."	https://dl.acm.org/doi/abs/10.1145/3610541.3614572	Alberto Chierici, Soojin Lee, Nizar Habash, Aaron Sherwood, Bishnu Dev, Gautham Kumar, Muhammad Ali
Break-A-Scene: Extracting Multiple Concepts from a Single Image	Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its synthesis in diverse contexts. However, current methods primarily focus on the case of learning a single concept from multiple images with variations in backgrounds and poses, and struggle when adapted to a different scenario. In this work, we introduce the task of textual scene decomposition: given a single image of a scene that may contain several concepts, we aim to extract a distinct text token for each concept, enabling fine-grained control over the generated scenes. To this end, we propose augmenting the input image with masks that indicate the presence of target concepts. These masks can be provided by the user or generated automatically by a pre-trained segmentation model. We then present a novel two-phase customization process that optimizes a set of dedicated textual embeddings (handles), as well as the model weights, striking a delicate balance between accurately capturing the concepts and avoiding overfitting. We employ a masked diffusion loss to enable handles to generate their assigned concepts, complemented by a novel loss on cross-attention maps to prevent entanglement. We also introduce union-sampling, a training strategy aimed to improve the ability of combining multiple concepts in generated images. We use several automatic metrics to quantitatively compare our method against several baselines, and further affirm the results using a user study. Finally, we showcase several applications of our method.	https://dl.acm.org/doi/abs/10.1145/3610548.3618154	Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, Dani Lischinski
Bridging the Gap: Sustainable Collaboration between CG Production and Educational Institutions	"One of the major challenges that new employees in Japan's CG entertainment industry faces is the skill gap between the industry and academia. Recognizing the urgent need for an effective approach to bridge this gap, this study with a new initiative with Nihon Kogakuin College, proposes and implements an innovative educational model, drawing analogies from the ""Melting Pot"" and the ""Salad Bowl."" The ""Salad Bowl"" model honors the uniqueness and needs of both participating companies and students, emphasizing the preservation of diversity. Through its implementation, it has been elucidated that this new educational approach brings about an improvement in student skills and high satisfaction levels, paving the way for success in future workplace environments through real-world industry experience. Additionally, it has been effective in resolving recruitment challenges of smaller studios, overcoming faculty shortages, providing a concrete evaluation of soft skills, and creating employment opportunities tailored to each company. These investigative outcomes underscore the significance and effectiveness of establishing a sustainable collaboration between industry and academia, realizing the provision of education programs that cater to the diverse industrial demands of Japan."	https://dl.acm.org/doi/abs/10.1145/3610540.3627003	Harutaka Matsunaga, Yukari Nagai, Kazunori Miyata
Bye Bear	A robot story about friendship, change and farewell.	https://dl.acm.org/doi/abs/10.1145/3626964.3626977	Jan Bitzer, Phil Varkki
Bézier Spline Simplification Using Locally Integrated Error Metrics	Inspired by surface mesh simplification methods, we present a technique for reducing the number of Bézier curves in a vector graphics while maintaining high fidelity. We propose a curve-to-curve distance metric to repeatedly conduct local segment removal operations. By construction, we identify all possible lossless removal operations ensuring the smallest possible zero-error representation of a given design. Subsequent lossy operations are computed via local Gauss-Newton optimization and processed in a priority queue. We tested our method on the OpenClipArts dataset of 20,000 real-world vector graphics images and show significant improvements over representative previous methods. The generality of our method allows us to show results for curves with varying thickness and for vector graphics animations.	https://dl.acm.org/doi/abs/10.1145/3610548.3618248	Siqi Wang, Chenxi Liu, Daniele Panozzo, Denis Zorin, Alec Jacobson
C-Shells: Deployable Gridshells with Curved Beams	We introduce a computational pipeline for simulating and designing , a new class of planar-to-spatial deployable linkage structures. A C-shell is composed of curved flexible beams connected at rotational joints that can be assembled in a stress-free planar configuration. When actuated, the elastic beams deform and the assembly deploys towards the target 3D shape. We propose two alternative computational design approaches for C-shells: (i) Forward exploration simulates the deployed shape from a planar beam layout provided by the user. Once a satisfactory overall shape is found, a subsequent design optimization adapts the beam geometry to reduce the elastic energy of the linkage while preserving the target shape. (ii) Inverse design is facilitated by a new geometric flattening method that takes a design surface as input and computes an initial layout of piecewise straight linkage beams. Our design optimization algorithm then calculates the smooth curved beams to best reproduce the target shape at minimal elastic energy. We find that C-shells offer a rich space for design and show several studies that highlight new shape topologies that cannot be achieved with existing deployable linkage structures.	https://dl.acm.org/doi/abs/10.1145/3618366	Quentin Becker, Seiichi Suzuki, Yingying Ren, Davide Pellis, Julian Panetta, Mark Pauly
CLIP-Head: Text-Guided Generation of Textured Neural Parametric 3D Head Models	We propose CLIP-Head, a novel approach towards text-driven neural parametric 3D head model generation. Our method takes simple text prompts in natural language, describing the appearance & facial expressions, and generates 3D neural head avatars with accurate geometry and high-quality texture maps. Unlike existing approaches, which use conventional parametric head models with limited control and expressiveness, we leverage Neural Parametric Head Models (NPHM), offering disjoint latent codes for the disentangled encoding of identities and expressions. To facilitate the text-driven generation, we propose two weakly-supervised mapping networks to map the CLIP's encoding of input text prompt to NPHM's disjoint identity and expression vector. The predicted latent codes are then fed to a pre-trained NPHM network to generate 3D head geometry. Since NPHM mesh doesn't support textures, we propose a novel aligned parametrization technique, followed by text-driven generation of texture maps by leveraging a recently proposed controllable diffusion model for the task of text-to-image synthesis. Our method is capable of generating 3D head meshes with arbitrary appearances and a variety of facial expressions, along with photoreal texture details. We show superior performance with existing state-of-the-art methods, both qualitatively & quantitatively, and demonstrate potentially useful applications of our method. We have released our code at https://raipranav384.github.io/clip_head.	https://dl.acm.org/doi/abs/10.1145/3610543.3626169	Pranav Manu, Astitva Srivastava, Avinash Sharma
CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration	This paper presents CLIPXPlore, a new framework that leverages a vision-language model to guide the exploration of the 3D shape space. Many recent methods have been developed to encode 3D shapes into a learned latent shape space to enable generative design and modeling. Yet, existing methods lack effective exploration mechanisms, despite the rich information. To this end, we propose to leverage CLIP, a powerful pre-trained vision-language model, to aid the shape-space exploration. Our idea is threefold. First, we couple the CLIP and shape spaces by generating paired CLIP and shape codes through sketch images and training a mapper network to connect the two spaces. Second, to explore the space around a given shape, we formulate a co-optimization strategy to search for the CLIP code that better matches the geometry of the shape. Third, we design three exploration modes, binary-attribute-guided, text-guided, and sketch-guided, to locate suitable exploration trajectories in shape space and induce meaningful changes to the shape. We perform a series of experiments to quantitatively and visually compare CLIPXPlore with different baselines in each of the three exploration modes, showing that CLIPXPlore can produce many meaningful exploration results that cannot be achieved by the existing solutions.	https://dl.acm.org/doi/abs/10.1145/3610548.3618144	Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Hao Zhang, Chi-Wing Fu
CamP: Camera Preconditioning for Neural Radiance Fields	Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input --- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.	https://dl.acm.org/doi/abs/10.1145/3618321	Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T. Barron, Ricardo Martin-Brualla
Capturing Animation-Ready Isotropic Materials Using Systematic Poking	"Capturing material properties of real-world elastic solids is both challenging and highly relevant to many applications in computer graphics, robotics and related fields. We give a non-intrusive, in-situ and inexpensive approach to measure the nonlinear elastic energy density function of man-made materials and biological tissues. We poke the elastic object with 3d-printed rigid cylinders of known radii, and use a precision force meter to record the contact force as a function of the indentation depth, which we measure using a force meter stand, or a novel unconstrained laser setup. We model the 3D elastic solid using the Finite Element Method (FEM), and elastic energy using a compressible Valanis-Landel material that generalizes Neo-Hookean materials by permitting arbitrary tensile behavior under large deformations. We then use optimization to fit the nonlinear isotropic elastic energy so that the FEM contact forces and indentations match their measured real-world counterparts. Because we use carefully designed cubic splines, our materials are accurate in a large range of stretches and robust to inversions, and are therefore ""animation-ready"" for computer graphics applications. We demonstrate how to exploit radial symmetry to convert the 3D elastostatic contact problem to the mathematically equivalent 2D problem, which vastly accelerates optimization. We also greatly improve the theory and robustness of stretch-based elastic materials, by giving a simple and elegant formula to compute the tangent stiffness matrix, with rigorous proofs and singularity handling. We also contribute the observation that volume compressibility can be estimated by poking with rigid cylinders of different radii, which avoids optical cameras and greatly simplifies experiments. We validate our method by performing full 3D simulations using the optimized materials and confirming that they match real-world forces, indentations and real deformed 3D shapes. We also validate it using a ""Shore 00"" durometer, a standard device for measuring material hardness."	https://dl.acm.org/doi/abs/10.1145/3618406	Huanyu Chen, Danyong Zhao, Jernej Barbič
Cheeky	Butt is life. Friendship is forever.	https://dl.acm.org/doi/abs/10.1145/3626964.3626971	Jack Yuster
Close the Design-to-Manufacturing Gap in Computational Optics with a 'Real2Sim' Learned Two-Photon Neural Lithography Simulator	We introduce neural lithography to address the 'design-to-manufacturing' gap in computational optics. Computational optics with large design degrees of freedom enable advanced functionalities and performance beyond traditional optics. However, the existing design approaches often overlook the numerical modeling of the manufacturing process, which can result in significant performance deviation between the design and the fabricated optics. To bridge this gap, we, for the first time, propose a fully differentiable design framework that integrates a pre-trained photolithography simulator into the model-based optical design loop. Leveraging a blend of physics-informed modeling and data-driven training using experimentally collected datasets, our photolithography simulator serves as a regularizer on fabrication feasibility during design, compensating for structure discrepancies introduced in the lithography process. We demonstrate the effectiveness of our approach through two typical tasks in computational optics, where we design and fabricate a holographic optical element (HOE) and a multi-level diffractive lens (MDL) using a two-photon lithography system, showcasing improved optical performance on the task-specific metrics. The source code for this work is available on the project page: https://neural-litho.github.io.	https://dl.acm.org/doi/abs/10.1145/3610548.3618251	Cheng Zheng, Guangyuan Zhao, Peter So
ClothCombo: Modeling Inter-Cloth Interaction for Draping Multi-Layered Clothes	We present ClothCombo, a pipeline to drape arbitrary combinations of clothes on 3D human models with varying body shapes and poses. While existing learning-based approaches for draping clothes have shown promising results, multi-layered clothing remains challenging as it is non-trivial to model inter-cloth interaction. To this end, our method utilizes a GNN-based network to efficiently model the interaction between clothes in different layers, thus enabling multi-layered clothing. Specifically, we first create feature embedding for each cloth using a topology-agnostic network. Then, the draping network deforms all clothes to fit the target body shape and pose without considering inter-cloth interaction. Lastly, the untangling network predicts the per-vertex displacements in a way that resolves interpenetration between clothes. In experiments, the proposed model demonstrates strong performance in complex multi-layered scenarios. Being agnostic to cloth topology, our method can be readily used for layered virtual try-on of real clothes in diverse poses and combinations of clothes.	https://dl.acm.org/doi/abs/10.1145/3618376	Dohae Lee, Hyun Kang, In-Kwon Lee
Collapsing Embedded Cell Complexes for Safer Hexahedral Meshing	We present a set of operators to perform modifications, in particular collapses and splits, in volumetric cell complexes which are discretely embedded in a background mesh. Topological integrity and geometric embedding validity are carefully maintained. We apply these operators strategically to volumetric block decompositions, so-called T-meshes or base complexes, in the context of hexahedral mesh generation. This allows circumventing the expensive and unreliable global volumetric remapping step in the versatile meshing pipeline based on 3D integer-grid maps. In essence, we reduce this step to simpler local cube mapping problems, for which reliable solutions are available. As a consequence, the robustness of the mesh generation process is increased, especially when targeting coarse or block-structured hexahedral meshes. We furthermore extend this pipeline to support feature alignment constraints, and systematically respect these throughout, enabling the generation of meshes that align to points, curves, and surfaces of special interest, whether on the boundary or in the interior of the domain.	https://dl.acm.org/doi/abs/10.1145/3618384	Hendrik Brückler, Marcel Campen
Combining Resampled Importance and Projected Solid Angle Samplings for Many Area Light Rendering	Direct lighting from many area light sources is challenging due to variance from both choosing an important light and then a point on it. Resampled Importance Sampling (RIS) achieves low variance in such situations. However, it is limited to simple sampling strategies for its candidates. Specifically for area lights, we can improve the convergence of RIS by incorporating a better sampling strategy: Projected Solid Angle Sampling (ProjLTC). Naively combining RIS and ProjLTC improves equal sample convergence. However, it achieves little to no gain in equal time. We identify the core issue for the high run times and reformulate RIS for better integration with ProjLTC. Our method achieves better convergence and results in both equal sample and equal time. We evaluate our method on challenging scenes with varying numbers of area light sources and compare it to uniform sampling, RIS, and ProjLTC. In all cases, our method seldom performs worse than RIS and often performs better.	https://dl.acm.org/doi/abs/10.1145/3610543.3626165	Ishaan Nikhil Shah, Aakash KT, P. J. Narayanan
Commonsense Knowledge-Driven Joint Reasoning Approach for Object Retrieval in Virtual Reality	National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), China Retrieving out-of-reach objects is a crucial task in virtual reality (VR). One of the most commonly used approaches for this task is the gesture-based approach, which allows for bare-hand, eyes-free, and direct retrieval. However, previous work has primarily focused on assigned gesture design, neglecting the context. This can make it challenging to accurately retrieve an object from a large number of objects due to the one-to-one mapping metaphor, limitations of finger poses, and memory burdens. There is a general consensus that objects and contexts are related, which suggests that the object expected to be retrieved is related to the context, including the scene and the objects with which users interact. As such, we propose a commonsense knowledge-driven joint reasoning approach for object retrieval, where human grasping gestures and context are modeled using an And-Or graph (AOG). This approach enables users to accurately retrieve objects from a large number of candidate objects by using natural grasping gestures based on their experience of grasping physical objects. Experimental results demonstrate that our proposed approach improves retrieval accuracy. We also propose an object retrieval system based on the proposed approach. Two user studies show that our system enables efficient object retrieval in virtual environments (VEs).	https://dl.acm.org/doi/abs/10.1145/3618320	Haiyan Jiang, Dongdong Weng, Xiaonuo Dongye, Le Luo, Zhenliang Zhang
Compact Neural Graphics Primitives with Learned Hash Probing	Neural graphics primitives are faster and achieve higher quality when their neural networks are augmented by spatial data structures that hold trainable features arranged in a grid. However, existing feature grids either come with a large memory footprint (dense or factorized grids, trees, and hash tables) or slow performance (index learning and vector quantization). In this paper, we show that a hash table with learned probes has neither disadvantage, resulting in a favorable combination of size and speed. Inference is faster than unprobed hash tables at equal quality while training is only 1.2–2.6 × slower, significantly outperforming prior index learning approaches. We arrive at this formulation by casting all feature grids into a common framework: they each correspond to a lookup function that indexes into a table of feature vectors. In this framework, the lookup functions of existing data structures can be combined by simple arithmetic combinations of their indices, resulting in Pareto optimal compression and speed.	https://dl.acm.org/doi/abs/10.1145/3610548.3618167	Towaki Takikawa, Thomas Müller, Merlin Nimier-David, Alex Evans, Sanja Fidler, Alec Jacobson, Alexander Keller
Comparing Cinematic Conventions through Emotional Responses in Cinematic VR and Traditional Mediums	This paper compares emotional responses to cinematic conventions in cinematic virtual reality (CVR) versus traditional mediums. We conducted a between-subjects experiment to evaluate viewer experiences across 10 cinematic shot types displayed through three viewing mediums: computer screen, cinema projector, and VR headset (N=15 per condition). Using the Self-Assessment Manikin scale (SAM), we measured pleasure, arousal, and dominance reactions. The statistical analysis showed that CVR elicited significantly higher arousal and lower dominance than traditional formats, indicating enhanced emotional engagement. Overall patterns were found to be similar, but the viewers' emotions were intensified in CVR for shots like close-up and ground-level. Our findings suggest CVR's embodied, interactive qualities altered the impact of cinematic techniques. This comparative study reveals CVR invokes different emotional resonance versus traditional mediums, contributing insights on adapting storytelling strategies for the VR cinematographic experience.	https://dl.acm.org/doi/abs/10.1145/3610543.3626175	Zhiyuan Yu, Cheng-Hung Lo, Mutian Niu, Hai-Ning Liang
Computational Design of Flexible Planar Microstructures	Mechanical metamaterials enable customizing the elastic properties of physical objects by altering their fine-scale structure. A broad gamut of effective material properties can be produced even from a single fabrication material by optimizing the geometry of a periodic microstructure tiling. Past work has extensively studied the capabilities of microstructures in the small-displacement regime, where periodic homogenization of linear elasticity yields computationally efficient optimal design algorithms. However, many applications involve flexible structures undergoing large deformations for which the accuracy of linear elasticity rapidly deteriorates due to geometric nonlinearities. Design of microstructures at finite strains involves a massive increase in computation and is much less explored; no computational tool yet exists to design metamaterials emulating target hyperelastic laws We make an initial step in this direction, developing algorithms to accelerate homogenization and metamaterial design for nonlinear elasticity and building a complete framework for the optimal design of planar metamaterials. Our nonlinear homogenization method works by efficiently constructing an accurate interpolant of a microstructure's deformation over a finite space of macroscopic strains likely to be endured by the metamaterial. From this interpolant, the homogenized energy density, stress, and tangent elasticity tensor describing the microstructure's effective properties can be inexpensively computed at any strain. Our design tool then fits the effective material properties to a target constitutive law over a region of strain space using a parametric shape optimization approach, producing a directly manufacturable geometry. We systematically test our framework by designing a catalog of materials fitting isotropic Hooke's laws as closely as possible. We demonstrate significantly improved accuracy over traditional linear metamaterial design techniques by fabricating and testing physical prototypes.	https://dl.acm.org/doi/abs/10.1145/3618396	Zhan Zhang, Christopher Brandt, Jean Jouve, Yue Wang, Tian Chen, Mark Pauly, Julian Panetta
Computational Design of LEGO® Sketch Art	This paper presents computational methods to aid the creation of LEGO sketch models from simple input images. Beyond conventional LEGO mosaics, we aim to improve the expressiveness of LEGO models by utilizing LEGO tiles with sloping and rounding edges, together with rectangular bricks, to reproduce smooth curves and sharp features in the input. This is a challenging task, as we have limited brick shapes to use and limited space to place bricks. Also, the search space is immense and combinatorial in nature. We approach the task by decoupling the LEGO construction into two steps: first approximate the shape with a LEGO -buildable contour then filling the contour polygon with LEGO bricks. Further, we formulate this contour approximation into a graph optimization with our objective and constraints and effectively solve for the contour polygon that best approximates the input shape. Further, we extend our optimization model to handle multi-color and multi-layer regions, and formulate a grid alignment process and various perceptual constraints to refine the results. We employ our method to create a large variety of LEGO models and compare it with humans and baseline methods to manifest its compelling quality and speed.	https://dl.acm.org/doi/abs/10.1145/3618306	Mingjun Zhou, Jiahao Ge, Hao Xu, Chi-Wing Fu
Computational Design of Wiring Layout on Tight Suits with Minimal Motion Resistance	An increasing number of electronics are directly embedded on the clothing to monitor human status (e.g., skeletal motion) or provide haptic feedback. A specific challenge to prototype and fabricate such a clothing is to design the wiring layout, while minimizing the intervention to human motion. We address this challenge by formulating the topological optimization problem on the clothing surface as a deformation-weighted Steiner tree problem on a 3D clothing mesh. Our method proposed an energy function for minimizing strain energy in the wiring area under different motions, regularized by its total length. We built the physical prototype to verify the effectiveness of our method and conducted user study with participants of both design experts and smart cloth users. On three types of commercial products of smart clothing, the optimized layout design reduced wire strain energy by an average of 77% among 248 actions compared to baseline design, and 18% over the expert design.	https://dl.acm.org/doi/abs/10.1145/3610548.3618200	Kai Wang, Xiaoyu Xu, Yinping Zheng, Da Zhou, Shihui Guo, Yipeng Qin, Xiaohu Guo
Computational Three Distances: Exploring the Aesthetics of the Southern Song Dynasty and Its Adapted Simulation and Rendering	"This paper references the painting theory of the ""three distances"" concept from the Southern Song Dynasty. It introduces a contemporary reinterpretation termed ""Computational Three Distances,"" which employs computer algorithms as the means of painting, departing from traditional ink-brush techniques. The three distances encompass ""micro-distance, volume-distance,"" and ""time-distance."" This study reexamines the adjustability of microscopic world units, the density of medium within volumes along with light-scattering effects, and the accumulation and erosion of time. It then puts this theory into practice, offering illustrative methods for each aspect."	https://dl.acm.org/doi/abs/10.1145/3610591.3616426	Chi-Min Hsieh, Hsiao-Ching Chou
Computer Graphics and Extended Reality Courses for the Programmophobic	This paper describes the challenges and solutions to teaching computer graphics as well as extended reality concepts to students from a variety of backgrounds in the context of the School of Future Environments at the Auckland University of Technology, New Zealand. Examples are provided for the content and assessment strategies for two courses, as well as a summary of student work and feedback collected over the last three years.	https://dl.acm.org/doi/abs/10.1145/3610540.3627004	Stefan Marks, Sebastián Gil Parga
Concept Decomposition for Visual Exploration and Inspiration	A creative idea is often born from transforming, combining, and modifying ideas from existing visual examples capturing various concepts. However, one cannot simply copy the concept as a whole, and inspiration is achieved by examining certain aspects of the concept. Hence, it is often necessary to separate a concept into different aspects to provide new perspectives. In this paper, we propose a method to decompose a visual concept, represented as a set of images, into different visual encoded in a hierarchical tree structure. We utilize large vision-language models and their rich latent space for concept decomposition and generation. Each node in the tree represents a sub-concept using a learned vector embedding injected into the latent space of a pretrained text-to-image model. We use a set of regularizations to guide the optimization of the embedding vectors encoded in the nodes to follow the hierarchical structure of the tree. Our method allows to explore and discover new concepts derived from the original one. The tree provides the possibility of endless visual sampling at each node, allowing the user to explore the hidden sub-concepts of the object of interest. The learned aspects in each node can be combined within and across trees to create new visual ideas, and can be used in natural language sentences to apply such aspects to new designs. Project page: https://inspirationtree.github.io/inspirationtree/	https://dl.acm.org/doi/abs/10.1145/3618315	Yael Vinker, Andrey Voynov, Daniel Cohen-Or, Ariel Shamir
Conditional Resampled Importance Sampling and ReSTIR	Recent work on generalized resampled importance sampling (GRIS) enables importance-sampled Monte Carlo integration with random variable weights replacing the usual division by probability density. This enables very flexible spatiotemporal sample reuse, even if neighboring samples (e.g., light paths) have intractable probability densities. Unlike typical Monte Carlo integration, which samples according to some PDF, GRIS instead resamples existing samples. But resampling with GRIS assumes samples have tractable marginal contribution weights, which is problematic if reusing, for example, light subpaths from unidirectionally-sampled paths. Reusing such subpaths requires conditioning by (non-reused) segments of the path prefixes. In this paper, we extend GRIS to conditional probability spaces, showing correctness given certain conditional independence between integration variables and their unbiased contribution weights. We show proper conditioning when using GRIS over randomized conditional domains and how to formulate a joint unbiased contribution weight for unbiased integration. To show our theory has practical impact, we prototype a modified ReSTIR PT with a final gather pass. This reuses subpaths, postponing reuse at least one bounce along each light path. As in photon mapping, such a final gather reduces blotchy artifacts from sample correlation and reduced correlation improves the behavior of modern denoisers on ReSTIR PT signals.	https://dl.acm.org/doi/abs/10.1145/3610548.3618245	Markus Kettunen, Daqi Lin, Ravi Ramamoorthi, Thomas Bashford-Rogers, Chris Wyman
Constrained Delaunay Tetrahedrization: A Robust and Practical Approach	We present a numerically robust algorithm for computing the constrained Delaunay tetrahedrization (CDT) of a piecewise-linear complex, which has a 100% success rate on the 4408 valid models in the Thingi10k dataset. We build on the underlying theory of the well-known tetgen software, but use a floating-point implementation based on indirect geometric predicates to implicitly represent Steiner points: this new approach dramatically simplifies the implementation, removing the need for ad-hoc tolerances in geometric operations. Our approach leads to a robust and parameter-free implementation, with an empirically manageable number of added Steiner points. Furthermore, our algorithm addresses a major gap in tetgen's theory which may lead to algorithmic failure on valid models, even when assuming perfect precision in the calculations. Our output tetrahedrization conforms with the input geometry without approximations. We can further round our output to floating-point coordinates for downstream applications, which almost always results in valid floating-point meshes unless the input triangulation is very close to being degenerate.	https://dl.acm.org/doi/abs/10.1145/3618352	Lorenzo Diazzi, Daniele Panozzo, Amir Vaxman, Marco Attene
Constructive Solid Geometry on Neural Signed Distance Fields	Signed Distance Fields (SDFs) parameterized by neural networks have recently gained popularity as a fundamental geometric representation. However, editing the shape encoded by a neural SDF remains an open challenge. A tempting approach is to leverage common geometric operators (e.g., boolean operations), but such edits often lead to incorrect non-SDF outputs (which we call Pseudo-SDFs), preventing them from being used for downstream tasks. In this paper, we characterize the space of Pseudo-SDFs, which are eikonal yet not true distance functions, and derive the closest point loss, a novel regularizer that encourages the output to be an exact SDF. We demonstrate the applicability of our regularization to many operations in which traditional methods cause a Pseudo-SDF to arise, such as CSG and swept volumes, and produce a true (neural) SDF for the result of these operations.	https://dl.acm.org/doi/abs/10.1145/3610548.3618170	Zoë Marschner, Silvia Sellán, Hsueh-Ti Derek Liu, Alec Jacobson
Content-based Search for Deep Generative Models	The growing proliferation of customized and pretrained generative models has made it infeasible for a user to be fully cognizant of every model in existence. To address this need, we introduce the task of content-based model search: given a query and a large set of generative models, finding the models that best match the query. As each generative model produces a distribution of images, we formulate the search task as an optimization problem to select the model with the highest probability of generating similar content as the query. We introduce a formulation to approximate this probability given the query from different modalities, e.g., image, sketch, and text. Furthermore, we propose a contrastive learning framework for model retrieval, which learns to adapt features for various query modalities. We demonstrate that our method outperforms several baselines on Generative Model Zoo, a new benchmark we create for the model retrieval task.	https://dl.acm.org/doi/abs/10.1145/3610548.3618189	Daohan Lu, Sheng-Yu Wang, Nupur Kumari, Rohan Agarwal, Mia Tang, David Bau, Jun-Yan Zhu
Controllable Group Choreography Using Contrastive Diffusion	Music-driven group choreography poses a considerable challenge but holds significant potential for a wide range of industrial applications. The ability to generate synchronized and visually appealing group dance motions that are aligned with music opens up opportunities in many fields such as entertainment, advertising, and virtual performances. However, most of the recent works are not able to generate high-fidelity long-term motions, or fail to enable controllable experience. In this work, we aim to address the demand for high-quality and customizable group dance generation by effectively governing the consistency and diversity of group choreographies. In particular, we utilize a diffusion-based generative approach to enable the synthesis of flexible number of dancers and long-term group dances, while ensuring coherence to the input music. Ultimately, we introduce a Group Contrastive Diffusion (GCD) strategy to enhance the connection between dancers and their group, presenting the ability to control the consistency or diversity level of the synthesized group animation via the classifier-guidance sampling technique. Through intensive experiments and evaluation, we demonstrate the effectiveness of our approach in producing visually captivating and consistent group dance motions. The experimental results show the capability of our method to achieve the desired levels of consistency and diversity, while maintaining the overall quality of the generated group choreography.	https://dl.acm.org/doi/abs/10.1145/3618356	Nhat Le, Tuong Do, Khoa Do, Hien Nguyen, Erman Tjiputra, Quang D. Tran, Anh Nguyen
Conversation Echo: Communication in virtual environments that reflects conversation contents	In this study, we propose Conversation Echo, a system that reflects the topics of conversation in a VR environment in real time.This method uses AI to convert speech data into text, extract conversation topics, and generate panoramic images to generate a VR environment, which dynamically changes the environment in real-time. This method aims to realize an experience that generates conversation topics and inspiration.	https://dl.acm.org/doi/abs/10.1145/3610542.3626127	Shun Hachisu, Sohei Wakisaka, Kouta Minamizawa
Corrupted	Some memories you can't keep.	https://dl.acm.org/doi/abs/10.1145/3626964.3626981	Alex Weight, Chris French, Dylan Neill
Crossing Narrative: Exploring the Possibilities of Crossing the Virtuality and Reality in Interactive Narrative Experiences	"With the development of mixed reality technology, the convergence of virtual and real experiences has emerged as a trend in the interactive narratives. In this paper, we introduce ""Crossing Narrative"", an interactive narrative experience that seamlessly blends virtuality and reality by utilizing real-world views and bystanders. We discuss specific methods for designing cross-reality narrative experience, focusing on three key aspects of cross-reality interactions: diegetic objects, transition effects, and bystander's avatar. Our design methods aim to enhance audience social interactions and understanding of the immersive narrative."	https://dl.acm.org/doi/abs/10.1145/3610542.3626125	Zixiao Liu, Shuo Yan, Xukun Shen
Cubic Gymnastics	"""Cubic Gymnastics"" is an XR system that enables users to engage in a wide range of exercises specifically designed for geometric shapes, which are visualized within the HMD space. By wearing the HMD, users can mentally immerse themselves in a geometrical body viewed in the mirror referred as the ""cubic body,"" comprised of multiple scalable cuboids and/or curved solid shapes. The physical restriction imposed on the user's body by a long rod, typically limiting hand motion, creates a perception of being transformed into a flat form, thereby enhancing the illusion of owning the cubic body. These geometric exercises involve unique movements that are physically impossible for the human body, including the separation of the upper and lower body, adhesion, stretching, deformation, and rotation-on-the-ground. Results from a survey conducted during a laboratory exhibition confirmed that a significant number of participants strongly experienced the sensation of performing these unconventional object-like movements specific to the transformed geometric shapes."	https://dl.acm.org/doi/abs/10.1145/3610549.3614605	Kenri Kodaka, Kousuke Motohashi, Tsuyoshi Suzuki
Curl Noise Jittering	We propose a method for implicitly generating blue noise point sets. Our method is based on the observations that curl noise vector fields are volume-preserving and that jittering can be construed as moving points along the streamlines of a vector field. We demonstrate that the volume preservation keeps the points well separated when jittered using a curl noise vector field. At the same time, the anisotropy that stems from regular lattices is significantly reduced by such jittering. In combination, these properties entail that jittering by curl noise effectively transforms a regular lattice into a point set with blue noise properties. Our implicit method does not require computing the point set in advance. This makes our technique valuable when an arbitrarily large set of points with blue noise properties is needed. We compare our method to several other methods based on jittering as well as other methods for blue noise point set generation. Finally, we show several applications of curl noise jittering in two and three dimensions.	https://dl.acm.org/doi/abs/10.1145/3610548.3618163	J. Andreas Bærentzen, Jeppe Revall Frisvad, Jonàs Martínez
Cymatic Ground	Cymatic Ground is an interactive sound installation doubling as a model of a dynamic urban landscape. Its body, a metallic scaffold designed following the plans of an old neighborhood in Hong Kong, is covered by a fine layer of sand. This exposed skin morphs in response to the sounds and vibrations of the larger environment it is immersed in. In response to fortuitous pressures and occasional gentle beats from the public, Cymatic Ground produces audible moaning and earthquake-like shakings that displace the grains of sand, destroying or revealing transient geometric patterns and complex networks of canals analogous to avenues, streets and alleys faithful to its original plans. On the other hand, when these external influences are not in tune to its fundamental mechanical resonances, its skin wrinkles and breaks apart, and its metallic body produces strident notes of despair as the installation searches for an appearance better attuned to the new reality it is immersed in. In an introspective and concerted effort, each plate - each neighborhood-continuously scans its own mechanical resonances, probing every part of its body with precisely tuned waves of energy, not unlike a city striving to reinvent itself in order to rekindle its appeal in response to global economic crisis, the effects of the climate change or the continuous displacement of its dwellers.	https://dl.acm.org/doi/abs/10.1145/3610537.3622961	Alvaro Cassinelli, Tobias Klein
C·ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters	We present C · ASE, an efficient and effective framework that learns Conditional Adversarial Skill Embeddings for physics-based characters. C · ASE enables the physically simulated character to learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. This is achieved by dividing the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn the conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character's skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, outperforming state-of-the-art models, and can be repurposed in various downstream tasks. In particular, the explicit skill control handle allows a high-level policy or a user to direct the character with desired skill specifications, which we demonstrate is advantageous for interactive character animation.	https://dl.acm.org/doi/abs/10.1145/3610548.3618205	Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang
DR-Occluder: Generating Occluders Using Differentiable Rendering	The target of the occluder is to use very few faces to maintain similar occlusion properties of the original 3D model. In this paper, we present DR-Occluder, a novel coarse-to-fine framework for occluder generation that leverages differentiable rendering to optimize a triangle set to an occluder. Unlike prior work, which has not utilized differentiable rendering for this task, our approach provides the ability to optimize a 3D shape to defined targets. Given a 3D model as input, our method first projects it to silhouette images, which are then processed by a convolution network to output a group of vertex offsets. These offsets are used to transform a group of distributed triangles into a preliminary occluder, which is further optimized by differentiable rendering. Finally, triangles whose area is smaller than a threshold are removed to obtain the final occluder. Our extensive experiments demonstrate that DR-Occluder significantly outperforms state-of-the-art methods in terms of occlusion quality. Furthermore, we compare the performance of our method with other approaches in a commercial engine, providing compelling evidence of its effectiveness.	https://dl.acm.org/doi/abs/10.1145/3618346	Jiaxian Wu, Yue Lin, Dehui Lu
DROP: Dynamics Responses from Human Motion Prior and Projective Dynamics	Synthesizing realistic human movements, dynamically responsive to the environment, is a long-standing objective in character animation, with applications in computer vision, sports, and healthcare, for motion prediction and data augmentation. Recent kinematics-based generative motion models offer impressive scalability in modeling extensive motion data, albeit without an interface to reason about and interact with physics. While simulator-in-the-loop learning approaches enable highly physically realistic behaviors, the challenges in training often affect scalability and adoption. We introduce DROP, a novel framework for modeling Dynamics Responses of humans using generative mOtion prior and Projective dynamics. DROP can be viewed as a highly stable, minimalist physics-based human simulator that interfaces with a kinematics-based generative motion prior. Utilizing projective dynamics, DROP allows flexible and simple integration of the learned motion prior as one of the projective energies, seamlessly incorporating control provided by the motion prior with Newtonian dynamics. Serving as a model-agnostic plug-in, DROP enables us to fully leverage recent advances in generative motion models for physics-based motion synthesis. We conduct extensive evaluations of our model across different motion tasks and various physical perturbations, demonstrating the scalability and diversity of responses.	https://dl.acm.org/doi/abs/10.1145/3610548.3618175	Yifeng Jiang, Jungdam Won, Yuting Ye, C. Karen Liu
Dandelion	A coal-burning robot working on a mine finds a dandelion.	https://dl.acm.org/doi/abs/10.1145/3626964.3626973	Ling Zhao, Zhengwu Gu
Darkening	How is the world perceived by someone with depression? The animated immersive film uses virtual reality to address depression and the ways to cope with it. Director and protagonist Ondřej guides us through diverse landscapes associating the story of his struggle with depression since puberty. We share his feelings during the first depressive episodes at a family trip in his childhood, at university when striving for perfect results, at work in his everyday fights with the depressive 'darkening'. Through animation, combining a stylised form of Ondřej's environment and abstract images of his emotions, the viewers will experience and understand what it is like to live with this illness, how to tackle it and what mechanisms are used by people with depression to feel better. Most of the interactions are voice controlled. The main character Ondřej finds out that his tool to get the depression under control is his voice. He uses humming, singing and even shouting as a calming and relieving technique.	https://dl.acm.org/doi/abs/10.1145/3610549.3614595	Hana Blaha Šilarová
Data-Driven Expressive 3D Facial Animation Synthesis for Digital Humans	This doctoral research focuses on generating expressive 3D facial animation for digital humans by studying and employing data-driven techniques. Face is the first point of interest during human interaction, and it is not any different for interacting with digital humans. Even minor inconsistencies in facial animation can disrupt user immersion. Traditional animation workflows prove realistic but time-consuming and labor-intensive that cannot meet the ever-increasing demand for 3D contents in recent years. Moreover, recent data-driven approaches focus on speech-driven lip synchrony, leaving out facial expressiveness that resides throughout the face. To address the emerging demand and reduce production efforts, we explore data-driven deep learning techniques for generating controllable, emotionally expressive facial animation. We evaluate the proposed models against state-of-the-art methods and ground-truth, quantitatively, qualitatively, and perceptually. We also emphasize the need for non-deterministic approaches in addition to deterministic methods in order to ensure natural randomness in the non-verbal cues of facial animation.	https://dl.acm.org/doi/abs/10.1145/3623053.3623369	Kazi Injamamul Haque
DazzleVR: Enhancement of Brightness by Presenting Afterimage and Dazzle Reflex Sensation in Virtual Reality	The presentation of brightness is essential in virtual reality. In this paper, we propose a system that can induce the sensation of dazzle by presenting a realistic afterimage and a pseudo dazzle reflex sensation with visuo-haptic feedback. Our system is expected to present realistic environments and emotional performance settings.	https://dl.acm.org/doi/abs/10.1145/3610541.3614579	Juro Hosoi, Takahiro Ito, Yuki Ban, Shin'Ichi Warisawa
Decaf: Monocular Deformation Capture for Face and Hand Interactions	Existing methods for 3D tracking from monocular RGB videos predominantly consider articulated and rigid objects ( , two hands or humans interacting with rigid environments). Modelling dense non-rigid object deformations in this setting ( when hands are interacting with a face), remained largely unaddressed so far, although such effects can improve the realism of the downstream applications such as AR/VR, 3D virtual avatar communications, and character animations. This is due to the severe ill-posedness of the monocular view setting and the associated challenges ( , in acquiring a dataset for training and evaluation or obtaining the reasonable non-uniform stiffness of the deformable object). While it is possible to naïvely track multiple non-rigid objects independently using 3D templates or parametric 3D models, such an approach would suffer from multiple artefacts in the resulting 3D estimates such as depth ambiguity, unnatural intra-object collisions and missing or implausible deformations. Hence, this paper introduces the first method that addresses the fundamental challenges depicted above and that allows tracking human hands interacting with human faces in 3D from single monocular RGB videos. We model hands as articulated objects inducing non-rigid face deformations during an active interaction. Our method relies on a new hand-face motion and interaction capture dataset with realistic face deformations acquired with a markerless multi-view camera system. As a pivotal step in its creation, we process the reconstructed raw 3D shapes with position-based dynamics and an approach for non-uniform stiffness estimation of the head tissues, which results in plausible annotations of the surface deformations, hand-face contact regions and head-hand positions. At the core of our neural approach are a variational auto-encoder supplying the hand-face depth prior and modules that guide the 3D tracking by estimating the contacts and the deformations. Our final 3D hand and face reconstructions are realistic and more plausible compared to several baselines applicable in our setting, both quantitatively and qualitatively. https://vcai.mpi-inf.mpg.de/projects/Decaf	https://dl.acm.org/doi/abs/10.1145/3618329	Soshi Shimada, Vladislav Golyanik, Patrick Pérez, Christian Theobalt
DeepBasis: Hand-Held Single-Image SVBRDF Capture via Two-Level Basis Material Model	Recovering spatial-varying bi-directional reflectance distribution function (SVBRDF) from a single hand-held captured image has been a meaningful but challenging task in computer graphics. Benefiting from the learned data priors, some previous methods can utilize the potential material correlations between image pixels to serve for SVBRDF estimation. To further reduce the ambiguity from single-image estimation, it is necessary to integrate additional explicit material correlations. Given the flexible expressive ability of basis material assumption, we propose DeepBasis, a deep-learning-based method integrated with this assumption. It jointly predicts basis materials and their blending weights. Then the estimated SVBRDF is their linear combination. To facilitate the extraction of data priors, we introduce a two-level basis model to keep the sufficient representative while using a fixed number of basis materials. Moreover, considering the absence of ground-truth basis materials and weights during network training, we propose a variance-consistency loss and adopt a joint prediction strategy, thereby enabling the existing SVBRDF dataset available for training. Additionally, due to the hand-held capture setting, the exact lighting directions are unknown. We model the lighting direction estimation as a sampling problem and propose an optimization-based algorithm to find the optimal estimation. Quantitative evaluation and qualitative analysis demonstrate that DeepBasis can produce a higher quality SVBRDF estimation than previous methods. All source codes will be publicly released.	https://dl.acm.org/doi/abs/10.1145/3610548.3618239	Li Wang, Lianghao Zhang, Fangzhou Gao, Jiawan Zhang
Depolarized Holography with Polarization-Multiplexing Metasurface	The evolution of computer-generated holography (CGH) algorithms has prompted significant improvements in the performances of holographic displays. Nonetheless, they start to encounter a limited degree of freedom in CGH optimization and physical constraints stemming from the coherent nature of holograms. To surpass the physical limitations, we consider polarization as a new degree of freedom by utilizing a novel optical platform called metasurface. Polarization-multiplexing metasurfaces enable incoherent-like behavior in holographic displays due to the mutual incoherence of orthogonal polarization states. We leverage this unique characteristic of a metasurface by integrating it into a holographic display and exploiting polarization diversity to bring an additional degree of freedom for CGH algorithms. To minimize the speckle noise while maximizing the image quality, we devise a fully differentiable optimization pipeline by taking into account the metasurface proxy model, thereby jointly optimizing spatial light modulator phase patterns and geometric parameters of metasurface nanostructures. We evaluate the metasurface-enabled depolarized holography through simulations and experiments, demonstrating its ability to reduce speckle noise and enhance image quality.	https://dl.acm.org/doi/abs/10.1145/3618395	Seung-Woo Nam, Youngjin Kim, Dongyeon Kim, Yoonchan Jeong
Developable Quad Meshes and Contact Element Nets	The property of a surface being developable can be expressed in different equivalent ways, by vanishing Gauss curvature, or by the existence of isometric mappings to planar domains. Computational contributions to this topic range from special parametrizations to discrete-isometric mappings. However, so far a local criterion expressing developability of general quad meshes has been lacking. In this paper, we propose a new and efficient discrete developability criterion that is applied to quad meshes equipped with vertex weights, and which is motivated by a well-known characterization in differential geometry, namely a rank-deficient second fundamental form. We assign contact elements to the faces of meshes and ruling vectors to the edges, which in combination yield a developability condition per face. Using standard optimization procedures, we are able to perform interactive design and developable lofting. The meshes we employ are combinatorially regular quad meshes with isolated singularities but are otherwise not required to follow any special curves on a developable surface. They are thus easily embedded into a design workflow involving standard operations like remeshing, trimming, and merging operations. An important feature is that we can directly derive a watertight, rational bi-quadratic spline surface from our meshes. Remarkably, it occurs as the limit of weighted Doo-Sabin subdivision, which acts in an interpolatory manner on contact elements.	https://dl.acm.org/doi/abs/10.1145/3618355	Victor Ceballos Inza, Florian Rist, Johannes Wallner, Helmut Pottmann
Developing a Realistic VR Interface to Recreate a Full-body Immersive Fire Scene Experience	This paper describes a research project on the development of a VR fire training system. We aimed to create a multi-sensory experience that simulates a real-world fire scene. To achieve this, we developed a motion simulator to provide physical stimulation, haptic firefighting nozzles to provide a realistic sense of tool use, and a firefighting suit that can transmit the hot/cold sensations of VR content to the entire body. We evaluated firefighter and public satisfaction with the VR firefighting experience, and outlined future research challenges to improve usability in the field.	https://dl.acm.org/doi/abs/10.1145/3610542.3626117	Ungyeon Yang, Hyungki Son, Kyungsik Han
DiffFR: Differentiable SPH-Based Fluid-Rigid Coupling for Rigid Body Control	Differentiable physics simulation has shown its efficacy in inverse design problems. Given the pervasiveness of the diverse interactions between fluids and solids in life, a differentiable simulator for the inverse design of the motion of rigid objects in two-way fluid-rigid coupling is also demanded. There are two main challenges to develop a differentiable two-way fluid-solid coupling simulator for rigid body control tasks: the ubiquitous, discontinuous contacts in fluid-solid interactions, and the high computational cost of gradient formulation due to the large number of degrees of freedom (DoF) of fluid dynamics. In this work, we propose a novel differentiable SPH-based two-way fluid-rigid coupling simulator to address these challenges. Our purpose is to provide a differentiable simulator for SPH which incorporates a unified representation for both fluids and solids using particles. However, naively differentiating the forward simulation of the particle system encounters gradient explosion issues. We investigate the instability in differentiating the SPH-based fluid-rigid coupling simulator and present a feasible gradient computation scheme to address its differentiability. In addition, we also propose an efficient method to compute the gradient of fluid-rigid coupling without incurring the high computational cost of differentiating the entire high-DoF fluid system. We show the efficacy, scalability, and extensibility of our method in various challenging rigid body control tasks with diverse fluid-rigid interactions and multi-rigid contacts, achieving up to an order of magnitude speedup in optimization compared to baseline methods in experiments.	https://dl.acm.org/doi/abs/10.1145/3618318	Zhehao Li, Qingyu Xu, Xiaohan Ye, Bo Ren, Ligang Liu
Differentiable Dynamic Visible-Light Tomography	We propose the first visible-light tomography system for real-time acquisition and reconstruction of general temporally-varying 3D phenomena. Using a single high-speed camera, a high-performance LED array and optical fibers with a total length of 5 km, we build a novel acquisition setup with no mechanical movements to simultaneously sample using 1,920 interleaved sources and detectors with a complete 360 ° coverage. Next, we introduce a novel differentiable framework to map both tomography acquisition and reconstruction to a carefully designed autoencoder. This allows the joint and automatic optimization of both processes in an end-to-end fashion, essentially learning to physically compress and computationally decompress the target information. Our framework can adapt to various factors, and trade between capture speed and reconstruction quality. We achieve an acquisition speed of up to 36.8 volumes per second at a spatial resolution of 32 × 128 × 128; each volume is captured with as few as 8 images. The effectiveness of the system is demonstrated on acquiring various dynamic scenes. Our results are also validated with the reconstructions computed from the measurements with one source on at a time, and compare favorably with state-of-the-art techniques.	https://dl.acm.org/doi/abs/10.1145/3610548.3618166	Kaizhang Kang, Zoubin Bi, Xiang Feng, Yican Dong, Kun Zhou, Hongzhi Wu
Differentiable Rendering of Parametric Geometry	We propose an efficient method for differentiable rendering of parametric surfaces and curves, which enables their use in inverse graphics problems. Our central observation is that a representative triangle mesh can be extracted from a continuous parametric object in a and way. We derive differentiable meshing operators for surfaces and curves that provide varying levels of approximation granularity. With triangle mesh approximations, we can readily leverage existing machinery for differentiable mesh rendering to handle parametric geometry. Naively combining differentiable tessellation with inverse graphics settings lacks robustness and is prone to reaching undesirable local minima. To this end, we draw a connection between our setting and the optimization of triangle meshes in inverse graphics and present a set of optimization techniques, including regularizations and coarse-to-fine schemes. We show the viability and efficiency of our method in a set of image-based computer-aided design applications.	https://dl.acm.org/doi/abs/10.1145/3618387	Markus Worchel, Marc Alexa
Diffusing Colors: Image Colorization with Text Guided Diffusion	The colorization of grayscale images is a complex and subjective task with significant challenges. Despite recent progress in employing large-scale datasets with deep neural networks, difficulties with controllability and visual quality persist. To tackle these issues, we present a novel image colorization framework that utilizes image diffusion techniques with granular text prompts. This integration not only produces colorization outputs that are semantically appropriate but also greatly improves the level of control users have over the colorization process. Our method provides a balance between automation and control, outperforming existing techniques in terms of visual quality and semantic coherence. We leverage a pretrained generative Diffusion Model, and show that we can finetune it for the colorization task without losing its generative power or attention to text prompts. Moreover, we present a novel CLIP-based ranking model that evaluates color vividness, enabling automatic selection of the most suitable level of vividness based on the specific scene semantics. Our approach holds potential particularly for color enhancement and historical image colorization.	https://dl.acm.org/doi/abs/10.1145/3610548.3618180	Nir Zabari, Aharon Azulay, Alexey Gorkor, Tavi Halperin, Ohad Fried
Diffusion Posterior Illumination for Ambiguity-Aware Inverse Rendering	Inverse rendering, the process of inferring scene properties from images, is a challenging inverse problem. The task is ill-posed, as many different scene configurations can give rise to the same image. Most existing solutions incorporate priors into the inverse-rendering pipeline to encourage plausible solutions, but they do not consider the inherent ambiguities and the multi-modal distribution of possible decompositions. In this work, we propose a novel scheme that integrates a denoising diffusion probabilistic model pre-trained on natural illumination maps into an optimization framework involving a differentiable path tracer. The proposed method allows sampling from combinations of illumination and spatially-varying surface materials that are, both, natural and explain the image observations. We further conduct an extensive comparative study of different priors on illumination used in previous work on inverse rendering. Our method excels in recovering materials and producing highly realistic and diverse environment map samples that faithfully explain the illumination of the input images.	https://dl.acm.org/doi/abs/10.1145/3618357	Linjie Lyu, Ayush Tewari, Marc Habermann, Shunsuke Saito, Michael Zollhöfer, Thomas Leimkühler, Christian Theobalt
Diffusion-based Holistic Texture Rectification and Synthesis	We present a novel framework for rectifying occlusions and distortions in degraded texture samples from natural images. Traditional texture synthesis approaches focus on generating textures from pristine samples, which necessitate meticulous preparation by humans and are often unattainable in most natural images. These challenges stem from the frequent occlusions and distortions of texture samples in natural images due to obstructions and variations in object surface geometry. To address these issues, we propose a framework that synthesizes holistic textures from degraded samples in natural images, extending the applicability of exemplar-based texture synthesis techniques. Our framework utilizes a conditional Latent Diffusion Model (LDM) with a novel occlusion-aware latent transformer. This latent transformer not only effectively encodes texture features from partially-observed samples necessary for the generation process of the LDM, but also explicitly captures long-range dependencies in samples with large occlusions. To train our model, we introduce a method for generating synthetic data by applying geometric transformations and free-form mask generation to clean textures. Experimental results demonstrate that our framework significantly outperforms existing methods both quantitatively and quantitatively. Furthermore, we conduct comprehensive ablation studies to validate the different components of our proposed framework. Results are corroborated by a perceptual user study which highlights the efficiency of our proposed approach.	https://dl.acm.org/doi/abs/10.1145/3610548.3618233	Guoqing Hao, Satoshi Iizuka, Kensho Hara, Edgar Simo-Serra, Hirokatsu Kataoka, Kazuhiro Fukui
Discontinuity-Aware 2D Neural Fields	Neural image representations offer the possibility of high fidelity, compact storage, and resolution-independent accuracy, providing an attractive alternative to traditional pixel- and grid-based representations. However, coordinate neural networks fail to capture discontinuities present in the image and tend to blur across them; we aim to address this challenge. In many cases, such as rendered images, vector graphics, diffusion curves, or solutions to partial differential equations, the locations of the discontinuities are known. We take those locations as input, represented as linear, quadratic, or cubic Bézier curves, and construct a feature field that is discontinuous across these locations and smooth everywhere else. Finally, we use a shallow multi-layer perceptron to decode the features into the signal value. To construct the feature field, we develop a new data structure based on a curved triangular mesh, with features stored on the vertices and on a subset of the edges that are marked as discontinuous. We show that our method can be used to compress a 100, 000 -pixel rendered image into a 25MB file; can be used as a new diffusion-curve solver by combining with Monte-Carlo-based methods or directly supervised by the diffusion-curve energy; or can be used for compressing 2D physics simulation data.	https://dl.acm.org/doi/abs/10.1145/3618379	Yash Belhe, Michaël Gharbi, Matthew Fisher, Iliyan Georgiev, Ravi Ramamoorthi, Tzu-Mao Li
Discovering Fatigued Movements for Virtual Character Animation	Virtual character animation and movement synthesis have advanced rapidly during recent years, especially through a combination of extensive motion capture datasets and machine learning. A remaining challenge is interactively simulating characters that fatigue when performing extended motions, which is indispensable for the realism of generated animations. However, capturing such movements is problematic, as performing movements like backflips with fatigued variations up to exhaustion raises capture cost and risk of injury. Surprisingly, little research has been done on faithful fatigue modeling. To address this, we propose a deep reinforcement learning-based approach, which—for the first time in literature—generates control policies for full-body physically simulated agents aware of cumulative fatigue. For this, we first leverage Generative Adversarial Imitation Learning (GAIL) to learn an expert policy for the skill; Second, we learn a fatigue policy by limiting the generated constant torque bounds based on endurance time to non-linear, state- and time-dependent limits in the joint-actuation space using a Three-Compartment Controller (3CC) model. Our results demonstrate that agents can adapt to different fatigue and rest rates interactively, and discover realistic recovery strategies without the need for any captured data of fatigued movement.	https://dl.acm.org/doi/abs/10.1145/3610548.3618176	Noshaba Cheema, Rui Xu, Nam Hee Kim, Perttu Hämäläinen, Vladislav Golyanik, Marc Habermann, Christian Theobalt, Philipp Slusallek
Discrete Laplacians for General Polygonal and Polyhedral Meshes	The Laplace-Beltrami operator is one of the essential tools in geometric processing. It allows us to solve numerous partial differential equations on discrete surface and volume meshes, which is a fundamental building block in many computer graphics applications. Discrete Laplacians are typically limited to standard elements like triangles or quadrilaterals, which severely constrains the tessellation of the mesh. But in recent years, several approaches were able to generalize the Laplace Beltrami and its closely related gradient and divergence operators to more general meshes. This allows artists and engineers to work with a wider range of elements which are sometimes required and beneficial in their field. This course, which extends the state-of-the-art report by Bunge and Botsch [2023], discusses the different constructions of these three ubiquitous differential operators on arbitrary polygons and polyhedra and analyzes their individual advantages and properties in common computer graphics applications.	https://dl.acm.org/doi/abs/10.1145/3610538.3614620	Astrid Bunge, Marc Alexa, Mario Botsch
Distributed Solution of the Blendshape Rig Inversion Problem	The problem of rig inversion is central in facial animation, but with the increasing complexity of modern blendshape models, execution times increase beyond practically feasible solutions. A possible approach towards a faster solution is clustering, which exploits the spacial nature of the face, leading to a distributed method. In this paper, we go a step further, involving cluster coupling to get more confident estimates of the overlapping components. Our algorithm applies the Alternating Direction Method of Multipliers, sharing the overlapping weights between the subproblems and show a clear advantage over the naive clustered approach. The method applies to an arbitrary clustering of the face. We also introduce a novel method for choosing the number of clusters in a data-free manner, resulting in a sparse clustering graph without losing essential information. Finally, we give a new variant of a data-free clustering algorithm that produces good scores with respect to the mentioned strategy for choosing the optimal clustering.	https://dl.acm.org/doi/abs/10.1145/3610543.3626166	Stevo Racković, Cláudia Soares, Dušan Jakovetić
Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models	Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts. Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times. However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts. In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models. This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods.	https://dl.acm.org/doi/abs/10.1145/3610548.3618173	Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, Amit H. Bermano
Doppler Time-of-Flight Rendering	We introduce Doppler time-of-flight (D-ToF) rendering, an extension of ToF rendering for dynamic scenes, with applications in simulating D-ToF cameras. D-ToF cameras use high-frequency modulation of illumination and exposure, and measure the Doppler frequency shift to compute the radial velocity of dynamic objects. The time-varying scene geometry and high-frequency modulation functions used in such cameras make it challenging to accurately and efficiently simulate their measurements with existing ToF rendering algorithms. We overcome these challenges in a twofold manner: To achieve accuracy, we derive path integral expressions for D-ToF measurements under global illumination and form unbiased Monte Carlo estimates of these integrals. To achieve efficiency, we develop a tailored time-path sampling technique that combines antithetic time sampling with correlated path sampling. We show experimentally that our sampling technique achieves up to two orders of magnitude lower variance compared to naive time-path sampling. We provide an open-source simulator that serves as a digital twin for D-ToF imaging systems, allowing imaging researchers, for the first time, to investigate the impact of modulation functions, material properties, and global illumination on D-ToF imaging performance.	https://dl.acm.org/doi/abs/10.1145/3618335	Juhyeon Kim, Wojciech Jarosz, Ioannis Gkioulekas, Adithya Pediredla
DreamEditor: Text-Driven 3D Scene Editing with Neural Fields	Neural fields have achieved impressive advancements in view synthesis and scene reconstruction. However, editing these neural fields remains challenging due to the implicit encoding of geometry and texture information. In this paper, we propose DreamEditor, a novel framework that enables users to perform controlled editing of neural fields using text prompts. By representing scenes as mesh-based neural fields, DreamEditor allows localized editing within specific regions. DreamEditor utilizes the text encoder of a pretrained text-to-Image diffusion model to automatically identify the regions to be edited based on the semantics of the text prompts. Subsequently, DreamEditor optimizes the editing region and aligns its geometry and texture with the text prompts through score distillation sampling [Poole et al. 2022]. Extensive experiments have demonstrated that DreamEditor can accurately edit neural fields of real-world scenes according to the given text prompts while ensuring consistency in irrelevant areas. DreamEditor generates highly realistic textures and geometry, significantly surpassing previous works in both quantitative and qualitative evaluations.	https://dl.acm.org/doi/abs/10.1145/3610548.3618190	Jingyu Zhuang, Chen Wang, Liang Lin, Lingjie Liu, Guanbin Li
Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic Clothing Driven by Sparse RGB-D Input	Clothing is an important part of human appearance but challenging to model in photorealistic avatars. In this work we present avatars with dynamically moving loose clothing that can be faithfully driven by sparse RGB-D inputs as well as body and face motion. We propose a Neural Iterative Closest Point (N-ICP) algorithm that can efficiently track the coarse garment shape given sparse depth input. Given the coarse tracking results, the input RGB-D images are then remapped to texel-aligned features, which are fed into the drivable avatar models to faithfully reconstruct appearance details. We evaluate our method against recent image-driven synthesis baselines, and conduct a comprehensive analysis of the N-ICP algorithm. We demonstrate that our method can generalize to a novel testing environment, while preserving the ability to produce high-fidelity and faithful clothing dynamics and appearance.	https://dl.acm.org/doi/abs/10.1145/3610548.3618136	Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, Timur Bagautdinov
Dynamic Ocean Explorer: XR Experience	The Dynamic Ocean Explorer XR experience allows users to explore oceanographic volumes in mixed reality; combining standard 2D ocean science visualisation approaches such as flow fields with approaches only possible in dynamic 3D such as arbitrary 6-axis clipping, as well as the embodied interaction techniques of natural gestural interaction and direct manipulation. The system is instantiated as mixed reality with pass-through video in a VR headset, and also using Web3D in a standard browser.	https://dl.acm.org/doi/abs/10.1145/3610549.3614616	Philip Grimmett, Emma Krantz, Nagida Helsby-Clark, Dominic Branchaud, Viveka Weiley
Dynamic Split Body: Changing Body Perception and Self-Location by Manipulating Half-Body Position	Some studies have been conducted to induce the sense of being in two locations by inducing body ownership in two bodies. However, the feeling of being in two locations was weak. In our previous study, we induced body ownership in a split avatar in advance to induce the sensation of being at two locations. As a result, although participants' self-location extended to the right, the split avatar was perceived as a single body and they did not feel as if they were in two locations. Our demonstration expand our previous work by allowing participants to dynamically manipulate the position of the split body. We attempt to present an experience in which the split body is perceived as one body even though it is split, or the participants feel as if they are in two locations as two independent bodies. We also changed tracking system from OptiTrack to Lighthouse.	https://dl.acm.org/doi/abs/10.1145/3610541.3614588	Ryota Kondo, Maki Sugimoto
EMS: 3D Eyebrow Modeling from Single-View Images	Eyebrows play a critical role in facial expression and appearance. Although the 3D digitization of faces is well explored, less attention has been drawn to 3D eyebrow modeling. In this work, we propose EMS, the first learning-based framework for single-view 3D eyebrow reconstruction. Following the methods of scalp hair reconstruction, we also represent the eyebrow as a set of fiber curves and convert the reconstruction to fibers growing problem. Three modules are then carefully designed: firstly localizes the fiber root positions which indicate where to grow; predicts an orientation field in the 3D space to guide the growing of fibers; is designed to determine when to stop the growth of each fiber. Our directly borrows the method used in hair reconstruction. Considering the differences between hair and eyebrows, both and are newly proposed. Specifically, to cope with the challenge that the root location is severely occluded, we formulate root localization as a density map estimation task. Given the predicted density map, a density-based clustering method is further used for finding the roots. For each fiber, the growth starts from the root point and moves step by step until the ending, where each step is defined as an oriented line segment with a constant length according to the predicted orientation field. To determine when to end, a pixel-aligned RNN architecture is designed to form a binary classifier, which outputs stop or not for each growing step. To support the training of all proposed networks, we build the first 3D synthetic eyebrow dataset that contains 400 high-quality eyebrow models manually created by artists. Extensive experiments have demonstrated the effectiveness of the proposed pipeline on a variety of different eyebrow styles and lengths, ranging from short and sparse to long bushy eyebrows.	https://dl.acm.org/doi/abs/10.1145/3618323	Chenghong Li, Leyang Jin, Yujian Zheng, Yizhou Yu, Xiaoguang Han
EXIM: A Hybrid Explicit-Implicit Representation for Text-Guided 3D Shape Generation	This paper presents a new text-guided technique for generating 3D shapes. The technique leverages a hybrid 3D shape representation, namely EXIM, combining the strengths of explicit and implicit representations. Specifically, the explicit stage controls the topology of the generated 3D shapes and enables local modifications, whereas the implicit stage refines the shape and paints it with plausible colors. Also, the hybrid approach separates the shape and color and generates color conditioned on shape to ensure shape-color consistency. Unlike the existing state-of-the-art methods, we achieve high-fidelity shape generation from natural-language descriptions without the need for time-consuming per-shape optimization or reliance on human-annotated texts during training or test-time optimization. Further, we demonstrate the applicability of our approach to generate indoor scenes with consistent styles using text-induced 3D shapes. Through extensive experiments, we demonstrate the compelling quality of our results and the high coherency of our generated shapes with the input texts, surpassing the performance of existing methods by a significant margin. Codes and models are released at https://github.com/liuzhengzhe/EXIM.	https://dl.acm.org/doi/abs/10.1145/3618312	Zhengzhe Liu, Jingyu Hu, Ka-Hei Hui, Xiaojuan Qi, Daniel Cohen-Or, Chi-Wing Fu
Editing Motion Graphics Video via Motion Vectorization and Transformation	Motion graphics videos are widely used in Web design, digital advertising, animated logos and film title sequences, to capture a viewer's attention. But editing such video is challenging because the video provides a low-level sequence of pixels and frames rather than higher-level structure such as the objects in the video with their corresponding motions and occlusions. We present a pipeline for converting motion graphics video into an SVG motion program that provides such structure. The resulting SVG program can be rendered using any SVG renderer (e.g. most Web browsers) and edited using any SVG editor. We also introduce a API that facilitates editing of a SVG motion program to create variations that adjust the timing, motions and/or appearances of objects. We show how the API can be used to create a variety of effects including retiming object motion to match a music beat, adding motion textures to objects, and collision preserving appearance changes.	https://dl.acm.org/doi/abs/10.1145/3618316	Sharon Zhang, Jiaju Ma, Jiajun Wu, Daniel Ritchie, Maneesh Agrawala
Educational Perilune: Visual effects practices at Media Design School	This work introduces and explores learning experience design (LXD) methods to teaching and learning in the creative technologies space. We examine LXD techniques as a method to provide educational solutions to the application of knowledge and skills in project-based learning with the goal of creating an engaging, interactive, rich, and effective learning environment. We analyse LXD techniques implemented in instruction designed specifically for CGI, 3D animation and visual effects subject matter.	https://dl.acm.org/doi/abs/10.1145/3610540.3627007	Stephen Dorner
Efficient Cone Singularity Construction for Conformal Parameterizations	We propose an efficient method to construct sparse cone singularities under distortion-bounded constraints for conformal parameterizations. Central to our algorithm is using the technique of shape derivatives to move cones for distortion reduction without changing the number of cones. In particular, the supernodal sparse Cholesky update significantly accelerates this movement process. To satisfy the distortion-bounded constraint, we alternately move cones and add cones. The capability and feasibility of our approach are demonstrated over a data set containing 3885 models. Compared with the state-of-the-art method, we achieve an average acceleration of 15 times and slightly fewer cones for the same amount of distortion.	https://dl.acm.org/doi/abs/10.1145/3618407	Mo Li, Qing Fang, Zheng Zhang, Ligang Liu, Xiao-Ming Fu
Efficient Graphics Representation with Differentiable Indirection	We introduce differentiable indirection – a novel learned primitive that employs differentiable multi-scale lookup tables as an effective substitute for traditional compute and data operations across the graphics pipeline. We demonstrate its flexibility on a number of graphics tasks, i.e., geometric and image representation, texture mapping, shading, and radiance field representation. In all cases, differentiable indirection seamlessly integrates into existing architectures, trains rapidly, and yields both versatile and efficient results.	https://dl.acm.org/doi/abs/10.1145/3610548.3618203	Sayantan Datta, Carl Marshall, Zhao Dong, Zhengqin Li, Derek Nowrouzezahrai
Efficient Human Motion Reconstruction from Monocular Videos with Physical Consistency Loss	Vision-only motion reconstruction from monocular videos often produces artifacts such as foot sliding and jittering. Existing physics-based methods typically either simplify the problem to focus solely on foot-ground contacts, or they reconstruct full-body contacts within a physics simulator, necessitating the solution of a time-consuming bilevel optimization problem. To overcome these limitations, we present an efficient gradient-based method for reconstructing complex human motions (including highly dynamic and acrobatic movements) with physical constraints. Our approach reformulates human motion dynamics through a differentiable physical consistency loss within an augmented search space that accounts both for contacts and camera alignment. This enables us to transform the motion reconstruction task into a single-level trajectory optimization problem. Experimental results demonstrate that our method can reconstruct complex human motions from real-world videos in minutes, which is substantially faster than previous approaches. Additionally, the reconstructed results show enhanced physical realism compared to existing methods.	https://dl.acm.org/doi/abs/10.1145/3610548.3618169	Lin Cong, Philipp Ruppel, Yizhou Wang, Xiang Pan, Norman Hendrich, Jianwei Zhang
Efficient Hybrid Zoom Using Camera Fusion on Mobile Phones	DSLR cameras can achieve multiple zoom levels via shifting lens distances or swapping lens types. However, these techniques are not possible on smart-phone devices due to space constraints. Most smartphone manufacturers adopt a hybrid zoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T) camera at a high zoom level. To simulate zoom levels between W and T, these systems crop and digitally upsample images from W, leading to significant detail loss. In this paper, we propose an efficient system for hybrid zoom super-resolution on mobile devices, which captures a synchronous pair of W and T shots and leverages machine learning models to align and transfer details from T to W. We further develop an adaptive blending method that accounts for depth-of-field mismatches, scene occlusion, flow uncertainty, and alignment errors. To minimize the domain gap, we design a dual-phone camera rig to capture real-world inputs and ground-truths for supervised training. Our method generates a 12-megapixel image in 500ms on a mobile platform and compares favorably against state-of-the-art methods under extensive evaluation on real-world scenarios.	https://dl.acm.org/doi/abs/10.1145/3618362	Xiaotong Wu, Wei-Sheng Lai, Yichang Shih, Charles Herrmann, Michael Krainin, Deqing Sun, Chia-Kai Liang
Efficient Incremental Potential Contact for Actuated Face Simulation	We present a quasi-static finite element simulator for human face animation. We model the face as an actuated soft body, which can be efficiently simulated using Projective Dynamics (PD). We adopt Incremental Potential Contact (IPC) to handle self-intersection. However, directly integrating IPC into the simulation would impede the high efficiency of the PD solver, since the stiffness matrix in the global step is no longer constant and cannot be pre-factorized. We notice that the actual number of vertices affected by the collision is only a small fraction of the whole model, and by utilizing this fact we effectively decrease the scale of the linear system to be solved. With the proposed optimization method for collision, we achieve high visual fidelity at a relatively low performance overhead.	https://dl.acm.org/doi/abs/10.1145/3610543.3626161	Bo Li, Lingchen Yang, Barbara Solenthaler
Efficient Visualization of Light Pollution for the Night Sky	Artificial light sources make our daily life convenient, but cause a severe problem called We propose a novel system for efficient visualization of light pollution in the night sky. Numerous methods have been proposed for rendering the sky, but most of these focus on rendering of the daytime or the sunset sky where the sun is the only, or dominant light source. For the visualization of the light pollution, however, we must consider many city light sources on the ground, resulting in excessive computational cost. We address this problem by precomputing a set of intensity distributions for the sky illuminated by city light at various locations and with different atmospheric conditions. We apply a principal component analysis and fast Fourier transform to the precomputed distributions, allowing us to efficiently visualize the extent of the light pollution. Using this method, we can achieve one to two orders of magnitudes faster computation compared to a naive approach that simply accumulates the scattered intensity for each viewing ray. Furthermore, the fast computation allows us to interactively solve the inverse problem that determines the city light intensity needed to reduce light pollution. Our system provides the user with both a forward and inverse investigation tool for the study and minimization of light pollution.	https://dl.acm.org/doi/abs/10.1145/3618337	Yoshinori Dobashi, Naoto Ishikawa, Kei Iwasaki
Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views	We present Ego3DPose, a highly accurate binocular egocentric 3D pose reconstruction system. The binocular egocentric setup offers practicality and usefulness in various applications, however, it remains largely under-explored. It has been suffering from low pose estimation accuracy due to viewing distortion, severe self-occlusion, and limited field-of-view of the joints in egocentric 2D images. Here, we notice that two important 3D cues, stereo correspondences, and perspective, contained in the egocentric binocular input are neglected. Current methods heavily rely on 2D image features, implicitly learning 3D information, which introduces biases towards commonly observed motions and leads to low overall accuracy. We observe that they not only fail in challenging occlusion cases but also in estimating visible joint positions. To address these challenges, we propose two novel approaches. First, we design a two-path network architecture with a path that estimates pose per limb independently with its binocular heatmaps. Without full-body information provided, it alleviates bias toward trained full-body distribution. Second, we leverage the egocentric view of body limbs, which exhibits strong perspective variance (e.g., a significantly large-size hand when it is close to the camera). We propose a new perspective-aware representation using trigonometry, enabling the network to estimate the 3D orientation of limbs. Finally, we develop an end-to-end pose reconstruction network that synergizes both techniques. Our comprehensive evaluations demonstrate that Ego3DPose outperforms state-of-the-art models by a pose estimation error (i.e., MPJPE) reduction of 23.1% in the UnrealEgo dataset. Our qualitative results highlight the superiority of our approach across a range of scenarios and challenges.	https://dl.acm.org/doi/abs/10.1145/3610548.3618147	Taeho Kang, Kyungjin Lee, Jinrui Zhang, Youngki Lee
Emotional Speech-Driven Animation with Content-Emotion Disentanglement	To be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE  (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control.	https://dl.acm.org/doi/abs/10.1145/3610548.3618183	Radek Daněček, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael Black, Timo Bolkart
Enhancing Diffusion Models with 3D Perspective Geometry Constraints	While perspective is a well-studied topic in art, it is generally taken for granted in images. However, for the recent wave of high-quality image synthesis methods such as latent diffusion models, perspective accuracy is not an explicit requirement. Since these methods are capable of outputting a wide gamut of possible images, it is difficult for these synthesized images to adhere to the principles of linear perspective. We introduce a novel geometric constraint in the training process of generative models to enforce perspective accuracy. We show that outputs of models trained with this constraint both appear more realistic and improve performance of downstream models trained on generated images. Subjective human trials show that images generated with latent diffusion models trained with our constraint are preferred over images from the Stable Diffusion V2 model 70% of the time. SOTA monocular depth estimation models such as DPT and PixelFormer, fine-tuned on our images, outperform the original models trained on real images by up to 7.03% in RMSE and 19.3% in SqRel on the KITTI test set for zero-shot transfer.	https://dl.acm.org/doi/abs/10.1145/3618389	Rishi Upadhyay, Howard Zhang, Yunhao Ba, Ethan Yang, Blake Gella, Sicheng Jiang, Alex Wong, Achuta Kadambi
Environmental Inversion: A Collection of Artworks that Subvert Methods of Climate Change Action	"This paper details Environmental Inversion, a collection of artworks that propose a critical examination of climate change, waste creation and removal. ""Inversion"" is defined and its relevance to climate change research, followed by detailed descriptions of projects by the author that examine this exploratory integration. Each project consists of an energy source that reverses its intended use. Examples include creating pollution through green energy to contaminating air through filtering hardware powered by fossil fuels. The works act as a critical reminder of attempts to greenify our planet, why they are pursued, and the energy-wasteful methods employed to reach these solutions."	https://dl.acm.org/doi/abs/10.1145/3610591.3616423	Jonah Brucker-Cohen
Erased Murmurs	Erased Murmurs is an interactive installation that brings attention to the disappearing and often overlooked graffiti, focusing on emotionally significant words written in public spaces in Hong Kong. The artwork explores themes of public expression and the lasting impact of the 2019 - 2020 Hong Kong protests. The installation features a book containing a collection of photographs capturing the graffiti in its original state. To mimic the attempts to conceal graffiti with patches of paint, which paradoxically makes the act of covering up stand out, the artist has covered the words in the photos with special ink, rendering them invisible to the naked eye. To reveal the hidden text, visitors must employ infrared reflectography using a handheld infrared light provided. This interactive element of the artwork engages the audience in a process of discovery. The act of revealing these messages creates a sense of connection between the viewers and the anonymous authors of the graffiti, whose voices are momentarily brought back to life. The words hidden behind the special ink are not random but have been carefully selected for their emotional significance. The order of the words is crafted into a cohesive narrative. Through personal confessions, expressions of inner struggle, complaints against society, and messages of consolation and support, the graffiti serves as a window into the emotional landscape of Hong Kong residents in the aftermath of the protests. Erased Murmurs is an examination of the impermanence of public expression. It invites viewers to confront their own emotional responses to these erased messages and to consider the broader implications of silencing voices in public spaces. Ultimately, the installation stands as a tribute to the unbreakable spirit of those who continue to make their voices heard, even in the most challenging circumstances.	https://dl.acm.org/doi/abs/10.1145/3610537.3622952	Miu Ling Lam
Example-Based Sampling with Diffusion Models	Much effort has been put into developing samplers with specific properties, such as producing blue noise, low-discrepancy, lattice or Poisson disk samples. These samplers can be slow if they rely on optimization processes, may rely on a wide range of numerical methods, are not always differentiable. The success of recent diffusion models for image generation suggests that these models could be appropriate for learning how to generate point sets from examples. However, their convolutional nature makes these methods impractical for dealing with scattered data such as point sets. We propose a generic way to produce 2-d point sets imitating existing samplers from observed point sets using a diffusion model. We address the problem of convolutional layers by leveraging neighborhood information from an optimal transport matching to a uniform grid, that allows us to benefit from fast convolutions on grids, and to support the example-based learning of non-uniform sampling patterns. We demonstrate how the differentiability of our approach can be used to optimize point sets to enforce properties.	https://dl.acm.org/doi/abs/10.1145/3610548.3618243	Bastien Doignies, Nicolas Bonneel, David Coeurjolly, Julie Digne, Loïs Paulin, Jean-Claude Iehl, Victor Ostromoukhov
Explorable Mesh Deformation Subspaces from Unstructured 3D Generative Models	Exploring variations of 3D shapes is a time-consuming process in traditional 3D modeling tools. Deep generative models of 3D shapes often feature continuous latent spaces that can, in principle, be used to explore potential variations starting from a set of input shapes; in practice, doing so can be problematic—latent spaces are high dimensional and hard to visualize, contain shapes that are not relevant to the input shapes, and linear paths through them often lead to sub-optimal shape transitions. Furthermore, one would ideally be able to explore variations in the original high-quality meshes used to train the generative model, not its lower-quality output geometry. In this paper, we present a method to explore variations among a given set of landmark shapes by constructing a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model. We first describe how to find a mapping that spans the set of input landmark shapes and exhibits smooth variations between them. We then show how to turn the variations in this subspace into deformation fields, to transfer those variations to high-quality meshes for the landmark shapes. Our results show that our method can produce visually-pleasing and easily-navigable 2D exploration spaces for several different shape categories, especially as compared to prior work on learning deformation spaces for 3D shapes. https://github.com/ArmanMaesumi/generative-mesh-subspaces	https://dl.acm.org/doi/abs/10.1145/3610548.3618192	Arman Maesumi, Paul Guerrero, Noam Aigerman, Vladimir Kim, Matthew Fisher, Siddhartha Chaudhuri, Daniel Ritchie
Exploration and Reflection on Design Research Methods in the design education in China's art academies: The preliminary exploration though World Building practice at China Academy of Art	"Taking as a case study the ""World Building"" course taught by the author in the Interactive Media major at the School of Animation and Games at the China Academy of Art, this article provides an initial analysis of the design education methodologies at the academy. It critiques the prevailing focus within China's design education system on ""Tao"" (philosophy/principles) over ""Qi"" (tools) and ""Art"" over ""Technique"". Additionally, it presents strategic recommendations for future reform in Chinese design education, ensuring it is adaptive to contemporary needs."	https://dl.acm.org/doi/abs/10.1145/3610540.3627013	Bin Shen
Exploring Embodiment and Usability of Autonomous Prosthetic Limbs through Virtual Reality	We propose the utilization of motion capture and immersive virtual reality to explore embodiment and user perception associated with prosthetic limbs. We developed a virtual reality simulation where a user could control an amputated avatar in first-person view with full body motion capture to experimentally investigate how the movement speed of an autonomous prosthetic limb affects its embodiment, usability, competence, warmth, and discomfort. In a within-subjects design experiment, participants performed a reaching task using a virtual prosthetic lower arm that moved at six different speeds in a minimum jerk trajectory. Results showed that extremely fast movements and extremely slow movements equally reduce embodiment, usability, and competence while movements at moderate speeds maximize embodiment and usability while reducing discomfort. Our findings provide insights into developing autonomous prosthetic limbs with higher user satisfaction that take embodiment, user perception, and usability into consideration.	https://dl.acm.org/doi/abs/10.1145/3610542.3626115	Harin Hapuarachchi, Yasuyuki Inoue, Michiteru Kitazaki
Exploring the Eco-Digital: Performative Sensing with Plants and Data	Artistic practices influenced by philosophical understandings that extend 'beyond the human', such as posthuman and post-Anthropocene approaches, partially connect on a qualitative level with ancient understandings of an expanded nature that engulfs both human and nonhuman kin. Preserved in Indigenous epistemes, such as mātauranga Māori (Māori knowledge), this research brings selected aspects of this complex knowledge system into contact with artistic practice generated with networked technologies, to manifest speculative methods of data visualization and signal flow. While there are many different approaches of the eco-digital currently at play, this research looks at practices engaging plants as co-composers. Articulating various situational assemblages that entangle humans, plants, and data, this paper explores theoretical positions that cross-pollinate natural ecology and digital technology. We do this through the prism of two iterative works by the authors: Contact/Sense (2019) and Nga manawataki o te koiora: Biorhythms (2022).	https://dl.acm.org/doi/abs/10.1145/3610591.3616428	Rewa Wright, Simon Howden
Exquisite Corpus	As humans, we regard our bodies through their visual surface components. The interior, when considered at all, is typically only due to medical concern for one's-self - rarely envisioning that of others. While radiological tools have dramatically improved our capacity for noninvasive representation, their use is often confined to the domains of personal health. This work seeks to instead uncover the possibilities they represent to show the full scope of our bodily form. In their obfuscation of the accustomed visual boundary, they remove associations of race and many aspects of gender. To further the dissolution of perceived identity, it excavates our inner sameness through algorithmically merging bodily interiors into 3d human chimeras - hybrid beings existing beyond the possibilities of genetic merger. Through the collection of simple participant biometrics, blended avatars constructed from real patient data are selected to give viewers a bodily representation that extends beyond the surface manifold commonly regarded as the self in both physical and virtual worlds. These avatars expand the representations usually seen within virtual spaces by, rather than existing as a 3d rendered hollow shell - absent the organs necessary for the operating individual, providing a volumetric representation of those elements unnecessary in the virtual space as the participant's character.	https://dl.acm.org/doi/abs/10.1145/3610537.3632938	Kevin Blackistone
Extended Path Space Manifolds for Physically Based Differentiable Rendering	Physically based differentiable rendering has become an increasingly important topic in recent years. A common pipeline computes local color derivatives of light paths or pixels with respect to arbitrary scene parameters, and enables optimizing or recovering the scene parameters through iterative gradient descent by minimizing the difference between rendered and target images. However, existing approaches cannot robustly handle complex illumination effects including reflections, refractions, caustics, shadows, and highlights, especially when the initial and target locations of such illumination effects are not close to each other in the image space. To address this problem, we propose a novel data structure named extended path space manifolds. The manifolds are defined in the combined space of path vertices and scene parameters. By enforcing geometric constraints, the path vertices could be implicitly and uniquely determined by perturbed scene parameters. This enables the manifold to track specific illumination effects and the corresponding paths, i.e., specular paths will still be specular paths after scene parameters are perturbed. Besides, the path derivatives with respect to scene parameters could be computed by solving small linear systems. We further propose a physically based differentiable rendering method built upon the theoretical results of extended path space manifolds. By incorporating the path derivatives computed from the manifolds and an optimal transport based loss function, our method is demonstrated to be more effective and robust than state-of-the-art approaches in inverse rendering applications involving complex illumination effects.	https://dl.acm.org/doi/abs/10.1145/3610548.3618195	Jiankai Xing, Xuejun Hu, Fujun Luan, Ling-Qi Yan, Kun Xu
ExtraSS: A Framework for Joint Spatial Super Sampling and Frame Extrapolation	We introduce ExtraSS, a novel framework that combines spatial super sampling and frame extrapolation to enhance real-time rendering performance. By integrating these techniques, our approach achieves a balance between performance and quality, generating temporally stable and high-quality, high-resolution results. Leveraging lightweight modules on warping and the ExtraSSNet for refinement, we exploit spatial-temporal information, improve rendering sharpness, handle moving shadings accurately, and generate temporally stable results. Computational costs are significantly reduced compared to traditional rendering methods, enabling higher frame rates and alias-free high resolution results. Evaluation using Unreal Engine demonstrates the benefits of our framework over conventional individual spatial or temporal super sampling methods, delivering improved rendering speed and visual quality. With its ability to generate temporally stable high-quality results, our framework creates new possibilities for real-time rendering applications, advancing the boundaries of performance and photo-realistic rendering in various domains.	https://dl.acm.org/doi/abs/10.1145/3610548.3618224	Songyin Wu, Sungye Kim, Zheng Zeng, Deepak Vembar, Sangeeta Jha, Anton Kaplanyan, Ling-Qi Yan
FIRE: Mid-Air Thermo-Tactile Display	We demonstrate an ultrasound haptic display-based mid-air thermo-tactile feedback system. We design a proof-of-concept thermo-tactile feedback system with an open-top chamber, heat modules, and an ultrasound display. Our method involves directing heated airflow toward the focused pressure point produced by the ultrasound display to deliver thermal and tactile cues in mid-air simultaneously. We present this system with three different virtual environments (CampFire, Water Fountain, and Kitchen) to show the rich user experiences of integrating thermal and tactile feedback.	https://dl.acm.org/doi/abs/10.1145/3610541.3614584	Yatharth Singhal, Haokun Wang, Jin Ryong Kim
FLARE: Fast Learning of Animatable and Relightable Mesh Avatars	Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the prefiltered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches.	https://dl.acm.org/doi/abs/10.1145/3618401	Shrisha Bharadwaj, Yufeng Zheng, Otmar Hilliges, Michael J. Black, Victoria Fernandez Abrevaya
Face0: Instantaneously Conditioning a Text-to-Image Model on a Face	We present Face0, a novel way to instantaneously condition a text-to-image generation model on a face without any optimization procedures such as fine-tuning or inversions. We augment a dataset of annotated images with embeddings of the included faces and train an image generation model on the augmented dataset. Once trained, our system is practically identical at inference time to the underlying base model, and is therefore able to generate face-conditioned images in just a couple of seconds. Our method achieves pleasing results, is remarkably simple, extremely fast, and equips the underlying model with new capabilities, like controlling the generated images both via text or via direct manipulation of the input face embeddings. In addition, when using a fixed random vector instead of a face embedding from a user supplied image, our method essentially solves the problem of consistent character generation across images. Finally, our method decouples the model's textual biases from its biases on faces. While requiring further research, we hope that this may help reduce biases in future text-to-image models.	https://dl.acm.org/doi/abs/10.1145/3610548.3618249	Dani Valevski, Danny Lumen, Yossi Matias, Yaniv Leviathan
Fast Remeshing-Free Methods for Complex Cutting and Fracture Simulation	Simulating complex cuts and fractures robustly and accurately benefits a broad spectrum of researchers. This includes rendering realistic and spectacular animations in computer graphics and interactive techniques, as well as conducting material strength analysis in industrial design and mechanical engineering. In this thesis, we develop a graph-based Finite Element Method (FEM) model that reformulates the hyper-elastic strain energy for fracture simulation and thus adds negligible computational overhead over a regular FEM. Our algorithm models fracture on the graph induced in a volumetric mesh with tetrahedral elements. We relabel the edges of the graph using a computed damage variable to initialize and propagate the fracture. Following that, we extend graph-based FEM to simulate dynamic fracture in anisotropic materials. We further enhance this model by developing novel probabilistic damage mechanics for modelling materials with impurities using a random graph-based formulation. We demonstrate how this formulation can be used by artists for directing and controlling fracture. Finally, we combine graph-based FEM with a Galerkin multigrid method to run fracture and cutting simulation at a real-time, interactive rate even for high-resolution meshes.	https://dl.acm.org/doi/abs/10.1145/3623053.3623366	Avirup Mandal
Fast-MSX: Fast Multiple Scattering Approximation	Classical microfacet theory suffers from energy loss on materials with high roughness due to the single bounce assumption of most microfacet models. When roughness is high, there is a large chance of multiple scattering occurring among the microfacets of the surface. Without explicitly modelling for this behaviour, rough surfaces appear darker than they should. To address this issue, we present a novel method to estimate the multiple scattering contribution from a second light bounce. Our method is inspired by Zipin's geometric construction approach, which simplifies the calculation of the light transport inside a V-groove cavity. Our experimental results demonstrate that our method is visually pleasing, physically plausible, and artifact-free compared to recent multiple scattering works. Additionally, the low computational cost makes our model suitable for real-time rendering.	https://dl.acm.org/doi/abs/10.1145/3610548.3618231	Enrique Rosales, Fatemeh Teimury, Joshua Horacsek, Aria Salari, Xuebin Qin, Adi Bar-Lev, Xiaoqiang Zhe, Ligang Liu
Ficusia	The Magical Banyan Tree's Desire That Leads to its Downfall.	https://dl.acm.org/doi/abs/10.1145/3626964.3626998	Ahmad Saropi, Selly Artaty Zega, Lusia Kiroyan
Flow: Connect the movement of Virtual Actor and Audience	Composed of light, movement, and AR, <Flow> is an AR immersive performance in which the virtual actor and the audience create movement together. The virtual actor appears on stage through AR technology, can predict and flexibly respond to the audience's behavior based on AI. The audience interacts with the virtual actor through hand interaction designed based on 'Contact Improvisation'. The movements they form together are visualized on stage and tell a story of memory, while implicitly capturing the flow of relationships between two different worlds: virtual and reality, past and present, existence and non-existent.	https://dl.acm.org/doi/abs/10.1145/3610549.3614610	Jeongmin Lee, Jisu Lee, Hyerim Jung, Taekyung Yoo
Fluid Simulation on Neural Flow Maps	We introduce Neural Flow Maps, a novel simulation method bridging the emerging paradigm of implicit neural representations with fluid simulation based on the theory of flow maps, to achieve state-of-the-art simulation of in-viscid fluid phenomena. We devise a novel hybrid neural field representation, Spatially Sparse Neural Fields (SSNF), which fuses small neural networks with a pyramid of overlapping, multi-resolution, and spatially sparse grids, to compactly represent long-term spatiotemporal velocity fields at high accuracy. With this neural velocity buffer in hand, we compute long-term, bidirectional flow maps and their Jacobians in a mechanistically symmetric manner, to facilitate drastic accuracy improvement over existing solutions. These long-range, bidirectional flow maps enable high advection accuracy with low dissipation, which in turn facilitates high-fidelity incompressible flow simulations that manifest intricate vortical structures. We demonstrate the efficacy of our neural fluid simulation in a variety of challenging simulation scenarios, including leapfrogging vortices, colliding vortices, vortex reconnections, as well as vortex generation from moving obstacles and density differences. Our examples show increased performance over existing methods in terms of energy conservation, visual complexity, adherence to experimental observations, and preservation of detailed vortical structures.	https://dl.acm.org/doi/abs/10.1145/3618392	Yitong Deng, Hong-Xing Yu, Diyang Zhang, Jiajun Wu, Bo Zhu
Flying Over Tourist Attractions: A Novel Augmented Reality Tourism System Using Miniature Dioramas	This paper presents a novel augmented reality tourism system using miniature dioramas. It offers a unique, immersive experience simulating aerial exploration of tourist attractions. Our system employs advanced 3D scanning and tracking for user interaction with attraction features, along with informative voice guides. This fresh approach enhances engagement and learning in diverse contexts.	https://dl.acm.org/doi/abs/10.1145/3610542.3626129	Suwon Lee, Sanghyeon Kim, Seongwon Kim, Hyunwoo Cho, Sang-Min Choi
Focus Range: Production Ray Tracing of Depth of Field	Defocus blur adds realism to computer generated images by modelling the restricted focus range of physical camera lenses. It allows artists to draw the viewer's attention to in-focus regions of the image by separating them from the background. The widely used thin lens model provides a simple approach to achieve this effect, allowing artists to control the frame's appearance through the distance to the focus plane and the aperture size. Our proposed focus range model extends the focus distance to a range. This allows artists to keep characters fully in focus and independently define the out-of-focus blur. We demonstrate how to achieve this robustly in the presence of specular BSDFs, ray-oriented geometry and multiple light bounces. Additionally, we share our practical experience of integrating this model into our production renderer Glimpse.	https://dl.acm.org/doi/abs/10.1145/3610543.3626156	Edoardo Alberto Dominici, Emanuel Schrade, Basile Fraboni, Luke Emrose, Curtis Black
FoodMorph: Changing Food Appearance Towards Less Unhealthy Food Intake	Human dietary experiences are influenced by multiple senses. To promote healthy eating and reduce the consumption of unhealthy food, we have developed FoodMorph, a virtual reality system that immerses users in visually simulated food textures that are inedible. By presenting users with these textures, FoodMorph aims to diminish interest in and intake of unhealthy food while also assessing the dining enjoyment associated with common non-food textures. We found that concrete textures on food tend to lead to the lowest enjoyment score, which potentially effects their dietary habits.	https://dl.acm.org/doi/abs/10.1145/3610542.3626149	Ruoxin Cui, Weijen Chen, Danyang Peng, Kouta Minamizawa, Yun Suen Pai
Footstep Detection for Film Sound Production	In this paper, we presented a footstep detection method, which could assist sound editors in positioning the character's footsteps on the timeline of the film. Based on it, a footstep detection system was designed for film sound production. Considering the characteristics of human motion and the needs of film sound production, our method included two parts: data preprocessing and footstep detection modeling. Experiments on various types of shots showed the good generalization and high accuracy of our method. The application evaluation demonstrated the high efficiency of the system.	https://dl.acm.org/doi/abs/10.1145/3610543.3626171	Xiaojuan Gu, Junliang Chen, Bo Li, Jun Chen
ForceField: Visualizing Intermaterial Interaction through Floor and Depth Sensing	ForceField realizes the measurement of intermaterial contact forces across any surface within a room-scale environment without the need for additional device attachments to bodies or objects. It achieves this by acquiring a 3D model through a depth sensor and capturing ground pressure through a floor sensor, and then reconstructing contact forces between surfaces where no sensors are attached, such as when a hand pushes a desk, or people push against each other, from information on force and geometry. Through this unobtrusive environmental measurement approach, ForceField opens up the possibility of highly physical spatial interactions in domains such as healthcare, sports, and entertainment.	https://dl.acm.org/doi/abs/10.1145/3610541.3614587	Takatoshi Yoshida, Takafumi Watanabe, Hirosuke Asahi, Kashun Rin, Haruki Kozuka, Masaharu Hirose, Masahiko Inami
Fortune	It's all about money! Money, from bills to bitcoins, has no intrinsic value beyond what we've collectively agreed to grant it. However, there's no denying that money governs our lives. FORTUNE offers a modern & pop exploration of the power of money, its psychology and the arbitrary and unequal dimensions of our economic system. FORTUNE is a series of 8 animated documentary shorts in augmented reality, 8 filters for Snapchat & Instagram. 8 people tell how money dictates their lives, how they have to rely on it to survive, or how they think it will bring happiness or freedom. From counterfeiting, to dumpster diving, from selling everything for bitcoin to retiring at age 30: what are we willing to do and give to make a little more money? The series is developed on Snap Lens AR Studio and Meta Spark AR to bring narrative, animated and interactive AR stories to life on social media.	https://dl.acm.org/doi/abs/10.1145/3610549.3614602	Aurelie Leduc, Claire Meinhard
Fresh Memories: The Look	"Bombed houses, schools, hospitals, streets and many others. War in Ukraine from its beginning has taken away thousands of households and lives as well. Kharkiv was one of the towns which was hit in a massive way. In this immersive experience in virtual reality inspired by the famous Marina Abramovich performance ""The Artist Is Present"" you look into the eyes of those who lost their homes or places closely connected with them. What can this simple look awake in us? Will it bring despair, grief or hope?"	https://dl.acm.org/doi/abs/10.1145/3610549.3614617	Ondrej Moravec
From Reconstruction to Generation: State-of-Art Approaches for 3D Visualization	Our course provides an essential overview of 3D reconstruction and generative AI-based 3D generation approaches that have transformed the field. The course provides an understanding of the fundamentals of photogrammetry, neural radiance fields-based techniques such as Instant NeRF, and generative AI-based 3D generation such as text-to-3D, with a focus on reflecting on these approaches, their applications in 3D world building, and how participants can integrate them into their research. The course firstly introduces the core concepts of photogrammetry, through which participants can understand its importance in reconstructing accurate 3D models. Through case studies, participants will reflect on the applications of photogrammetry in various domains, such as cultural heritage preservation and VR experiences, and can view some examples themselves. We then delve into a gentle introduction to neural rendering with a broad overview of NVIDIA Instant NeRF, a state-of-the-art technique for real-time 3D reconstruction from multiple images. Participants will have the opportunity to watch several demonstrations of InstantNGP and discover its applications in interactive 3D world building. Furthermore, participants will be introduced to emerging generative AI approaches that simplify 3D content generation. In particular, we explore the use of text-to-3D techniques, which leverage natural language descriptions to automatically generate 3D models. We will also examine the impact of generative AI techniques on the creation of realistic and immersive 3D environments. By the end of the course, participants will have a solid understanding of the basics of two salient 3D reconstruction methods, as well as other generative AI techniques for 3D content. We also hope to enable participants to critically evaluate and harness the potential of these approaches in 3D world building.	https://dl.acm.org/doi/abs/10.1145/3610538.3614647	Pallavi Mohan, June Kim
From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans	"Great progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses. We build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to ""upgrade"" existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained and more realistic model of human articulation. The model, code, and data are available for research at https://skel.is.tue.mpg.de."	https://dl.acm.org/doi/abs/10.1145/3618381	Marilyn Keller, Keenon Werling, Soyong Shin, Scott Delp, Sergi Pujades, C. Karen Liu, Michael J. Black
From fledglings to wise owls:: Nurturing talent to new heights.	For over 32 years Animal Logic has played a major role in the growth of the Animation and Visual Effects industry in Australia and globally. In this talk we will look at the breadth of initiatives, from hosting work experience student programs for high schools, through partnering with universities and hosting trainees. We explore how the company's values have been expressed through these, demonstrating how these have benefited the company and the greater community.	https://dl.acm.org/doi/abs/10.1145/3610540.3627014	Mark Flanagan
Funeral at Nine	In a small town, a funeral is held after the death of a gardener. Three brothers find themselves lost in their imaginations on the day of the burial.	https://dl.acm.org/doi/abs/10.1145/3626964.3626983	Mamadou Barry, Rodrigo Veras, Ziyu Wang, Junhao Xiang, Wang Yu, Lifeng Zhou
FurAir: Non-contact Presentation of Soft Fur Texture by Psuedo-haptics and Mid-air Ultrasound Haptic Feedback	We propose a non-contact method to present the tactile sensation of soft fur texture using ultrasound haptic feedback and pseudo-haptics. By responsively adjusting haptic and visual feedback according to user interaction, our approach effectively simulates a realistic fur stroking experience.	https://dl.acm.org/doi/abs/10.1145/3610541.3614585	Juro Hosoi, Du Jin, Yuki Ban, Shin'Ichi Warisawa
FuseSR: Super Resolution for Real-time Rendering through Efficient Multi-resolution Fusion	The workload of real-time rendering is steeply increasing as the demand for high resolution, high refresh rates, and high realism rises, overwhelming most graphics cards. To mitigate this problem, one of the most popular solutions is to render images at a low resolution to reduce rendering overhead, and then manage to accurately upsample the low-resolution rendered image to the target resolution, a.k.a. super-resolution techniques. Most existing methods focus on exploiting information from low-resolution inputs, such as historical frames. The absence of high frequency details in those LR inputs makes them hard to recover fine details in their high-resolution predictions. In this paper, we propose an efficient and effective super-resolution method that predicts high-quality upsampled reconstructions utilizing low-cost high-resolution auxiliary G-Buffers as additional input. With LR images and HR G-buffers as input, the network requires to align and fuse features at multi resolution levels. We introduce an efficient and effective H-Net architecture to solve this problem and significantly reduce rendering overhead without noticeable quality deterioration. Experiments show that our method is able to produce temporally consistent reconstructions in 4 × 4 and even challenging 8 × 8 upsampling cases at 4K resolution with real-time performance, with substantially improved quality and significant performance boost compared to existing works.Project page: https://isaac-paradox.github.io/FuseSR/	https://dl.acm.org/doi/abs/10.1145/3610548.3618209	Zhihua Zhong, Jingsen Zhu, Yuxin Dai, Chuankun Zheng, Guanlin Chen, Yuchi Huo, Hujun Bao, Rui Wang
Fusing Monocular Images and Sparse IMU Signals for Real-time Human Motion Capture	Either RGB images or inertial signals have been used for the task of motion capture (mocap), but combining them together is a new and interesting topic. We believe that the combination is complementary and able to solve the inherent difficulties of using one modality input, including occlusions, extreme lighting/texture, and out-of-view for visual mocap and global drifts for inertial mocap. To this end, we propose a method that fuses monocular images and sparse IMUs for real-time human motion capture. Our method contains a dual coordinate strategy to fully explore the IMU signals with different goals in motion capture. To be specific, besides one branch transforming the IMU signals to the camera coordinate system to combine with the image information, there is another branch to learn from the IMU signals in the body root coordinate system to better estimate body poses. Furthermore, a hidden state feedback mechanism is proposed for both two branches to compensate for their own drawbacks in extreme input cases. Thus our method can easily switch between the two kinds of signals or combine them in different cases to achieve a robust mocap. Quantitative and qualitative results demonstrate that by delicately designing the fusion method, our technique significantly outperforms the state-of-the-art vision, IMU, and combined methods on both global orientation and local pose estimation. Our codes are available for research at https://shaohua-pan.github.io/robustcap-page/.	https://dl.acm.org/doi/abs/10.1145/3610548.3618145	Shaohua Pan, Qi Ma, Xinyu Yi, Weifeng Hu, Xiong Wang, Xingkang Zhou, Jijunnan Li, Feng Xu
Fusion: Landscape and Beyond 2.0: An interactive AI generated art installation	"""Fusion: Landscape and Beyond 2.0"" (2023) is an interactive art installation harnessing the potential of AI to redefine our relationship with urban and natural landscapes. Central to the concept of the installation is the synthetic memory, which dynamically adapts and responds to myriads of instructions, and in turn, influences our understanding of the environment. The installation offers an immersive experience where viewers play an active role. As participants traverse the exhibit, their movements act as triggers, instigating a real-time transformation of the AI-generated landscape. This interactivity reveals layers of cityscapes and landscapes, serving as a visualization of AI's evolving memory and its interpretation of our environment. The aesthetics of traditional Chinese landscape painting are deliberately incorporated to reinforce our discourse on ecological balance. The Chinese philosophies of nature entice us to embrace a worldview that deeply values the harmonious coexistence of humanity and the natural world. The relationship being sought is one characterized by compatibility, participation, and interconnectedness. Using the traditional Chinese brushwork technique, Cun, we have devised a model that fuses AI's textual interpretation of city aesthetics with traditional brushstrokes. This integration results in a unique visual calligram that blurs the boundaries between physical and digital experiences. The workflow of this project involves the use of a self-fine-tuned Stable Diffusion model and real-time visualization system. The system continuously synthesizes Chinese city images that echo our real-world urban experiences. These city images then metamorphose into an artificial nature imbued with the aesthetics of Chinese landscape painting, creating a visual poem or calligram. In the end, the installation not only blurs the lines between AI and human cognition but also emphasizes the symbiotic relationship between humans, AI, and the world we inhabit. This project harnesses AI to echo our collective consciousness, weaving a narrative of co-existence within our shared world."	https://dl.acm.org/doi/abs/10.1145/3610537.3622954	Mingyong Cheng, Xuexi Dang, Zetao Yu
GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields	Neural Radiance Fields (NeRF) have shown impressive novel view synthesis results; nonetheless, even thorough recordings yield imperfections in reconstructions, for instance due to poorly observed areas or minor lighting changes. Our goal is to mitigate these imperfections from various sources with a joint solution: we take advantage of the ability of generative adversarial networks (GANs) to produce realistic images and use them to enhance realism in 3D scene reconstruction with NeRFs. To this end, we learn the patch distribution of a scene using an adversarial discriminator, which provides feedback to the radiance field reconstruction, thus improving realism in a 3D-consistent fashion. Thereby, rendering artifacts are repaired directly in the underlying 3D representation by imposing multi-view path rendering constraints. In addition, we condition a generator with multi-resolution NeRF renderings which is adversarially trained to further improve rendering quality. We demonstrate that our approach significantly improves rendering quality, e.g., nearly halving LPIPS scores compared to Nerfacto while at the same time improving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.	https://dl.acm.org/doi/abs/10.1145/3618402	Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Niessner
GPU Programming Primitives for Computer Graphics	Various parallel algorithms can be decomposed into programming primitives that share similar patterns. This course focuses on studying these programming primitives and their applicability in computer graphics, specifically in the context of massively parallel processing on GPUs. The course begins by establishing a theoretical foundation, followed by practical examples and real-world applications. We explain two pivotal algorithms: parallel reduction and parallel prefix scan in detail, discussing their variants and different implementations. Afterward, we provide a collection of more advanced techniques and tricks applicable across various domains. At the end of the course, we also briefly discuss code optimization.	https://dl.acm.org/doi/abs/10.1145/3610538.3614632	Daniel Meister, Atsushi Yoshimura, Chih-Chen Kao
GarmentCode: Programming Parametric Sewing Patterns	Garment modeling is an essential task of the global apparel industry and a core part of digital human modeling. Realistic representation of garments with valid sewing patterns is key to their accurate digital simulation and eventual fabrication. However, little-to-no computational tools provide support for bridging the gap between high-level construction goals and low-level editing of pattern geometry, e.g., combining or switching garment elements, semantic editing, or design exploration that maintains the validity of a sewing pattern. We suggest the first DSL for garment modeling - GarmentCode - that applies principles of object-oriented programming to garment construction and allows designing sewing patterns in a hierarchical, component-oriented manner. The programming-based paradigm naturally provides unique advantages of component abstraction, algorithmic manipulation, and free-form design parametrization. We additionally support the construction process by automating typical low-level tasks like placing a dart at a desired location. In our prototype garment configurator, users can manipulate meaningful design parameters and body measurements, while the construction of pattern geometry is handled by garment programs implemented with GarmentCode. Our configurator enables the free exploration of rich design spaces and the creation of garments using interchangeable, parameterized components. We showcase our approach by producing a variety of garment designs and retargeting them to different body shapes using our configurator. The library and garment configurator are available at https://github.com/maria-korosteleva/GarmentCode.	https://dl.acm.org/doi/abs/10.1145/3618351	Maria Korosteleva, Olga Sorkine-Hornung
General Software Platform for Designing and Developing of Augmented Reality Task Support Systems	Augmented reality (AR) task support systems have not yet been widely spread in actual companies despite the fact that many researchers have worked tirelessly to validate their effectiveness for various industrial tasks. One of the reasons for this is the excessive development cost of a practical AR system. Development still strongly depends on outstanding developers' skills and trial-and-error. To solve this problem, we propose to separate an AR task support system into a reusable software part (including basic functions and its user interfaces) and task support contents that differ depending on each task. We have developed a general software platform and an original file format for describing task support contents based on analyzing various tasks. In addition, we have also proposed a web tool that enables easy design of AR task support content even for those without AR development experience. The output of the tool, an XML file written in the format, is loaded into the software platform to enable operators to use the AR task support system. We demonstrate a series of steps from the creation of task support content to the use of the AR task support system.	https://dl.acm.org/doi/abs/10.1145/3610549.3614599	Soshiro Ueda, Keishi Tainaka, Yiming Shen, Shuntaro Ueda, Konstantin Kulik, Yuichiro Fujimoto, Taishi Sawabe, Masayuki Kanbara, Hirokazu Kato
GeoLatent: A Geometric Approach to Latent Space Design for Deformable Shape Generators	We study how to optimize the latent space of neural shape generators that map latent codes to 3D deformable shapes. The key focus is to look at a deformable shape generator from a differential geometry perspective. We define a Riemannian metric based on as-rigid-as-possible and as-conformal-as-possible deformation energies. Under this metric, we study two desired properties of the latent space: 1) straight-line interpolations in latent codes follow geodesic curves; 2) latent codes disentangle pose and shape variations at different scales. Strictly enforcing the geometric interpolation property, however, only applies if the metric matrix is a constant. We show how to achieve this property approximately by enforcing that geodesic interpolations are axis-aligned, i.e., interpolations along coordinate axis follow geodesic curves. In addition, we introduce a novel approach that decouples pose and shape variations via generalized eigendecomposition. We also study efficient regularization terms for learning deformable shape generators, e.g., that promote smooth interpolations. Experimental results on benchmark datasets show that our approach leads to interpretable latent codes, improves the generalizability of synthetic shapes, and enhances performance in geodesic interpolation and geodesic shooting.	https://dl.acm.org/doi/abs/10.1145/3618371	Haitao Yang, Bo Sun, Liyan Chen, Amy Pavel, Qixing Huang
Geomart-ut7: Encountering Geometric Patterns in Media Arts	Geometric patterns sometimes appear to reflect an order we observe in nature and sometimes as abstract results of an intuitive process that emerges with different techniques applied. The geometric patterns obtained with the help of a simple compass and ruler reveal a prosperous world with their complex structures that appear at different levels in the visual content they offer us and their perfect layout. In the art of geometry, which takes its inspiration from deep abstractions, we encounter some mysterious bridges that are tried to be built between the representation and the imagined. When these bridges are skillfully built, they have a simple structure that allows us to look through a window that leads to absoluteness, repetition, eternity, simplicity, complexity, order, and chaos. In this series of works titled Geomart-ut7, I try to present a transformation where simple geometric forms can gain movement and turn into complex structures in a world of perception where meaning takes its breath away from expression. I wish to make visible the connections between the past's geometric art and the present's generative art, shrouded in mist.	https://dl.acm.org/doi/abs/10.1145/3610537.3622942	Selcuk Artut
Geometry Aware Texturing	In this work, we propose a novel approach to texture generation, making use of recent advancements in Latent Diffusion models, [Rombach et al. 2022] unlocked by [Zhang and Agrawala 2023], introducing control inputs to generation pipelines via ControlNet. We find that a special condition, where the mesh is encoded into UV space, can serve as a control input, producing textures that are geometrically and visually coherent, and of high quality. Using this approach, we are able to generate a unique look guided by text for existing meshes in a matter of seconds.	https://dl.acm.org/doi/abs/10.1145/3610542.3626152	Evgeniia Cheskidova, Aleksandr Arganaidi, Daniel-Ionut Rancea, Olaf Haag
Ghost in the Machine : Discourses with AI	"AI systems analyze vast amounts of data, uncover patterns, and make decisions - emulating a semblance of intelligence despite lacking qualia and embodiment that form the basis of the human condition. In this paper, we expound on ""Ghost in the Machine"", an interactive installation that delves into our pervasive tendency to anthropomorphize AI, ascribing human-like qualities, intentions, and even consciousness. Participants engage in dialogue with the AI as it collaboratively materializes the AI's thoughts in moving image and generative sound. The installation attempts to forge embodiment for an amorphous AI, revealing errors in its comprehension, represented by the metaphor of hallucinations."	https://dl.acm.org/doi/abs/10.1145/3610591.3616429	Purav Bhardwaj, Misha Sra
Go Fishboy	A respected chef from a lineage of sushi makers attempts to connect with his son through the shared knowledge of the family trade. Tensions arise as he begins to notice a strange pattern of behaviour in the young boy.	https://dl.acm.org/doi/abs/10.1145/3626964.3626984	Chiayu Liu, Sebastian Doringer, Andrey Kolesov, Lan Zhou, Zhen Tian, Denise Cirone
GroomGen: A High-Quality Generative Hair Model Using Hierarchical Latent Representations	Despite recent successes in hair acquisition that fits a high-dimensional hair model to a specific input subject, generative hair models, which establish general embedding spaces for encoding, editing, and sampling diverse hairstyles, are way less explored. In this paper, we present , the first generative model designed for hair geometry composed of highly-detailed dense strands. Our approach is motivated by two key ideas. First, we construct covering both individual strands and hairstyles. The latent spaces are compact, expressive, and well-constrained for high-quality and diverse sampling. Second, we adopt a that parameterizes a complete hair model to three levels: single strands, sparse guide hairs, and complete dense hairs. This representation is critical to the compactness of latent spaces, the robustness of training, and the efficiency of inference. Based on this hierarchical latent representation, our proposed pipeline consists of a and a that encode an individual strand and a set of guide hairs to their respective latent spaces, and a that populates sparse guide hairs to a dense hair model. not only enables novel hairstyle sampling and plausible hairstyle interpolation, but also supports interactive editing of complex hairstyles, or can serve as strong data-driven prior for hairstyle reconstruction from images. We demonstrate the superiority of our approach with qualitative examples of diverse sampled hairstyles and quantitative evaluation of generation quality regarding every single component and the entire pipeline.	https://dl.acm.org/doi/abs/10.1145/3618309	Yuxiao Zhou, Menglei Chai, Alessandro Pepe, Markus Gross, Thabo Beeler
GroundLink: A Dataset Unifying Human Body Movement and Ground Reaction Dynamics	The physical plausibility of human motions is vital to various applications in fields including but not limited to graphics, animation, robotics, vision, biomechanics, and sports science. While fully simulating human motions with physics is an extreme challenge, we hypothesize that we can treat this complexity as a black box in a data-driven manner if we focus on the ground contact, and have sufficient observations of physics and human activities in the real world. To prove our hypothesis, we present GroundLink, a unified dataset comprised of captured ground reaction force (GRF) and center of pressure (CoP) synchronized to standard kinematic motion captures. GRF and CoP of GroundLink are not simulated but captured at high temporal resolution using force platforms embedded in the ground for uncompromising measurement accuracy. This dataset contains 368 processed motion trials (∼ 1.59M recorded frames) with 19 different movements including locomotion and weight-shifting actions such as tennis swings to signify the importance of capturing physics paired with kinematics. GroundLinkNet, our benchmark neural network model trained with GroundLink, supports our hypothesis by predicting GRFs and CoPs accurately and plausibly on unseen motions from various sources. The dataset, code, and benchmark models are made public for further research on various downstream tasks leveraging the rich physics information at https://csr.bu.edu/groundlink/.	https://dl.acm.org/doi/abs/10.1145/3610548.3618247	Xingjian Han, Ben Senderling, Stanley To, Deepak Kumar, Emily Whiting, Jun Saito
Hair Tubes: Stylized Hair from Polygonal Meshes of Arbitrary Topology	In this paper, we describe a fast, topology-independent method to generate bundles of hair from a mesh defining the outward shape of the hair. This allows artists to focus on the outward appearance and create stylized painterly hairstyles. We describe a novel approach to parameterize a hair mesh using ideas from discrete differential geometry and offer simple controls to distribute hair within the volume of the mesh. We present real-world production examples of various hairstyles created using our proposed method.	https://dl.acm.org/doi/abs/10.1145/3610543.3626157	Soorya Narayan Jayaraman Mohan
Hair Universe: Meorikarak Woojoo	A Grieving Boy Gets Swept Into the Underneath the Bathtub	https://dl.acm.org/doi/abs/10.1145/3626964.3626974	Jinuk Choi, Hanjin Cho
Hand Pose Estimation with Mems-Ultrasonic Sensors	Hand tracking is an important aspect of human-computer interaction and has a wide range of applications in extended reality devices. However, current hand motion capture methods suffer from various limitations. For instance, visual hand pose estimation is susceptible to self-occlusion and changes in lighting conditions, while IMU-based tracking gloves experience significant drift and are not resistant to external magnetic field interference. To address these issues, we propose a novel and low-cost hand-tracking glove that utilizes several MEMS-ultrasonic sensors attached to the fingers, to measure the distance matrix among the sensors. Our lightweight deep network then reconstructs the hand pose from the distance matrix. Our experimental results demonstrate that this approach is both accurate, size-agnostic, and robust to external interference. We also show the design logic for the sensor selection, sensor configurations, circuit diagram, as well as model architecture.	https://dl.acm.org/doi/abs/10.1145/3610548.3618202	Qiang Zhang, Yuanqiao Lin, Yubin Lin, Szymon Rusinkiewicz
HangerBody: a Haptic Device Using Haptic Illusion for Multiple Parts of Body	The hanger reflex is a phenomenon in which a strong sense of rotational force is perceived when one wares a wire hanger on his/her head. This phenomenon is caused by lateral skin stretch generated by pressure from the wire hanger on the head, and similar phenomena have been observed in the wrist, ankle, knee and elbow. However, the hanger reflex has been applied to only one part of the body, but not to multiple parts of the body simultaneously. In this study, we propose the HangerBody system, which applies the hanger reflex to multiple parts of body simultaneously in order to expand the range of applications. In this paper, we introduce the system configuration and some examples of applying the system to VR games, sports training, rehabilitation, and remote assistance.	https://dl.acm.org/doi/abs/10.1145/3610541.3614586	Takuto Nakamura, Hideaki Kuzuoka
HapReel: A Racket-shaped Haptic Display Controller for Presenting Vibrotactile and Force Feedback through Fingertip Deformation	Racket sports in virtual spaces are rapidly gaining popularity. However, to approximate the real-world experience, particularly when hitting the ball, tactile feedback is crucial. Current feedback is primarily through vibrations and audio, and providing rich tactile feedback typically requires large, heavy devices. This research proposes a method to realize more realistic tactile feedback by presenting force sensations through fingertip deformation using a small device. This novel device has the potential to enable immersive experiences in VR racket sports and aid in the training for racket sports.	https://dl.acm.org/doi/abs/10.1145/3610541.3614583	Daichi Inoue, Kazuki Nishimoto, Takuto Nakamura, Koki Fukuda, Takuji Narumi
Haptic Hongi - Reiterated	Haptic Hongi – Reiterated uses Augmented Reality (AR) to recreate meeting an indigenous person on their own terms, and thereby reimagine first encounters that took place during the pre-colonisation era in Aotearoa New Zealand. These encounters were often a-symmetrical affairs, freighted with troubled power dynamics and violence, so there is a need to rebalance, restore and reconnect in a playful way. In the experience, AR technology is used to place a virtual woman in front of the user, and then haptic actuators are used to enable the user to feel a hongi, the traditional Māori greeting involving pressing of noses. In this way, AR technologies enable people to communicate across time and space so that they may feel as though they have met.	https://dl.acm.org/doi/abs/10.1145/3610549.3614618	Mairi Gunn, Prasanth Sasikumar, Sachith Muthukumarana, Tania Remana, Mark Billinghurst
Head in the Clouds	A quirky student outwits odds to stay in the clouds.	https://dl.acm.org/doi/abs/10.1145/3626964.3626972	Kaylee Tian Lin Tan
Healing Horizons: Adaptive VR for Traumatic Brain Injury Rehabilitation	Healing Horizons presents a novel, bio-adaptive Virtual Reality (VR) approach to Traumatic Brain Injury (TBI) rehabilitation. Traditional therapies often fall short, prompting the need for a more personalised, dynamic intervention. Healing Horizons meets this demand by combining real-time physiological data with VR technology, facilitating effective recovery. The system uses the novel Galea VR head-mounted display (HMD) with integrated physiological sensors to continuously monitor brain activity, skin responses, muscle movements, and heart rate variability. These real-time bio-signals predict the user's cognitive fatigue, a common challenge in TBI patients. The VR environment then adapts dynamically to these physiological cues, ensuring the therapeutic experience is optimised for the individual's current cognitive state. This personalised approach helps develop resilience and social skills while addressing cognitive fatigue within a realistic yet controlled setting. Our demo showcases the potential of VR HMDs with physiological sensors to be used for bio-responsive virtual rehabilitation.	https://dl.acm.org/doi/abs/10.1145/3610549.3614600	Kunal Gupta, Zhuang Chang, Tamil Selvan Gunasekaran, Yuewei Zhang, Joanne Nunnerley, Marcus King, Peta Murphy, Philip Pitts, Conor Russomanno, Mark Billinghurst
High Density Ratio Multi-Fluid Simulation with Peridynamics	Multiple fluid simulation has raised wide research interest in recent years. Despite the impressive successes of current works, simulation of scenes containing mixing or unmixing of high-density-ratio phases using particle-based discretizations still remains a challenging task. In this paper, we propose a peridynamic mixture-model theory that stably handles high-density-ratio multi-fluid simulations. With assistance of novel scalar-valued volume flow states, a particle based discretization scheme is proposed to calculate all the terms in the multi-phase Navier-Stokes equations in an integral form, We also design a novel mass updating strategy for enhancing phase mass conservation and reducing particle volume variations under high density ratio settings in multi-fluid simulations. As a result, we achieve significantly stabler simulations in mixture-model multi-fluid simulations involving mixing and unmixing of high density ratio phases. Various experiments and comparisons demonstrate the effectiveness of our approach.	https://dl.acm.org/doi/abs/10.1145/3618347	Han Yan, Bo Ren
High-Fidelity and Real-Time Novel View Synthesis for Dynamic Scenes	This paper aims to tackle the challenge of dynamic view synthesis from multi-view videos. The key observation is that while previous grid-based methods offer consistent rendering, they fall short in capturing appearance details of a complex dynamic scene, a domain where multi-view image-based rendering methods demonstrate the opposite properties. To combine the best of two worlds, we introduce Im4D, a hybrid scene representation that consists of a grid-based geometry representation and a multi-view image-based appearance representation. Specifically, the dynamic geometry is encoded as a 4D density function composed of spatiotemporal feature planes and a small MLP network, which globally models the scene structure and facilitates the rendering consistency. We represent the scene appearance by the original multi-view videos and a network that learns to predict the color of a 3D point from image features, instead of memorizing detailed appearance totally with networks, thereby naturally making the learning of networks easier. Our method is evaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap, NHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D exhibits state-of-the-art performance in rendering quality and can be trained efficiently, while realizing real-time rendering with a speed of 79.8 FPS for 512x512 images, on a single RTX 3090 GPU. The code is available at https://zju3dv.github.io/im4d.	https://dl.acm.org/doi/abs/10.1145/3610548.3618142	Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, Xiaowei Zhou
High-Order Moment-Encoded Kinetic Simulation of Turbulent Flows	Kinetic solvers for incompressible fluid simulation were designed to run efficiently on massively parallel architectures such as GPUs. While these lattice Boltzmann solvers have recently proven much faster and more accurate than the macroscopic Navier-Stokes-based solvers traditionally used in graphics, it systematically comes at the price of a very large memory requirement: a mesoscopic discretization of statistical mechanics requires over an order of magnitude more variables per grid node than most fluid solvers in graphics. In order to open up kinetic simulation to gaming and simulation software packages on commodity hardware, we propose a HighOrder Moment-Encoded Lattice-Boltzmann-Method solver which we coined HOME-LBM, requiring only the storage of a few moments per grid node, with little to no loss of accuracy in the typical simulation scenarios encountered in graphics. We show that our lightweight and lightspeed fluid solver requires three times less memory and runs ten times faster than state-of-the-art kinetic solvers, for a nearly-identical visual output.	https://dl.acm.org/doi/abs/10.1145/3618341	Wei Li, Tongtong Wang, Zherong Pan, Xifeng Gao, Kui Wu, Mathieu Desbrun
High-quality Color-animated CGH Using a Motor-driven Photomask	We propose a novel display system for a color-animated computer-generated hologram (CGH) with the world's highest level of image quality, screen size, and viewing angle. The conventional method uses a static CGH and pattern illumination to achieve color-animated CGH with large screens and wide viewing angles, but non-illuminated areas result in degraded image quality. Our system solves this problem by using a photomask with a motorized stage, which enables the projection of much finer patterns than with the conventional method. We demonstrated that our method achieves a significant quality improvement compared to the conventional method by developing a color animation prototype with 4 reconstructed images.	https://dl.acm.org/doi/abs/10.1145/3610543.3626182	Ryota Koiso, Tatsuya Kobayashi, Keisuke Nonaka, Kyoji Matsushima
Hitchhiking Hands: Remote Interaction by Switching Multiple Hand Avatars with Gaze	Virtual reality (VR) and augmented reality (AR) with eye tracking and hand tracking are widely used in entertainment, gaming, design, and training. However, most VR and AR interaction methods are limited in their interactable range and do not fully support the direct manipulation of VR and AR objects. This paper proposes a novel technique called Hitchhiking Hands, which allows the user to switch multiple hand avatars by staring them, and enables natural and direct interaction with VR/AR objects ranging from nearby objects to remote ones.	https://dl.acm.org/doi/abs/10.1145/3610541.3614580	Reigo Ban, Keigo Matsumoto, Takuji Narumi
Holographic Near-eye Display with Real-time Embedded Rendering	We present a wearable full-color holographic augmented reality headset with binocular vision support and real-time embedded hologram calculation. Contrarily to most previously proposed prototypes, our headset employs high-speed amplitude-only microdisplays and embeds a compact and lightweight electronic board to drive and synchronize the microdisplays and light source engines. In addition, to enable a standalone usage of the headset, we developed a real-time hologram rendering engine capable of computing full-color binocular holograms at over 35 frames per second on a NVIDIA Jetson AGX Orin embedded platform. Finally, we provide a comparison of the efficiency of laser diodes and superluminescent diodes for the reduction of speckle noise, which greatly affects the reconstructed image's quality. Experimental results show that our prototype enables full-color holographic images to be reconstructed with accurate focus cues and reduced speckle noise in real-time.	https://dl.acm.org/doi/abs/10.1145/3610548.3618179	Antonin Gilles, Pierre Le Gargasson, Grégory Hocquet, Patrick Gioia
How To Get Your Man Pregnant	After 10 failure IVF treatment, the couple tries male pregnancy.	https://dl.acm.org/doi/abs/10.1145/3626964.3626968	Gyeongmu Noh, Jae-Ok Park
HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image	3D content creation from a single image is a long-standing yet highly desirable task. Recent advances introduce 2D diffusion priors, yielding reasonable results. However, existing methods are not hyper-realistic enough for post-generation usage, as users cannot view, render and edit the resulting 3D content from a full range. To address these challenges, we introduce HyperDreamer with several key designs and appealing properties: 1) Full-range viewable: 360° mesh modeling with high-resolution textures enables the creation of visually compelling 3D models from a full range of observation points. 2) Full-range renderable: Fine-grained semantic segmentation and data-driven priors are incorporated as guidance to learn reasonable albedo, roughness, and specular properties of the materials, enabling semantic-aware arbitrary material estimation. 3) Full-range editable: For a generated model or their own data, users can interactively select any region via a few clicks and efficiently edit the texture with text-based guidance. Extensive experiments demonstrate the effectiveness of HyperDreamer in modeling region-aware materials with high-resolution textures and enabling user-friendly editing. We believe that HyperDreamer holds promise for advancing 3D content creation and finding applications in various domains.	https://dl.acm.org/doi/abs/10.1145/3610548.3618168	Tong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xingang Pan, Jiaqi Wang, Dahua Lin, Ziwei Liu
Ice Merchants	Every day, a father and his son jump with a parachute from their vertiginous cold house, attached to a cliff, to go to the village on the ground, far away where they sell the ice they produce daily.	https://dl.acm.org/doi/abs/10.1145/3626964.3627001	João Gonzalez, Bruno Caetano, Michaël Proença
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers	Scalable Vector Graphics (SVG) is a popular vector image format that offers good support for interactivity and animation. Despite its appealing characteristics, creating custom SVG content can be challenging for users due to the steep learning curve required to understand SVG grammars or get familiar with professional editing software. Recent advancements in text-to-image generation have inspired researchers to explore vector graphics synthesis using either image-based methods (i.e., text → raster image → vector graphics) combining text-to-image generation models with image vectorization, or language-based methods (i.e., text → vector graphics script) through pretrained large language models. Nevertheless, these methods suffer from limitations in terms of generation quality, diversity, and flexibility. In this paper, we introduce IconShop, a text-guided vector icon synthesis method using autoregressive transformers. The key to success of our approach is to sequentialize and tokenize SVG paths (and textual descriptions as guidance) into a uniquely decodable token sequence. With that, we are able to exploit the sequence learning power of autoregressive transformers, while enabling both unconditional and text-conditioned icon synthesis. Through standard training to predict the next token on a large-scale vector icon dataset accompanied by textural descriptions, the proposed IconShop consistently exhibits better icon synthesis capability than existing image-based and language-based methods both quantitatively (using the FID and CLIP scores) and qualitatively (through formal subjective user studies). Meanwhile, we observe a dramatic improvement in generation diversity, which is validated by the objective Uniqueness and Novelty measures. More importantly, we demonstrate the flexibility of IconShop with multiple novel icon synthesis tasks, including icon editing, icon interpolation, icon semantic combination, and icon design auto-suggestion.	https://dl.acm.org/doi/abs/10.1145/3618364	Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao
Ignis: Eulerian Fluid Simulation and Rendering at VR Frame Rates	We present Ignis, a GPU Eulerian fluid solver capable of simulating and rendering at VR resolutions and refresh rates. Core to our approach are an approximate shadowing technique and an adaptive dithering technique, which allow us to cheaply render fully lit volumes at high resolutions and under a range of visibility conditions. We discuss the design decisions which enable our solver to run at interactive rates.	https://dl.acm.org/doi/abs/10.1145/3610542.3626120	Charlie Shenton
Infinite Colours	"""Infinite Colours"" brings 2,499 videogame titles into a slow canvas of accumulative light. Each game adds a unique shape and colour onto the canvas and plays a unique string of notes. Over 8 hours, the canvas will be filled with infinite colours to celebrate LGBTQIA+ independent videogames. History has always been queer. Through this generative visual and sound work, we aim to demonstrate the collective activism, movement, and creative expressions that queer folks are making to be visible, heard, and to say that we are here. But queer movement does not happen overnight; queer resistance is accumulative and built over generations of selfsacrifice and self-acceptance. The multitude intersectionality of the unruly times slowly bleeds colour into the world, blends motion into the landscape, and accumulatively becomes a canvas of evermoving colourful light."	https://dl.acm.org/doi/abs/10.1145/3610537.3622958	Xavier Ho, Stephen Krol
Inovis: Instant Novel-View Synthesis	Novel-view synthesis is an ill-posed problem in that it requires inference of previously unseen information. Recently, reviving the traditional field of image-based rendering, neural methods proved particularly suitable for this interpolation/extrapolation task; however, they often require a-priori scene-completeness or costly preprocessing steps and generally suffer from long (scene-specific) training times. Our work draws from recent progress in neural spatio-temporal supersampling to enhance a state-of-the-art neural renderer's ability to infer novel-view information at inference time. We adapt a supersampling architecture [Xiao et al. 2020], which resamples previously rendered frames, to instead recombine nearby camera images in a multi-view dataset. These input frames are warped into a joint target frame, guided by the most recent (point-based) scene representation, followed by neural interpolation. The resulting architecture gains sufficient robustness to significantly improve transferability to previously unseen datasets. In particular, this enables novel applications for neural rendering where dynamically streamed content is directly incorporated in a (neural) image-based reconstruction of a scene. As we will show, our method reaches state-of-the-art performance when compared to previous works that rely on static and sufficiently densely sampled scenes; in addition, we demonstrate our system's particular suitability for dynamically streamed content, where our approach is able to produce high-fidelity novel-view synthesis even with significantly fewer available frames than competing neural methods.	https://dl.acm.org/doi/abs/10.1145/3610548.3618216	Mathias Harrer, Linus Franke, Laura Fink, Marc Stamminger, Tim Weyrich
Input-Dependent Uncorrelated Weighting for Monte Carlo Denoising	Image-space denoising techniques have been widely employed in Monte Carlo rendering, typically blending neighboring pixel estimates using a denoising kernel. It is widely recognized that a kernel should be adapted to characteristics of the input pixel estimates in order to ensure robustness to diverse image features and amount of noise. Denoising with such an input-dependent kernel, however, can introduce a bias that makes the denoised estimate even less accurate than the noisy input estimate. Consequently, it has been considered essential to balance the bias introduced by denoising and the reduction of noise. We propose a new framework to define an input-dependent kernel that departs from the existing approaches based on error estimation or supervised learning. Rather than seeking an optimal bias-noise balance as in those existing approaches, we propose to constrain the amount of bias introduced by denoising. Such a constraint is made possible by the concept of uncorrelated statistics, which has never been applied for denoising. By designing an input-dependent kernel with uncorrelated weights against the input pixel estimates, our denoising kernel can reduce data-dependent noise with a negligible amount of bias in most cases. We demonstrate the effectiveness of our method for various scenes.	https://dl.acm.org/doi/abs/10.1145/3610548.3618177	Jonghee Back, Binh-Son Hua, Toshiya Hachisuka, Bochang Moon
Interaction-Driven Active 3D Reconstruction with Object Interiors	We introduce an method which integrates visual perception, , and 3D scanning to recover both the exterior and , i.e., unexposed, geometries of a target 3D object. Unlike other works in active vision which focus on optimizing camera viewpoints to better investigate the environment, the primary feature of our reconstruction is an analysis of the of various parts of the target object and the ensuing part manipulation by a robot to enable scanning of occluded regions. As a result, an understanding of part articulations of the target object is obtained on top of complete geometry acquisition. Our method operates by a Fetch robot with built-in RGBD sensors. It iterates between interaction analysis and interaction-driven reconstruction, scanning and reconstructing detected moveable parts one at a time, where both the articulated part detection and mesh reconstruction are carried out by neural networks. In the final step, all the remaining, non-articulated parts, including all the interior structures that had been exposed by prior part manipulations and subsequently scanned, are reconstructed to complete the acquisition. We demonstrate the performance of our method via qualitative and quantitative evaluation, ablation studies, comparisons to alternatives, as well as experiments in a real environment.	https://dl.acm.org/doi/abs/10.1145/3618327	Zihao Yan, Fubao Su, Mingyang Wang, Ruizhen Hu, Hao Zhang, Hui Huang
Interactive Material Annotation on 3D Scanned Models leveraging Color-Material Correlation	3D scanning has made it possible to generate 3D models from real objects. Although 3D scanning can capture an object's shape and color texture, it is still technically difficult to analyze and reproduce material properties such as metalness, roughness, and transparency. Therefore, they need to be explicitly annotated after the scanning process. However, existing methods are highly labor-intensive such as a simple brush painting that requires delicate and inefficient handwork. To make this process more efficient and accurate, we propose a system that mitigates the costs by introducing a texture-aware annotation pipeline. This method is based on the observation that material distribution is correlated to color distribution. We segment the 3D surface into areas based on color similarity and let users annotate materials using the segmentations as masks. In an empirical user study, the participants could make quality annotations in a short time.	https://dl.acm.org/doi/abs/10.1145/3610543.3626170	Wataru Kawabe, Taisuke Hashimoto, Fabrice Matulic, Takeo Igarashi, Keita Higuchi
Interactive Relative Pose Estimation for 360° Indoor Panoramas through Wall-Wall Matching Selections	We present an interactive approach to estimating the relative camera pose of two panoramas shot in the same indoor environment. Compared to the trivial interactive baseline, which would require the user to precisely select 8 or more pairs of matching points by mouse clicks, our method just needs the user to select a pair of matching walls with two mouse clicks or keyboard strokes. Our method is based on the key observation that, in most cases, there exist at least one or multiple pairs of roughly matched walls in the room layouts estimated by neural networks - which alone are sufficient to generate accurate relative camera poses. Tested on a real-world indoor panorama dataset, our method outperforms current state-of-the-art automatic methods by large margins, compensating the additional human efforts. Through user studies, we found that matched wall-wall pairs can be easily recognized and selected by humans in relatively short time, indicating that such an interactive approach is practical.	https://dl.acm.org/doi/abs/10.1145/3610542.3626114	Bosheng Chen, Chihan Peng
Interactive Story Visualization with Multiple Characters	Accurate Story visualization requires several necessary elements, such as identity consistency across frames, the alignment between plain text and visual content, and a reasonable layout of objects in images. Most previous works endeavor to meet these requirements by fitting a text-to-image (T2I) model on a set of videos in the same style and with the same characters, e.g., the FlintstonesSV dataset. However, the learned T2I models typically struggle to adapt to new characters, scenes, and styles, and often lack the flexibility to revise the layout of the synthesized images. This paper proposes a system for generic interactive story visualization, capable of handling multiple novel characters and supporting the editing of layout and local structure. It is developed by leveraging the prior knowledge of large language and T2I models, trained on massive corpora. The system comprises four interconnected components: story-to-prompt generation (S2P), text-to-layout generation (T2L), controllable text-to-image generation (C-T2I), and image-to-video animation (I2V). First, the S2P module converts concise story information into detailed prompts required for subsequent stages. Next, T2L generates diverse and reasonable layouts based on the prompts, offering users the ability to adjust and refine the layout to their preferences. The core component, C-T2I, enables the creation of images guided by layouts, sketches, and actor-specific identifiers to maintain consistency and detail across visualizations. Finally, I2V enriches the visualization process by animating the generated images. Extensive experiments and a user study are conducted to validate the effectiveness and flexibility of interactive editing of the proposed system.	https://dl.acm.org/doi/abs/10.1145/3610548.3618184	Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang
Intrinsic Harmonization for Illumination-Aware Image Compositing	Despite significant advancements in network-based image harmonization techniques, there still exists a domain disparity between typical training pairs and real-world composites encountered during inference. Most existing methods are trained to reverse global edits made on segmented image regions, which fail to accurately capture the lighting inconsistencies between the foreground and background found in composited images. In this work, we introduce a self-supervised illumination harmonization approach formulated in the intrinsic image domain. First, we estimate a simple global lighting model from mid-level vision representations to generate a rough shading for the foreground region. A network then refines this inferred shading to generate a harmonious re-shading that aligns with the background scene. In order to match the color appearance of the foreground and background, we utilize ideas from prior harmonization approaches to perform parameterized image edits in the albedo domain. To validate the effectiveness of our approach, we present results from challenging real-world composites and conduct a user study to objectively measure the enhanced realism achieved compared to state-of-the-art harmonization methods. 	https://dl.acm.org/doi/abs/10.1145/3610548.3618178	Chris Careaga, S. Mahdi H. Miangoleh, Yağız Aksoy
Joint Sampling and Optimisation for Inverse Rendering	When dealing with difficult inverse problems such as inverse rendering, using Monte Carlo estimated gradients to optimise parameters can slow down convergence due to variance. Averaging many gradient samples in each iteration reduces this variance trivially. However, for problems that require thousands of optimisation iterations, the computational cost of this approach rises quickly. We derive a theoretical framework for interleaving sampling and optimisation. We update and reuse past samples with low-variance finite-difference estimators that describe the change in the estimated gradients between each iteration. By combining proportional and finite-difference samples, we continuously reduce the variance of our novel gradient meta-estimators throughout the optimisation process. We investigate how our estimator interlinks with Adam and derive a stable combination. We implement our method for inverse path tracing and demonstrate how our estimator speeds up convergence on difficult optimisation tasks.	https://dl.acm.org/doi/abs/10.1145/3610548.3618244	Martin Balint, Karol Myszkowski, Hans-Peter Seidel, Gurprit Singh
K-Surfaces: Bézier-Splines Interpolating at Gaussian Curvature Extrema	K-surfaces are an interactive modeling technique for Bézier-spline surfaces. Inspired by -curves by [Yan et al. 2017], each patch provides a single control point that is being interpolated at a local extremum of Gaussian curvature. The challenge is to solve the inverse problem of finding the center control point of a Bézier patch given the boundary control points and the handle. Unlike the situation in 2D, bi-quadratic Bézier patches may exhibit none, one, or several extrema, and finding them is non-trivial. We solve the difficult inverse problem, including the possible selection among several extrema, by learning the desired function from samples, generated by computing Gaussian curvature of random patches. This approximation provides a stable solution to the ill-defined inverse problem and is much more efficient than direct numerical optimization, facilitating the interactive modeling framework. The local solution is used in an iterative optimization incorporating continuity constraints across patches. We demonstrate that the surface varies smoothly with the handle location and that the resulting modeling system provides local and generally intuitive control. The idea of learning the inverse mapping from handles to patches may be applicable to other parametric surfaces.	https://dl.acm.org/doi/abs/10.1145/3618383	Tobias Djuren, Maximilian Kohlbrenner, Marc Alexa
Kirchhoff-Love Shells with Arbitrary Hyperelastic Materials	Kirchhoff-Love shells are commonly used in many branches of engineering, including in computer graphics, but have so far been simulated only under limited nonlinear material options. We derive the Kirchhoff-Love thin-shell mechanical energy for an arbitrary 3D volumetric hyperelastic material, including isotropic materials, anisotropic materials, and materials whereby the energy includes both even and odd powers of the principal stretches. We do this by starting with any 3D hyperelastic material, and then analytically computing the corresponding thin-shell energy limit. This explicitly identifies and separates in-plane stretching and bending terms, and avoids numerical quadrature. Thus, in-plane stretching and bending are shown to originate from one and the same process (volumetric elasticity of thin objects), as opposed to from two separate processes as done traditionally in cloth simulation. Because we can simulate materials that include both even and odd powers of stretches, we can accommodate standard mesh distortion energies previously employed for 3D solid simulations, such as Symmetric ARAP and Co-rotational materials. We relate the terms of our energy to those of prior work on Kirchhoff-Love thin-shells in computer graphics that assumed small in-plane stretches, and demonstrate the visual difference due to the presence of our exact stretching and bending terms. Furthermore, our formulation allows us to categorize all distinct hyperelastic Kirchhoff-Love thin-shell energies. Specifically, we prove that for Kirchhoff-Love thin-shells, the space of all hyperelastic materials collapses to two-dimensional hyperelastic materials. This observation enables us to create an interface for the design of thin-shell Kirchhoff-Love mechanical energies, which in turn enables us to create thin-shell materials that exhibit arbitrary stiffness profiles under large deformations.	https://dl.acm.org/doi/abs/10.1145/3618405	Jiahao Wen, Jernej Barbič
Kristine Is Not Well	Memes vs. algorithm in metaverse inspired by real rebellion stories. Kristine is Not Well is a 20-minute animated VR social media simulation that highlights the resistance of online activists against algorithmic surveillance and censorship.	https://dl.acm.org/doi/abs/10.1145/3610549.3614611	Seeyam Quine
L'Animal Sauce Ail	Gooseville, close to nothing and far away from everything. The place where Goosevillers lead themselves to destruction. The inhabitants live off the farming of species that they overexploit each time, from geese to tadpoles. Each new managing and self consuming environment is told throughout low budget homemade TV shows. Advertising, teleshopping and others show us the devolution of a small French village rushing towards its end.	https://dl.acm.org/doi/abs/10.1145/3626964.3626992	Ysaline Debut, Isa Cotte, Aurélien Duchez, Camille Rostan, Clément Mouchel, Diane Mazella, Philippe Meis
Landmark Guided 4D Facial Expression Generation	In this paper, we proposed a generative model that learns to synthesize the 4D facial expression with the neutral landmark. Existing works mainly focus on the generation of sequences guided by expression labels, speech, etc, while they are not robust to the change of different identities. Our LM-4DGAN utilizes neutral landmarks to guide the facial expression generation while adding an identity discriminator and a landmark autoencoder to the basic WGAN for achieving better identity robustness. Furthermore, we add a cross-attention mechanism to the existing displacement decoder which is suitable for the given identity.	https://dl.acm.org/doi/abs/10.1145/3610542.3626119	Xin Lu, Zhengda Lu, Yiqun Wang, Jun Xiao
Lapso	The older me wants to know the younger you.	https://dl.acm.org/doi/abs/10.1145/3626964.3626999	Monica Moura
Last Summer	As summer ends, Ren's bandmates and childhood friends are leaving for college. He struggles to write the lyrics of their final song.	https://dl.acm.org/doi/abs/10.1145/3626964.3626987	Gabriela Lewandowska, Alessandra De Stefano, Nicola Bernardi, Camille Van Delft, Chloé Van Becelaere, Elodie Xia, Cécile Blondel
LayerDiffusion: Layered Controlled Image Editing with Diffusion Models	Text-guided image editing has recently experienced rapid development. However, simultaneously performing multiple editing actions on a single image, such as background replacement and specific subject attribute changes, while maintaining consistency between the subject and the background remains challenging. In this paper, we propose LayerDiffusion, a semantic-based layered controlled image editing method. Our method enables non-rigid editing and attribute modification of specific subjects while preserving their unique characteristics and seamlessly integrating them into new backgrounds. We leverage a large-scale text-to-image model and employ a layered controlled optimization strategy combined with layered diffusion training. During the diffusion process, an iterative guidance strategy is used to generate a final image that aligns with the textual description. Experimental results demonstrate the effectiveness of our method in generating highly coherent images that closely align with the given textual description. The edited images maintain a high similarity to the features of the input image and surpass the performance of current leading image editing methods. LayerDiffusion opens up new possibilities for controllable image editing.	https://dl.acm.org/doi/abs/10.1145/3610543.3626172	Pengzhi Li, Qinxuan Huang, Yikang Ding, Zhiheng Li
Learning Assisted Interactive 3D modelling from 3D sketches	Sketching is one of the most natural ways for representing any object pictorially. With the advent of Virtual Reality (VR) and Augmented Reality (AR) technologies, 3D sketching has become more accessible. But it is challenging to convert these sketches to 3D models in a manner consistent with the intent of the artist. Learning based methods provide an effective alternate paradigm for solving this classical problem of sketch based modelling in interactive platforms. Surface patches, inferred from 3D sketch strokes, can be treated as a primitive which are assembled to create complex object models. We present our proposed framework for this problem and discuss our solution to one of its core challenges. Our deep neural network based method allows users to create surfaces from a stream of sparse 3D sketch strokes. We also show integration of our method into an existing Blender based 3D content creation pipeline. This serves as a basis to solve a more complex problem of creating complete 3D object models from sparse 3D sketch strokes.	https://dl.acm.org/doi/abs/10.1145/3623053.3623370	Sukanya Bhattacharjee
Learning Based 2D Irregular Shape Packing	2D irregular shape packing is a necessary step to arrange UV patches of a 3D model within a texture atlas for memory-efficient appearance rendering in computer graphics. Being a joint, combinatorial decision-making problem involving all patch positions and orientations, this problem has well-known NP-hard complexity. Prior solutions either assume a heuristic packing order or modify the upstream mesh cut and UV mapping to simplify the problem, which either limits the packing ratio or incurs robustness or generality issues. Instead, we introduce a learning-assisted 2D irregular shape packing method that achieves a high packing quality with minimal requirements from the input. Our method iteratively selects and groups subsets of UV patches into near-rectangular super patches, essentially reducing the problem to bin-packing, based on which a joint optimization is employed to further improve the packing ratio. In order to efficiently deal with large problem instances with hundreds of patches, we train deep neural policies to predict nearly rectangular patch subsets and determine their relative poses, leading to linear time scaling with the number of patches. We demonstrate the effectiveness of our method on three datasets for UV packing, where our method achieves a higher packing ratio over several widely used baselines with competitive computational speed.	https://dl.acm.org/doi/abs/10.1145/3618348	Zeshi Yang, Zherong Pan, Manyi Li, Kui Wu, Xifeng Gao
Learning Contact Deformations with General Collider Descriptors	This paper presents a learning-based method for the simulation of rich contact deformations on reduced deformation models. Previous works learn deformation models for specific pairs of objects; we lift this limitation by designing a neural model that supports general rigid collider shapes. We do this by formulating a novel collider descriptor that characterizes local geometry in a region of interest. The paper shows that the learning-based deformation model can be trained on a library of colliders, but it accurately supports unseen collider shapes at runtime. We showcase our method on interactive dynamic simulations with animation of rich deformation detail, manipulation and exploration of untrained objects, and augmentation of contact information suitable for high-fidelity haptics.	https://dl.acm.org/doi/abs/10.1145/3610548.3618229	Cristian Romero, Dan Casas, Maurizio Chiaramonte, Miguel A. Otaduy
Learning Gradient Fields for Scalable and Generalizable Irregular Packing	The packing problem, also known as cutting or nesting, has diverse applications in logistics, manufacturing, layout design, and atlas generation. It involves arranging irregularly shaped pieces to minimize waste while avoiding overlap. Recent advances in machine learning, particularly reinforcement learning, have shown promise in addressing the packing problem. In this work, we delve deeper into a novel machine learning-based approach that formulates the packing problem as conditional generative modeling. To tackle the challenges of irregular packing, including object validity constraints and collision avoidance, our method employs the score-based diffusion model to learn a series of gradient fields. These gradient fields encode the correlations between constraint satisfaction and the spatial relationships of polygons, learned from teacher examples. During the testing phase, packing solutions are generated using a coarse-to-fine refinement mechanism guided by the learned gradient fields. To enhance packing feasibility and optimality, we introduce two key architectural designs: multi-scale feature extraction and coarse-to-fine relation extraction. We conduct experiments on two typical industrial packing domains, considering translations only. Empirically, our approach demonstrates spatial utilization rates comparable to, or even surpassing, those achieved by the teacher algorithm responsible for training data generation. Additionally, it exhibits some level of generalization to shape variations. We are hopeful that this method could pave the way for new possibilities in solving the packing problem.	https://dl.acm.org/doi/abs/10.1145/3610548.3618235	Tianyang Xue, Mingdong Wu, Lin Lu, Haoxuan Wang, Hao Dong, Baoquan Chen
Learning multivariate empirical mode decomposition for spectral motion editing	Spectral motion editing provides an important perspective for character animation synthesis since biomechanical features are usually embedded in the frequency domain. However, heavy manual postprocessing and high computational cost were required to achieve high-quality character animations. In this paper, first, we propose a novel architecture for neural networks to learn multivariate empirical mode decomposition that can decompose motion into non-linear frequency components corresponding to their biomechanical features. Next, we demonstrate a spectral motion editing technique based on our proposed architecture. The results revealed that high-quality character animation synthesis could be achieved by editing these decomposed non-linear frequency components, providing novel tasks for character animation design.	https://dl.acm.org/doi/abs/10.1145/3610543.3626174	Ran Dong, Soichiro Ikuno, Xi Yang
Learning the Geodesic Embedding with Graph Neural Networks	We present GEGNN, a learning-based method for computing the approximate geodesic distance between two arbitrary points on discrete polyhedra surfaces with constant time complexity after fast precomputation. Previous relevant methods either focus on computing the geodesic distance between a single source and all destinations, which has linear complexity at least or require a long precomputation time. Our key idea is to train a graph neural network to embed an input mesh into a high-dimensional embedding space and compute the geodesic distance between a pair of points using the corresponding embedding vectors and a lightweight decoding function. To facilitate the learning of the embedding, we propose novel graph convolution and graph pooling modules that incorporate local geodesic information and are verified to be much more effective than previous designs. After training, our method requires only one forward pass of the network per mesh as precomputation. Then, we can compute the geodesic distance between a pair of points using our decoding function, which requires only several matrix multiplications and can be massively parallelized on GPUs. We verify the efficiency and effectiveness of our method on ShapeNet and demonstrate that our method is faster than existing methods by orders of magnitude while achieving comparable or better accuracy. Additionally, our method exhibits robustness on noisy and incomplete meshes and strong generalization ability on out-of-distribution meshes. The code and pretrained model can be found on https://github.com/IntelligentGeometry/GeGnn.	https://dl.acm.org/doi/abs/10.1145/3618317	Bo Pang, Zhongtian Zheng, Guoping Wang, Peng-Shuai Wang
Leveraging AR-Driven Visual Storytelling to Enhance Communication of Complex Social Issues: Principles, Strategies, and Multifaceted Roles of Visuals	By sharing principles and strategies for effectively using AR-driven Visual Storytelling to communicate complex social issues this paper adds to the fields of design and education. The insights offer visual designers a strong toolbox to create engaging experiences for audiences. Additionally, focusing on how to apply these strategies in various educational settings makes the proposed framework useful worldwide. By giving designers a clear way to blend augmented reality into visual storytelling, this study encourages creative communication about important social topics. This contribution not only boosts the impact of design but also helps education by promoting understanding across different cultures and sharing knowledge. Ultimately, the paper's approach connects design and education, opening new possibilities for teaching methods and positive changes in society.	https://dl.acm.org/doi/abs/10.1145/3610540.3627009	Anastasia Tyurina
LiCROM: Linear-Subspace Continuous Reduced Order Modeling with Neural Fields	Linear reduced-order modeling (ROM) simplifies complex simulations by approximating the behavior of a system using a simplified kinematic representation. Typically, ROM is trained on input simulations created with a specific spatial discretization, and then serves to accelerate simulations with the same discretization. This discretization-dependence is restrictive. Becoming independent of a specific discretization would provide flexibility to mix and match mesh resolutions, connectivity, and type (tetrahedral, hexahedral) in training data; to accelerate simulations with novel discretizations unseen during training; and to accelerate adaptive simulations that temporally or parametrically change the discretization. We present a flexible, discretization-independent approach to reduced-order modeling. Like traditional ROM, we represent the configuration as a linear combination of displacement fields. Unlike traditional ROM, our displacement fields are continuous maps from every point on the reference domain to a corresponding displacement vector; these maps are represented as implicit neural fields. With linear continuous ROM (LiCROM), our training set can include multiple geometries undergoing multiple loading conditions, independent of their discretization. This opens the door to novel applications of reduced order modeling. We can now accelerate simulations that modify the geometry at runtime, for instance via cutting, hole punching, and even swapping the entire mesh. We can also accelerate simulations of geometries unseen during training. We demonstrate one-shot generalization, training on a single geometry and subsequently simulating various unseen geometries.	https://dl.acm.org/doi/abs/10.1145/3610548.3618158	Yue Chang, Peter Yichen Chen, Zhecheng Wang, Maurizio M. Chiaramonte, Kevin Carlberg, Eitan Grinspun
Light-Efficient Holographic Illumination for Continuous-Wave Time-of-Flight Imaging	Time-of-flight (TOF) cameras have seen widespread adoption in recent years across the entire spectrum of commodity devices. However, these devices are fundamentally limited by their dynamic range, struggling with saturation from nearby, brighter objects and noisy depth from farther, darker objects. In this work, we explore overcoming these limitations in the context of continuous-wave time-of-flight (CWTOF) devices, by using a holographic light source capable of redistributing light according to arbitrary patterns. In particular, we propose using such a system to move light from overexposed to underexposed regions of a scene, such that the entire scene is well exposed. Such a methodology can be easily integrated with existing illumination schemes for TOF. Our proof-of-concept prototype is constructed from off-the-shelf optical components, and demonstrated on a number of lab scenes.	https://dl.acm.org/doi/abs/10.1145/3610548.3618152	Dorian Chan, Matthew O'Toole
LightSense - Long Distance	'LightSense - Long Distance' explores remote interaction with architectural space. It is a virtual extension of the project 'LightSense,' which is currently presented at the exhibition 'Cyber Physical: Architecture in Real Time' at EPFL Pavilions in Switzerland. Using numerous VR headsets, the setup at the Art Gallery at SIGGRAPH Asia establishes a direct connection between both exhibition sites in Sydney and Lausanne. 'LightSense' at EPFL Pavilions is an immersive installation that allows the audience to engage in intimate interaction with a living architectural body. It consists of a 12-meter-long construction that combines a lightweight structure with projected 3D holographic animations. At its core sits a neural network, which has been trained on sixty thousand poems. This allows the structure to engage, lead, and sustain conversations with the visitor. Its responses are truly associative, unpredictable, meaningful, magical, and deeply emotional. Analysing the emotional tenor of the conversation, 'LightSense' can transform into a series of hybrid architectural volumes, immersing the visitors in Pavilions of Love, Anger, Curiosity, and Joy. 'LightSense's' physical construction is linked to a digital twin. Movement, holographic animations, sound, and text responses are controlled by the cloud-based AI system. This combination creates a location-independent cyber-physical system. As such, the 'Long Distance' version, which premiered at SIGGRAPH Asia, enables the visitors in Sydney to directly engage with the physical setup in Lausanne. Using VR headsets with a new 360-degree 4K live streaming system, the visitors find themselves teleported to face 'LightSense', able to engage in a direct conversation with the structure on-site. 'LightSense - Long Distance' leaves behind the notion of architecture being a place-bound and static environment. Instead, it points toward the next generation of responsive buildings that transcend space, are capable of dynamic behaviour, and able to accompany their visitors as creative partners.	https://dl.acm.org/doi/abs/10.1145/3610537.3622963	Uwe Rieger, Yinan Liu, Tharindu Kaluarachchi, Amit Barde, Huidong Bai, Alaeddin Nassani, Suranga Nanayakkara, Mark Billinghurst
Limit Situation	"""Limit Situation"" is a virtual reality artwork that explores the universal existential dilemma of human existence through immersive and narrative-driven virtual spaces. The artwork is divided into six chapters of virtual spatial experiences, inspired by six books that provide background and context. These virtual experiences create a realm for emotions and contemplation. From conceptual to tangible, from real to virtual, the artwork represents the artist's understanding of existential dilemmas."	https://dl.acm.org/doi/abs/10.1145/3610549.3614607	Ziyao Lin
LitNeRF: Intrinsic Radiance Decomposition for High-Quality View Synthesis and Relighting of Faces	High-fidelity, photorealistic 3D capture of a human face is a long-standing problem in computer graphics – the complex material of skin, intricate geometry of hair, and fine scale textural details make it challenging. Traditional techniques rely on very large and expensive capture rigs to reconstruct explicit mesh geometry and appearance maps, and are limited by the accuracy of hand-crafted reflectance models. More recent volumetric methods (e.g., NeRFs) have enabled view-synthesis and sometimes relighting by learning an implicit representation of the density and reflectance basis, but suffer from artifacts and blurriness due to the inherent ambiguities in volumetric modeling. These problems are further exacerbated when capturing with few cameras and light sources. We present a novel technique for high-quality capture of a human face for 3D view synthesis and relighting using a sparse, compact capture rig consisting of 15 cameras and 15 lights. Our method combines a neural volumetric representation with traditional mesh reconstruction from multiview stereo. The proxy geometry allows us to anchor the 3D density field to prevent artifacts and guide the disentanglement of intrinsic radiance components of the face appearance such as diffuse and specular reflectance, and incident radiance (shadowing) fields. Our hybrid representation significantly improves the state-of-the-art quality for arbitrarily dense renders of a face from desired camera viewpoint as well as environmental, directional, and near-field lighting.	https://dl.acm.org/doi/abs/10.1145/3610548.3618210	Kripasindhu Sarkar, Marcel C. Bühler, Gengyan Li, Daoye Wang, Delio Vicini, Jérémy Riviere, Yinda Zhang, Sergio Orts-Escolano, Paulo Gotardo, Thabo Beeler, Abhimitra Meka
Live4D: A Real-time Capture System for Streamable Volumetric Video	Volumetric video holds promise for virtual and augmented reality (VR/AR) applications but faces challenges in interactive scenarios due to high hardware costs, complex processing and substantial data streams. In this paper, we introduce Live4D, a cost-effective, real-time volumetric video generation and streaming system using an RGB-only camera setup. We propose a novel deep implicit surface reconstruction algorithm, that combined neural signed distance field with observed truncated signed distance field to generate the watertight meshes with low latency. Moreover, we achieve a robust non-rigid tracking method that provides temporal stability to the meshes while resisting tracking failure cases. Experimental results show that Live4D achieves a performance of 24fps using mid-range graphic cards and exhibits an end-to-end latency of 95ms. The system enables live streaming of volumetric video within a 20Mbps bandwidth requirement, positioning Live4D as a promising solution for real-time 3D vision content creation in the growing VR/AR industry.	https://dl.acm.org/doi/abs/10.1145/3610543.3626178	Yifeng Zhou, Shuheng Wang, Wenfa Li, Chao Zhang, Li Rao, Pu Cheng, Yi Xu, Jinle Ke, Wenduo Feng, Wen Zhou, Hao Xu, Yukang Gao, Yang Ding, Weixuan Tang, Shaohui Jiao
LiveNVS: Neural View Synthesis on Live RGB-D Streams	Existing real-time RGB-D reconstruction approaches, like Kinect Fusion, lack real-time photo-realistic visualization. This is due to noisy, oversmoothed or incomplete geometry and blurry textures which are fused from imperfect depth maps and camera poses. Recent neural rendering methods can overcome many of such artifacts but are mostly optimized for offline usage, hindering the integration into a live reconstruction pipeline. In this paper, we present LiveNVS, a system that allows for neural novel view synthesis on a live RGB-D input stream with very low latency and real-time rendering. Based on the RGB-D input stream, novel views are rendered by projecting neural features into the target view via a densely fused depth map and aggregating the features in image-space to a target feature map. A generalizable neural network then translates the target feature map into a high-quality RGB image. LiveNVS achieves state-of-the-art neural rendering quality of unknown scenes during capturing, allowing users to virtually explore the scene and assess reconstruction quality in real-time.	https://dl.acm.org/doi/abs/10.1145/3610548.3618213	Laura Fink, Darius Rückert, Linus Franke, Joachim Keinert, Marc Stamminger
Locally-Adaptive Level-of-Detail for Hardware-Accelerated Ray Tracing	We introduce an adaptive level-of-detail technique for ray tracing triangle meshes that aims to reduce the memory bandwidth used during ray traversal, which can be the bottleneck for rendering time with large scenes and the primary consumer of energy. We propose a specific data structure for hierarchically representing triangle meshes, allowing localized decisions for the desired mesh resolution per ray. Starting with the lowest-resolution triangle mesh level, higher-resolution levels are generated by tessellating each triangle into four via splitting its edges with arbitrarily-placed vertices. We fit the resulting mesh hierarchy into a specialized acceleration structure to perform on-the-fly tessellation level selection during ray traversal. Our structure reduces both storage cost and data movement during rendering, which are the main consumers of energy. It also allows continuous transitions between detail levels, while locally adjusting the mesh resolution per ray and preserving watertightness. We present how this structure can be used with both primary and secondary rays for reflections and shadows, which can intersect with different tessellation levels, providing consistent results. We also propose specific hardware units to cover the cost of additional compute needed for level-of-detail operations. We evaluate our method using a cycle-accurate simulation of a custom ray tracing hardware architecture. Our results show that, as compared to traditional bounding volume hierarchies, our method can provide more than an order of magnitude reduction in energy use and render time, given sufficient computational resources.	https://dl.acm.org/doi/abs/10.1145/3618359	Jacob Haydel, Cem Yuksel, Larry Seiler
Lock-free Vertex Clustering for Multicore Mesh Reduction	Modern data collection methods can capture representations of 3D objects at resolutions much greater than they can be discretely rendered as an image. To improve the efficiency of storage, transmission, rendering, and editing of 3D models constructed from such data, it is beneficial to first employ a mesh reduction technique to reduce the size of a mesh. Vertex clustering, a technique that merges close vertices together, has particularly wide applicability, because it operates only on vertices and their spatial proximity. However, it is also very difficult to accelerate with parallelisation in a deterministic manner because it contains extensive algorithmic dependencies. Prior work treats the non-trivial clustering step of this process serially to preserve vertex priorities, which fundamentally limits to mid-single digits the acceleration rates that are possible for the process overall. This paper introduces a novel lock-free parallel algorithm, P-Weld, that exposes parallelism with a graph-theoretic lens that iteratively peels away layers of a mesh that have no remaining dependencies. Concurrent updates to shared data are managed with a linearisable sequence of atomic instructions that exactly reproduces the serial clustering. The resulting parallelism and improved spatial locality yield a 3.86 × speedup on a standard 14-million vertex mesh and a 2.93 × speedup on a 400-million vertex LiDaR point cloud covering the city of Vancouver, Canada, relative to a popular open source library.	https://dl.acm.org/doi/abs/10.1145/3610548.3618234	Nima Fathollahi, Sean Chester
Loup y es-tu ?	A little girl named Mischa lives in the suburbs of Moscow. She has just made a beautiful violin out of paper. She wants to play for the big monsters who live in the apartment. But the less they listen to her playing the violin, the more the Wolf dangerously prowls near her.	https://dl.acm.org/doi/abs/10.1145/3626964.3626995	Louise Laurent, Alizée Van De Valle, Emma Fessart, Jeanne Galland, Célina Lebon, Annouck François, Philippe Meis
Low-Light Image Enhancement with Wavelet-Based Diffusion Models	Diffusion models have achieved promising results in image restoration tasks, yet suffer from time-consuming, excessive computational resource consumption, and unstable restoration. To address these issues, we propose a robust and efficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL. Specifically, we present a wavelet-based conditional diffusion model (WCDM) that leverages the generative power of diffusion models to produce results with satisfactory perceptual fidelity. Additionally, it also takes advantage of the strengths of wavelet transformation to greatly accelerate inference and reduce computational resource usage without sacrificing information. To avoid chaotic content and diversity, we perform both forward diffusion and denoising in the training phase of WCDM, enabling the model to achieve stable denoising and reduce randomness during inference. Moreover, we further design a high-frequency restoration module (HFRM) that utilizes the vertical and horizontal details of the image to complement the diagonal information for better fine-grained restoration. Extensive experiments on publicly available real-world benchmarks demonstrate that our method outperforms the existing state-of-the-art methods both quantitatively and visually, and it achieves remarkable improvements in efficiency compared to previous diffusion-based methods. In addition, we empirically show that the application for low-light face detection also reveals the latent practical values of our method. Code is available at https://github.com/JianghaiSCU/Diffusion-Low-Light.	https://dl.acm.org/doi/abs/10.1145/3618373	Hai Jiang, Ao Luo, Haoqiang Fan, Songchen Han, Shuaicheng Liu
Lucky Brave's Sunshine	3D Animated short film, sci-fi western, drama, Unreal Engine.	https://dl.acm.org/doi/abs/10.1145/3626964.3626997	Andres Aguilar, Joseph Game
MCNeRF: Monte Carlo Rendering and Denoising for Real-Time NeRFs	The volume rendering step used in Neural Radiance Fields (NeRFs) produces highly photorealistic results, but is inherently slow because it evaluates an MLP at a large number of sample points per ray. Previous work has addressed this by either proposing neural scene representations that are faster to evaluate or by pre-computing (and approximating) scene properties to reduce render times. In this work, we propose MCNeRF, a general Monte Carlo-based rendering algorithm that can speed up any NeRF representation. We show that the NeRF volume rendering integral can be efficiently computed via Monte Carlo integration using an importance sampling scheme based on ray density distributions. This allows us to use a small number of MLP evaluations to estimate pixel radiance. These noisy Monte Carlo estimates can be further denoised using an inexpensive image-space denoiser trained per-scene. We demonstrate that MCNeRF can be used to speed up NeRF representations like TensoRF by 7 × while closely matching their visual quality and without making the scene approximations that real-time NeRF rendering methods usually make.	https://dl.acm.org/doi/abs/10.1145/3610548.3618221	Kunal Gupta, Milos Hasan, Zexiang Xu, Fujun Luan, Kalyan Sunkavalli, Xin Sun, Manmohan Chandraker, Sai Bi
MIPS-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online Neural RGB-D Reconstruction	We introduce MIPS-Fusion, a robust and scalable online RGB-D reconstruction method based on a novel neural implicit representation - multi-implicit-submap. Different from existing neural RGB-D reconstruction methods lacking either flexibility with a single neural map or scalability due to extra storage of feature grids, we propose a pure neural representation tackling both difficulties with a divide-and-conquer design. In our method, neural submaps are incrementally allocated alongside the scanning trajectory and efficiently learned with local neural bundle adjustments. The submaps can be refined individually in a back-end optimization and optimized jointly to realize submap-level loop closure. Meanwhile, we propose a hybrid tracking approach combining randomized and gradient-based pose optimizations. For the first time, randomized optimization is made possible in neural tracking with several key designs to the learning process, enabling efficient and robust tracking even under fast camera motions. The extensive evaluation demonstrates that our method attains higher reconstruction quality than the state of the arts for large-scale scenes and under fast camera motions.	https://dl.acm.org/doi/abs/10.1145/3618363	Yijie Tang, Jiazhao Zhang, Zhinan Yu, He Wang, Kai Xu
MOCHA: Real-Time Motion Characterization via Context Matching	Transforming neutral, characterless input motions to embody the distinct style of a notable character in real time is highly compelling for character animation. This paper introduces MOCHA, a novel online motion characterization framework that transfers both motion styles and body proportions from a target character to an input source motion. MOCHA begins by encoding the input motion into a motion feature that structures the body part topology and captures motion dependencies for effective characterization. Central to our framework is the Neural Context Matcher, which generates a motion feature for the target character with the most similar context to the input motion feature. The conditioned autoregressive model of the Neural Context Matcher can produce temporally coherent character features in each time frame. To generate the final characterized pose, our Characterizer network incorporates the characteristic aspects of the target motion feature into the input motion feature while preserving its context. This is achieved through a transformer model that introduces the adaptive instance normalization and context mapping-based cross-attention, effectively injecting the character feature into the source feature. We validate the performance of our framework through comparisons with prior work and an ablation study. Our framework can easily accommodate various applications, including characterization with only sparse input and real-time characterization. Additionally, we contribute a high-quality motion dataset comprising six different characters performing a range of motions, which can serve as a valuable resource for future research.	https://dl.acm.org/doi/abs/10.1145/3610548.3618252	Deok-Kyeong Jang, Yuting Ye, Jungdam Won, Sung-Hee Lee
MR BLS Trainer: A physical mixed reality CPR+AED rescue simulator	The MR BLS Rescue Trainer is a training system that allows users to experience realistic simulation with physical feeling in lifesaving procedures, that is BLS (basic life support), including CPR (cardiopulmonary resuscitation) and AED (Automated External Defibrillator) by applying physical mixed reality incorporated with partially tangible components	https://dl.acm.org/doi/abs/10.1145/3610549.3614591	Toshikazu Ohshima, Saina Matsui, Mizuki Yamane, Yali Ling, Katsuhito Muroi, Chihiro Sakai
Machine Learning & Neural Networks	Use and development of computer systems that are able to learn and adapt without following explicit instructions by using algorithms and statistical models to analyze and draw inferences from patterns in data.	https://dl.acm.org/doi/abs/10.1145/3610538.3614646	Rajesh Sharma, Mia Tang
Maintaining agency in AI-generated works of art and design: Deliberate creative processes	This paper responds to the unfolding debate on the use of generative AI tools in art and design. As the conversations and technological frameworks remain in their early stages the paper focuses on emerging possibilities and methodologies rather than on broader claims or conclusions. The discussed methods can serve as an effective entry point for education and curricula developments with practical advice and conceptual framing to contextualize emerging AI tools in a broader artistic culture and legacy. While the discussion on autonomous creativity is a significant part of the future of AI system developments and their relationship with humanity, this paper specifically limits its contribution to human-to-AI interactions as an important step in understanding intentionality, agency, and identity in that newly formed landscape.	https://dl.acm.org/doi/abs/10.1145/3610540.3627011	Andrzej Zarzycki
Manifold Path Guiding for Importance Sampling Specular Chains	Complex visual effects such as caustics are often produced by light paths containing multiple consecutive specular vertices (dubbed , which pose a challenge to unbiased estimation in Monte Carlo rendering. In this work, we study the light transport behavior within a sub-path that is comprised of a specular chain and two non-specular separators. We show that the specular manifolds formed by all the sub-paths could be exploited to provide coherence among sub-paths. By reconstructing continuous energy distributions from historical and coherent sub-paths, seed chains can be generated in the context of importance sampling and converge to admissible chains through manifold walks. We verify that importance sampling the seed chain in the continuous space reaches the goal of importance sampling the discrete admissible specular chain. Based on these observations and theoretical analyses, a progressive pipeline, , is designed and implemented to importance sample challenging paths featuring long specular chains. To our best knowledge, this is the first general framework for importance sampling discrete specular chains in regular Monte Carlo rendering. Extensive experiments demonstrate that our method outperforms state-of-the-art unbiased solutions with up to 40 × variance reduction, especially in typical scenes containing long specular chains and complex visibility.	https://dl.acm.org/doi/abs/10.1145/3618360	Zhimin Fan, Pengpei Hong, Jie Guo, Changqing Zou, Yanwen Guo, Ling-Qi Yan
Mapping and Recognition of Facial Expressions on Another Person's Look-Alike Avatars	As Virtual Reality (VR) continues to advance and gain popularity, one of the persisting concerns revolves around the level of fidelity achievable in creating lifelike avatars. This study aims to explore the possibility, feasibility, and effects of controlling avatars in Virtual Reality using actual facial expressions and eye movements from three different actors mapped onto one actor's look-alike avatar. The objective is to explore the authenticity and appeal of avatars, particularly when they accurately portray facial expressions of their respective users compared to when they display facial expressions from other individuals. By properly mapping facial expressions and eye movements onto the avatar, we seek to aid with the development of a more realistic and captivating virtual experience that closely mirrors real-life interactions. Furthermore, we investigate whether mapping one's facial expressions to another person's look-alike avatar affects identification and recognition.	https://dl.acm.org/doi/abs/10.1145/3610543.3626159	Birate Sonia, Trinity Suma, Kwame Agyemang, Oyewole Oyekoya
MatFusion: A Generative Diffusion Model for SVBRDF Capture	We formulate SVBRDF estimation from photographs as a diffusion task. To model the distribution of spatially varying materials, we first train a novel unconditional SVBRDF diffusion backbone model on a large set of 312, 165 synthetic spatially varying material exemplars. This SVBRDF diffusion backbone model, named MatFusion, can then serve as a basis for refining a conditional diffusion model to estimate the material properties from a photograph under controlled or uncontrolled lighting. Our backbone MatFusion model is trained using only a loss on the reflectance properties, and therefore refinement can be paired with more expensive rendering methods without the need for backpropagation during training. Because the conditional SVBRDF diffusion models are generative, we can synthesize multiple SVBRDF estimates from the same input photograph from which the user can select the one that best matches the users' expectation. We demonstrate the flexibility of our method by refining different SVBRDF diffusion models conditioned on different types of incident lighting, and show that for a single photograph under colocated flash lighting our method achieves equal or better accuracy than existing SVBRDF estimation methods.	https://dl.acm.org/doi/abs/10.1145/3610548.3618194	Sam Sartor, Pieter Peers
MathVR: Teaching Vector Arithmetic Using Virtual Reality	Traditionally, linear algebra has been taught using 2D representations of fundamentally 3D concepts. We present a visualisation of vector arithmetic in virtual reality with full hand presence and lateral motion, demonstrating how this new technology can be used to enhance student engagement and understanding. We detail sufficiently powerful affordances to convey ideas such as compound operations, yet simple enough to be digestible by average undergraduate students in a 1st year university mathematics course. This presentation aims to catalyse discussion and ideas around the use of immersive technologies for education in more abstract disciplines such as mathematics.	https://dl.acm.org/doi/abs/10.1145/3610540.3627012	Matt Cabanag
Media Interpretation: Revisiting McLuhans' Laws of Media and Ant Farm	This paper reexamines the work of Marshall McLuhan and Ant Farm, highlighting their enduring relevance for contemporary mediated urbanism and architecture. By exploring their historical context, connections, and influences, the authors provide insights for architects and artists navigating the complex interplay between media, technology, and the built environment. The analysis bridges the gap between historical context and contemporary practice, focusing on the motivations, possibilities, and limitations of media interpretation as a critical and creative practice. The paper addresses the pressing questions concerning the future design of architectural spaces and urban forms, ultimately fostering innovative approaches that challenge conventional design thinking	https://dl.acm.org/doi/abs/10.1145/3610591.3616424	Rem Rungu Lin, Kang Zhang
Meshes with Spherical Faces	Discrete surfaces with spherical faces are interesting from a simplified manufacturing viewpoint when compared to other double curved face shapes. Furthermore, by the nature of their definition they are also appealing from the theoretical side leading to a Möbius invariant discrete surface theory. We therefore systematically describe so called sphere meshes with spherical faces and circular arcs as edges where the Möbius transformation group acts on all of its elements. Driven by aspects important for manufacturing, we provide the means to cluster spherical panels by their radii. We investigate the generation of sphere meshes which allow for a geometric support structure and characterize all such meshes with triangular combinatorics in terms of non-Euclidean geometries. We generate sphere meshes with hexagonal combinatorics by intersecting tangential spheres of a reference surface and let them evolve - guided by the surface curvature - to visually convex hexagons, even in negatively curved areas. Furthermore, we extend meshes with circular faces of all combinatorics to sphere meshes by filling its circles with suitable spherical caps and provide a remeshing scheme to obtain quadrilateral sphere meshes with support structure from given sphere congruences. By broadening polyhedral meshes to sphere meshes we exploit the additional degrees of freedom to minimize intersection angles of neighboring spheres enabling the use of spherical panels that provide a softer perception of the overall surface.	https://dl.acm.org/doi/abs/10.1145/3618345	Martin Kilian, Anthony S Ramos Cisneros, Christian Müller, Helmut Pottmann
MetaLayer: A Meta-Learned BSDF Model for Layered Materials	Reproducing the appearance of arbitrary layered materials has long been a critical challenge in computer graphics, with regard to the demanding requirements of both physical accuracy and low computation cost. Recent studies have demonstrated promising results by learning-based representations that implicitly encode the appearance of complex (layered) materials by neural networks. However, existing generally-learned models often struggle between strong representation ability and high runtime performance, and also lack physical parameters for material editing. To address these concerns, we introduce , a new methodology leveraging meta-learning for modeling and rendering layered materials. MetaLayer contains two networks: a that compactly encodes layered materials into implicit neural representations, and a that establishes the mapping between the physical parameters of each material and the weights of its corresponding implicit neural representation. A new positional encoding method and a well-designed training strategy are employed to improve the performance and quality of the neural model. As a new learning-based representation, the proposed MetaLayer model provides both fast responses to material editing and high-quality results for a wide range of layered materials, outperforming existing layered BSDF models.	https://dl.acm.org/doi/abs/10.1145/3618365	Jie Guo, Zeru Li, Xueyan He, Beibei Wang, Wenbin Li, Yanwen Guo, Ling-Qi Yan
Metric Optimization in Penner Coordinates	Many parametrization and mapping-related problems in geometry processing can be viewed as metric optimization problems, i.e., computing a metric minimizing a functional and satisfying a set of constraints, such as flatness. are global coordinates on the space of metrics on meshes with a fixed vertex set and topology, but varying connectivity, making it homeomorphic to the Euclidean space of dimension equal to the number of edges in the mesh, without any additional constraints imposed. These coordinates play an important role in the theory of discrete conformal maps, enabling recent development of highly robust algorithms with convergence and solution existence guarantees for computing such maps. We demonstrate how Penner coordinates can be used to solve a general class of optimization problems involving metrics, including optimization and interpolation, while retaining the key solution existence guarantees available for discrete conformal maps.	https://dl.acm.org/doi/abs/10.1145/3618394	Ryan Capouellez, Denis Zorin
MicroGlam: Microscopic Skin Image Dataset with Cosmetics	In this paper, we present a cosmetic-specific skin image dataset.1 It consists of skin images from 45 patches (5 skin patches each from 9 participants) of size 8mm*8mm under three cosmetic products (i.e., foundation, blusher, and highlighter). We designed a novel capturing device inspired by LightStage [Debevec et al. 2000]. Using the device, we captured over 600 images of each skin patch under diverse lighting conditions in 30 seconds. We repeated the process for the same skin patch under three cosmetic products. Finally, we demonstrate the viability of the dataset with an image-to-image translation-based pipeline for cosmetic rendering and compared our data-driven approach to an existing cosmetic rendering method [Kim and Ko 2018].	https://dl.acm.org/doi/abs/10.1145/3610543.3626162	Toby Chong, Alina Chadwick, I-Chao Shen, Haoran Xie, Takeo Igarashi
Midnight Hotel	What if it were possible to meet one another in our sleep? One night, three strangers find themselves in a Hotel of dreams. They must navigate this strange world together before dawn.	https://dl.acm.org/doi/abs/10.1145/3626964.3626985	Marie Toury, May Taraud, Malo Doucet, Neïl Dieu, Vincent Albert
MiniGI: Guerilla Mappings in Miniature	We present Secret Mapping, an interactive tabletop installation coupled with a handheld Augmented Reality (AR) visualisation. The installation showcases five selected locations which are the human-scale twins of actual, real-word exhibits all located throughout former Yugoslavia. AR visualisation illuminates these miniature 3D replicas and encompasses them with an extended version of the archived visuals created by the artists, specifically to each spot. The installation invites visitors to join the artists on their trip and reenact their journey with an collaborative tabletop interface, while offering the possibility for backwards or forward exploration.	https://dl.acm.org/doi/abs/10.1145/3610549.3614612	Dávid Maruscsák, Gábor Szűcs, Jeanne Vézien, Christian Sandor
Missing 10 Hours VR	"M10H (Missing 10 Hours) is a narrative VR project directed by Fanni Fazakas. In the multi-ending experience, the user is embodied as the bystander (the buddy of the perpetrator), who can influence the narrative. By consulting with psychologists and experts, we made sure that the experience is ethically correct and has a measurable impact. In the experience, users can explore a similar night to that that of a real-life victim of GHB, the 'date rape drug from the Bystander's perspective. Multiple interactive gameplay elements will allow the user to influence how the story evolves and gain a multi-perspective understanding of the issue. After onboarding, the bystander can explore real-life scenarios where a ""pal"" called Greg puts GHB into a drink that belongs to their common friend, a young woman called Mara. Since GHB makes it impossible for Mara to defend herself, or even to ask for help, the only possibility is that the user will recognize what happens and save her from Greg and his questionable intentions. During the multi-ending storyline, the user has to think, react and challenge themselves."	https://dl.acm.org/doi/abs/10.1145/3610549.3614601	Fanni Fazakas
Mixed Reality as Empathy Machine. Case Study in Universal Design Course	Empathy has become a central focus in design and is clearly evident in various frameworks, such as universal design, inclusive design, and human-centered design. However, among numerous stakeholders, designers, and engineers are the ones who particularly require an expansion of their empathetic understanding. The emergence of immersive technologies has the potential to facilitate the development of empathy in individuals. Extended reality (XR) combines virtual reality with the real world, enabling the simulation of various physical states, health conditions, and limitations of the human body. This paper introduces three distinct XR scenarios that immerse potential users in the experiences of people with special needs. The elaborated tasks encompass aspects like mobility impairments, pregnancy, and challenges faced by the elderly. All exercises take place in a familiar supermarket environment, as shopping is a common, everyday activity for most people. The XR application is designed for the Oculus Quest 2 platform and is complemented by external equipment such as a geriatric suit, pregnancy belly simulator, or wheelchair. The proposed simulations were assessed and validated by 42 students from the Lodz University of Technology taking the Universal Design course. The testing process comprised several stages, which included an empathy pre-test, the execution of the XR experience, an empathy post-test, and a brief interview. Additionally, we registered the time for exercises and monitored users' movement activity during the experience.	https://dl.acm.org/doi/abs/10.1145/3610540.3627008	Dorota Kamińska, Grzegorz Zwolinski, Agnieszka Dubiel
Modeling Dynamic Clothing for Data-Driven Photorealistic Avatars	This thesis presents research on building photorealistic avatars of humans wearing complex clothing in a data-driven manner. Such avatars will be a critical technology to enable future applications such as VR/AR and virtual content creation. Loose-fitting clothing poses a significant challenge for avatar modeling due to its large deformation space. We address the challenge by unifying three components of avatar modeling: model-based statistical prior from pre-captured data, physics-based prior from simulation, and real-time measurement from sparse sensor input. First, we introduce a separate two-layer representation that allows us to disentangle the dynamics between the pose-driven body part and temporally-dependent clothing part. Second, we further combine physics-based cloth simulation with a physics-inspired neural rendering model to generate rich and natural dynamics and appearance even for challenging clothing such as a skirt and a dress. Last, we go beyond pose-driven animation and incorporate online sensor input into the avatars to achieve more faithful telepresence of clothing.	https://dl.acm.org/doi/abs/10.1145/3623053.3623373	Donglai Xiang
Moirai - Thread of Life	Film intertwines cosmic journeys, quantum physics, and fate.	https://dl.acm.org/doi/abs/10.1145/3626964.3626975	Ina Conradi Chavez, Mark Chavez
Monte Carlo Denoising via Multi-scale Auxiliary Feature Fusion Guided Transformer	Deep learning-based single-frame Monte Carlo denoising techniques have demonstrated remarkable results in photo-realistic rendering research. However, the current state-of-the-art methods relying on self-attention mechanisms underutilize auxiliary features and struggle to preserve intricate high-frequency details in complex scenes. Employing a generative adversarial architecture, we present a transformer-based denoising network guided by multi-scale auxiliary feature. The proposed U-shaped denoising network extracts multi-scale texture and geometric features from auxiliaries, modulating them to guide the improved transformer module's denoising process. The improved transformer module employs cross-channel self-attention to capture non-local relationships with near-linear computational complexity. Additionally, a gating mechanism is introduced in the transformer module's feed-forward network, enhancing information flow. Extensive experiments on noisy images with varied per-pixel sampling rates demonstrate the method's superiority in quantitative metrics and visual perception compared with state-of-the-art methods. Our method excels notably in intricate scenes with complex hair and texture details, which are historically challenging to denoise.	https://dl.acm.org/doi/abs/10.1145/3610543.3626179	Bingyi Chen, Zengyu Liu, Li Yuan, Zhitao Liu, Yi Li, Guan Wang, Ning Xie
Morning Shadows	A journey under the light of a brightening black sun.	https://dl.acm.org/doi/abs/10.1145/3626964.3626978	Rita Cruchinho Neves, Pedro Castro Neves
Motion to Dance Music Generation using Latent Diffusion Model	The role of music in games and animation, particularly in dance content, is essential for creating immersive and entertaining experiences. Although recent studies have made strides in generating dance music from videos, their practicality in integrating music into games and animation remains limited. In this context, we present a method capable of generating plausible dance music from 3D motion data and genre labels. Our approach leverages a combination of a UNET-based latent diffusion model and a pre-trained VAE model. To evaluate the performance of the proposed model, we employ evaluation metrics to assess various audio properties, including beat alignment, audio quality, motion-music correlation, and genre score. The quantitative results show that our approach outperforms previous methods. Furthermore, we demonstrate that our model can generate audio that seamlessly fits to in-the-wild motion data. This capability enables us to create plausible dance music that complements dynamic movements of characters and enhances overall audiovisual experience in interactive media. Examples from our proposed model are available at this link: https://dmdproject.github.io/.	https://dl.acm.org/doi/abs/10.1145/3610543.3626164	Vanessa Tan, Junghyun Nam, Juhan Nam, Junyong Noh
Motor-Skill-Transfer Technology for Piano Playing with Electrical Muscle Stimulation	We introduce a motor-skill-transfer technology using electrical muscle stimulation (EMS) for acquiring piano playing skills. While expert pianists use the coordination of multiple muscles, such as fingers and arms, novices are less aware of muscle coordination and tend to only move their fingers. Our EMS-based system encourages them to use their arms as well as their fingers. Based on the analysis of experts' muscle coordination, our system applies EMS to the novices' forearms and shoulders. With this system, novices should be able to improve their motor skills, such as playing octave tremolos by using wrist rotation to reduce fatigue and playing C major scales more smoothly by coordinating forearm and shoulder muscles to execute the thumb-under technique.	https://dl.acm.org/doi/abs/10.1145/3610541.3614570	Arinobu Niijima, Ryosuke Aoki, Yukio Koike, Shinji Miyahara
Multi-Stage Manufacturing for Preoperative Medical Models with Overhanging Components	Medical models play an instrumental role in replicating a patient's unique anatomy, enhancing surgical outcomes, and fostering effective communication between doctors and patients. While 3D printing technology offers bespoke solutions for such models, producing designs with multiple overhanging tissues often requires advanced 3D printers – making it unattainable for conventional FDM fabrication methods. To bridge this gap, we present a cost-efficient, multi-stage, printing-molding hybrid manufacturing technique that incrementally solidifies complex medical models. Leveraging our adaptive optimization algorithm, we determine the optimal molding direction and the most streamlined manufacturing process with the fewest stages. As a testament to our method's efficacy, we successfully printed a liver model featuring overhanging tumors.	https://dl.acm.org/doi/abs/10.1145/3610542.3626132	Mingli Xiang, Zun Li, Lin Lu, Lifang Wu
Multi-color Holograms Improve Brightness in Holographic Displays	Holographic displays generate Three-Dimensional (3D) images by displaying single-color holograms time-sequentially, each lit by a single-color light source. However, representing each color one by one limits brightness in holographic displays. This paper introduces a new driving scheme for realizing brighter images in holographic displays. Unlike the conventional driving scheme, our method utilizes three light sources to illuminate each displayed hologram simultaneously at various intensity levels. In this way, our method reconstructs a multiplanar three-dimensional target scene using consecutive multi-color holograms and persistence of vision. We co-optimize multi-color holograms and required intensity levels from each light source using a gradient descent-based optimizer with a combination of application-specific loss terms. We experimentally demonstrate that our method can increase the intensity levels in holographic displays up to three times, reaching a broader range and unlocking new potentials for perceptual realism in holographic displays.	https://dl.acm.org/doi/abs/10.1145/3610548.3618135	Koray Kavaklı, Liang Shi, Hakan Urey, Wojciech Matusik, Kaan Akşit
Multichannel Haptic Communication Platform with Wearable Sensing and Display	Our goal has been to create an environment in which anyone can take advantage of tactile technology. Content that incorporates tactile sensations in addition to visual and auditory sensations will activate people's interaction with it. On the other hand, both sensing and display are complicated to create high-quality content, and ease of use is lost. In this paper, we propose a platform that can easily develop multi-channel vibrotactile contents by utilizing existing workflows. By preparing a 4-channel fingertip-mounted vibrotactile device as a display, a high-resolution experience with less load during the experience is realized. We demonstrate sensor-based pre-recorded or real-time multi-channel tactile contents and show the concept of near-future tactile technology including use cases.	https://dl.acm.org/doi/abs/10.1145/3610541.3614573	Harunobu Taguchi, Youichi Kamiyama, Kenta Kan, Yulan Ju, Arata Horie, Yoshihiro Tanaka, Hironori Ishikawa, Kouta Minamizawa
Multiple-bounce Smith Microfacet BRDFs using the Invariance Principle	Smith microfacet models are widely used in computer graphics to represent materials. Traditional microfacet models do not consider the multiple bounces on microgeometries, leading to visible energy missing, especially on rough surfaces. Later, as the equivalence between the microfacets and volume has been revealed, random walk solutions have been proposed to introduce multiple bounces, but at the cost of high variance. Recently, the position-free property has been introduced into the multiple-bounce model, resulting in much less noise, but also bias or a complex derivation.In this paper, we propose a simple way to derive the multiple-bounce Smith microfacet bidirectional reflectance distribution functions (BRDFs) using the invariance principle. At the core of our model is a shadowing-masking function for a path consisting of direction collections, rather than separated bounces. Our model ensures unbiasedness and can produce less noise compared to the previous work with equal time, thanks to the simple formulation. Furthermore, we also propose a novel probability density function (PDF) for BRDF multiple importance sampling, which has a better match with the multiple-bounce BRDFs, producing less noise than previous naive approximations.	https://dl.acm.org/doi/abs/10.1145/3610548.3618198	Yuang Cui, Gaole Pan, Jian Yang, Lei Zhang, Ling-Qi Yan, Beibei Wang
Multisource Holography	Holographic displays promise several benefits including high quality 3D imagery, accurate accommodation cues, and compact form-factors. However, holography relies on coherent illumination which can create undesirable speckle noise in the final image. Although smooth phase holograms can be speckle-free, their non-uniform eyebox makes them impractical, and speckle mitigation with partially coherent sources also reduces resolution. Averaging sequential frames for speckle reduction requires high speed modulators and consumes temporal bandwidth that may be needed elsewhere in the system. In this work, we propose multisource holography, a novel architecture that uses an array of sources to suppress speckle in a single frame without sacrificing resolution. By using two spatial light modulators, arranged sequentially, each source in the array can be controlled almost independently to create a version of the target content with different speckle. Speckle is then suppressed when the contributions from the multiple sources are averaged at the image plane. We introduce an algorithm to calculate multisource holograms, analyze the design space, and demonstrate up to a 10 dB increase in peak signal-to-noise ratio compared to an equivalent single source system. Finally, we validate the concept with a benchtop experimental prototype by producing both 2D images and focal stacks with natural defocus cues.	https://dl.acm.org/doi/abs/10.1145/3618380	Grace Kuo, Florian Schiffers, Douglas Lanman, Oliver Cossairt, Nathan Matsuda
MuscleVAE: Model-Based Controllers of Muscle-Actuated Characters	In this paper, we present a simulation and control framework for generating biomechanically plausible motion for muscle-actuated characters. We incorporate a fatigue dynamics model, the 3CC-r model, into the widely-adopted Hill-type muscle model to simulate the development and recovery of fatigue in muscles, which creates a natural evolution of motion style caused by the accumulation of fatigue from prolonged activities. To address the challenging problem of controlling a musculoskeletal system with high degrees of freedom, we propose a novel muscle-space control strategy based on PD control. Our simulation and control framework facilitates the training of a generative model for muscle-based motion control, which we refer to as MuscleVAE. By leveraging the variational autoencoders (VAEs), MuscleVAE is capable of learning a rich and flexible latent representation of skills from a large unstructured motion dataset, encoding not only motion features but also muscle control and fatigue properties. We demonstrate that the MuscleVAE model can be efficiently trained using a model-based approach, resulting in the production of high-fidelity motions and enabling a variety of downstream tasks.	https://dl.acm.org/doi/abs/10.1145/3610548.3618137	Yusen Feng, Xiyan Xu, Libin Liu
My name is Edgar and I have a cow	Edgar's ordinary life is disrupted by a newborn calf he sees on a tourist trip to a slaughterhouse.	https://dl.acm.org/doi/abs/10.1145/3626964.3626982	Filip Diviak, Bara Prikaska
MyStyle++: A Controllable Personalized Generative Prior	In this paper, we propose an approach to obtain a personalized generative prior with explicit control over a set of attributes. We build upon MyStyle, a recently introduced method, that tunes the weights of a pre-trained StyleGAN face generator on a few images of an individual. This system allows synthesizing, editing, and enhancing images of the target individual with high fidelity to their facial features. However, MyStyle does not demonstrate precise control over the attributes of the generated images. We propose to address this problem through a novel optimization system that organizes the latent space in addition to tuning the generator. Our key contribution is to formulate a loss that arranges the latent codes, corresponding to the input images, along a set of specific directions according to their attributes. We demonstrate that our approach, dubbed MyStyle++, is able to synthesize, edit, and enhance images of an individual with great control over the attributes, while preserving the unique facial characteristics of that individual.	https://dl.acm.org/doi/abs/10.1145/3610548.3618171	Libing Zeng, Lele Chen, Yi Xu, Nima Khademi Kalantari
Mākū, te hā o Haupapa: Moisture, the breath of Haupapa	The cracking and melting Haupapa glacier and lake, Aotearoa New Zealand's fastest growing body of water, are presented in a live cast of mākū, life-giving moisture. Tiny bubbles of ancient breath and atmosphere are pressed inside Haupapa's ancient glacial ice - including sea breezes, pollens, carbon dioxide and methane, as well as the ash of Australian fires. Single words and names of the elemental ancestors in Māori elder Ron Bull's voice, recorded live on the lake Haupapa, are woven through the sound and images to gift and acknowledge Kāi Tahu matauraka (knowledge) in a weather-responsive audio-visual installation. The project bridges meteorology, indigenous cosmologies, and science to create an active and unruly response to this rapidly changing icescape. The artists relinquish the ordering and qualities of sound and video to the weather conditions of Aoraki, recorded by NIWA instruments (New Zealand Institute for Water and Atmosphere) in place near the Haupapa glacier, then turned to digital information which feeds live into the installation, subtly altering the brightness, direction, and movement of the images and sounds according to the real-time weather conditions, and wind direction. Depending on the weather, the image changes and the sound and vocal sequence is endlessly variable. On days of high solar radiation, bright, clear ice and sun predominate and also move the images on screen accordingly, on cloudy days, the image darkens. La Niña conditions for the past three years have brought sunny settled weather to this region in the central South Island and melting has accelerated, indicating the changing climate. This installation expresses what it feels like to be inside that ice and water, responsive to heat, rain and bright sunlight.	https://dl.acm.org/doi/abs/10.1145/3610537.3622943	Stefan Marks, Janine Randerson, Rachel Shearer, Ron Bull, Heather Purdie
NVSHU: Virtual Reality Design and Narrative Popularization for Intangible Cultural Heritage Characters	"This project reconstructs and reproduces the ecological context of ""NVSHU"" culture in an immersive (Virtual Reality) VR experience. We explore interactive approaches of communication for this unique intangible heritage context. ""NVSHU"" amplifies the effectiveness of intangible cultural heritage (ICH) preservation through narrative interaction and virtual character popularization techniques, aiming to enhance audiences' experiential learning, and augment their sense of presence and immersion."	https://dl.acm.org/doi/abs/10.1145/3610549.3614615	Xuanmiao Zhang, Linqi Sun, Shuo Yan
Neural Caches for Monte Carlo Partial Differential Equation Solvers	This paper presents a method that uses neural networks as a caching mechanism to reduce the variance of Monte Carlo Partial Differential Equation solvers, such as the Walk-on-Spheres algorithm [Sawhney and Crane 2020]. While these Monte Carlo PDE solvers have the merits of being unbiased and discretization-free, their high variance often hinders real-time applications. On the other hand, neural networks can approximate the PDE solution, and evaluating these networks at inference time can be very fast. However, neural-network-based solutions may suffer from convergence difficulties and high bias. Our hybrid system aims to combine these two potentially complementary solutions by training a neural field to approximate the PDE solution using supervision from a WoS solver. This neural field is then used as a cache in the WoS solver to reduce variance during inference. We demonstrate that our neural field training procedure is better than the commonly used self-supervised objectives in the literature. We also show that our hybrid solver exhibits lower variance than WoS with the same computational budget: it is significantly better for small compute budgets and provides smaller improvements for larger budgets, reaching the same performance as WoS in the limit.	https://dl.acm.org/doi/abs/10.1145/3610548.3618141	Zilu Li, Guandao Yang, Xi Deng, Christopher De Sa, Bharath Hariharan, Steve Marschner
Neural Categorical Priors for Physics-Based Character Control	Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with improved motion quality and diversity over existing methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervision of the encoder's output, it follows the original motion clip distribution in the dataset and could lead to imbalanced behaviors in our setting. To address the issue, we further propose a technique named prior shifting to adjust the prior distribution using curiosity-driven RL. The outcome distribution is demonstrated to offer sufficient behavioral diversity and significantly facilitates upper-level policy learning for downstream tasks. We conduct comprehensive experiments using humanoid characters on two challenging downstream tasks, sword-shield striking and two-player boxing game. Our results demonstrate that the proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism. Videos, codes, and data are available at https://tencent-roboticsx.github.io/NCP/.	https://dl.acm.org/doi/abs/10.1145/3618397	Qingxu Zhu, He Zhang, Mengting Lan, Lei Han
Neural Collision Fields for Triangle Primitives	We present neural collision fields as an alternative to contact point sampling in physics simulations. Our approach is built on top of a novel smoothed integral formulation for the contact surface patches between two triangle meshes. By reformulating collisions as an integral, we avoid issues of sampling common to many collision-handling algorithms. Because the resulting integral is difficult to evaluate numerically, we store its solution in an integrated neural collision field — a 6D neural field in the space of triangle pair vertex coordinates. Our network generalizes well to new triangle meshes without retraining. We demonstrate the effectiveness of our method by implementing it as a constraint in a position-based dynamics framework and show that our neural formulation successfully handles collisions in practical simulations involving both volumetric and thin-shell geometries.	https://dl.acm.org/doi/abs/10.1145/3610548.3618225	Ryan S. Zesch, Vismay Modi, Shinjiro Sueda, David I.W. Levin
Neural Field Convolutions by Repeated Differentiation	Neural fields are evolving towards a general-purpose continuous representation for visual computing. Yet, despite their numerous appealing properties, they are hardly amenable to signal processing. As a remedy, we present a method to perform general continuous convolutions with general continuous signals such as neural fields. Observing that piecewise polynomial kernels reduce to a sparse set of Dirac deltas after repeated differentiation, we leverage convolution identities and train a repeated integral field to efficiently execute large-scale convolutions. We demonstrate our approach on a variety of data modalities and spatially-varying kernels.	https://dl.acm.org/doi/abs/10.1145/3618340	Ntumba Elie Nsampi, Adarsh Djeacoumar, Hans-Peter Seidel, Tobias Ritschel, Thomas Leimkühler
Neural Gradient Learning and Optimization for Oriented Point Normal Estimation	We propose Neural Gradient Learning (NGL), a deep learning approach to learn gradient vectors with consistent orientation from 3D point clouds for normal estimation. It has excellent gradient approximation properties for the underlying geometry of the data. We utilize a simple neural network to parameterize the objective function to produce gradients at points using a global implicit representation. However, the derived gradients usually drift away from the ground-truth oriented normals due to the lack of local detail descriptions. Therefore, we introduce Gradient Vector Optimization (GVO) to learn an angular distance field based on local plane geometry to refine the coarse gradient vectors. Finally, we formulate our method with a two-phase pipeline of coarse estimation followed by refinement. Moreover, we integrate two weighting functions, i.e., anisotropic kernel and inlier score, into the optimization to improve the robust and detail-preserving performance. Our method efficiently conducts global gradient approximation while achieving better accuracy and generalization ability of local feature description. This leads to a state-of-the-art normal estimator that is robust to noise, outliers and point density variations. Extensive evaluations show that our method outperforms previous works in both unoriented and oriented normal estimation on widely used benchmarks. The source code and pre-trained models are available at https://github.com/LeoQLi/NGLO .	https://dl.acm.org/doi/abs/10.1145/3610548.3618253	Qing Li, Huifang Feng, Kanle Shi, Yi Fang, Yu-Shen Liu, Zhizhong Han
Neural Metamaterial Networks for Nonlinear Material Design	Nonlinear metamaterials with tailored mechanical properties have applications in engineering, medicine, robotics, and beyond. While modeling their macromechanical behavior is challenging in itself, finding structure parameters that lead to ideal approximation of high-level performance goals is a challenging task. In this work, we propose Neural Metamaterial Networks (NMN)---smooth neural representations that encode the nonlinear mechanics of entire metamaterial families. Given structure parameters as input, NMN return continuously differentiable strain energy density functions, thus guaranteeing conservative forces by construction. Though trained on simulation data, NMN do not inherit the discontinuities resulting from topo-logical changes in finite element meshes. They instead provide a smooth map from parameter to performance space that is fully differentiable and thus well-suited for gradient-based optimization. On this basis, we formulate inverse material design as a nonlinear programming problem that leverages neural networks for both objective functions and constraints. We use this approach to automatically design materials with desired strain-stress curves, prescribed directional stiffness and Poisson ratio profiles. We furthermore conduct ablation studies on network nonlinearities and show the advantages of our approach compared to native-scale optimization.	https://dl.acm.org/doi/abs/10.1145/3618325	Yue Li, Stelian Coros, Bernhard Thomaszewski
Neural Motion Graph	Deep learning techniques have been employed to design a controllable human motion synthesizer. Despite their potential, however, designing a neural network-based motion synthesis that enables flexible user interaction, fine-grained controllability, and the support of new types of motions at reduced time and space consumption costs remains a challenge. In this paper, we propose a novel approach, a neural motion graph, that addresses the challenge by enabling scalability to new motions while using compact neural networks. Our approach represents each type of motion with a separate neural node to reduce the cost of adding new motion types. In addition, designing a separate neural node for each motion type enables task-specific control strategies and has greater potential to achieve a high-quality synthesis of complex motions, such as the Mongolian dance. Furthermore, a single transition network, which acts as neural edges, is used to model the transition between two motion nodes. The transition network is designed with a lightweight control module to achieve a fine-grained response to user control signals. Overall, the design choice makes the neural motion graph highly controllable and scalable. In addition to being fully flexible to user interaction through high-level and fine-grained user-control signals, our experimental and subjective evaluation results demonstrate that our proposed approach, neural motion graph, outperforms state-of-the-art human motion synthesis methods in terms of the quality of controlled motion generation.	https://dl.acm.org/doi/abs/10.1145/3610548.3618181	Hongyu Tao, Shuaiying Hou, Changqing Zou, Hujun Bao, Weiwei Xu
Neural Packing: from Visual Sensing to Reinforcement Learning	We present a novel learning framework to solve the transport-and-packing (TAP) problem in 3D. It constitutes a full solution pipeline from partial observations of input objects via RGBD sensing and recognition to final box placement, via robotic motion planning, to arrive at a compact packing in a target container. The technical core of our method is a neural network for TAP, trained via (RL), to solve the NP-hard combinatorial optimization problem. Our network simultaneously selects an object to pack and determines the final packing location, based on a judicious encoding of the continuously evolving states of partially observed source objects and available spaces in the target container, using separate encoders both enabled with attention mechanisms. The encoded feature vectors are employed to compute the matching scores and feasibility masks of different pairings of box selection and available space configuration for packing strategy optimization. Extensive experiments, including ablation studies and physical packing execution by a real robot (Universal Robot UR5e), are conducted to evaluate our method in terms of its design choices, scalability, generalizability, and comparisons to baselines, including the most recent RL-based TAP solution. We also contribute the first benchmark for TAP which covers a variety of input settings and difficulty levels.	https://dl.acm.org/doi/abs/10.1145/3618354	Juzhan Xu, Minglun Gong, Hao Zhang, Hui Huang, Ruizhen Hu
Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar	Rendering photorealistic and dynamically moving human heads is crucial for ensuring a pleasant and immersive experience in AR/VR and video conferencing applications. However, existing methods often struggle to model challenging facial regions (e.g., mouth interior, eyes, and beard), resulting in unrealistic and blurry results. In this paper, we propose Neural Point-based Volumetric Avatar (NPVA), a method that adopts the neural point representation as well as the neural volume rendering process and discards the predefined connectivity and hard correspondence imposed by mesh-based approaches. Specifically, the neural points are strategically constrained around the surface of the target expression via a high-resolution UV displacement map, achieving increased modeling capacity and more accurate control. We introduce three technical innovations to improve the rendering and training efficiency: a patch-wise depth-guided (shading point) sampling strategy, a lightweight radiance decoding process, and a Grid-Error-Patch (GEP) ray sampling strategy during training. By design, our NPVA is better equipped to handle topologically changing regions and thin structures while also ensuring accurate expression control when animating avatars. Experiments conducted on three subjects from the Multiface dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods, especially in handling challenging facial regions.	https://dl.acm.org/doi/abs/10.1145/3610548.3618204	Cong Wang, Di Kang, Yan-Pei Cao, Linchao Bao, Ying Shan, Song-Hai Zhang
Neural Spectro-polarimetric Fields	Modeling the spatial radiance distribution of light rays in a scene has been extensively explored for applications, including view synthesis. Spectrum and polarization, the wave properties of light, are often neglected due to their integration into three RGB spectral bands and their non-perceptibility to human vision. However, these properties are known to encompass substantial material and geometric information about a scene. Here, we propose to model spectro-polarimetric fields, the spatial Stokes-vector distribution of any light ray at an arbitrary wavelength. We present Neural Spectro-polarimetric Fields (NeSpoF), a neural representation that models the physically-valid Stokes vector at given continuous variables of position, direction, and wavelength. NeSpoF manages inherently noisy raw measurements, showcases memory efficiency, and preserves physically vital signals — factors that are crucial for representing the high-dimensional signal of a spectro-polarimetric field. To validate NeSpoF, we introduce the first multi-view hyperspectral-polarimetric image dataset, comprised of both synthetic and real-world scenes. These were captured using our compact hyperspectral-polarimetric imaging system, which has been calibrated for robustness against system imperfections. We demonstrate the capabilities of NeSpoF on diverse scenes.	https://dl.acm.org/doi/abs/10.1145/3610548.3618172	Youngchan Kim, Wonjoon Jin, Sunghyun Cho, Seung-Hwan Baek
Neural Stochastic Poisson Surface Reconstruction	Reconstructing a surface from a point cloud is an underdetermined problem. We use a neural network to study and quantify this reconstruction uncertainty under a Poisson smoothness prior. Our algorithm addresses the main limitations of existing work and can be fully integrated into the 3D scanning pipeline, from obtaining an initial reconstruction to deciding on the next best sensor position and updating the reconstruction upon capturing more data.	https://dl.acm.org/doi/abs/10.1145/3610548.3618162	Silvia Sellán, Alec Jacobson
Neural Stress Fields for Reduced-order Elastoplasticity and Fracture	We propose a hybrid neural network and physics framework for reduced-order modeling of elastoplasticity and fracture. State-of-the-art scientific computing models like the Material Point Method (MPM) faithfully simulate large-deformation elastoplasticity and fracture mechanics. However, their long runtime and large memory consumption render them unsuitable for applications constrained by computation time and memory usage, e.g., virtual reality. To overcome these barriers, we propose a reduced-order framework. Our key innovation is training a low-dimensional manifold for the Kirchhoff stress field via an implicit neural representation. This low-dimensional neural stress field (NSF) enables efficient evaluations of stress values and, correspondingly, internal forces at arbitrary spatial locations. In addition, we also train neural deformation and affine fields to build low-dimensional manifolds for the deformation and affine momentum fields. These neural stress, deformation, and affine fields share the same low-dimensional latent space, which uniquely embeds the high-dimensional simulation state. After training, we run new simulations by evolving in this single latent space, which drastically reduces the computation time and memory consumption. Our general continuum-mechanics-based reduced-order framework is applicable to any phenomena governed by the elastodynamics equation. To showcase the versatility of our framework, we simulate a wide range of material behaviors, including elastica, sand, metal, non-Newtonian fluids, fracture, contact, and collision. We demonstrate dimension reduction by up to 100,000 × and time savings by up to 10 ×.	https://dl.acm.org/doi/abs/10.1145/3610548.3618207	Zeshun Zong, Xuan Li, Minchen Li, Maurizio M. Chiaramonte, Wojciech Matusik, Eitan Grinspun, Kevin Carlberg, Chenfanfu Jiang, Peter Yichen Chen
Neural-Singular-Hessian: Implicit Neural Representation of Unoriented Point Clouds by Enforcing Singular Hessian	Neural implicit representation is a promising approach for reconstructing surfaces from point clouds. Existing methods combine various regularization terms, such as the Eikonal and Laplacian energy terms, to enforce the learned neural function to possess the properties of a Signed Distance Function (SDF). However, inferring the actual topology and geometry of the underlying surface from poor-quality unoriented point clouds remains challenging. In accordance with Differential Geometry, the Hessian of the SDF is singular for points within the differential thin-shell space surrounding the surface. Our approach enforces the Hessian of the neural implicit function to have a zero determinant for points near the surface. This technique aligns the gradients for a near-surface point and its on-surface projection point, producing a rough but faithful shape within just a few iterations. By annealing the weight of the singular-Hessian term, our approach ultimately produces a high-fidelity reconstruction result. Extensive experimental results demonstrate that our approach effectively suppresses ghost geometry and recovers details from unoriented point clouds with better expressiveness than existing fitting-based methods.	https://dl.acm.org/doi/abs/10.1145/3618311	Zixiong Wang, Yunxiao Zhang, Rui Xu, Fan Zhang, Peng-Shuai Wang, Shuangmin Chen, Shiqing Xin, Wenping Wang, Changhe Tu
NinjaHeads: Gaze-Oriented Parallel View System for Asynchronous Tasks	"Virtual parallel experience enables a person in Virtual Reality (VR) to receive multiple and concurrent perspectives. It also enables the person to synchronously participate in activities or solve multiple tasks at once. This research focuses on utilizing parallel views for asynchronous tasks by providing the user with intuitively extended view perspectives driven by gaze. In this demonstration, we present NinjaHeads, a parallel view system that generates four additional perspectives ""parallel views"", based on the user's gaze point. We introduce a technique where a user can manipulate parallel views and their behaviors through eye, head, and hand movements. This allows the user to concurrently examine an object or a point of interest from different perspectives, as if the user has more than a pair of eyes."	https://dl.acm.org/doi/abs/10.1145/3610549.3614593	Theophilus Teo, Maki Sugimoto, Gun Lee, Mark Billinghurst
NodeGit: Diffing and Merging Node Graphs	The use of version control is pervasive in collaborative software projects. Version control systems are based on two primary operations: diffing two versions to compute the change between them and merging two versions edited concurrently. Recent works provide solutions to diff and merge graphics assets such as images, meshes and scenes. In this work, we present a practical algorithm to diff and merge procedural programs written as node graphs. To obtain more precise diffs, we version the graphs directly rather than their textual representations. Diffing graphs is equivalent to computing the graph edit distance, which is known to be computationally infeasible. Following prior work, we propose an approximate algorithm tailored to our problem domain. We validate the proposed algorithm by applying it both to manual edits and to a large set of randomized modifications of procedural shapes and materials. We compared our method with existing state-of-the-art algorithms, showing that our approach is the only one that reliably detects user edits.	https://dl.acm.org/doi/abs/10.1145/3618343	Eduardo Rinaldi, Davide Sforza, Fabio Pellacini
Non-Newtonian ViRheometry via Similarity Analysis	We estimate the three Herschel-Bulkley parameters (yield stress , power-law index , and consistency parameter ) for shear-dependent fluid-like materials possibly with large-scale inclusions, for which rheometers may fail to provide a useful measurement. We perform experiments using the unknown material for dam-break (or column collapse) setups and capture video footage. We then use simulations to optimize for the material parameters. For better match up with the simple shear flow encountered in a rheometer, we modify the flow rule for the elasto-viscoplastic Herschel-Bulkley model. Analyzing the loss landscape for optimization, we realize a ; material parameters far away within this relation would result in matched simulations, making it hard to distinguish the parameters. We found that by exploiting the setup dependency of the similarity relation, we can improve on the estimation using multiple setups, which we propose by analyzing the Hessian of the similarity relation. We validate the efficacy of our method by comparing the estimations to the measurements from a rheometer (for materials without large-scale inclusions) and show applications to materials with large-scale inclusions, including various salad or pasta sauces, and congee.	https://dl.acm.org/doi/abs/10.1145/3618310	Mitsuki Hamamichi, Kentaro Nagasawa, Masato Okada, Ryohei Seto, Yonghao Yue
Nonlinear Ray Tracing for Displacement and Shell Mapping	Displacement mapping and shell mapping add fine-scale geometric features to meshes and can significantly enhance the realism of an object's surface representation. Both methods generate geometry within a layer between the base mesh and its offset mesh called a shell. It is not easy to simultaneously achieve high ray tracing performance, low memory consumption, interactive feedback, and ease of implementation, partly because the mapping between shell and texture space is nonlinear. This paper introduces a new efficient approach to perform acceleration structure traversal and intersection tests against microtriangles entirely in texture space by formulating nonlinear rays as degree-2 rational functions. Our method simplifies the implementation of tessellation-free displacement mapping and smooth shell mapping and works even if base mesh triangles are degenerated in uv space.	https://dl.acm.org/doi/abs/10.1145/3610548.3618199	Shinji Ogaki
Object Motion Guided Human Motion Synthesis	Modeling human behaviors in contextual environments has a wide range of applications in character animation, embodied AI, VR/AR, and robotics. In real-world scenarios, humans frequently interact with the environment and manipulate various objects to complete daily tasks. In this work, we study the problem of full-body human motion synthesis for the manipulation of large-sized objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a conditional diffusion framework that can generate full-body manipulation behaviors from only the object motion. Since naively applying diffusion models fails to precisely enforce contact constraints between the hands and the object, OMOMO learns two separate denoising processes to first predict hand positions from object motion and subsequently synthesize full-body poses based on the predicted hand positions. By employing the hand positions as an intermediate representation between the two denoising processes, we can explicitly enforce contact constraints, resulting in more physically plausible manipulation motions. With the learned model, we develop a novel system that captures full-body human manipulation motions by simply attaching a smartphone to the object being manipulated. Through extensive experiments, we demonstrate the effectiveness of our proposed pipeline and its ability to generalize to unseen objects. Additionally, as high-quality human-object interaction datasets are scarce, we collect a large-scale dataset consisting of 3D object geometry, object motion, and human motion. Our dataset contains human-object interaction motion for 15 objects, with a total duration of approximately 10 hours.	https://dl.acm.org/doi/abs/10.1145/3618333	Jiaman Li, Jiajun Wu, C. Karen Liu
Octopus Ninja	Octopus ninja is on a mission to rescue his family.	https://dl.acm.org/doi/abs/10.1145/3626964.3626970	Donghoe Kim
Online Scene CAD Recomposition via Autonomous Scanning	Autonomous surface reconstruction of 3D scenes has been intensely studied in recent years, however, it is still difficult to accurately reconstruct all the surface details of complex scenes with complicated object relations and severe occlusions, which makes the reconstruction results not suitable for direct use in applications such as gaming and virtual reality. Therefore, instead of reconstructing the detailed surfaces, we aim to recompose the scene with CAD models retrieved from a given dataset to faithfully reflect the object geometry and arrangement in the given scene. Moreover, unlike most of the previous works on scene CAD recomposition requiring an offline reconstructed scene or captured video as input, which leads to significant data redundancy, we propose a novel online scene CAD recomposition method with autonomous scanning, which efficiently recomposes the scene with the guidance of automatically optimized Next-Best-View (NBV) in a single online scanning pass. Based on the key observation that spatial relation in the scene can not only constrain the object pose and layout optimization but also guide the NBV generation, our system consists of two key modules: relation-guided CAD recomposition module that uses relation-constrained global optimization to get accurate object pose and layout estimation, and relation-aware NBV generation module that makes the exploration during the autonomous scanning tailored for our composition task. Extensive experiments have been conducted to show the superiority of our method over previous methods in scanning efficiency and retrieval accuracy as well as the importance of each key component of our method.	https://dl.acm.org/doi/abs/10.1145/3618339	Changhao Li, Junfu Guo, Ruizhen Hu, Ligang Liu
OpenSVBRDF: A Database of Measured Spatially-Varying Reflectance	We present the first large-scale database of measured spatially-varying anisotropic reflectance, consisting of 1,000 high-quality near-planar SVBRDFs, spanning 9 material categories such as wood, fabric and metal. Each sample is captured in 15 minutes, and represented as a set of high-resolution texture maps that correspond to spatially-varying BRDF parameters and local frames. To build this database, we develop a novel integrated system for robust, high-quality and -efficiency reflectance acquisition and reconstruction. Our setup consists of 2 cameras and 16,384 LEDs. We train 64 lighting patterns for efficient acquisition, in conjunction with a network that predicts per-point reflectance in a neural representation from carefully aligned two-view measurements captured under the patterns. The intermediate results are further fine-tuned with respect to the photographs acquired under 63 effective linear lights, and finally fitted to a BRDF model. We report various statistics of the database, and demonstrate its value in the applications of material generation, classification as well as sampling. All related data, including future additions to the database, can be downloaded from https://opensvbrdf.github.io/.	https://dl.acm.org/doi/abs/10.1145/3618358	Xiaohe Ma, Xianmin Xu, Leyao Zhang, Kun Zhou, Hongzhi Wu
Optimal Design of Robotic Character Kinematics	The kinematic motion of a robotic character is defined by its mechanical joints and actuators that restrict the relative motion of its rigid components. Designing robots that perform a given target motion as closely as possible with a fixed number of actuated degrees of freedom is challenging, especially for robots that form kinematic loops. In this paper, we propose a technique that simultaneously solves for optimal design and control parameters for a robotic character whose design is parameterized with configurable joints. At the technical core of our technique is an efficient solution strategy that uses dynamic programming to solve for optimal state, control, and design parameters, together with a strategy to remove redundant constraints that commonly exist in general robot assemblies with kinematic loops. We demonstrate the efficacy of our approach by either editing the design of an existing robotic character, or by optimizing the design of a new character to perform a desired motion.	https://dl.acm.org/doi/abs/10.1145/3618404	Guirec Maloisel, Christian Schumacher, Espen Knoop, Ruben Grandia, Moritz Bächer
OwnDiffusion: A Design Pipeline Using Design Generative AI to preserve Sense Of Ownership	Generative Artificial Intelligence (AI) has been a fast-growing technology, well known for generating high-quality design drawings and images in seconds with a simple text input. However, users often feel uncertain about whether generative art should be considered created by AI or by themselves. Losing the sense of ownership of the outcome might impact the learning process and confidence of novice designers and design learners who seek to benefit from using Generative Design Tools. In this context, we propose OwnDiffusion, a design pipeline that utilizes Generative AI to assist in the physical prototype ideation process for novice product designers and industrial design learners while preserving their sense of ownership. The pipeline incorporates a prompt weight assessing tool, allowing designers to fine-tune the AI's input based on their sense of ownership. We envision this method as a solution for AI-assisted design, enabling designers to maintain confidence in their creativity and ownership of a design.	https://dl.acm.org/doi/abs/10.1145/3610542.3626142	Yaokun Wu, Minamizawa Kouta, Pai Yun Suen
PSDR-Room: Single Photo to Scene using Differentiable Rendering	A 3D digital scene contains many components: lights, materials and geometries, interacting to reach the desired appearance. Staging such a scene is time-consuming and requires both artistic and technical skills. In this work, we propose PSDR-Room, a system allowing to optimize lighting as well as the pose and materials of individual objects to match a target image of a room scene, with minimal user input. To this end, we leverage a recent path-space differentiable rendering approach that provides unbiased gradients of the rendering with respect to geometry, lighting, and procedural materials, allowing us to optimize all of these components using gradient descent to visually match the input photo appearance. We use recent single-image scene understanding methods to initialize the optimization and search for appropriate 3D models and materials. We evaluate our method on real photographs of indoor scenes and demonstrate the editability of the resulting scene components.	https://dl.acm.org/doi/abs/10.1145/3610548.3618165	Kai Yan, Fujun Luan, Miloš Hašan, Thibault Groueix, Valentin Deschaintre, Shuang Zhao
Paumo D'amour	Instead of giving his wife the attention she deserves, Dédé sends all his love to his vegetable garden. One day, all his delicious tomatoes mysteriously disappear.	https://dl.acm.org/doi/abs/10.1145/3626964.3626994	Ian Halley, Nathan Hermouet, Luka Croisez, Laurène Vaubourdolle, Jade Van De Veire, Philippe Meis
Pedagogical strategies for teaching Virtual Production pipelines	The incorporation of LED walls and virtual production tools in the film industry is a recent development that has significant pedagogical implications (BLISTEIN, 2020; FARID and TORRALBA, 2021). The use of LED walls combines physical and digital realities, potentially reducing post-production time and resource usage (ONG, 2020). Virtual production can facilitate immediate on-set decision-making and mitigate the need for certain post-production adjustments (EPIC GAMES, 2020). MIT researchers posit that such practices may also reduce the carbon footprint associated with location filming (FARID and TORRALBA, 2021). Nonetheless, further research and innovation are needed to overcome any limitations and fully harness the potential of this technology. The utilization of Unreal Engine software and other real-time tools in visual effects education aligns with industry trends and enhances student preparedness for professional practice (FLEISCHER, 2020). It has been suggested that engaging students with these tools can foster an understanding of the virtual production pipeline, thus aligning the educational curriculum with evolving industry standards (BALSAMO et al., 2021). As educators at Auckland University of Technology, we recognized the necessity of early integration of these paradigm-shifting tools into our curriculum to prepare students for impending and ongoing industry changes. In anticipation of procuring LED walls, we explored the potential of leveraging existing resources to initiate a pedagogical foray into the virtual production sphere. The available resources encompassed a large green screen studio, a motion capture studio, and virtual reality (VR) headsets and trackers. This paper offers two pedagogical responses that were deployed in courses situated in the final (third) year of undergraduate studies for an Animation, Visual Effects and Game Design Major, Bachelor of Design, at the School of Art and Design, Auckland University of Technology. One course is positioned within a motion capture minor and the other an option within a major capstone project framework. These two responses form case studies in the technical modification of existing teaching equipment and the need to push the limits of existing software and hardware resources within the attendant budgetary constraints of a tertiary education institution. This, in turn, is in the service of meeting shifting tertiary curriculum demands in response to a technologically fluid and future-focused industry.	https://dl.acm.org/doi/abs/10.1145/3610540.3627010	Gregory Bennett, Hossein Najafi, Lee Jackson
Penumbra2.0	"Penumbra2.0 is an AI immersive art installation exploring the visualisation of an unpredictable extreme wildfire scenario using a pyro-aesthetic, an aesthetic based on the perceptual qualities of this fire type. Currently seen in Canada, these wildfires are extreme in their speed and scale. They are unpredictable as their behaviour moves across terrains in unforeseen ways, unlike the linear and predictable paths of bushfires. Using geo-located data, Penumbra2.0 recreates an actual wildfire in the Vosges mountains, France, 2020. It has been collaboratively developed by art, AI and fire researchers. Penumbra2.0 forms part of a larger research program entitled iFire. iFire consists of an artistic and scientific project series, the Penumbra series comprising the artistic and Umbra the scientific. Both use the same database of atmospheres, flora, pyro-histories and topographies. Penumbra explores the palpable and sensorial qualities of wildfire experiences, while Umbra investigates the dynamic variables of wildfire events. To amplify the evocative viscerality of these encounters, Penumbra is rendered in monochrome. To underscore its complex pyro-turbulent processes, Umbra is rendered in color. Penumbra2 0 investigates pyro-aesthetics as a two-way dialogue between the viewer and a fire-laden landscape, rather than a linear relationship between an active human protagonist and a passive ""natural disaster"". It aims to model the uncertainties that characterize such exchanges. On the one hand, the actions of the user, and on the other, the fire's behavior. As the user traverses the landscape, they attempt to control their perspective through their movement and orientation of gaze. The fire responds in autonomous ways, by changing its behavior. Conversely, unexpected changes in the wildfire induce shifts in the user's actions as they attempt to manage the uncertainty."	https://dl.acm.org/doi/abs/10.1145/3610537.3622946	Dennis Del Favero, Yang Song, Khalid Moinuddin, Charles Green, Jason Sharples, Alex Ong, Navin Brohier
Perceptual Requirements for World-Locked Rendering in AR and VR	Stereoscopic, head-tracked display systems can show users realistic, world-locked virtual objects and environments. However, discrepancies between the rendering pipeline and physical viewing conditions can lead to perceived instability in the rendered content resulting in reduced immersion and, potentially, visually-induced motion sickness. Precise requirements to achieve perceptually stable world-locked rendering (WLR) are unknown due to the challenge of constructing a wide field of view, distortion-free display with highly accurate head and eye tracking. We present a system capable of rendering virtual objects over real-world references without perceivable drift under such constraints. This platform is used to study acceptable errors in render camera position for WLR in augmented and virtual reality scenarios, where we find an order of magnitude difference in perceptual sensitivity. We conclude with an analytic model which examines changes to apparent depth and visual direction in response to camera displacement errors.	https://dl.acm.org/doi/abs/10.1145/3610548.3618134	Phillip Guan, Eric Penner, Joel Hegland, Benjamin Letham, Douglas Lanman
Perceptual error optimization for Monte Carlo animation rendering	Independently estimating pixel values in Monte Carlo rendering results in a perceptually sub-optimal white-noise distribution of error in image space. Recent works have shown that perceptual fidelity can be improved significantly by distributing pixel error as blue noise instead. Most such works have focused on static images, ignoring the temporal perceptual effects of animation display. We extend prior formulations to simultaneously consider the spatial and temporal domains, and perform an analysis to motivate a perceptually better spatio-temporal error distribution. We then propose a practical error optimization algorithm for spatio-temporal rendering and demonstrate its effectiveness in various configurations.	https://dl.acm.org/doi/abs/10.1145/3610548.3618146	Miša Korać, Corentin Salaün, Iliyan Georgiev, Pascal Grittmann, Philipp Slusallek, Karol Myszkowski, Gurprit Singh
Perceptually Adaptive Real-Time Tone Mapping	Tone mapping operators aim to remap content to a display's dynamic range. Virtual reality is a popular new display modality that has significant differences from other media, making the use of traditional tone mapping techniques difficult. Moreover, real-time adaptive estimation of tone curves that faithfully maintain appearance remains a significant challenge. In this work, we propose a real-time perceptual contrast-matching framework, that allows us to optimally remap scenes for target displays. Our framework is optimized for efficiency and runs on a mobile Quest 2 headset in under 1ms per frame. A subjective study on an HDR-VR prototype demonstrates our method's effectiveness across a wide range of display luminances, producing imagery that is preferred to alternatives tone mapped at peak luminances an order of magnitude higher. This result highlights the importance of good tone mapping for visual quality in VR.	https://dl.acm.org/doi/abs/10.1145/3610548.3618222	Taimoor Tariq, Nathan Matsuda, Eric Penner, Jerry Jia, Douglas Lanman, Ajit Ninan, Alexandre Chapiro
PerfectDart: Automatic Dart Design for Garment Fitting	Dart, a triangle-shaped folded and stitched tuck in a garment, is a common sewing technique used to provide custom-fit garments. Unfortunately, designing and optimally placing these darts requires knowledge and practice, making it challenging for novice users. We propose a novel computational dart design framework that takes rough user cues (the region where the dart will be inserted) and computes the optimal dart configurations to improve fitness. To be more specific, our framework utilizes the body-garment relationship to quantify the fitting using a novel energy composed of three geometric terms: 1) closeness term encoding the proximity between the garment and the target body, 2) stretchability term favouring area-preserving cloth deformation, and 3) smoothness term promoting an unwrinkled and unfolded garment. We evaluate these three geometric terms via off-the-shelf cloth simulation and use it to optimize the dart configuration by minimizing the energy. As demonstrated by our results, our method is able to automatically generate darts to improve fitness for various garment designs and a wide range of body shapes, including animals.	https://dl.acm.org/doi/abs/10.1145/3610543.3626154	Charles de Malefette, Anran Qi, Amal Dev Parakkat, Marie-Paule Cani, Takeo Igarashi
Perinatal Dreaming - Understanding Country	Perinatal Dreaming is a ground-breaking virtual reality experience developed by fEEL felt Experience and Empathy Lab (University of New South Wales Sydney) and led by midwife, nurse, artist and trauma support worker, Marianne Wobcke. As one of thousands of Indigenous children forcibly removed from her mother at birth, Marianne has spent her career researching and supporting perinatal and intergenerational trauma, focusing especially on work with mothers and babies. The VR artwork presents a visually stunning immersive audio-visual experience evoking early life in the womb and entry into the world, taking us through experiences the 'good' and 'toxic' womb and first encounters with breast, skin and the world. Designed as a unique art experience, the piece can also be used in conjunction with therapeutic work.	https://dl.acm.org/doi/abs/10.1145/3610549.3614603	Volker Kuchelmeister, Jill Bennett, Marianne Wobcke, Lucia Barrera, Glenn Barry, Naomi Sunderland, Phil Graham
Phantom Walls : Spatial perception and navigation without vision	"""Phantom Walls"" is a novel technique that establishes continuous spatial perception without vision. By creating an auditory environment where obstacles emit sounds, users can perceive and navigate around visually imperceptible ""Phantom"" obstacles by listening to the generated ""soundscapes"". This method allows individuals to perceive and avoid these obstacles while walking without vision."	https://dl.acm.org/doi/abs/10.1145/3610541.3614578	Takumi Ikeda, Sohei Wakisaka, Kouta Minamizawa
Plastic Landscape - The Reversible World	"""Plastic Landscape - The Reversible World"" is an Al-generated 3D animated video design that shows the apocalyptic and surreal world surrounded by artificial plastic mixtures and objects in the ocean, urban city, Antarctica, and forest. Four different scenes are animated, with the camera panning slowly from left to right. Viewers can observe how the plastics are decomposed at a slower speed by looking at particle animations. Sound is created by the data of the decomposition of plastics. Different types of plastics and speed of decomposition determine the frequency, amplitude, and parameters of audio synthesis. This scene animation is inspired by Ilwalobongbyeong (a folding screen) behind the king's throne of the Joseon Dynasty. This animation depicts the twist of the landscape. Surreal objects/buildings in this animation made out of plastic look beautiful and mesmerizing at first glance. However, the viewers can notice that they are the decayed objects and destroyed nature impacted by human beings. This new multi-sensory artwork addresses the awareness of plastic pollution through the apocalyptic lens."	https://dl.acm.org/doi/abs/10.1145/3610537.3622949	Yoon Chung Han, Seong-Lyun Kim, Hung Tsai
Portrait Expression Editing With Mobile Photo Sequence	Mobile cameras have revolutionized content creation, allowing casual users to capture professional-looking photos. However, capturing the perfect moment can still be challenging, making post-capture editing desirable. In this work, we introduce ExShot, a mobile-oriented expression editing system that delivers high-quality, fast, and interactive editing experiences. Unlike existing methods that rely on learning expression priors, we leverage mobile photo sequences to extract expression information on demand. This design insight enables ExShot to address challenges related to diverse expressions, facial details, environment entanglement, and interactive editing. At the core lies ExprNet, a lightweight deep learning model that extracts and refines expression features. To train our model, we captured portrait images with diverse expressions, incorporating pre-processing and lighting augmentation techniques to ensure data quality. Our comprehensive evaluation results demonstrate that ExShot outperforms other editing approaches by up to 29.02% in PSNR. Ablation studies validate the effectiveness of our design choices, and user studies with 28 participants confirm the strong desire for expression editing and the superior synthesis quality of ExShot, while also identifying areas for further investigation.	https://dl.acm.org/doi/abs/10.1145/3610543.3626160	Yiqin Zhao, Rohit Pandey, Yinda Zhang, Ruofei Du, Feitong Tan, Chetan Ramaiah, Tian Guo, Sean Fanello
Pose and Skeleton-aware Neural IK for Pose and Motion Editing	Posing a 3D character for film or game is an iterative and laborious process where many control handles (e.g. joints) need to be manipulated to achieve a compelling result. Neural Inverse Kinematics (IK) is a new type of IK that enables sparse control over a 3D character pose, and leverages full body correlations to complete the un-manipulated joints of the body. While neural IK is promising, current methods are not designed to preserve previous edits in posing workflows. Current models generate a single pose from the handles only—regardless of what was there previously—making it difficult to preserve any variations and hindering tasks such as pose and motion editing. In this paper, we introduce SKEL-IK, a novel architecture and training scheme that is conditioned on a base pose, and designed to flow information directly onto the skeletal graph structure, such that hard constraints can be enforced by blocking information flows at certain joints. As a result, we are able to satisfy both hard and soft constraints, as well as preserve un-manipulated parts of the body when desired. Finally, by controlling the base pose in different ways, we demonstrate the ability of our model to perform tasks such as generating variations and quickly editing poses and motions; with less erosion of the base poses compared to the current state-of-the-art.	https://dl.acm.org/doi/abs/10.1145/3610548.3618217	Dhruv Agrawal, Martin Guay, Jakob Buhmann, Dominik Borer, Robert W. Sumner
Power Plastics: A Hybrid Lagrangian/Eulerian Solver for Mesoscale Inelastic Flows	We present a novel hybrid Lagrangian/Eulerian method for simulating inelastic flows that generates high-quality particle distributions with adaptive volumes. At its core, our approach integrates an updated Lagrangian time discretization of continuum mechanics with the Power Particle-In-Cell geometric representation of deformable materials. As a result, we obtain material points described by optimized density kernels that precisely track the varying particle volumes both spatially and temporally. For efficient CFL-rate simulations, we also propose an implicit time integration for our system using a non-linear Gauss-Seidel solver inspired by X-PBD, viewing Eulerian nodal velocities as primal variables. We demonstrate the versatility of our method with simulations of mesoscale bubbles, sands, liquid, and foams.	https://dl.acm.org/doi/abs/10.1145/3618344	Ziyin Qu, Minchen Li, Yin Yang, Chenfanfu Jiang, Fernando De Goes
Practical Course for Creating Stereoscopic VTuber	"The authors conduct a practical class for third-year students in the Information Media Studies Department, focused on creating stereoscopic CG content. They focus on virtual YouTubers (VTubers), that has been gaining popularity on the internet. Despite the current VTubers being created as 3D models, they lack actual stereoscopic capabilities. VTuber characters can feel more lifelike upon introduction of stereoscopic visuals, allowing creators to expand their ideas and unlocking significant potential in the metaverse era. In this study, each student captures video footage of their upper body using their laptop's web camera. Then, they combine various freely available software tools online to create stereoscopic VTuber videos without the need for special equipment. The students produce content using techniques, such as anaglyph videos (using red-blue glasses), lenticular lenses, and fly's eye lenses for autostereoscopic viewing. They also add audio to the VTuber content, and at the end of the exercise, they hold a ""Stereoscopic VTuber"" presentation, where mutual evaluations are conducted. Results of the student surveys indicate a positive outcome in terms of their increased interest in related technologies and enhanced creative motivation. This practical course holds significant values as an education that combines information media art, system-oriented thinking, and AI technologies."	https://dl.acm.org/doi/abs/10.1145/3610540.3627005	Katsuhiro Kanamori, Kazuhisa Yanaka
Prison X	Prison X is an innovative VR experience transporting audiences into a mythological Neo-Andean underworld. As a foundational part of the Neo-Andean Metaverse, it translates Andean perspectives on time, space, philosophy, and mythology into an immersive 3D realm. This collaborative project by indigenous artists reclaims indigenous aesthetics using pioneering technology. The prison environment was created through photogrammetry and hand-painted in Tilt Brush then imported into Unity. Characters represent Andean futurism, designed based on pop culture and folklore. Animation utilizes motion capture while spatial audio and interactivity enhance immersion. Premiering at Sundance, Prison X combines storytelling, immersion and interactivity to innovate emerging film futurism. It explores anti-colonial narratives, challenging notions of identity and reality. The diversity of perspectives expands possibilities for XR as a tool of empowerment. Prison X points to an inclusive creative future, pioneering new visual language and storytelling.	https://dl.acm.org/doi/abs/10.1145/3610549.3614614	Violeta Ayala
ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models	Personalizing generative models offers a way to guide image generation with user-provided references. Current personalization methods can invert an object or concept into the textual conditioning space and compose new natural sentences for text-to-image diffusion models. However, representing and editing specific visual attributes such as material, style, and layout remains a challenge, leading to a lack of disentanglement and editability. To address this problem, we propose a novel approach that leverages the step-by-step generation process of diffusion models, which generate images from low to high frequency information, providing a new perspective on representing, generating, and editing images. We develop the Prompt Spectrum Space P*, an expanded textual conditioning space, and a new image representation method called represents an image as a collection of inverted textual token embeddings encoded from per-stage prompts, where each prompt corresponds to a specific generation stage (i.e., a group of consecutive steps) of the diffusion model. Experimental results demonstrate that P* and offer better disentanglement and controllability compared to existing methods. We apply in various personalized attribute-aware image generation applications, such as image-guided or text-driven manipulations of materials, style, and layout, achieving previously unattainable results from a single image input without fine-tuning the diffusion models. Our source code is available at https://github.com/zyxElsa/ProSpect.	https://dl.acm.org/doi/abs/10.1145/3618342	Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, Changsheng Xu
Progressive Shell Qasistatics for Unstructured Meshes	Thin shell structures exhibit complex behaviors critical for modeling and design across wide-ranging applications. Capturing their mechanical response requires finely detailed, high-resolution meshes. Corresponding simulations for predicting equilibria with these meshes are expensive, whereas coarse-mesh simulations can be fast but generate unacceptable artifacts and inaccuracies. The recently proposed progressive simulation framework [Zhang et al. 2022] offers a promising avenue to address these limitations with consistent and progressively improving simulation over a hierarchy of increasingly higher-resolution models. Unfortunately, it is currently severely limited in application to meshes and shapes generated via Loop subdivision. We propose Progressive Shells Quasistatics to extend progressive simulation to the high-fidelity modeling and design of all input shell (and plate) geometries with unstructured (as well as structured) triangle meshes. To do so, we construct a fine-to-coarse hierarchy with a novel nonlinear prolongation operator custom-suited for curved-surface simulation that is rest-shape preserving, supports complex curved boundaries, and enables the reconstruction of detailed geometries from coarse-level meshes. Then, to enable convergent, high-quality solutions with robust contact handling, we propose a new, safe, and efficient shape-preserving upsampling method that ensures non-intersection and strain limits during refinement. With these core contributions, Progressive Shell Quasistatics enables, for the first time, wide generality for progressive simulation, including support for arbitrary curved-shell geometries, progressive collision objects, curved boundaries, and unstructured triangle meshes - all while ensuring that preview and final solutions remain free of intersections. We demonstrate these features across a wide range of stress-tests where progressive simulation captures the wrinkling, folding, twisting, and buckling behaviors of frictionally contacting thin shells with orders-of-magnitude speed-up in examples over direct fine-resolution simulation.	https://dl.acm.org/doi/abs/10.1145/3618388	Jiayi Eris Zhang, Jérémie Dumas, Yun (Raymond) Fei, Alec Jacobson, Doug L. James, Danny M. Kaufman
Projective Sampling for Differentiable Rendering of Geometry	Discontinuous visibility changes at object boundaries remain a persistent source of difficulty in the area of differentiable rendering. Left untreated, they bias computed gradients so severely that even basic optimization tasks fail. Prior path-space methods addressed this bias by decoupling boundaries from the interior, allowing each part to be handled using specialized Monte Carlo sampling strategies. While conceptually powerful, the full potential of this idea remains unrealized since existing methods often fail to adequately sample the boundary proportional to its contribution. This paper presents theoretical and algorithmic contributions. On the theoretical side, we transform the boundary derivative into a remarkably simple local integral that invites present and future developments. Building on this result, we propose a new strategy that projects ordinary samples produced during forward rendering onto nearby boundaries. The resulting projections establish a variance-reducing guiding distribution that accelerates convergence of the subsequent differential phase. We demonstrate the superior efficiency and versatility of our method across a variety of shape representations, including triangle meshes, implicitly defined surfaces, and cylindrical fibers based on Bézier curves.	https://dl.acm.org/doi/abs/10.1145/3618385	Ziyi Zhang, Nicolas Roussel, Wenzel Jakob
Quantum Ray Marching: Reformulating Light Transport for Quantum Computers	The use of quantum computers in computer graphics has gained interest in recent years, especially for the application to rendering. The current state of the art in quantum rendering relies on Grover's search for finding ray intersections in for M primitives. This quantum approach is faster than the naive approach of O(M) but slower than O(log M) of modern ray tracing with an acceleration data structure. Furthermore, this quantum ray tracing method is fundamentally limited to casting one ray at a time, leaving quantum rendering scales for the number of rays the same as non-quantum algorithms. We present a new quantum rendering method, quantum ray marching, based on the reformulation of ray marching as a quantum random walk. Our work is the first complete quantum rendering pipeline capable of light transport simulation and remains asymptotically faster than non-quantum counterparts. Our quantum ray marching can trace an exponential number of paths with polynomial cost, and it leverages quantum numerical integration to converge in O(1/N) for N estimates as opposed to non-quantum . These properties led to first quantum rendering that is asymptotically faster than non-quantum Monte Carlo rendering. We numerically tested our algorithm by rendering 2D and 3D scenes.	https://dl.acm.org/doi/abs/10.1145/3610548.3618151	Logan Mosier, Morgan Mcguire, Toshiya Hachisuka
Quem Salva	Sidnei is a young recruit of the fire department fighting forest fires in the Amazon. During his first mission, he will be separated from the group and will have to undertake a perilous mission, guided by his mentor, Joao. Sidnei will be confronted with a choice: follow the orders of the hierarchy or save a thousand-year-old tree at the risk of his life.	https://dl.acm.org/doi/abs/10.1145/3626964.3626991	Laure Devin, Maxime Bourstin, Nathan Medam, Charles Hechinger, Titouan Jaouen, Philippe Meis
Quest Driven Spatial Songs: Exploring how narrative structures of Quests in Games can be mapped onto songs.	This paper briefly examines the relationship between music recordings and narrative structures found in video games. It highlights the immersive and time-consuming nature of game quests, where players invest substantial time exploring virtual worlds to reveal the narrative. The paper explores the potential of spatialised music experiences, particularly in the context of Dolby Atmos and the concept of degrees of freedom (3DoF and 6DoF). It investigates the application of spatial algorithms and game engines' middleware to create immersive music experiences that dynamically adapt to the listener's spatial and temporal position. The research acknowledges limited instances of studies on compositional narrative design frameworks specifically tailored for completely spatialised music, particularly regarding the virtual spatialisation of individual instruments. It recognizes that the prevalence of non-diegetic music in games, which serves to represent the player's emotional state, has impeded the exploration of spatialised music elements. Consequently, the paper proposes the adoption of narrative design frameworks derived from video games as a promising approach for songwriters to develop Quest Driven Spatial Songs (QDSS).	https://dl.acm.org/doi/abs/10.1145/3623053.3623367	Yunyu Ong
RMIP: Displacement ray tracing via inversion and oblong bounding	High-performance ray tracing of triangle meshes equipped with displacement maps is a challenging task. Existing methods either rely on pre-tessellation, taking full advantage of the hardware but with a poor memory/quality tradeoff, or use custom displacement-centric acceleration structures, preserving all the geometric details but being orders of magnitude slower. We introduce a method that efficiently probes the displacement-map space to find ray-surface intersections without relying on pre-tessellation. Our method combines inverse displacement mapping and on-the-fly surface-bound computation. It employs a novel data structure that provides tight displacement bounds over rectangular regions in the displacement-map space. We demonstrate the effectiveness of our approach in a production GPU path tracer. It can achieve over an order of magnitude speed-up in render time compared to state of the art in the most challenging real-time path-tracing scenarios, while maintaining a low memory footprint.	https://dl.acm.org/doi/abs/10.1145/3610548.3618182	Theo Thonat, Iliyan Georgiev, François Beaune, Tamy Boubekeur
RT-Octree: Accelerate PlenOctree Rendering with Batched Regular Tracking and Neural Denoising for Real-time Neural Radiance Fields	Neural Radiance Fields (NeRF) has demonstrated its ability to generate high-quality synthesized views. Nonetheless, due to its slow inference speed, there is a need to explore faster inference methods. In this paper, we propose RT-Octree, which uses batched regular tracking based on PlenOctree with neural denoising to achieve better real-time performance. We achieve this by modifying the volume rendering algorithm to regular tracking. We batch all samples for each pixel in one single ray-voxel intersection process to further improve the real-time performance. To reduce the variance caused by insufficient samples while ensuring real-time speed, we propose a lightweight neural network named GuidanceNet, which predicts the guidance map and weight maps utilized for the subsequent multi-layer denoising module. We evaluate our method on both synthetic and real-world datasets, obtaining a speed of 100 + frames per second (FPS) with a resolution of 1920 × 1080. Compared to PlenOctree, our method is 1.5 to 2 times faster in inference time and significantly outperforms NeRF by several orders of magnitude. The experimental results demonstrate the effectiveness of our approach in achieving real-time performance while maintaining similar rendering quality.	https://dl.acm.org/doi/abs/10.1145/3610548.3618214	Zixi Shu, Ran Yi, Yuqi Meng, Yutong Wu, Lizhuang Ma
ReShader: View-Dependent Highlights for Single Image View-Synthesis	In recent years, novel view synthesis from a single image has seen significant progress thanks to the rapid advancements in 3D scene representation and image inpainting techniques. While the current approaches are able to synthesize geometrically consistent novel views, they often do not handle the view-dependent effects properly. Specifically, the highlights in their synthesized images usually appear to be glued to the surfaces, making the novel views unrealistic. To address this major problem, we make a key observation that the process of synthesizing novel views requires changing the shading of the pixels based on the novel camera, and moving them to appropriate locations. Therefore, we propose to split the view synthesis process into two independent tasks of pixel reshading and relocation. During the reshading process, we take the single image as the input and adjust its shading based on the novel camera. This reshaded image is then used as the input to an existing view synthesis method to relocate the pixels and produce the final novel view image. We propose to use a neural network to perform reshading and generate a large set of synthetic input-reshaded pairs to train our network. We demonstrate that our approach produces plausible novel view images with realistic moving highlights on a variety of real world scenes.	https://dl.acm.org/doi/abs/10.1145/3618393	Avinash Paliwal, Brandon G. Nguyen, Andrii Tsarov, Nima Khademi Kalantari
Reach For the Spheres: Tangency-aware surface reconstruction of SDFs	Signed distance fields (SDFs) are a widely used implicit surface representation, with broad applications in computer graphics, computer vision, and applied mathematics. To reconstruct an explicit triangle mesh surface corresponding to an SDF, traditional isosurfacing methods, such as Marching Cubes and and its variants, are typically used. However, these methods overlook fundamental properties of SDFs, resulting in reconstructions that exhibit severe oversmoothing and feature loss. To address this shortcoming, we propose a novel method based on a key insight: each SDF sample corresponds to a spherical region that must lie fully inside or outside the surface, depending on its sign, and that must be tangent to the surface at some point. Leveraging this understanding, we formulate an energy that gauges the degree of violation of tangency constraints by a proposed surface. We then employ a gradient flow that minimizes our energy, starting from an initial triangle mesh that encapsulates the surface. This algorithm yields superior reconstructions to previous methods, even with sparsely sampled SDFs. Our approach provides a more nuanced understanding of SDFs and offers significant improvements in surface reconstruction.	https://dl.acm.org/doi/abs/10.1145/3610548.3618196	Silvia Sellán, Christopher Batty, Oded Stein
Real-time Height-field Simulation of Sand and Water Mixtures	We propose a height-field-based real-time simulation method for sand and water mixtures. Inspired by the shallow-water assumption, our approach extends the governing equations to handle two-phase flows of sand and water using height fields. Our depth-integrated governing equations can model the elastoplastic behavior of sand, as well as sand-water-mixing phenomena such as friction, diffusion, saturation, and momentum exchange. We further propose an operator-splitting time integrator that is both GPU-friendly and stable under moderate time step sizes. We have evaluated our method on a set of benchmark scenarios involving large bodies of heterogeneous materials, where our GPU-based algorithm runs at real-time frame rates. Our method achieves a desirable trade-off between fidelity and performance, bringing an unprecedentedly immersive experience for real-time applications.	https://dl.acm.org/doi/abs/10.1145/3610548.3618159	Haozhe Su, Siyu Zhang, Zherong Pan, Mridul Aanjaneya, Xifeng Gao, Kui Wu
Real-time Rendering of Glossy Reflections using Ray Tracing and Two-level Radiance Caching	Estimation of glossy reflections remains a challenging topic for real-time renderers. Ray tracing is a robust solution for evaluating the specular lobe of a given BRDF; however, it is computationally expensive and introduces noise that requires filtering. Other solutions, such as light probe systems, offer to approximate the signal with little to no noise and better performance but tend to introduce additional bias in the form of overly blurred visuals. This paper introduces a novel approach to rendering reflections in real time that combines the radiance probes of an existing diffuse global illumination framework with denoised ray-traced reflections calculated at a low sampling rate. We will show how combining these two sources allows producing an efficient and high-quality estimation of glossy reflections that is suitable for real-time applications such as games.	https://dl.acm.org/doi/abs/10.1145/3610543.3626167	Kenta Eto, Sylvain Meunier, Takahiro Harada, Guillaume Boissé
Recognition-Independent Handwritten Text Alignment Using Lightweight Recurrent Neural Network	Legibility refers to the ease with which handwritten content can be read and understood accurately. However, existing approaches to handwriting beautification either rely on the result of handwriting recognition and accumulate errors from the recognition system or do not address the alignment problem and are difficult to generalize to other languages. This paper presents a novel approach to improve handwriting legibility by straightening the written content. It utilizes a recurrent neural network that operates without the need for recognition, supports connected writing, and accommodates various writing styles. The results obtained with this method demonstrate significant improvements in handwriting alignment. Moreover, a single neural network model can effectively cater to multiple languages within the same writing system.	https://dl.acm.org/doi/abs/10.1145/3610542.3626136	Karina Korovai, Dmytro Zhelezniakov, Olga Radyvonenko, Oleg Yakovchuk, Ivan Deriuga, Nataliya Sakhnenko
Reconstructing Close Human Interactions from Multiple Views	This paper addresses the challenging task of reconstructing the poses of multiple individuals engaged in close interactions, captured by multiple calibrated cameras. The difficulty arises from the noisy or false 2D keypoint detections due to inter-person occlusion, the heavy ambiguity in associating keypoints to individuals due to the close interactions, and the scarcity of training data as collecting and annotating motion data in crowded scenes is resource-intensive. We introduce a novel system to address these challenges. Our system integrates a learning-based pose estimation component and its corresponding training and inference strategies. The pose estimation component takes multi-view 2D keypoint heatmaps as input and reconstructs the pose of each individual using a 3D conditional volumetric network. As the network doesn't need images as input, we can leverage known camera parameters from test scenes and a large quantity of existing motion capture data to synthesize massive training data that mimics the real data distribution in test scenes. Extensive experiments demonstrate that our approach significantly surpasses previous approaches in terms of pose accuracy and is generalizable across various camera setups and population sizes. The code is available on our project page: https://github.com/zju3dv/CloseMoCap.	https://dl.acm.org/doi/abs/10.1145/3618336	Qing Shuai, Zhiyuan Yu, Zhize Zhou, Lixin Fan, Haijun Yang, Can Yang, Xiaowei Zhou
Reconstructing Hand Shape and Appearance for Accurate Tracking from Monocular Video	A virtual animatable hand avatar capable of representing a user's hand shape and appearance, and tracking the articulated motion is essential for an immersive experience in AR/VR. Recent approaches use implicit representations to capture geometry and appearance combined with neural rendering. However, they fail to generalize to unseen shapes, don't handle lighting leading to baked-in illumination and self-shadows, and cannot capture complex poses. In this thesis, we 1) introduce a novel hand shape model that augments a data-driven shape model and adapt its local scale to represent unseen hand shapes, 2) propose a method to reconstruct a detailed hand avatar from monocular RGB video captured under real-world environment lighting by jointly optimizing shape, appearance, and lighting parameters using a realistic shading model in a differentiable rendering framework incorporating Monte Carlo path tracing, and 3) present a robust hand tracking framework that accurately registers our hand model to monocular depth data utilizing a modified skinning function with blend shapes. Our evaluation demonstrates that our approach outperforms existing hand shape and appearance reconstruction methods on all commonly used metrics. Further, our tracking framework improves over existing generative and discriminative hand pose estimation methods.	https://dl.acm.org/doi/abs/10.1145/3623053.3623371	Pratik Kalshetti
Reconstruction of Machine-Made Shapes from Bitmap Sketches	We propose a method of reconstructing 3D machine-made shapes from bitmap sketches by separating an input image into individual patches and jointly optimizing their geometry. We rely on two main observations: (1) human observers interpret sketches of man-made shapes as a collection of simple geometric primitives, and (2) sketch strokes often indicate occlusion contours or sharp ridges between those primitives. Using these main observations we design a system that takes a single bitmap image of a shape, estimates image depth and segmentation into primitives with neural networks, then fits primitives to the predicted depth while determining occlusion contours and aligning intersections with the input drawing via optimization. Unlike previous work, our approach does not require additional input, annotation, or templates, and does not require retraining for a new category of man-made shapes. Our method produces triangular meshes that display sharp geometric features and are suitable for downstream applications, such as editing, rendering, and shading.	https://dl.acm.org/doi/abs/10.1145/3618361	Ivan Puhachov, Cedric Martens, Paul G. Kry, Mikhail Bessmeltsev
Recovering Detailed Neural Implicit Surfaces from Blurry Images	In this poster, we present a method for recovering surface details from blurry images. We achieve this by transforming input position features of the neural implicit surface and radiance field using a blur kernel and simulating the motion blur process through weighted averaging of different transformations. Then, we can obtain more clear surface reconstruction results by discarding the blur kernel during the testing phase. Experimental results on the blurred DTU dataset demonstrate that our approach exhibits robustness to blurry inputs and effectively reconstructs surface details.	https://dl.acm.org/doi/abs/10.1145/3610542.3626147	Zihui Xu, Yiqun Wang, Zhengda Lu, Jun Xiao
Rectifying Strip Patterns	Straight flat strips of inextensible material can be bent into curved strips aligned with arbitrary space curves. The large shape variety of these so-called rectifying strips makes them candidates for shape modeling, especially in applications such as architecture where simple elements are preferred for the fabrication of complex shapes. In this paper, we provide computational tools for the design of shapes from rectifying strips. They can form various patterns and fulfill constraints which are required for specific applications such as gridshells or shading systems. The methodology is based on discrete models of rectifying strips, a discrete level-set formulation and optimization-based constrained mesh design and editing. We also analyse the geometry at nodes and present remarkable quadrilateral arrangements of rectifying strips with torsion-free nodes.	https://dl.acm.org/doi/abs/10.1145/3618378	Bolun Wang, Hui Wang, Eike Schling, Helmut Pottmann
Reflection on Abstract Art through Kandinsky's Teaching: Reflection on Abstract Art through Reconstruction of Kandinsky's Teaching at the Bauhaus	Wassily Kandinsky is considered as one of the pioneers of abstract painting in Europe. In this paper, our goal is to develop a user interface that models Kandinsky's analytical drawing process within a computational context to advance research in artificial intelligence and the arts. A user first chooses a photograph from the ObjectNet3D database, objects of which are aligned with the 3D shapes. Using the 3D shape information, the software extracts 2D projection contours. Then, it performs corner detection and finds structural networks. Lastly, the software transforms the photograph into an abstract image based on a student example.	https://dl.acm.org/doi/abs/10.1145/3610591.3616436	Jungah Son, Marko Peljhan, George Legrady
Reimagining Animation Making through Style Transfer	"""Dissolution"" and ""Cunabula"" are experimental animation projects that explore the potential of artistic style transfer with neural networks in animation filmmaking. The objective is to discover novel production methods using fast, arbitrary, and stroke-adjustable style transfer networks that allow for unconventional technical approaches during the production process and the creation of unique and unexplored aesthetics in moving images powered by deep learning. These projects exemplify a creative approach to leveraging existing neural style transfer techniques to create compelling animations that go beyond mere passive pastiches of certain artistic styles, thereby advancing the aesthetics of contemporary animation."	https://dl.acm.org/doi/abs/10.1145/3610591.3616435	Sujin Kim
ReparamCAD: Zero-shot CAD Re-Parameterization for Interactive Manipulation	Parametric CAD models encode entire families of shapes that should, in principle, be easy for designers to explore. However, in practice, parametric CAD models can be difficult to manipulate due to implicit semantic constraints among parameter values. Finding and enforcing these semantic constraints solely from geometry or programmatic shape representations is not possible because these constraints ultimately reflect design intent. They are informed by the designer's experience and semantics in the real world. To address this challenge, we introduce ReparamCAD, a zero-shot pipeline that leverages pre-trained large language and image model to infer meaningful space of variations for a shape We then re-parameterize a new constrained parametric CAD program that captures these variations, enabling effortless exploration of the design space along meaningful design axes. We evaluated our approach through five examples and a user study. The result showed that the inferred spaces are meaningful and comparable to those defined by experts. Code and data are at: https://github.com/milmillin/ReparamCAD.	https://dl.acm.org/doi/abs/10.1145/3610548.3618219	Milin Kodnongbua, Benjamin Jones, Maaz Bin Safeer Ahmad, Vladimir Kim, Adriana Schulz
Repurposing Diffusion Inpainters for Novel View Synthesis	In this paper, we present a method for generating consistent novel views from a single source image. Our approach focuses on maximizing the reuse of visible pixels from the source image. To achieve this, we use a monocular depth estimator that transfers visible pixels from the source view to the target view. Starting from a pre-trained 2D inpainting diffusion model, we train our method on the large-scale Objaverse dataset to learn 3D object priors. While training we use a novel masking mechanism based on epipolar lines to further improve the quality of our approach. This allows our framework to perform zero-shot novel view synthesis on a variety of objects. We evaluate the zero-shot abilities of our framework on three challenging datasets: Google Scanned Objects, Ray Traced Multiview, and Common Objects in 3D.	https://dl.acm.org/doi/abs/10.1145/3610548.3618149	Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky, Riza Alp Guler, Jian Ren, Sergey Tulyakov, Igor Gilitschenski
Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation	Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos. Code is available at our project page: https://www.mmlab-ntu.com/project/rerender/	https://dl.acm.org/doi/abs/10.1145/3610548.3618160	Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy
Robust Skin Weights Transfer via Weight Inpainting	We present a new method for the robust transfer of skin weights from a source mesh to a target mesh with significantly different geometric shapes. Rigging garments is a typical application of skin weight transfer where weights are copied from a source body mesh to avoid tedious weight painting from scratch. However, existing techniques struggle with non-skin-tight garments and require additional manual weight painting. We introduce a fully automatic two-stage skin weight transfer process. First, an initial transfer is performed by copying weights from the source mesh only for those vertices on the target mesh where we have high confidence in obtaining the ground truth weights from the source. Then, we automatically compute weights for all other vertices by interpolating the weights computed in stage one. This approach is robust and easy to implement in practice, yet it far outperforms the methods used in existing commercial software and previous research works.	https://dl.acm.org/doi/abs/10.1145/3610543.3626180	Rinat Abdrashitov, Kim Raichstat, Jared Monsen, David Hill
Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on Double Covering	In this paper, we propose a new method, called DoubleCoverUDF, for extracting the zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a learned UDF and a user-specified parameter (a small positive real number) as input and extracts an iso-surface with an iso-value using the conventional marching cubes algorithm. We show that the computed iso-surface is the boundary of the -offset volume of the target zero level-set , which is an orientable manifold, regardless of the topology of Next, the algorithm computes a covering map to project the boundary mesh onto , preserving the mesh's topology and avoiding folding. If is an orientable manifold surface, our algorithm separates the double-layered mesh into a single layer using a robust minimum-cut post-processing step. Otherwise, it keeps the double-layered mesh as the output. We validate our algorithm by reconstructing 3D surfaces of open models and demonstrate its efficacy and effectiveness on synthetic models and benchmark datasets. Our experimental results confirm that our method is robust and produces meshes with better quality in terms of both visual evaluation and quantitative measures than existing UDF-based methods. The source code is available at https://github.com/jjjkkyz/DCUDF.	https://dl.acm.org/doi/abs/10.1145/3618314	Fei Hou, Xuhui Chen, Wencheng Wang, Hong Qin, Ying He
Room to Room Mapping: Seamlessly Connecting Different Rooms	In this study, we propose a projection mapping technique designed to connect rooms in disparate locations virtually, creating a continuous, immersive space. Despite technologies related to remote communication becoming widespread in recent years, the physical presence of display screens often hinders the sense of realism and immersion. Therefore, this study aims to create a sense of continuity between a local room and a remote room, irrespective of display device constraints. We achieve this by utilizing a wide-area image projection technique in a room. First, we realize a projection mapping system that can project images over a wide area, including in limited indoor spaces. This system improves the sense of connectivity with remote locations by expressing a seamless illusion of continuity between the remote and local environments. We conduct user studies and show the effectiveness of the proposed method.	https://dl.acm.org/doi/abs/10.1145/3610542.3626113	Naoki Hashimoto, Yuki Inada
Rule-of-Thirds or Centered? A study in preference in photo composition	"The Rule of Thirds is a well known heuristic in photo composition. The professional photography community both uses it and derides it. We report on an experiment to test the validity of the Rule of Thirds in the simplest case: composition of a single object. Our results show that our participants overwhelmingly preferred a centered object in the image to one positioned according to the Rule of Thirds. We speculate why this is so and point to other research that addresses how we can take advantage of this ""salient centeredness""."	https://dl.acm.org/doi/abs/10.1145/3610542.3626121	Weng Khuan Hoh, Fang-Lue Zhang, Neil Anthony Dodgson
SAILOR: Synergizing Radiance and Occupancy Fields for Live Human Performance Capture	Immersive user experiences in live VR/AR performances require a fast and accurate free-view rendering of the performers. Existing methods are mainly based on Pixel-aligned Implicit Functions (PIFu) or Neural Radiance Fields (NeRF). However, while PIFu-based methods usually fail to produce photorealistic view-dependent textures, NeRF-based methods typically lack local geometry accuracy and are computationally heavy ( , dense sampling of 3D points, additional fine-tuning, or pose estimation). In this work, we propose a novel generalizable method, named SAILOR, to create high-quality human free-view videos from very sparse RGBD live streams. To produce view-dependent textures while preserving locally accurate geometry, we integrate PIFu and NeRF such that they work synergistically by conditioning the PIFu on depth and then rendering view-dependent textures through NeRF. Specifically, we propose a novel network, named SRONet, for this hybrid representation. SRONet can handle unseen performers without fine-tuning. Besides, a neural blending-based ray interpolation approach, a tree-based voxel-denoising scheme, and a parallel computing pipeline are incorporated to reconstruct and render live free-view videos at 10 fps on average. To evaluate the rendering performance, we construct a real-captured RGBD benchmark from 40 performers. Experimental results show that SAILOR outperforms existing human reconstruction and performance capture methods.	https://dl.acm.org/doi/abs/10.1145/3618370	Zheng Dong, Ke Xu, Yaoan Gao, Qilin Sun, Hujun Bao, Weiwei Xu, Rynson W. H. Lau
SAME: Skeleton-Agnostic Motion Embedding for Character Animation	Learning deep neural networks on human motion data has become common in computer graphics research, but the heterogeneity of available datasets poses challenges for training large-scale networks. This paper presents a framework that allows us to solve various animation tasks in a skeleton-agnostic manner. The core of our framework is to learn an embedding space to disentangle skeleton-related information from input motion while preserving semantics, which we call Skeleton-Agnostic Motion Embedding (SAME). To efficiently learn the embedding space, we develop a novel autoencoder with graph convolution networks and provide new formulations of various animation tasks operating in the SAME space. We showcase various examples, including retargeting, reconstruction, and interactive character control, and conduct an ablation study to validate design choices made during development.	https://dl.acm.org/doi/abs/10.1145/3610548.3618206	Sunmin Lee, Taeho Kang, Jungnam Park, Jehee Lee, Jungdam Won
SCOOT:Self-supervised Centric Open-set Object Tracking	We propose a novel and comprehensive general-purpose object tracking system named Self-supervised Centric Open-set Object Tracking or 'SCOOT'. Our SCOOT encompasses a self-supervised appearance model, a fusion module for combining textual and visual features, and an object association algorithm based on reconstruction and observation. Through this system, we unlock new possibilities for enhancing the capability of open-set object tracking with the aid of language cues in real-world scenarios.	https://dl.acm.org/doi/abs/10.1145/3610542.3626130	Wei Li, Weiliang Meng, Bowen Li, Jiguang Zhang, Xiaopeng Zhang
SFLSH: Shape-Dependent Soft-Flesh Avatars	We present a multi-person soft-tissue avatar model. This model maps a body shape descriptor to heterogeneous geometric and mechanical parameters of a soft-tissue model across the body, effectively producing a shape-dependent parametric soft avatar model. The design of the model overcomes two major challenges, the potential redundancy of geometric and mechanical parameters, and the complexity to obtain abundant subject data, which together induce major risk of overfitting the resulting model. To overcome these challenges, we introduce a local shape-dependent regularization of the model. We demonstrate accurate results, on par with independent per-subject estimation, accurate interpolation within the range of body shapes of the training subjects, and good generalization to unseen body shapes. As a result, we obtain a parametric soft-flesh avatar model easy to integrate in many existing applications.	https://dl.acm.org/doi/abs/10.1145/3610548.3618242	Pablo Ramón, Cristian Romero, Javier Tapia, Miguel A. Otaduy
SLANG.D: Fast, Modular and Differentiable Shader Programming	We introduce SLANG.D, an extension to the Slang shading language that incorporates first-class automatic differentiation support. The new shading language allows us to transform a Direct3D-based path tracer to be fully differentiable with minor modifications to existing code. SLANG.D enables a shared ecosystem between machine learning frameworks and pre-existing graphics hardware API-based rendering systems, promoting the interchange of components and ideas across these two domains. Our contributions include a differentiable type system designed to ensure type safety and semantic clarity in codebases that blend differentiable and non-differentiable code, language primitives that automatically generate both forward and reverse gradient propagation methods, and a compiler architecture that generates efficient derivative propagation shader code for graphics pipelines. Our compiler supports differentiating code that involves arbitrary control-flow, dynamic dispatch, generics and higher-order differentiation, while providing developers flexible control of checkpointing and gradient aggregation strategies for best performance. Our system allows us to differentiate an existing real-time path tracer, Falcor, with minimal change to its shader code. We show that the compiler-generated derivative kernels perform as efficiently as handwritten ones. In several benchmarks, the SLANG.D code achieves significant speedup when compared to prior automatic differentiation systems.	https://dl.acm.org/doi/abs/10.1145/3618353	Sai Praveen Bangaru, Lifan Wu, Tzu-Mao Li, Jacob Munkberg, Gilbert Bernstein, Jonathan Ragan-Kelley, Frédo Durand, Aaron Lefohn, Yong He
SOL-NeRF: Sunlight Modeling for Outdoor Scene Decomposition and Relighting	Outdoor scenes often involve large-scale geometry and complex unknown lighting conditions, making it difficult to decompose them into geometry, reflectance and illumination. Recently researchers made attempts to decompose outdoor scenes using Neural Radiance Fields (NeRF) and learning-based lighting and shadow representations. However, diverse lighting conditions and shadows in outdoor scenes are challenging for learning-based models. Moreover, existing methods may produce rough geometry and normal reconstruction and introduce notable shading artifacts when the scene is rendered under a novel illumination. To solve the above problems, we propose SOL-NeRF to decompose outdoor scenes with the help of a hybrid lighting representation and a signed distance field geometry reconstruction. We use a single Spherical Gaussian (SG) lobe to approximate the sun lighting, and a first-order Spherical Harmonic (SH) mixture to resemble the sky lighting. This hybrid representation is specifically designed for outdoor settings, and compactly models the outdoor lighting, ensuring robustness and efficiency. The shadow of the direct sun lighting can be obtained by casting the ray against the mesh extracted from the signed distance field, and the remaining shadow can be approximated by Ambient Occlusion (AO). Additionally, sun lighting color prior and a relaxed Manhattan-world assumption can be further applied to boost decomposition and relighting performance. When changing the lighting condition, our method can produce consistent relighting results with correct shadow effects. Experiments conducted on our hybrid lighting scheme and the entire decomposition pipeline show that our method achieves better reconstruction, decomposition, and relighting performance compared to previous methods both quantitatively and qualitatively.	https://dl.acm.org/doi/abs/10.1145/3610548.3618143	Jia-Mu Sun, Tong Wu, Yong-Liang Yang, Yu-Kun Lai, Lin Gao
ScaNeRF: Scalable Bundle-Adjusting Neural Radiance Fields for Large-Scale Scene Rendering	High-quality large-scale scene rendering requires a scalable representation and accurate camera poses. This research combines tile-based hybrid neural fields with parallel distributive optimization to improve bundle-adjusting neural radiance fields. The proposed method scales with a divide-and-conquer strategy. We partition scenes into tiles, each with a multi-resolution hash feature grid and shallow chained diffuse and specular multilayer perceptrons (MLPs). Tiles unify foreground and background via a spatial contraction function that allows both distant objects in outdoor scenes and planar reflections as virtual images outside the tile. Decomposing appearance with the specular MLP allows a specular-aware warping loss to provide a second optimization path for camera poses. We apply the alternating direction method of multipliers (ADMM) to achieve consensus among camera poses while maintaining parallel tile optimization. Experimental results show that our method outperforms state-of-the-art neural scene rendering method quality by 5%--10% in PSNR, maintaining sharp distant objects and view-dependent reflections across six indoor and outdoor scenes.	https://dl.acm.org/doi/abs/10.1145/3618369	Xiuchao Wu, Jiamin Xu, Xin Zhang, Hujun Bao, Qixing Huang, Yujun Shen, James Tompkin, Weiwei Xu
Scene-Aware Activity Program Generation with Language Guidance	We address the problem of scene-aware activity program generation, which requires decomposing a given activity task into instructions that can be sequentially performed within a target scene to complete the activity. While existing methods have shown the ability to generate rational or executable programs, generating programs with both high rationality and executability still remains a challenge. Hence, we propose a novel method where the key idea is to explicitly combine the language rationality of a powerful language model with dynamic perception of the target scene where instructions are executed, to generate programs with high rationality executability. Our method iteratively generates instructions for the activity program. Specifically, a two-branch feature encoder operates on a language-based and graph-based representation of the current generation progress to extract language features and scene graph features, respectively. These features are then used by a predictor to generate the next instruction in the program. Subsequently, another module performs the predicted action and updates the scene for perception in the next iteration. Extensive evaluations are conducted on the VirtualHome-Env dataset, showing the advantages of our method over previous work. Key algorithmic designs are validated through ablation studies, and results on other types of inputs are also presented to show the generalizability of our method.	https://dl.acm.org/doi/abs/10.1145/3618338	Zejia Su, Qingnan Fan, Xuelin Chen, Oliver Van Kaick, Hui Huang, Ruizhen Hu
SeamlessNeRF: Stitching Part NeRFs with Gradient Propagation	"Neural Radiance Fields (NeRFs) have emerged as promising digital mediums of 3D objects and scenes, sparking a surge in research to extend the editing capabilities in this domain. The task of seamless editing and merging of multiple NeRFs, resembling the ""Poisson blending"" in 2D image editing, remains a critical operation that is under-explored by existing work. To fill this gap, we propose SeamlessNeRF, a novel approach for seamless appearance blending of multiple NeRFs. In specific, we aim to optimize the appearance of a target radiance field in order to harmonize its merge with a source field. We propose a well-tailored optimization procedure for blending, which is constrained by 1) pinning the radiance color in the intersecting boundary area between the source and target fields and 2) maintaining the original gradient of the target. Extensive experiments validate that our approach can effectively propagate the source appearance from the boundary area to the entire target field through the gradients. To the best of our knowledge, SeamlessNeRF is the first work that introduces gradient-guided appearance editing to radiance fields, offering solutions for seamless stitching of 3D objects represented in NeRFs. Our code and more results are available at https://sites.google.com/view/seamlessnerf."	https://dl.acm.org/doi/abs/10.1145/3610548.3618238	Bingchen Gong, Yuehao Wang, Xiaoguang Han, Qi Dou
Search for the Human	In a fantasy world, a griffin embraces his fate and goes on an epic journey to find a legendary creature : The Human.	https://dl.acm.org/doi/abs/10.1145/3626964.3626986	Claire Pellet, Faustine Merle, Caroline Leibel, Lucie Juric, Mélina Ienco, Cécile Blondel
Second-Order Finite Elements for Deformable Surfaces	We present a computational framework for simulating deformable surfaces from planar rest shape with second-order triangular finite elements. Our method develops numerical schemes for discretizing stretching, shearing, and bending energies of deformable surfaces in a second-order finite-element setting. In particular, we introduce a novel discretization scheme for approximating mean curvatures on a curved triangle mesh. Our framework also integrates a virtual-node finite-element scheme that supports two-way coupling between cut-cell rods without expensive remeshing. We compare our approach with traditional simulation methods using linear and higher-order finite elements and demonstrate its advantages in several challenging settings, such as low-resolution meshes, anisotropic triangulation, and stiff materials. Finally, we showcase several applications of our framework, including cloth simulation, mixed Origami and Kirigami, and biologically-inspired soft wing simulation.	https://dl.acm.org/doi/abs/10.1145/3610548.3618186	Qiqin Le, Yitong Deng, Jiamu Bu, Bo Zhu, Tao Du
Seeing the Invisible: Next-Generation Cameras Leveraging Hidden Properties of Light	Conventional cameras, inspired by human vision, only capture the intensity and color of light, restricting our ability to fully comprehend the visual environment. My thesis aims to transcend the limitations of traditional RGB imaging by delving into three underexplored light properties: polarization, ray direction, and time-of-flight. Firstly, this thesis leverages polarization of surface reflections for geometry estimation, reflectance separation, and lighting estimation, resulting in accurate estimation and editing of visual appearance. Then this thesis demonstrates how to leverage reflections on multi-view images of glossy objects to recover the reflected 3D environment, essentially turning the glossy objects into cameras. Finally, this thesis develops imaging systems that exploit the distribution of time-of-flight of photons, also known as transients, to reconstruct objects hidden from the camera's line of sight. By harnessing the full potential of light, this work lays the foundation for next-generation vision systems that can recover information seemingly invisible to human eyes and conventional cameras.	https://dl.acm.org/doi/abs/10.1145/3623053.3623374	Akshat Dave
Self-Calibrating, Fully Differentiable NLOS Inverse Rendering	Existing time-resolved non-line-of-sight (NLOS) imaging methods reconstruct hidden scenes by inverting the optical paths of indirect illumination measured at visible relay surfaces. These methods are prone to reconstruction artifacts due to inversion ambiguities and capture noise, which are typically mitigated through the manual selection of filtering functions and parameters. We introduce a fully-differentiable end-to-end NLOS inverse rendering pipeline that self-calibrates the imaging parameters during the reconstruction of hidden scenes, using as input only the measured illumination while working both in the time and frequency domains. Our pipeline extracts a geometric representation of the hidden scene from NLOS volumetric intensities and estimates the time-resolved illumination at the relay wall produced by such geometric information using differentiable transient rendering. We then use gradient descent to optimize imaging parameters by minimizing the error between our simulated time-resolved illumination and the measured illumination. Our end-to-end differentiable pipeline couples diffraction-based volumetric NLOS reconstruction with path-space light transport and a simple ray marching technique to extract detailed, dense sets of surface points and normals of hidden scenes.We demonstrate the robustness of our method to consistently reconstruct geometry and albedo, even under significant noise levels.	https://dl.acm.org/doi/abs/10.1145/3610548.3618140	Kiseok Choi, Inchul Kim, Dongyoung Choi, Julio Marco, Diego Gutierrez, Min H. Kim
Sensitive Floral	"""Sensitive Floral"" is an interactive, generative artwork that ventures into the exploration of a unique generative system, biomimetically emulating the reactive behaviors of the Mimosa plant. By synthesizing the complexity of fractal tree data structures with the Cellular Automata mechanisms of grid computations, the artwork elegantly mirrors nature's adaptive and responsive traits. The interactive interface allows users to initiate a ripple of movement by simply touching the screen, triggering a cascade of changes across thousands of leaves, akin to the group leaf movements observed in Mimosa. The system, instantaneously detects which branch of the tree structure on the touch screen is being externally triggered. It then notifies the grid calculation system, based on the Cellular Automata mechanism, to generate parameters for angle gradients of closing behavior between branches. These parameters are applied in real time to the image generation system of the floral structure, altering the overall appearance of the flower. The flower shaping heavily employs the recursion mathematical mechanism. Through a set of geometric relationship designs, a concept like cell division is used to grow the next branch. In the iteration process, two characteristics emerge: (1) the development of an organizational relationship of length and angle between lines that presents an organic gradient sense, and (2) the development of movable joints between lines that can display activity for subsequent imitation of the Mimosa pudica motion cell mechanism, demonstrating dynamic deformations ""Close"". In existing research, it inspired that the pulvinar cells at the base of Mimosa leaves propagate electrical and chemical signals in response to external stimuli, leading to a chain reaction of leaf movement in adjacent cells. To implement this interlinking characteristic, I employ the computational mechanism of Cellular Automata, both for aesthetic representation and user sensation, within the tree structure's relationships."	https://dl.acm.org/doi/abs/10.1145/3610537.3622962	Scottie Chih-Chieh Huang
SensoryScape: Context-Aware Empathic VR Photography	SensoryScape presents a novel approach to context-aware empathic interactions (CAEIxs) using a VR head-mounted display (HMD) integrated with various biosensors. This innovative system addresses the need for personal, emotionally resonant, and empathic digital experiences by dynamically responding to users' real-time emotional and situational context. SensoryScape harnesses the Galea VR HMD, a device with integrated physiological sensors that track bio-signals such as brain activity, skin responses, muscle movements, and heart rate variability. These real-time biosignals, along with situational context, help estimate the user's emotional state within its context. The VR environment and empathic companion adapt to these cues, ensuring an optimized experience for the user's current emotional state. The demonstration of SensoryScape highlights the potential of VR HMDs with integrated physiological sensors for crafting empathic digital experiences. Our research contributes to immersive, adaptive, and user-focused empathic interactions, marking a promising shift in human-computer interaction.	https://dl.acm.org/doi/abs/10.1145/3610549.3614596	Kunal Gupta, Yuewei Zhang, Tamil Selvan Gunasekaran, Prasanth Sasikumar, Nanditha Krishna, Philip Pits, Conor Russomanno, Mark Billinghurst
ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering	We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is interactive, taking less than 1 second to produce a 3D model with voxel resolutions up to 5123. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase the ability of our method to learn geometric details and textures from shapes reconstructed from real-world photos. In addition, we have developed an interactive modeling application to demonstrate the generalizability of our method to various user inputs and the controllability it offers, allowing users to interactively sculpt a coarse voxel shape to define the overall structure of the detailized 3D shape. Code and data are available at https://github.com/qiminchen/ShaDDR.	https://dl.acm.org/doi/abs/10.1145/3610548.3618201	Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang
Shadow Harmonization for Realistic Compositing	Compositing virtual objects into real background images requires one to carefully match the scene's camera parameters, surface geometry, textures, and lighting to obtain plausible renderings. Recent learning approaches have shown many scene properties can be estimated from images, resulting in robust automatic single-image compositing systems, but many challenges remain. In particular, interactions between real and synthetic shadows are not handled gracefully by existing methods, which typically assume a shadow-free background. As a result, they tend to generate double shadows when the synthetic object's cast shadow overlaps a background shadow, and ignore shadows from the background that should be cast onto the synthetic object. In this paper, we present a compositing method for outdoor scenes that addresses these issues and produces realistic cast shadows. This requires identifying existing shadows, including soft shadow boundaries, then reasoning about the ambiguity of unknown ground albedo and scene lighting to match the color and intensity of shaded areas. Using supervision from shadow removal and detection datasets, we propose a generative adversarial pipeline and improved composition equations that simultaneously handle both shadow interaction scenarios. We evaluate our method on challenging, real outdoor images from multiple distributions and datasets. Quantitative and qualitative comparisons show our approach produces more realistic results than existing alternatives. Our code, datasets, and trained models are publicly available at https://lvsn.github.io/shadowcompositing.	https://dl.acm.org/doi/abs/10.1145/3610548.3618227	Lucas Valença, Jinsong Zhang, Michaël Gharbi, Yannick Hold-Geoffroy, Jean-François Lalonde
ShapeSonic: Sonifying Fingertip Interactions for Non-Visual Virtual Shape Perception	"For sighted users, computer graphics and virtual reality allow them to model and perceive imaginary objects and worlds. However, these approaches are inaccessible to blind and visually impaired (BVI) users, since they primarily rely on visual feedback. To this end, we introduce ShapeSonic, a system designed to convey vivid 3D shape perception using purely audio feedback or sonification. ShapeSonic tracks users' fingertips in 3D and provides real-time sound feedback (sonification). The shape's geometry and sharp features (edges and corners) are expressed as sounds whose volumes modulate according to fingertip distance. ShapeSonic is based on a mass-produced, commodity hardware platform (Oculus Quest). In a study with 15 sighted and 6 BVI users, we demonstrate the value of ShapeSonic in shape landmark localization and recognition. ShapeSonic users were able to quickly and relatively accurately ""touch"" points on virtual 3D shapes in the air."	https://dl.acm.org/doi/abs/10.1145/3610548.3618246	Jialin Huang, Alexa Siu, Rana Hanocka, Yotam Gingold
Shrink & Morph: 3D-Printed Self-Shaping Shells Actuated by a Shape Memory Effect	While 3D printing enables the customization and home fabrication of a wide range of shapes, fabricating freeform thin-shells remains challenging. As layers misalign with the curvature, they incur structural deficiencies, while the curved shells require large support structures, typically using more material than the part itself. We present a computational framework for optimizing the internal structure of 3D printed plates such that they morph into a desired freeform shell when heated. This exploits the shrinkage effect of thermoplastics such as PLA, which store internal stresses along the deposition directions. These stresses get released when the material is heated again above its glass transition temperature, causing an anisotropic deformation that induces curvature. Our inverse design method takes as input a freeform surface and finds an optimized set of deposition trajectories in each layer such that their anisotropic shrinkage deforms the plate into the prescribed surface geometry. We optimize for a continuous vector field that varies across the plate and within its thickness. The algorithm then extracts a set of deposition trajectories from the vector field in order to fabricate the flat plates on standard FFF printers. We validate our algorithm on freeform, doubly-curved surfaces.	https://dl.acm.org/doi/abs/10.1145/3618386	David Jourdan, Pierre-Alexandre Hugron, Camille Schreck, Jonàs Martínez, Sylvain Lefebvre
SimpleNeRF: Regularizing Sparse Input Neural Radiance Fields with Simpler Solutions	Neural Radiance Fields (NeRF) show impressive performance for the photo-realistic free-view rendering of scenes. However, NeRFs require dense sampling of images in the given scene, and their performance degrades significantly when only a sparse set of views are available. Researchers have found that supervising the depth estimated by the NeRF helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the NeRF. We design augmented models that encourage simpler solutions by exploring the role of positional encoding and view-dependent radiance in training the few-shot NeRF. The depth estimated by these simpler models is used to supervise the NeRF depth estimates. Since the augmented models can be inaccurate in certain regions, we design a mechanism to choose only reliable depth estimates for supervision. Finally, we add a consistency loss between the coarse and fine multi-layer perceptrons of the NeRF to ensure better utilization of hierarchical sampling. We achieve state-of-the-art view-synthesis performance on two popular datasets by employing the above regularizations. The source code for our model can be found on our project page: https://nagabhushansn95.github.io/publications/2023/SimpleNeRF.html	https://dl.acm.org/doi/abs/10.1145/3610548.3618188	Nagabhushan Somraj, Adithyan Karanayil, Rajiv Soundararajan
Simultaneous Color Computer Generated Holography	Computer generated holography has long been touted as the future of augmented and virtual reality (AR/VR) displays, but has yet to be realized in practice. Previous high-quality, color holographic displays have made either a 3 × sacrifice on frame rate by using a sequential color illumination scheme or used more than one spatial light modulator (SLM) and/or bulky, complex optical setups. The reduced frame rate of sequential color introduces distracting judder and color fringing in the presence of head motion while the form factor of current simultaneous color systems is incompatible with a head-mounted display. In this work, we propose a framework for simultaneous color holography that allows the use of the full SLM frame rate while maintaining a compact and simple optical setup. Simultaneous color holograms are optimized through the use of a perceptual loss function, a physics-based neural network wavefront propagator, and a camera-calibrated forward model. We measurably improve hologram quality compared to other simultaneous color methods and move one step closer to the realization of color holographic displays for AR/VR.	https://dl.acm.org/doi/abs/10.1145/3610548.3618250	Eric Markley, Nathan Matsuda, Florian Schiffers, Oliver Cossairt, Grace Kuo
SinMPI: Novel View Synthesis from a Single Image with Expanded Multiplane Images	Single-image novel view synthesis is a challenging and ongoing problem that aims to generate an infinite number of consistent views from a single input image. Although significant efforts have been made to advance the quality of generated novel views, less attention has been paid to the expansion of the underlying scene representation, which is crucial to the generation of realistic novel view images. This paper proposes SinMPI, a novel method that uses an expanded multiplane image (MPI) as the 3D scene representation to significantly expand the perspective range of MPI and generate high-quality novel views from a large multiplane space. The key idea of our method is to use Stable Diffusion [Rombach et al. 2021] to generate out-of-view contents, project all scene contents into an expanded multiplane image according to depths predicted by monocular depth estimators, and then optimize the multiplane image under the supervision of pseudo multi-view data generated by a depth-aware warping and inpainting module. Both qualitative and quantitative experiments have been conducted to validate the superiority of our method to the state of the art. Our code and data are available at https://github.com/TrickyGo/SinMPI.	https://dl.acm.org/doi/abs/10.1145/3610548.3618155	Guo Pu, Peng-Shuai Wang, Zhouhui Lian
Single-Image 3D Human Digitization with Shape-guided Diffusion	We present an approach to generate a 360-degree view of a person with a consistent, high-resolution appearance from a single input image. NeRF and its variants typically require videos or images from different viewpoints. Most existing approaches taking monocular input either rely on ground-truth 3D scans for supervision or lack 3D consistency. While recent 3D generative models show promise of 3D consistent human digitization, these approaches do not generalize well to diverse clothing appearances, and the results lack photorealism. Unlike existing work, we utilize high-capacity 2D diffusion models pretrained for general image synthesis tasks as an appearance prior of clothed humans. To achieve better 3D consistency while retaining the input identity, we progressively synthesize multiple views of the human in the input image by inpainting missing regions with shape-guided diffusion conditioned on silhouette and surface normal. We then fuse these synthesized multi-view images via inverse rendering to obtain a fully textured high-resolution 3D mesh of the given person. Experiments show that our approach outperforms prior methods and achieves photorealistic 360-degree synthesis of a wide range of clothed humans with complex textures from a single image.	https://dl.acm.org/doi/abs/10.1145/3610548.3618153	Badour Albahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, Jia-Bin Huang
Slippage-Preserving Reshaping of Human-Made 3D Content	Artists often need to 3D models of human-made objects by changing the relative proportions or scales of different model parts or elements while preserving the look and structure of the inputs. Manually reshaping inputs to satisfy these criteria is highly time-consuming; the edit in our teaser took an artist 5 hours to complete. However, existing methods for 3D shape editing are largely designed for other tasks and produce undesirable outputs when repurposed for reshaping. Prior work on 2D curve network reshaping suggests that in 2D settings the user-expected outcome is achieved when the reshaping edit keeps the of the different model elements and when these elements However, our observations suggest that in 3D viewers are tolerant of tangential scaling if and when this scaling preserves and reduces changes in element size, or , relative to the input. Slippage preservation requires surfaces which are locally with respect to a given rigid motion to retain this property post-reshaping (a motion is slippable if when applied to the surface, it slides the surface along itself without gaps). We build on these observations by first extending the 2D ALUP framework to 3D and then modifying it to allow non-uniform scaling while promoting slippage and scale preservation. Our 3D ALUP extension produces reshaped outputs better aligned with viewer expectations than prior alternatives; our slippage-aware method further improves the outcome producing results on par with manual reshaping ones. Our method does not require any user input beyond specifying control handles and their target locations. We validate our method by applying it to over one hundred diverse inputs and by comparing our results to those generated by alternative approaches and manually. Comparative study participants preferred our outputs over the best performing traditional deformation method by a 65% margin and over our 3D ALUP extension by a 61% margin; they judged our outputs as at least on par with manually produced ones.	https://dl.acm.org/doi/abs/10.1145/3618391	Chrystiano Araújo, Nicholas Vining, Silver Burla, Manuel Ruivo De Oliveira, Enrique Rosales, Alla Sheffer
Smile Mask	A boy witnessed a girl's death in his childhood.	https://dl.acm.org/doi/abs/10.1145/3626964.3626979	Yuanyuan Lu
Somatic Music: Enhancing Musical Experiences through the Performer's Embodiment	Music serves as a tangible embodiment of the performer's expression, bearing a distinct musicality that transcends auditory perception. This study delves into the distinctive musicality inherent to musicians through physical data analysis. It introduces a novel approach to the auditory experience by incorporating tactile stimulation via vibration and pressure to present an innovative channel for conveying musicality to the audience. The primary aim is to enhance the realm of music appreciation by amplifying the scope of performers' expressive capacities, thereby revolutionizing the conventional paradigm of music experiences.	https://dl.acm.org/doi/abs/10.1145/3610542.3626131	Aoi Uyama, Youichi Kamiyama, Sohei Wakisaka, Arata Horie, Tatsuya Saito, Kouta Minamizawa
Sonus Maris; Strange Attractor	"Sonus Maris; Strange Attractor- is a two-channel video work that navigates the intersections between art and science, developed during an ongoing collaboration between artist Dr. Nigel Helyer and water engineers and scientists at the UNSW Water Research Laboratory (WRL). Working in close partnership with WRL postdoctoral researcher Dr. Tino Heimhuber, Helyer employs audio-visual media to reinterpret data charting the unique dynamics of intermittently closed and open lakes and lagoons (ICOLLs). ICOLLs are the most prominent type of estuaries found on the NSW coastline and are unique in that they alternate between open and closed oceanic entrance conditions, driven by the dynamic interplay between oceanic and land-based forces. The fluctuations of water flow act as 'canary in the mine' indicators - reacting dynamically to our increasingly kinetic weather systems. Through data archaeology and a novel algorithm ""Inlet Tracker', the collaborators extract valuable information from a fourdecade archive of public satellite imagery, drawing attention to long-term morphological and eco-hydrological variations in these crucial sites. Helyer interprets this detail-rich source material to compose a series of musical scores translating the flow dynamics of the four ICOLLs sites as a multisensory experience. Helyer's animations of satellite imagery and experimental music invite audiences to rethink knowledge systems by seeing, feeling, and hearing the flows and patterns of coastal environments."	https://dl.acm.org/doi/abs/10.1145/3610537.3622944	Nigel Llwyd William Helyer
Sparse Stress Structures from Optimal Geometric Measures	Identifying optimal structural designs given loads and constraints is a primary challenge in topology optimization and shape optimization. We propose a novel approach to this problem by finding a minimal tensegrity structure—a network of cables and struts in equilibrium with a given loading force. Through the application of geometric measure theory and compressive sensing techniques, we show that this seemingly difficult graph-theoretic problem can be reduced to a numerically tractable continuous optimization problem. With a light-weight iterative algorithm involving only Fast Fourier Transforms and local algebraic computations, we can generate sparse supporting structures featuring detailed branches, arches, and reinforcement structures that respect the prescribed loading forces and obstacles.	https://dl.acm.org/doi/abs/10.1145/3610548.3618193	Dylan Rowe, Albert Chern
Stable Discrete Bending by Analytic Eigensystem and Adaptive Orthotropic Geometric Stiffness	In this paper, we address two limitations of dihedral angle based discrete bending (DAB) models, i.e. the indefiniteness of their energy Hessian and their vulnerability to geometry degeneracies. To tackle the indefiniteness issue, we present novel analytic expressions for the eigensystem of a DAB energy Hessian. Our expressions reveal that DAB models typically have positive, negative, and zero eigenvalues, with four of each, respectively. By using these expressions, we can efficiently project an indefinite DAB energy Hessian as positive semi-definite analytically. To enhance the stability of DAB models at degenerate geometries, we propose rectifying their indefinite geometric stiffness matrix by using orthotropic geometric stiffness matrices with adaptive parameters calculated from our analytic eigensystem. Among the twelve motion modes of a dihedral element, our resulting Hessian for DAB models retains only the desirable bending modes, compared to the undesirable altitude-changing modes of the exact Hessian with original geometric stiffness, all modes of the Gauss-Newton approximation without geometric stiffness, and no modes of the projected Hessians with inappropriate geometric stiffness. Additionally, we suggest adjusting the compression stiffness according to the Kirchhoff-Love thin plate theory to avoid over-compression. Our method not only ensures the positive semidefiniteness but also avoids instability caused by large bending forces at degenerate geometries. To demonstrate the benefit of our approaches, we show comparisons against existing methods on the simulation of cloth and thin plates in challenging examples.	https://dl.acm.org/doi/abs/10.1145/3618372	Zhendong Wang, Yin Yang, Huamin Wang
Standard Shader Ball: A Modern and Feature-Rich Render Test Scene	We discuss our recent material test scene contribution to the USD Assets Working Group [2022] and enumerate some of its desirable test qualities. The scene employs modern geometry and material standards and has been released to the rendering community under a permissive licence, so that others may benefit from its useful properties, to encourage standardisation, and to remove the need for continuing reinvention.	https://dl.acm.org/doi/abs/10.1145/3610543.3626181	André Mazzone, Chris Rydalch
Story-to-Motion: Synthesizing Infinite and Controllable Character Animation from Long Text	Generating natural human motion from a story has the potential to transform the landscape of animation, gaming, and film industries. A new and challenging task, Story-to-Motion, arises when characters are required to move to various locations and perform specific motions based on a long text description. This task demands a fusion of low-level control (trajectories) and high-level control (motion semantics). Previous works in character control and text-to-motion have addressed related aspects, yet a comprehensive solution remains elusive: character control methods do not handle text description, whereas text-to-motion methods lack position constraints and often produce unstable motions. In light of these limitations, we propose a novel system that generates controllable, infinitely long motions and trajectories aligned with the input text. 1) We leverage contemporary Large Language Models to act as a text-driven motion scheduler to extract a series of (text, position, duration) pairs from long text. 2) We develop a text-driven motion retrieval scheme that incorporates motion matching with motion semantic and trajectory constraints. 3) We design a progressive mask transformer that addresses common artifacts in the transition motion such as unnatural pose and foot sliding. Beyond its pioneering role as the first comprehensive solution for Story-to-Motion, our system undergoes evaluation across three distinct sub-tasks: trajectory following, temporal action composition, and motion blending, where it outperforms previous state-of-the-art (SOTA) motion synthesis methods across the board. Homepage: https://story2motion.github.io/	https://dl.acm.org/doi/abs/10.1145/3610543.3626176	Zhongfei Qing, Zhongang Cai, Zhitao Yang, Lei Yang
Subspace Mixed Finite Elements for Real-Time Heterogeneous Elastodynamics	Real-time elastodynamic solvers are well-suited for the rapid simulation of homogeneous elastic materials, with high-rates generally enabled by aggressive early termination of timestep solves. Unfortunately, the introduction of strong domain heterogeneities can make these solvers slow to converge. Stopping the solve short creates visible damping artifacts and rotational errors. To address these challenges we develop a reduced mixed finite element solver that preserves rich rotational motion, even at low-iteration regimes. Specifically, this solver augments time-step solve optimizations with auxillary stretch degrees of freedom at mesh elements, and maintains consistency with the primary positional degrees of freedoms at mesh nodes via explicit constraints. We make use of a Skinning Eigenmode subspace for our positional degrees of freedom. We accelerate integration of non-linear elastic energies with a cubature approximation, placing stretch degrees of freedom at cubature points. Across a wide range of examples we demonstrate that this subspace is particularly well suited for heterogeneous material simulation. Our resulting method is a subspace mixed finite element method completely decoupled from the resolution of the mesh that is well-suited for real-time simulation of heterogeneous domains.	https://dl.acm.org/doi/abs/10.1145/3610548.3618220	Ty Trusty, Otman Benchekroun, Eitan Grinspun, Danny M. Kaufman, David I.W. Levin
Subspace-Preconditioned GPU Projective Dynamics with Contact for Cloth Simulation	We propose an efficient cloth simulation method that combines the merits of two drastically different numerical procedures, namely the subspace integration and parallelizable iterative relaxation. We show those two methods can be organically coupled within the framework of projective dynamics (PD), where both low- and high-frequency cloth motions are effectively and efficiently computed. Our method works seamlessly with the state-of-the-art contact handling algorithm, the incremental potential contact (IPC), to offer the non-penetration guarantee of the resulting animation. Our core ingredient centers around the utilization of subspace for the expedited convergence of Jacobi-PD. This involves solving the reduced global system and smartly employing its precomputed factorization. In addition, we incorporate a time-splitting strategy to handle the frictional self-contacts. Specifically, during the PD solve, we employ a quadratic proxy to approximate the contact barrier. The prefactorized subspace system matrix is exploited in a reduced-space LBFGS. The LBFGS method starts with the reduced system matrix of the rest shape as the initial Hessian approximation, incorporating contact information into the reduced system progressively, while the full-space Jacobi iteration captures high-frequency details. Furthermore, we address penetration issues through a penetration correction step. It minimizes an incremental potential without elasticity using Newton-PCG. Our method can be efficiently executed on modern GPUs. Experiments show significant performance improvements over existing GPU solvers for high-resolution cloth simulation.	https://dl.acm.org/doi/abs/10.1145/3610548.3618157	Xuan Li, Yu Fang, Lei Lan, Huamin Wang, Yin Yang, Minchen Li, Chenfanfu Jiang
Superb Lyrebird Sequences, 2023.	In Unruly Times, like many of us, I have experienced two significant stand-out events, the devastating bushfires of 2019- 2020 and the COVID-19 pandemic. In response, my focus has been on reconnecting with the natural world by closely observing local Superb Lyrebirds, documenting their unique behaviours, vocalizations, and dance. As the field of deep fake algorithms continues to evolve, their ability to convincingly simulate the visual appearance and acoustic characteristics of real individuals becomes increasingly advanced. Interestingly, the Australian Lyrebird has been a master of mimicry since ancient times, mimicking birds within its environment and more recently, chainsaws and cameras. Inspired by this natural and artificial phenomenon of mimicry, this project explores the intricate dynamics of representation, perception, and deception. The first sequence documents the Lyrebird's mimicry in realtime, disrupting the interplay between real, reversed, and negative time to distort traditional conventions of cinematic time. In the second sequence slowly animated hybrid creatures, are morphed together. Created using genetic algorithms trained on millions of images, new images were created by cross-breeding multiple image genes. These morphing entities exist in a synthesised latent space, created by the artist, the community, and algorithms, challenging notions of abstraction, representation, and authorship. Thirdly, a text-to-image-generated image of a person converses using the lyrical language of the Lyrebird. This convergence hints at a future where interconnectedness between humanity, technology, and nature becomes further intertwined. By blurring the boundaries between these seemingly separate domains, I invite viewers to contemplate the interplay between the enigmatic aspects of nature, our conceptual understanding of representation and perception, the potential dangers of hyper-realistic fakes, and the potential futures of virtual characters embodied with animal behaviours.	https://dl.acm.org/doi/abs/10.1145/3610537.3622950	Wade Marynowsky
Synced Drift: A Novel Sport Using a System that Harmonises Human Movement to Transcend Distance and Ability	"""Synced Drift"" provides a new inclusive sport experience that allows two individuals to play a drifting race together while sharing a single body. Synced drift leverages a system consisting of a large-diameter omni-wheel mechanism enabling omnidirectional movement, two sensing seats that detects the user's center of gravity shift, a mobile vehicle, and a viewpoint sharing system. The movement of the mobile vehicle is achieved by proportional control of the body movements acquired by the two sensing seats, one placed on the vehicle and one remote, from two or more individuals. The speed and direction are adjusted based on the harmonization of the inputs from the two users. This system allows people who were previously unable to participate in sports due to physical limitations, for example people with spinal cord injuries, or those with who had a stroke, to participate equally, regardless of individual impairments. Using the above system, the rules of the sport were designed and implemented. In this demonstration, the system explores the possibility of people coming together, racing and competing as one, transcending physical characteristics and physical locations."	https://dl.acm.org/doi/abs/10.1145/3610541.3614582	Ryoichi Ando, Giulia Barbareschi, Midori Kawaguchi, Kouta Minamizawa
SynthDa: Exploiting Existing Real-World Data for Usable and Accessible Synthetic Data Generation	"Acquiring real-world data for computer vision presents challenges such as data scarcity, high costs, and privacy concerns. We introduce SynthDa, an automated approach for usable synthetic data generation (SDG) that empowers users with varying expertise to create diverse synthetic data from existing real-world datasets. It combines pose estimation, synthetic scene creation, and domain randomization to offer data variants. Ease of SDG through SynthDa enables different permutations and combinations of synthetic data that allow users to explore efficacy of various data configurations in relation to their specific AI tasks. Our experiments across multiple existing datasets and models demonstrate the utility of SynthDa in challenging nuances such as the ""more data, the better"" paradigm; revealing that excessive synthetic data may degrade performance and vice versa. In a pilot user study with 24 participants, we show the perceived usefulness of SynthDa as a promising SDG tool for overcoming challenges related to real-world data acquisition."	https://dl.acm.org/doi/abs/10.1145/3610543.3626168	Megani Rajendran, Chek Tien Tan, Indriyati Atmosukarto, Aik Beng Ng, Zhihua Zhou, Andrew Grant, Simon See
Table Cape VR Education Technology Project: An Immersive Learning Experience of Walking with Giants	"This paper profiles the Table Cape VR Education Technology Project titled ""Walking with Giants,"" which was designed to offer an immersive learning experience for community storytelling. Table Cape is located in Wynyard, a town in rural Northwest Tasmania, Australia; it is an extinct volcano renowned for its rich tourism and heritage values. As part of a wider education technology and digital tourism initiative, Walking with Giants provides educators, students, and community members with the opportunity to visualise the high-fidelity 3D scanned terrain of Table Cape in virtual reality. Currently, the project is being exhibited at the Wonders of Wynyard Exhibition and Visitor Information Centre - a joint initiative involving creators and researchers from the University of Tasmania, industry partners from Business Northwest, and the Waratah Wynyard Council. In this paper, we describe the technical aspects of this digital technologies project and discuss the specialised knowledge-based requirements for teaching and learning incorporating such resources."	https://dl.acm.org/doi/abs/10.1145/3610540.3627002	Zi Siang See, Greg Oates, David Hicks, Cyndia Hilliger, Justin McErlain, Jean-Luc Schmid, Kaitlyn Dierikx
TableMorph: Haptic Experience with Movable Tables and Redirection	"We presented ""TableMorph,"" a novel method that combines redirection techniques and encountered-type haptic displays. In this method, concave and convex-shaped tables with mobile robots move through the real environment according to the relative positions of the user and the virtual tables in the virtual environment. In the process, by using redirection techniques that change the position and orientation of the user's virtual hand and the shape of the virtual tables visually presented through the head-mounted display from the actual one, the system can make them feel, both visually and haptically, that the table has a shape very different from the real one. In the demonstration at SIGGRAPH ASIA 2023 E-tech, participants can try virtual mazes as an application of the proposed method. As the users move around in the virtual maze separated by tables, the arrangement of the tables in the real environment changes. This allows for the presentation of various types of mazes that are four times larger than the physical space."	https://dl.acm.org/doi/abs/10.1145/3610541.3614574	Amane Yamaguchi, Sotaro Yokoi, Keigo Matsumoto, Takuji Narumi
Text-Guided Synthesis of Eulerian Cinemagraphs	We introduce Text2Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions --- an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. We focus on cinemagraphs of fluid elements, such as flowing rivers, and drifting clouds, which exhibit continuous motion and repetitive textures. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image from a single text prompt --- a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predicted motion can then be transferred to the artistic image to create the final cinemagraph. Our method outperforms existing approaches in creating cinemagraphs for natural landscapes as well as artistic and other-worldly scenes, as validated by automated metrics and user studies. Finally, we demonstrate two extensions: animating existing paintings and controlling motion directions using text.	https://dl.acm.org/doi/abs/10.1145/3618326	Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee, Sergey Tulyakov, Jun-Yan Zhu
Text-Guided Vector Graphics Customization	Vector graphics are widely used in digital art and valued by designers for their scalability and layer-wise topological properties. However, the creation and editing of vector graphics necessitate creativity and design expertise, leading to a time-consuming process. In this paper, we propose a novel pipeline that generates high-quality customized vector graphics based on textual prompts while preserving the properties and layer-wise information of a given exemplar SVG. Our method harnesses the capabilities of large pre-trained text-to-image models. By fine-tuning the cross-attention layers of the model, we generate customized raster images guided by textual prompts. To initialize the SVG, we introduce a semantic-based path alignment method that preserves and transforms crucial paths from the exemplar SVG. Additionally, we optimize path parameters using both image-level and vector-level losses, ensuring smooth shape deformation while aligning with the customized raster image. We extensively evaluate our method using multiple metrics from vector-level, image-level, and text-level perspectives. The evaluation results demonstrate the effectiveness of our pipeline in generating diverse customizations of vector graphics with exceptional quality. The project page is https://intchous.github.io/SVGCustomization.	https://dl.acm.org/doi/abs/10.1145/3610548.3618232	Peiying Zhang, Nanxuan Zhao, Jing Liao
Text-driven Tree Modeling on L-System	Text-driven methods have recently gained substantial attention in the realm of image and 3D model generation. A critical aspect of these methods is CLIP (Contrastive Language-Image Pre-training), which computes semantic similarities between input texts and resultant images. This paper introduces a text-driven approach to tree modeling, adopting an optimization technique with CLIP. Tree models are generated through L-System. We adopt genetic algorithms for optimization, determining fitness through CLIP. The efficacy of our method is demonstrated through various examples.	https://dl.acm.org/doi/abs/10.1145/3610542.3626126	Yudai Ichimura, Syuhei Sato
Texture Atlas Compression Based on Repeated Content Removal	Optimizing the memory footprint of 3D models can have a major impact on the user experiences during real-time rendering and streaming visualization, where the major memory overhead lies in the high-resolution texture data. In this work, we propose a robust and automatic pipeline to content-aware, lossy compression for texture atlas. The design of our solution lies in two observations: 1) mapping multiple surface patches to the same texture region is seamlessly compatible with the standard rendering pipeline, requiring no decompression before any usage; 2) a texture image has background regions and salient structural features, which can be handled separately to achieve a high compression rate. Accordingly, our method contains joint operations of image segmentation, re-meshing, UV unwrapping, and texture baking. To evaluate the efficacy of our approach, we batch-processed a dataset containing 100 models collected online. On average, our method achieves a texture atlas compression ratio of 81.41% with an averaged PSNR and MS-SSIM scores of 40.90 and 0.98, a marginal error in visual appearance.	https://dl.acm.org/doi/abs/10.1145/3610548.3618150	Yuzhe Luo, Xiaogang Jin, Zherong Pan, Kui Wu, Qilong Kou, Xiajun Yang, Xifeng Gao
Thaba Ye	Under the shadow of a mysterious Mountain in a remote part of South Africa, where legends of the local Bapedi people say that those who go there never come back, Thato has to face her fears in order to save her sick brother.	https://dl.acm.org/doi/abs/10.1145/3626964.3626988	Preetam Dhar, Daria Batueva, Hannah Judd, Mogau Kekana, Merel Hamers, Leroy Le Roux, Cécile Blondel
The Day Spring Comes	Spring of four seasons and the spring of our lives.	https://dl.acm.org/doi/abs/10.1145/3626964.3626990	Sooyeon Shin
The Effect of Wearing Knee Supporters on the Applicable Gain of Redirected Walking	Redirected Walking (RDW) is a locomotion technique that enables users to explore extensive virtual environments while confined to limited real-world environment by manipulating orientation and coordinates within the virtual environment. Previous research has shown that RDW can be made more effective by wearing a knee-tightening device. In this study, we examined the effects of two different knee supporters—band-type and soft-type—on the applicable gain of RDW. A significant difference in applicable gain between the two supporter conditions was confirmed. This result indicates that knee tightening affects the applicable gains of RDW and suggest that a more comfortable RDW experience may be possible with the use of appropriately shaped supporters.	https://dl.acm.org/doi/abs/10.1145/3610542.3626148	Gaku Fukui, Takuto Nakamura, Keigo Matsumoto, Takuji Narumi, Hideaki Kuzuoka
The Effects of Avatar Voice and Facial Expression Intensity on Emotional Recognition and User Perception	The use of avatars of various rendering styles (e.g., abstract, cartoon, realistic) in virtual reality is ever-increasing. However, little is known about the effects of auditory stimuli, specifically avatar voices, on users' perceived realism. This paper aims to investigate and better understand the role of a look-alike avatar's vocal and facial expression intensity on users' perceived realism and emotional recognition using a virtual bystander scenario. Results show that avatars' vocal intensity generally affected study participants' emotional recognition while facial expression intensity affected their perceived realism. The results have implications for the perception and effectiveness of look-alike avatars in virtual environments, specifically industry training for dangerous or non-replicable situations, such as school shootings and exposure therapy.	https://dl.acm.org/doi/abs/10.1145/3610543.3626158	Trinity Suma, Birate Sonia, Kwame Agyemang Baffour, Oyewole Oyekoya
The Fusion Nexus: Exploring the Confluence of Virtual and Real Worlds through Biocognitive Audio-Verbal Interface in Immersive XR Environments	In this demo, we present FusionNexus, an asymmetric collaboration experience where a virtual-world player needs to guide a real-world player to make certain actions. A machine learning algorithm was used to do the pose detection of the real-world player. We collect the virtual-world player's heartbeat to control drum beats in background music played to the real-world player to convey emotional status of the virtual-world player. In this demonstration, two players will collaborate to accomplish a time task.	https://dl.acm.org/doi/abs/10.1145/3610549.3614594	Bowen Yuan, Zirui Xiao, Tamil Selvan Gunasekaran, Qianyuan Zou, Zhuang Chang, Misha Mirza, Gun Lee, Mark Billinghurst
The Garden of Unearthly Delights	The Garden of Unearthly Desires is an interactive physical/digital installation in which three, real-time simulated biomes evolve over the course of a day as users make ethical decisions that alter the paramaters of the virtual worlds. The work is inspired by the paintings of Renaissance master Hieronymus Bosch, which depicted surreal worlds with mythological characters in order to explore the dynamics between the spiritual and the physical. Bosch's masterpiece 'The Garden of Earthly Delights' is a triptych altarpiece depiecting Heaven, Earth and Hell according to Bosch's visualisation of the moral characteristics of each sphere. My work takes Bosch's original and turns it into an interactive morality play in which user inputs create chaos, ecological harmony, or hedonistic effects within three evolving biomes. Each virtual world is controlled by artificial intelligence behaviours: characters explore, interact, hunt and fight; plants grow, flower and wither, all reacting dynamically to stimulus. Audience behaviours influence the evolution of the worlds. Periodically, a narrator asks questions that prompt the audience to make decisions that reflect their attitudes towards the world. These questions will ask audiences to think carefully about issues such as climate change, social responsibility, future industries and culture all couched within a poetic narrative based upon medieval literature. Users respond by selecting from a range of responses that cause the narrative to branch. Their selections are fed back into the virtual world, changing the way its characters and environments adapt. Each selection is mapped to a set of ethical variables, with corresponding algoithms that control environmental simulations. The 'health' of the virtual world is charted on an onscreen data dashboard, providing real time statistics of the sentiments of the audience. In this way, the visual and simulative evolution of The Garden becomes a data visualisation of the behaviours of the audience.	https://dl.acm.org/doi/abs/10.1145/3610537.3622960	Andrew Yip
The Most Expensive Museum in the World: Three Player Cooperative Game Between VR and PC Platforms Investigating Empathy between Players and Historical Characters.	The paper describes an art installation based on a local cooperative cross platform VR/PC game for three players. The story is based on the historical story of a nuclear power plant, which was built, but never turned on because of a public referendum. The game mechanics are built upon the interaction of three playable historical characters: engineer, activist, and politician. The main goal was to foster empathy in the players by replaying the game from each character's perspective. We present the story implementation, custom-made interfaces, physical setup, and an evaluation of recordings from a four-month display in a public gallery.	https://dl.acm.org/doi/abs/10.1145/3610591.3616425	Vojtěch Radakulan, David Sedláček
The Shortest Route is Not Always the Fastest: Probability-Modeled Stereoscopic Eye Movement Completion Time in VR	Speed and consistency of target-shifting play a crucial role in human ability to perform complex tasks. Shifting our gaze between objects of interest quickly and consistently requires changes both in depth and direction. Gaze changes in depth are driven by slow, inconsistent which rotate the eyes in opposite directions, while changes in direction are driven by ballistic, consistent movements called , which rotate the eyes in the same direction. In the natural world, most of our eye movements are a combination of both types. While scientific consensus on the nature of saccades exists, vergence and combined movements remain less understood and agreed upon. We eschew the lack of scientific consensus in favor of proposing an operationalized computational model which predicts the completion time of any type of gaze movement during target-shifting in 3D. To this end, we conduct a psychophysical study in a stereo VR environment to collect more than 12,000 gaze movement trials, analyze the temporal distribution of the observed gaze movements, and fit a probabilistic model to the data. We perform a series of objective measurements and user studies to validate the model. The results demonstrate its predictive accuracy, generalization, as well as applications for optimizing visual performance by altering content placement. Lastly, we leverage the model to measure differences in human target-changing time relative to the natural world, as well as suggest scene-aware projection depth. By incorporating the complexities and randomness of human oculomotor control, we hope this research will support new behavior-aware metrics for VR/AR display design, interface layout, and gaze-contingent rendering.	https://dl.acm.org/doi/abs/10.1145/3618334	Budmonde Duinkharjav, Benjamin Liang, Anjul Patney, Rachel Brown, Qi Sun
The Witch's Cat	Jealousy strikes a witch's cat in this ultimately heartwarming tale.	https://dl.acm.org/doi/abs/10.1145/3626964.3627000	Seth Holladay, Craig Van Dyke, Abby Staker, Jessica Fink Blaine
The Zoetop: a kinesthetic aware zoetrope	"We introduce the Zoetop, an innovative variation of the Zoetrope opening up new opportunities for design. At the core of the Zoetop is a small and inexpensive electronic device that can be embedded on any naturally spinning object, such as hand-spun spinning tops, car wheels, bouncing balls, or windmills. The device uses an inertial sensor to keep track of its own instantaneous axis of rotation and angular speed. It then produces a constant number of flashes per revolution regardless of speed variations, and corresponding to the number of frames in the animation. As a consequence animations can be created without strobe light synchronization, delicate mechanical setups including driving motors, or any form of calibration. As such, this ""kinaesthetic-aware"" computational Zoetrope offers new opportunities for deployment in the wild, augmenting hand-spun toys, wind-powered chimes, propellers or fans with animated graphics. We believe that the Zoetop can inspire designers and the DIY community to come up with new ideas centered around this age-old yet still fascinating Victorian-era optical gadget."	https://dl.acm.org/doi/abs/10.1145/3610541.3614577	Alvaro Cassinelli, Daniel Saakes
The effect of display capabilities on the gloss consistency between real and virtual objects	A faithful reproduction of gloss is inherently difficult because of the limited dynamic range, peak luminance, and 3D capabilities of display devices. This work investigates how the display capabilities affect gloss appearance with respect to a real-world reference object. To this end, we employ an accurate imaging pipeline to achieve a perceptual gloss match between a virtual and real object presented side-by-side on an augmented-reality high-dynamic-range (HDR) stereoscopic display, which has not been previously attained to this extent. Based on this precise gloss reproduction, we conduct a series of gloss matching experiments to study how gloss perception degrades based on individual factors: object albedo, display luminance, dynamic range, stereopsis, and tone mapping. We support the study with a detailed analysis of individual factors, followed by an in-depth discussion on the observed perceptual effects. Our experiments demonstrate that stereoscopic presentation has a limited effect on the gloss matching task on our HDR display. However, both reduced luminance and dynamic range of the display reduce the perceived gloss. This means that the visual system cannot compensate for the changes in gloss appearance across luminance (lack of gloss constancy), and the tone mapping operator should be carefully selected when reproducing gloss on a low dynamic range (LDR) display.	https://dl.acm.org/doi/abs/10.1145/3610548.3618226	Bin Chen, Akshay Jindal, Michal Piovarči, Chao Wang, Hans-Peter Seidel, Piotr Didyk, Karol Myszkowski, Ana Serrano, Rafał K. Mantiuk
The yellow triangle	As M.Bou, the boss of the city underworld, offers a big reward to whoever brings him the Yellow Triangle. Bart and Oscar, two gangsters who always work together, decide to get it!	https://dl.acm.org/doi/abs/10.1145/3626964.3626989	Yoann Leclerc, Catherine Totems
Thin On-Sensor Nanophotonic Array Cameras	Today's commodity camera systems rely on compound optics to map light originating from the scene to positions on the sensor where it gets recorded as an image. To record images without optical aberrations, i.e., deviations from Gauss' linear model of optics, typical lens systems introduce increasingly complex stacks of optical elements which are responsible for the height of existing commodity cameras. In this work, we investigate as an alternative that employs an array of skewed lenslets and a learned reconstruction approach. The optical array is embedded on a metasurface that, at 700 nm height, is flat and sits on the sensor cover glass at 2.5 mm focal distance from the sensor. To tackle the highly chromatic response of a metasurface and design the array over the entire sensor, we propose a differentiable optimization method that continuously samples over the visible spectrum and factorizes the optical modulation for different incident fields into individual lenses. We reconstruct a megapixel image from our flat imager with a method that employs a generative diffusion model to sample an implicit prior. To tackle , we propose a method for acquiring paired captured training data in varying illumination conditions. We assess the proposed flat camera design in simulation and with an experimental prototype, validating that the method is capable of recovering images from diverse scenes in broadband with a single nanophotonic layer.	https://dl.acm.org/doi/abs/10.1145/3618398	Praneeth Chakravarthula, Jipeng Sun, Xiao Li, Chenyang Lei, Gene Chou, Mario Bijelic, Johannes Froesch, Arka Majumdar, Felix Heide
ToRoS: A Topology Optimization Approach for Designing Robotic Skins	Soft robotics offers unique advantages in manipulating fragile or deformable objects, human-robot interaction, and exploring inaccessible terrain. However, designing soft robots that produce large, targeted deformations is challenging. In this paper, we propose a new methodology for designing soft robots that combines optimization-based design with a simple and cost-efficient manufacturing process. Our approach is centered around the concept of robotic skins---thin fabrics with 3D-printed reinforcement patterns that augment and control plain silicone actuators. By decoupling shape control and actuation, our approach enables a simpler and cost-efficient manufacturing process. Unlike previous methods that rely on empirical design heuristics for generating desired deformations, our approach automatically discovers complex reinforcement patterns without any need for domain knowledge or human intervention. This is achieved by casting reinforcement design as a nonlinear constrained optimization problem and using a novel, three-field topology optimization approach tailored to fabrics with 3D-printed reinforcements. We demonstrate the potential of our approach by designing soft robotic actuators capable of various motions such as bending, contraction, twist, and combinations thereof. We also demonstrate applications of our robotic skins to robotic grasping with a soft three-finger gripper and locomotion tasks for a soft quadrupedal robot.	https://dl.acm.org/doi/abs/10.1145/3618382	Juan Montes Maestre, Ronan Hinchet, Stelian Coros, Bernhard Thomaszewski
Topology Guaranteed B-Spline Surface/Surface Intersection	The surface/surface intersection technique serves as one of the most fundamental functions in modern Computer Aided Design (CAD) systems. Despite the long research history and successful applications of surface intersection algorithms in various CAD industrial software, challenges still exist in balancing computational efficiency, accuracy, as well as topology correctness. Specifically, most practical intersection algorithms fail to guarantee the correct topology of the intersection curve(s) when two surfaces are in near-critical positions, which brings instability to CAD systems. Even in one of the most successfully used commercial geometry engines ACIS, such complicated intersection topology can still be a tough nut to crack. In this paper, we present a practical topology guaranteed algorithm for computing the intersection loci of two B-spline surfaces. Our algorithm well treats all types of common and complicated intersection topology with practical efficiency, including those intersections with multiple branches or cross singularities, contacts in several isolated singular points or highorder contacts along a curve, as well as intersections along boundary curves. We present representative examples of these hard topology situations that challenge not only the open-source geometry engine OCCT but also the commercial engine ACIS. We compare our algorithm in both efficiency and topology correctness on plenty of common and complicated models with the open-source intersection package in SISL, OCCT, and the commercial engine ACIS.	https://dl.acm.org/doi/abs/10.1145/3618349	Jieyin Yang, Xiaohong Jia, Dong-Ming Yan
Towards Efficient Local 3D Conditioning	Recently, Neural Implicit Representations (NIRs) have gained popularity for learning-based 3D shape representation. General representations, i.e. ones that share a decoder across a family of geometries have multiple advantages such as ability of generating previously unseen samples and smoothly interpolating between training examples. These representations, however, impose a trade-off between quality of reconstruction and memory footprint stored per sample. Globally conditioned NIRs suffer from a lack of quality in capturing intricate shape details, while densely conditioned NIRs demand excessive memory resources. In this work we suggest using a Neural Network to approximate a grid of latent codes, while sharing the decoder across the entire category. Our model achieves a significantly better reconstruction quality compared to globally conditioned methods, while using less memory per sample to store single geometry.	https://dl.acm.org/doi/abs/10.1145/3610542.3626151	Dingxi Zhang, Artem Lukoianov
Towards Garment Sewing Pattern Reconstruction from a Single Image	Garment sewing pattern represents the intrinsic rest shape of a garment, and is the core for many applications like fashion design, virtual try-on, and digital avatars. In this work, we explore the challenging problem of recovering garment sewing patterns from daily photos for augmenting these applications. To solve the problem, we first synthesize a versatile dataset, named SewFactory, which consists of around 1M images and ground-truth sewing patterns for model training and quantitative evaluation. SewFactory covers a wide range of human poses, body shapes, and sewing patterns, and possesses realistic appearances thanks to the proposed human texture synthesis network. Then, we propose a two-level Transformer network called Sewformer, which significantly improves the sewing pattern prediction performance. Extensive experiments demonstrate that the proposed framework is effective in recovering sewing patterns and well generalizes to casually-taken human photos. Code, dataset, and pre-trained models will be released.	https://dl.acm.org/doi/abs/10.1145/3618319	Lijuan Liu, Xiangyu Xu, Zhijie Lin, Jiabin Liang, Shuicheng Yan
Towards Practical Capture of High-Fidelity Relightable Avatars	In this paper, we propose a novel framework, Tracking-free Relightable Avatar (TRAvatar), for capturing and reconstructing high-fidelity 3D avatars. Compared to previous methods, TRAvatar works in a more practical and efficient setting. Specifically, TRAvatar is trained with dynamic image sequences captured in a Light Stage under varying lighting conditions, enabling realistic relighting and real-time animation for avatars in diverse scenes. Additionally, TRAvatar allows for tracking-free avatar capture and obviates the need for accurate surface tracking under varying illumination conditions. Our contributions are two-fold: First, we propose a novel network architecture that explicitly builds on and ensures the satisfaction of the linear nature of lighting. Trained on simple group light captures, TRAvatar can predict the appearance in real-time with a single forward pass, achieving high-quality relighting effects under illuminations of arbitrary environment maps. Second, we jointly optimize the facial geometry and relightable appearance from scratch based on image sequences, where the tracking is implicitly learned. This tracking-free approach brings robustness for establishing temporal correspondences between frames under different lighting conditions. Extensive qualitative and quantitative experiments demonstrate that our framework achieves superior performance for photorealistic avatar animation and relighting.	https://dl.acm.org/doi/abs/10.1145/3610548.3618138	Haotian Yang, Mingwu Zheng, Wanquan Feng, Haibin Huang, Yu-Kun Lai, Pengfei Wan, Zhongyuan Wang, Chongyang Ma
Towards a Psychophysically Plausible Simulation of Translucent Appearance	Understanding visual perception of materials is critical for informing image-based approaches to real-time rendering. This poster presents a new cue to translucency that can be efficiently modeled using graphical rendering.	https://dl.acm.org/doi/abs/10.1145/3610542.3626145	Takehiro Nagai, Hiroaki Kiyokawa, Stephen Palmisano, Juno Kim
Town Hall Square	Bernard's life is turned upside down by a little tiger.	https://dl.acm.org/doi/abs/10.1145/3626964.3626976	Christian Kaufmann, Paulina Larson
Training Orchestral Conductors in Beating Time	Orchestral conducting involves a rich vocabulary of gestures and so training conductors is challenging. We discuss how virtual reality and gesture detection could be used to aid this process. We describe our pilot interface for training conductors in basic beat patterns, using gestural input and virtual reality output. We investigated both positional and acceleration-based detection of beats, concluding that that the best way to detect beats reliably is to identify maxima in acceleration, that is, those moments in a beat that would appear as a flick to a human player. This practical evidence supports Wöllner's theory of how human players detect beats. We trialled our system with novices and experts, with a range of beating styles.	https://dl.acm.org/doi/abs/10.1145/3610543.3626155	Neil Anthony Dodgson, Kathleen Griffin
Transcale: Embodiment Transition Toward Multi-Verse Exploration	Transcale is a VR experience where the users can change the body scale smoothly while walking. By continuously modifying their perceived body scale, this interaction method provides the users to explore the virtual world from various points of view from a dwarf to a giant.	https://dl.acm.org/doi/abs/10.1145/3610549.3614598	Haruka Onoda, Sohei Wakisaka, Tatsuya Saito, Kouta Minamizawa
Transparent Object Reconstruction via Implicit Differentiable Refraction Rendering	Reconstructing the geometry of transparent objects has been a long-standing challenge. Existing methods rely on complex setups, such as manual annotation or darkroom conditions, to obtain object silhouettes and usually require controlled environments with designed patterns to infer ray-background correspondence. However, these intricate arrangements limit the practical application for common users. In this paper, we significantly simplify the setups and present a novel method that reconstructs transparent objects in unknown natural scenes without manual assistance. Our method incorporates two key technologies. Firstly, we introduce a volume rendering-based method that estimates object silhouettes by projecting the 3D neural field onto 2D images. This automated process yields highly accurate multi-view object silhouettes from images captured in natural scenes. Secondly, we propose transparent object optimization through differentiable refraction rendering with the neural SDF field, enabling us to optimize the refraction ray based on color rather than explicit ray-background correspondence. Additionally, our optimization includes a ray sampling method to supervise the object silhouette at a low computational cost. Extensive experiments and comparisons demonstrate that our method produces high-quality results while offering much more convenient setups.	https://dl.acm.org/doi/abs/10.1145/3610548.3618236	Fangzhou Gao, Lianghao Zhang, Li Wang, Jiamin Cheng, Jiawan Zhang
TreeGAN	TreeGAN is an investigation into how machine learning and generative adversarial networks (GANS) create 3-dimensional objects. As machine learning finds an increasing number of applications within visual culture, we was interested to see how such systems might influence how we think about 3D objects. When this project started in 2019, there were relatively few art projects that used machine learning to produce 3D objects and even fewer that were trained on 3D objects to produce 3D objects (as opposed to synthesising 3D forms from 2D images), partly due to the paucity of conditional datasets of 3D objects. We synthesised a dataset of 3D objects using a form that is easy to produce and recognise - trees. Previous studies for 3D machine learning tended to focus on geometrically simple objects such as IKEA furniture (Lim et al 2013) and industrial objects (Wu et al 2016), therefore, trees presented an opportunity to observe how a 3D machine learning system would approach complex yet familiar organic forms. Trees are often used in visual art as metaphors for the human experience, from the scholarly pines of Chinese ink painting (Clunas 2002) (McMahon 2003) to the martyred oaks of German Romanticism (Rosenblum 1975), and thus add an empathetic layer to our formal exploration. Three-dimensional trees are easy to produce on a large scale using Lindenmayer systems, and we made 76 unique tree templates, based on art historical references and exported 350 random variations of these templates, giving us a dataset of just over 26,000 3D trees. We watched the transition of beautiful abstractions as the system progressed from random 3D noise to recognizable trees, a process we likened to the analytical cubism of Picasso and Braque in the early 20th century, where we could observe a new technological system developing its own form of figuration.	https://dl.acm.org/doi/abs/10.1145/3610537.3622953	Peter Andrew Clarke Nelson
TwinTex: Geometry-Aware Texture Generation for Abstracted 3D Architectural Models	Coarse architectural models are often generated at scales ranging from individual buildings to scenes for downstream applications such as Digital Twin City, Metaverse, LODs, etc. Such piece-wise planar models can be abstracted as twins from 3D dense reconstructions. However, these models typically lack realistic texture relative to the real building or scene, making them unsuitable for vivid display or direct reference. In this paper, we present , the first automatic texture mapping framework to generate a photorealistic texture for a piece-wise planar proxy. Our method addresses most challenges occurring in such twin texture generation. Specifically, for each primitive plane, we first select a small set of photos with greedy heuristics considering photometric quality, perspective quality and facade texture completeness. Then, different levels of line features (LoLs) are extracted from the set of selected photos to generate guidance for later steps. With LoLs, we employ optimization algorithms to align texture with geometry from local to global. Finally, we fine-tune a diffusion model with a multi-mask initialization component and a new dataset to inpaint the missing region. Experimental results on many buildings, indoor scenes and man-made objects of varying complexity demonstrate the generalization ability of our algorithm. Our approach surpasses state-of-the-art texture mapping methods in terms of high-fidelity quality and reaches a human-expert production level with much less effort.	https://dl.acm.org/doi/abs/10.1145/3618328	Weidan Xiong, Hongqian Zhang, Botao Peng, Ziyu Hu, Yongli Wu, Jianwei Guo, Hui Huang
UVDoc: Neural Grid-based Document Unwarping	Restoring the original, flat appearance of a printed document from casual photographs of bent and wrinkled pages is a common everyday problem. In this paper we propose a novel method for grid-based single-image document unwarping. Our method performs geometric distortion correction via a fully convolutional deep neural network that learns to predict the 3D grid mesh of the document and the corresponding 2D unwarping grid in a dual-task fashion, implicitly encoding the coupling between the shape of a 3D piece of paper and its 2D image. In order to allow unwarping models to train on data that is more realistic in appearance than the commonly used synthetic Doc3D dataset, we create and publish our own dataset, called UVDoc, which combines pseudo-photorealistic document images with physically accurate 3D shape and unwarping function annotations. Our dataset is labeled with all the information necessary to train our unwarping network, without having to engineer separate loss functions that can deal with the lack of ground-truth typically found in document in the wild datasets. We perform an in-depth evaluation that demonstrates that with the inclusion of our novel pseudo-photorealistic dataset, our relatively small network architecture achieves state-of-the-art results on the DocUNet benchmark. We show that the pseudo-photorealistic nature of our UVDoc dataset allows for new and better evaluation methods, such as lighting-corrected MS-SSIM. We provide a novel benchmark dataset that facilitates such evaluations, and propose a metric that quantifies line straightness after unwarping. Our code, results and UVDoc dataset are available at https://github.com/tanguymagne/UVDoc.	https://dl.acm.org/doi/abs/10.1145/3610548.3618174	Floor Verhoeven, Tanguy Magne, Olga Sorkine-Hornung
Usability Evaluation of VR Shopping System not Imitating Real Stores	Virtual reality (VR) shopping has been attracting attention for its potential to offer people a new purchasing experience. However, most previous studies on VR shopping have dealt with systems that imitate real stores. Therefore, we focused on VR shopping systems that do not imitate real stores (non-store type systems) and evaluated their usability.First, we conducted an experiment to compare the shopping experience between a store type system and a circularly displayed non-store type system. The results revealed that the non-store type system was superior in terms of usefulness and ease of mobility but inferior in terms of spatial recognition and likability. On the basis of these results, we then proposed a retractable non-store type system and conducted an experiment to clarify its usability. As a result, the retractable non-store type system was rated better in terms of visibility and ease of mobility than the store type system. To create a better system, we have fixed and improved some problems with the retractable system pointed out by the experiment participants.	https://dl.acm.org/doi/abs/10.1145/3610542.3626150	Ikumi Hisamatsu, Yuji Sakamoto
Use of Physics-Based AI for Simulations and Modeling in the Era of Digital Twins	What an AI powered future might look like? Rapid evolution of computing capabilities enables transforming process of scientific discovery and inventing new ways to experience and interact with simulation outcomes in real-time, especially in the context of modern Digital Twins.	https://dl.acm.org/doi/abs/10.1145/3610538.3614627	Tomasz Bednarz, Ram Cherukuri
VASCO: Volume and Surface Co-Decomposition for Hybrid Manufacturing	Additive and subtractive hybrid manufacturing (ASHM) involves the alternating use of additive and subtractive manufacturing techniques, which provides unique advantages for fabricating complex geometries with otherwise inaccessible surfaces. However, a significant challenge lies in ensuring tool accessibility during both fabrication procedures, as the object shape may change dramatically, and different parts of the shape are interdependent. In this study, we propose a computational framework to optimize the planning of additive and subtractive sequences while ensuring tool accessibility. Our goal is to minimize the switching between additive and subtractive processes to achieve efficient fabrication while maintaining product quality. We approach the problem by formulating it as a Volume-And-Surface-CO-decomposition (VASCO) problem. First, we slice volumes into slabs and build a dynamic-directed graph to encode manufacturing constraints, with each node representing a slab and direction reflecting operation order. We introduce a novel geometry property called hybrid-fabricability for a pair of additive and subtractive procedures. Then, we propose a beam-guided top-down block decomposition algorithm to solve the VASCO problem. We apply our solution to a 5-axis hybrid manufacturing platform and evaluate various 3D shapes. Finally, we assess the performance of our approach through both physical and simulated manufacturing evaluations.	https://dl.acm.org/doi/abs/10.1145/3618324	Fanchao Zhong, Haisen Zhao, Haochen Li, Xin Yan, Jikai Liu, Baoquan Chen, Lin Lu
VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering	In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.	https://dl.acm.org/doi/abs/10.1145/3610548.3618212	Linus Franke, Darius Rückert, Laura Fink, Matthias Innmann, Marc Stamminger
VMesh: Hybrid Volume-Mesh Representation for Efficient View Synthesis	With the emergence of neural radiance fields (NeRFs), view synthesis quality has reached an unprecedented level. Compared to traditional mesh-based assets, this volumetric representation is more powerful in expressing scene geometry but inevitably suffers from high rendering costs and can hardly be involved in further processes like editing, posing significant difficulties in combination with the existing graphics pipeline. In this paper, we present a hybrid volume-mesh representation, VMesh, which depicts an object with a textured mesh along with an auxiliary sparse volume. VMesh retains the advantages of mesh-based assets, such as efficient rendering and compact storage, while also incorporating the ability to represent subtle geometric structures provided by the volumetric counterpart. VMesh can be obtained from multi-view images of an object and renders at 2K 60FPS on common consumer devices with high fidelity, unleashing new opportunities for real-time immersive applications.	https://dl.acm.org/doi/abs/10.1145/3610548.3618161	Yuan-Chen Guo, Yan-Pei Cao, Chen Wang, Yu He, Ying Shan, Song-Hai Zhang
VR-NeRF: High-Fidelity Virtualized Walkable Spaces	We present an end-to-end system for the high-fidelity capture, model reconstruction, and real-time rendering of walkable spaces in virtual reality using neural radiance fields. To this end, we designed and built a custom multi-camera rig to densely capture walkable spaces in high fidelity and with multi-view high dynamic range images in unprecedented quality and density. We extend instant neural graphics primitives with a novel perceptual color space for learning accurate HDR appearance, and an efficient mip-mapping mechanism for level-of-detail rendering with anti-aliasing, while carefully optimizing the trade-off between quality and speed. Our multi-GPU renderer enables high-fidelity volume rendering of our neural radiance field model at the full VR resolution of dual 2K × 2K at 36 Hz on our custom demo machine. We demonstrate the quality of our results on our challenging high-fidelity datasets, and compare our method and datasets to existing baselines. We release our dataset on our project website: https://vr-nerf.github.io.	https://dl.acm.org/doi/abs/10.1145/3610548.3618139	Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, Aljaž Božič, Dahua Lin, Michael Zollhöfer, Christian Richardt
Variational Barycentric Coordinates	We propose a variational technique to optimize for generalized barycentric coordinates that offers additional control compared to existing models. Prior work represents barycentric coordinates using meshes or closed-form formulae, in practice limiting the choice of objective function. In contrast, we directly parameterize the continuous function that maps any coordinate in a polytope's interior to its barycentric coordinates using a neural field. This formulation is enabled by our theoretical characterization of barycentric coordinates, which allows us to construct neural fields that parameterize the entire function class of valid coordinates. We demonstrate the flexibility of our model using a variety of objective functions, including multiple smoothness and deformation-aware energies; as a side contribution, we also present mathematically-justified means of measuring and minimizing objectives like total variation on discontinuous neural fields. We offer a practical acceleration strategy, present a thorough validation of our algorithm, and demonstrate several applications.	https://dl.acm.org/doi/abs/10.1145/3618403	Ana Dodik, Oded Stein, Vincent Sitzmann, Justin Solomon
Vector Gradient Stroke Stylized Neural Network Painting	This study focuses on the oil painting brush style transfer in deep convolutional network-based style painting models. We proposes an SVG gradient vectorization process to preserve brush stroke structures while avoiding the generation of a large number of paths. Most of the images in non-photorealistic rendering painting are raster images, suffering from blurriness and quality degradation when zoomed in, whereas vector graphics offer advantages such as scalability and detail preservation. However, existing SVG vectorization methods struggle with images containing gradient colors. The proposed method involves vectorizing each brush, analyzing the positions of main color tones, and incorporating gradient color control points. Finally, the vectorized brush results are stacked and merged. Experimental results demonstrate that this process can preserve the brush stroke structure and present gradient color effects in non-photorealistic rendering style transfer, enhancing editing flexibility and printing quality for brush style transfer.	https://dl.acm.org/doi/abs/10.1145/3610542.3626118	Jia-Shuan Lin, Tung-Ju Hsieh
ViCMA: Visual Control of Multibody Animations	Motion control of large-scale, multibody physics animations with contact is difficult. Existing approaches, such as those based on optimization, are computationally daunting, and, as the number of interacting objects increases, can fail to find satisfactory solutions. We present a new, complementary method for the visual control of multibody animations that exploits object motion and visibility, and has overall cost comparable to a single simulation. Our method is highly practical, and is demonstrated on numerous large-scale, contact-rich examples involving both rigid and deformable bodies.	https://dl.acm.org/doi/abs/10.1145/3610548.3618223	Doug L. James, David I. W. Levin
Visions of Destruction	"""Visions of Destruction"" is an interactive AI-aided artwork that critiques the human impact on the environment. A viewer's gaze, detected by an eye-tracking sensor, causes transformations in the landscape imagery. Hence, merely by observing the digital scenery, the spectator induces dramatic changes at the points their gaze touches. AI-generated 'beautiful landscapes', constructed by Stable Diffusion, present viewers with a romanticized version of nature derived from the collective human memory, as represented by a web-based training dataset. The piece operates in real-time, providing a unique experience for each viewer, symbolizing the Anthropocene and the urgency to protect the natural environment. A viewer's gaze acts as a metaphor for human presence and the irreversible actions leading to today's climate crisis. Technically speaking, an eye-tracker registers the gaze, which then triggers an image change precisely where the audience's eyes land. Using an array of pre-set prompts and inpainting with Stable Diffusion, viewers witness how pristine nature begins to deform before their eyes. Consequently, participants can reshape mountains, carve rivers, erect cities, and disrupt the initial idyllic nature, experiencing the metaphorical destruction and tension between technology and nature. When the eye-tracking detects no viewers, the landscapes begin a regeneration journey. Nature finds solace in this symbiotic dance between human presence and absence, its beauty flourishing. Additionally, the project brings interactivity into the realm of AI art. The artwork effectively utilises generative models to emphasize the urgency of the climate crisis. By transitioning from serene landscapes to scenes of ecological devastation, it captures the stark realities of our evolving world. This aligns with the audience's crucial role in molding our environment and highlights everyone's duty to nature. As such, ""Visions of Destruction"" stands not only as artwork but also as a call to action."	https://dl.acm.org/doi/abs/10.1145/3610537.3622947	Mar Canet Sola, Varvara Guljajeva
Visual Signatures of Music Mood	Majority of the existing methods of music visualization utilized mostly frequency, tempo and volume which are rendered in realtime as animated images for the music being played. Visualization of music as static images is rarely addressed. In this paper, we propose visual signatures – static images which are generated using artificial intelligence to visualize the music mood.	https://dl.acm.org/doi/abs/10.1145/3610542.3626122	Hanqin Wang, Alexei Sourin
Visual-gestural Interface for Auslan Virtual Assistant	For Deaf people to use natural language interfaces, technologies must understand sign language input and respond in the same language. We developed a prototype smart home assistant that utilises gesture-based controls, improving accessibility and convenience for Deaf users. The prototype features Zelda, an interactive signing avatar that provides responses in Auslan (Australian Sign Language), enabling effective two-way communication. Our live demonstration includes gesture recognition and sign production.	https://dl.acm.org/doi/abs/10.1145/3610541.3614566	Maria Zelenskaya, Scott Whittington, Julie Lyons, Adele Vogel, Jessica Korte
Volumetric Scenography for the Fashion Film Genre: 444.2, a Posthumanist Journey in Virtual Reality	444.2 is a volumetric 6-DoF VR fashion film presented for the category of XR theatre. It is the outcome of transdisciplinary practice-based research at the nexus of fashion, expanded imaging and digital media. The researcher's praxis emerges as rhizomatic, within a network of human and non-human agents. The project interrogates the notion of digital fashion bodies from a critical speculative and posthumanist [Braidotti, 2013] lens using Deleuzian concepts and diffractive methods. 444.2 stages fashion bodies as morphing figurations unfolding to a cinematic Afro-diasporic score. The volumetric scenography of black female bodies as celestial bodies in the virtual environment of the Southern African Large Telescope becomes an enactment of African cosmology. Posthumanistic performance emerges as an experience of pluriversal and decolonial [Escobar, 2018] performativity in VR.	https://dl.acm.org/doi/abs/10.1145/3610549.3614589	Nirma Madhoo
Warped-Area Reparameterization of Differential Path Integrals	Physics-based differentiable rendering is becoming increasingly crucial for tasks in inverse rendering and machine learning pipelines. To address discontinuities caused by geometric boundaries and occlusion, two classes of methods have been proposed: 1) the edge-sampling methods that directly sample light paths at the scene discontinuity boundaries, which require nontrivial data structures and precomputation to select the edges, and 2) the reparameterization methods that avoid discontinuity sampling but are currently limited to hemispherical integrals and unidirectional path tracing. We introduce a new mathematical formulation that enjoys the benefits of both classes of methods. Unlike previous reparameterization work that focused on hemispherical integral, we derive the reparameterization in the path space. As a result, to estimate derivatives using our formulation, we can apply advanced Monte Carlo rendering methods, such as bidirectional path tracing, while avoiding explicit sampling of discontinuity boundaries. We show differentiable rendering and inverse rendering results to demonstrate the effectiveness of our method.	https://dl.acm.org/doi/abs/10.1145/3618330	Peiyu Xu, Sai Bangaru, Tzu-Mao Li, Shuang Zhao
Waylet: Self-Contained Haptic Device for Park-Scale Interactions	Although various haptic devices have been proposed so far, most of them are limited to use in laboratory or indoor environments, because such haptic devices are wired to external equipment for power supply and control. This can be a critical issue, particularly when using them in freely moving situations. In this study, we proposed a self-contained haptic device called Waylet for park-scale interactions. Waylets can provide translational and rotational pseudo-forces via asymmetric vibrations. To demonstrate the feasibility of our concept, we demonstrate an intuitive haptic navigation system in a park-scale mixed-reality environment with haptic rendering.	https://dl.acm.org/doi/abs/10.1145/3610541.3614567	Tomosuke Maeda, Junnosuke Yamamoto, Takayoshi Yoshimura, Hiroyuki Sakai, Kouta Minamizawa
WearSway: Wearable Device to Reproduce Tactile Stimuli of Strong Wind through Swaying Clothes	Wind displays reproduce cutaneous stimuli of wind by blowing actual airflow to make virtual content more immersive. However, reproducing the sensation of a strong wind across the whole body, as might be used in educational or entertainment scenarios, typically requires large and cumbersome wind fans. WearSway is a compact and lightweight wearable haptic device that simulates the swaying of clothes in the wind to express the strong wind without large equipment. The device achieves this by using motors and strings to sway the clothing and a small fan to provide wind sensation. While most existing wind displays stimulate the skin directly, WearSway introduces a novel approach by leveraging the movement of clothing to deliver a strong wind simulation.	https://dl.acm.org/doi/abs/10.1145/3610541.3614576	Kenichi Ito, Juro Hosoi, Kei Takanohashi, Yuki Ban, Shin'ichi Warisawa
What is the Best Automated Metric for Text to Motion Generation?	There is growing interest in generating skeleton-based human motions from natural language descriptions. While most efforts have focused on developing better neural architectures for this task, there has been no significant work on determining the proper evaluation metric. Human evaluation is the ultimate accuracy measure for this task, and automated metrics should correlate well with human quality judgments. Since descriptions are compatible with many motions, determining the right metric is critical for evaluating and designing effective generative models. This paper systematically studies which metrics best align with human evaluations and proposes new metrics that align even better. Our findings indicate that none of the metrics currently used for this task show even a moderate correlation with human judgments on a sample level. However, for assessing average model performance, commonly used metrics such as R-Precision and less-used coordinate errors show strong correlations. Additionally, several recently developed metrics are not recommended due to their low correlation compared to alternatives. We also introduce a novel metric based on a multimodal BERT-like model, MoBERT, which offers strongly human-correlated sample-level evaluations while maintaining near-perfect model-level correlation. Our results demonstrate that this new metric exhibits extensive benefits over all current alternatives.	https://dl.acm.org/doi/abs/10.1145/3610548.3618185	Jordan Voas, Yili Wang, Qixing Huang, Raymond Mooney
Zero-Shot 3D Shape Correspondence	We propose a novel zero-shot approach to computing correspondences between 3D shapes. Existing approaches mainly focus on isometric and near-isometric shape pairs (e.g., human vs. human), but less attention has been given to strongly non-isometric and inter-class shape matching (e.g., human vs. cow). To this end, we introduce a fully automatic method that exploits the exceptional reasoning capabilities of recent foundation models in language and vision to tackle difficult shape correspondence problems. Our approach comprises multiple stages. First, we classify the 3D shapes in a zero-shot manner by feeding rendered shape views to a language-vision model (e.g., BLIP2) to generate a list of class proposals per shape. These proposals are unified into a single class per shape by employing the reasoning capabilities of ChatGPT. Second, we attempt to segment the two shapes in a zero-shot manner, but in contrast to the co-segmentation problem, we do not require a mutual set of semantic regions. Instead, we propose to exploit the in-context learning capabilities of ChatGPT to generate two different sets of semantic regions for each shape and a semantic mapping between them. This enables our approach to match strongly non-isometric shapes with significant differences in geometric structure. Finally, we employ the generated semantic mapping to produce coarse correspondences that can further be refined by the functional maps framework to produce dense point-to-point maps. Our approach, despite its simplicity, produces highly plausible results in a zero-shot manner, especially between strongly non-isometric shapes.	https://dl.acm.org/doi/abs/10.1145/3610548.3618228	Ahmed Abdelreheem, Abdelrahman Eldesokey, Maks Ovsjanikov, Peter Wonka
asmVR: Enhancing ASMR Tingles with Multimodal Triggers Based on Virtual Reality	Anxiety and stress have gradually become commonplace mental health problems afflicting many people worldwide. We propose asmVR, a novel approach to enhance the autonomous sensory meridian response (ASMR) experience by combining multimodal triggers including visual, auditory, tactile, and emotional stimuli. asmVR helps users enhance ASMR tingling sensations through online and offline modes, provides realistic VR environments and remote avatar ASMRtist for users, showcasing its potential for stress relief, emotion regulation, and customization. Additionally, it reveals new possibilities for the future application of VR in the field of psychotherapy.	https://dl.acm.org/doi/abs/10.1145/3610549.3614597	Danyang Peng, Tanner Person, Kinga Skierś, Ruoxin Cui, Mark Armstrong, Kouta Minamizawa, Yun Suen Pai
asmVR: VR-Based ASMR Experience with Multimodal Triggers for Mental Well-Being	Individuals are besieged by anxiety and stress throughout the world. Solutions to improve mental well-being can vary, and Autonomous Sensory Meridian Response (ASMR) is one potential method that has proven to be able to reduce stress. To that end, we introduce asmVR, a novel approach to enhancing ASMR experiences using multi-modal triggers. Combining online and offline modes, asmVR enhances ASMR tingling, offering immersive VR environments and remote ASMRist embodiments. Initial user testing shows tingles enhance and stress relief potential, along with new possibilities for VR in psychological therapy.	https://dl.acm.org/doi/abs/10.1145/3610542.3626146	Danyang Peng, Tanner Person, Ruoxin Cui, Mark Armstrong, Kouta Minamizawa, Yun Suen Pai
see-saw: A Kinetic Installation that Unfolds in Silence Driven by Liquid Flow	see-saw is a kinetic installation unfolding silently, driven by the behavior of liquid flowing inside the objects. A group of these objects, shaped like seesaws, are aligned. By changing the position of the center of gravity of the liquid inside, each see-saw alters its inclination dynamically. The behavior of liquid is realized using the Electrohydrodynamics (EHD) phenomenon. We generate these movements that appear to depart from conventional physics by electrically manipulating an invisible driving force to move the liquid against gravity. This installation embodies a natural and smooth kinetic aesthetic that blends seamlessly into the surrounding environment through silent movement.	https://dl.acm.org/doi/abs/10.1145/3610591.3616434	Takafumi Morita, Yasuaki Kakehi
Δt-Sphere	"This artwork has a unique name, ""Δt,"" which combines the Greek letter delta (Δ) representing a small increment in mathematics and physics, and the initial letter of ""Thickness."" This work :Δt-Sphere is part of the Δt series, and was born out of a study of the monement of living creatures. The principles of elasticity and pendulum is used by penetrating a piano wire through a stack of thin acrylic plates. The theme of this artwork is ""Floating Liquid Encased in a Sphere"" Through the organic fluctuations originating from artificial objects, I aim to provide an opportunity for people to recognize that the relationship between nature and artificiality is not separate, but rather by designing physical phenomena at a microscopic level, we can seamlessly experience natural elements like liquid. Participants can manipulate a sphere that sways in response to their hand movements, creating a sensation as if they were generating wind with the palm of their hands. Moreover, they can control the floating liquid within the sphere, which reflects LED lights, giving them an illusion of becoming a deity or a sorcerer. In today's chaotic and unpredictable society, this work has the power to harmonize even complex events under a designed law through the power of combination of art and science. The man-made objects are the crystallization of human wisdom while the liquid, which is swaying like a wave as a metaphor of this artwork, is a source of all things. The ""Δt-Sphere"" is a work that gives interactivity to the beauty of the fusion and harmony of natural and man-made objects, and can give people a new perspective."	https://dl.acm.org/doi/abs/10.1145/3610537.3622948	Masaki Kanayama
“Achi Mukua Experience Space”——A Multi-Player Immersive Experience of Yunnan ICH Dance	Increasingly, intangible cultural heritage (ICH) dance is being performed interactively through immersive experiences. However, these experiences often leaving participants isolated and lacking the connection to the cultural context of ICH dances. This project focuses on designing interactive mechanism that combine collaborative multi-players interactions with the collective nature of ICH dance. It explores the advantage of bio-signals as visual feedback in multi-player dance interactions. The project aims to create an immersive social space where participants can be fully immersed in the dance atmosphere and gain a deeper understanding of intangible heritage.	https://dl.acm.org/doi/abs/10.1145/3610591.3616431	Siyu Luo, Shuo Yan
“Dongba Script Character Construction Space”: VR science-based interactive experience of pictographs in intangible cultural heritage	"The ""Dongba Script Character Construction Space"" is an interactive science VR experience based on the Dongba script of the Naxi people in Yunnan Province, China, which provides an immersive virtual reality experience based on the traditional Yiyin script theory. [Selmanović et al., 2020] Specifically, this work combines the character construction characteristics of ""pictogram"", ""simple ideographs"" and ""compound ideographs"" to explore the meaning and visual symbolic characteristics of Naxi pictograms through three interesting forms of interaction, and provides more possibilities for the inheritance and development of Dongba in the future by using various forms of interaction."	https://dl.acm.org/doi/abs/10.1145/3610549.3614604	Yulu Lu, Wensi Dai, Shuo Yan
“Dream Songs”༚ Integrating Body Interaction into ICH Oral Literature Virtual Narrative Experience	"The intangible cultural heritage(ICH) of China contains a wealth of oral literature. Their narrative characteristics tell the historical memory of a region or a nation. Miao ancient songs are one of them. ""Dream Songs"" is an immersive virtual narrative experience integrated with body interaction. Based on the singing content and antiphonal form of ""Maple Song"" in Miao ancient songs, our work explored the design methods of digital narrative expression of oral literature of ICH from three aspects: antiphonal interactive narrative, metaphorical body interaction, and gamification of knowledge."	https://dl.acm.org/doi/abs/10.1145/3610549.3614606	Yu Wang, Shuo Yan
“Embark the Journey · Embrace the Universe”: An Interactive System of Astronomy Education Based on Spherical TUI in MR	The widespread adoption and application of MR technology have opened up new possibilities in the field of education. Integrating physical objects and virtual contents in the physical interactive interface provides a larger practical platform for astronomy science education. However, very few attempts are currently at this form of science education. Therefore, we have summarized the design principles for the spherical TUI regarding both interactive interfaces and behaviors. Based on this, we have developed a spherical TUI-based astronomical knowledge system. It delivers users with a more diverse knowledge presentation by a fusion of virtual and reality and mapping interaction of spherical TUI in MR.	https://dl.acm.org/doi/abs/10.1145/3610549.3614609	Xiaozhan Liang, Yong Hu, Xukun Shen
