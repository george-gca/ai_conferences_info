title	abstract	url	authors
A Liquid Sound Retrieval using History of Velocities in Physically-based Simulation	This paper presents a novel method for synthesizing sound effects for fluid animation. Previous approaches synthesize the sound for fluids by physically-based simulation, but these approaches need a huge computational cost. To address this, we propose a data-driven method for synthesizing sound effects for fluids. In this paper, we focus on liquid sound. A liquid sound database which consists of a set of recorded sound clips is prepared in advance, and then the most suitable sound clip for an input liquid motion is automatically retrieved from the database. The retrieval is achieved by comparing a waveform of the sound and a history of velocity of the liquid motion computed by the simulation. The velocity history is computed for the regions where the liquid sound is expected to occur. Then, a distance between the velocity history and the waveform of each sound clip in the database is calculated, and our system chooses the clip with the minimum distance. Our method achieves fast synthesis of liquid sound for simulated liquid motion, once the database is prepared. In this paper, we use a small database containing a few sound clips and evaluate the effectiveness of our retrieval approach, as a preliminary experiment of this research.	https://dl.acm.org/doi/abs/10.1145/3476124.3488643	Hyuga Saito, Syuhei Sato, Yoshinori Dobashi
A Multi-Stage Advanced Deep Learning Graphics Pipeline	In this paper we propose the Advanced Deep Learning Graphics Pipeline (ADLGP). ADLGP is a novel approach that uses existing deep learning architectures to convert scene data into rendered images. Our goal of generating frames from semantic data has produced successful renderings with similar structures and composition as target frames. We demonstrate the success of ADLGP with side-by-side comparisons of frames generated through standard rendering procedures. We assert that a fully implemented ADLGP framework would reduce the time spent in visualizing 3D environments, and help selectively offload the requirements of the current graphics rendering pipeline.	https://dl.acm.org/doi/abs/10.1145/3478512.3488609	Mark Wesley Harris, Sudhanshu Kumar Semwal
A Procedural MatCap System for Cel-Shaded Japanese Animation Production	MatCap is an expressive approach to shade 3D models and is promising for the production of typical Japanese-style cel-shaded animations. However, we experienced an asset management problem in our previous short film production; we needed to manually create many MatCap assets to achieve variations shot by shot or even frame by frame. In this work, we identify requirements for shading systems in Japanese animation production and describe our procedural MatCap system, which satisfies the requirements. Procedural MatCap generates customizable MatCap assets fully procedurally in run time, which drastically improves the asset manageability.	https://dl.acm.org/doi/abs/10.1145/3476124.3488620	Yuki Koyama, Takeshi Tsuruta, Heisuke Saito, Daisuke Takizawa, Hiroshi Moriguchi
A Robust Display Delay Compensation Technique Considering User's Head Motion Direction for Cloud XR	Conventionally, it has been difficult to realize both photorealistic and geometrically consistent 3DCG rendering on head mounted displays (HMDs), due to the trade-off between rendering quality and low motion-to-photon latency (M2PL). In order to solve this problem, we propose a novel rendering framework, where the server renders RGB-D images of a 3D model with the optimally arranged rendering viewpoints, and the client HMD realizes geometric compensation of M2PL by employing depth image based rendering (DIBR). Experiments with real smart glasses show that the proposed method can display binocular images closer to the ground truth than the conventional approaches.	https://dl.acm.org/doi/abs/10.1145/3476124.3488648	Tatsuya Kobayashi, Tomoaki Konno, Haruhisa Kato
A Walk Alone: Triggering Fear and Simulating Empathy to Raise Awareness about the Dangers Women Face when Walking Alone at Night: Triggering Fear and Simulating Empathy to Raise Awareness about the Dangers Women Face when Walking Alone at Night	A Walk Alone is a virtual reality experience that simulates what it feels like to walk alone at night as a woman. The concept is inspired by the kidnapping and murder of Sarah Everard, a 33-year-old woman who went missing during her walk home from her friend's apartment in March 2021. It is one thing to discuss this universal issue and highlight the precautions women take when walking alone, but it is another thing to experience it. A Walk Alone focuses on displaying the vulnerability of the user by triggering multiple senses in the virtual reality environment. The experience is centered around a linear story involving (1) one night-city environment, (2) the user in first person point-of-view, (3) eerie sound design, and (5) dim street lighting.	https://dl.acm.org/doi/abs/10.1145/3478514.3487627	Eman Al-Zubeidi, Jinsil Hwaryoung Seo, Julia DeLaney, Dominic Nguyen, Jonathan Konderla, Jaime Diaz
A material point method for nonlinearly magnetized materials	We propose a novel numerical scheme to simulate interactions between a magnetic field and nonlinearly magnetized objects immersed in it. Under our nonlinear magnetization framework, the strength of magnetic forces is effectively saturated to produce stable simulations without requiring any parameter tuning. The mathematical model of our approach is based upon Langevin's nonlinear theory of paramagnetism, which bridges microscopic structures and macroscopic equations after a statistical derivation. We devise a hybrid Eulerian-Lagrangian numerical approach to simulating this strongly nonlinear process by leveraging the discrete material points to transfer both material properties and the number density of magnetic micro-particles in the simulation domain. The magnetic equations can then be built and solved efficiently on a background Cartesian grid, followed by a finite difference method to incorporate magnetic forces. The multi-scale coupling can be processed naturally by employing the established particle-grid interpolation schemes in a conventional MLS-MPM framework. We demonstrate the efficacy of our approach with a host of simulation examples governed by magnetic-mechanical coupling effects, ranging from magnetic deformable bodies to magnetic viscous fluids with nonlinear elastic constitutive laws.	https://dl.acm.org/doi/abs/10.1145/3478513.3480541	Yuchen Sun, Xingyu Ni, Bo Zhu, Bin Wang, Baoquan Chen
A parallel guaranteed projector-camera system for dual videography	Thanks to the optical duality between a projector and a camera, an image from the viewpoint of the projector can be computationally generated using camera images;however, to achieve dual photography in real time, the geometric configuration of the projector and the camera must be strongly constrained. In this research, to substantially relax this constraint, we improved the optical components of the projector–camera system by employing a stabilizer to guarantee that both epipolar planes are parallel. As a result, it is possible to move the projector to change the synthesized virtual viewpoint more freely in real–time.	https://dl.acm.org/doi/abs/10.1145/3476124.3488621	Yushi Yamagiwa, Hiroyuki Kubo
ARMixer: Live Stage Monitor Mixing through Gestural Interaction in Augmented Reality	Existing stage monitor mixing systems are inefficient and cannot accommodate the communication between the musicians and sound engineers. We introduce ARMixer, which allows musicians to perform self-stage monitor mixing through gestures in augmented reality to provide an intuitive mixing experience. We performed two usability tests and found that ARMixer is acceptable to the user and has excellent psychoacoustic intuitiveness in terms of mixing parameter controls by gestures and identifying mixing target.	https://dl.acm.org/doi/abs/10.1145/3476124.3488632	Weihan Huang, Stephanie Bourgeois, Yun Suen Pai, Kouta Minamizawa
ARinVR: Bringing Mobile AR into VR	Augmented Reality (AR) and Virtual Reality (VR) techniques are widely used for people to experience and interact with 3D digital content. However, the user has to choose either AR or VR separately with different devices for different interaction modalities. This work presents ARinVR, a novel concept that seamlessly and synchronously integrates handheld mobile AR and head-worn VR to allow users to see an augmented scenario on a phone screen and in a VR space at the same time. We developed a prototype system that enabled users to operate a smartphone and observe mobile AR scenarios in VR environment as naturally as in the real world. The system extends the AR space and occludes the physical surroundings, enabling users to stay focused on task performance and enhancing spatial perception.	https://dl.acm.org/doi/abs/10.1145/3476124.3488636	Li Zhang, Weiping He, Yizhe Liu, Qianyuan Zou, Huidong Bai, Mark Billinghurst
AdaptiBrush: adaptive general and predictable VR ribbon brush	Virtual reality drawing applications let users draw 3D shapes using brushes that form shaped, or ruled-surface, strokes. Each ribbon is uniquely defined by its user-specified ruling length, path, and the ruling directions at each point along this path. Existing brushes use the trajectory of a handheld controller in 3D space as the ribbon path, and compute the ruling directions using a mapping from a specific controller coordinate-frame axis. This fixed mapping forces users to rotate the controller and thus their wrists to change ribbon normal or ruling directions, and requires substantial physical effort to draw even medium complexity ribbons. Since human ability to rotate their wrists continuously is heavily restricted, the space of ribbon geometries users can comfortably draw using these brushes is limited. These brushes can be unpredictable, producing ribbons with unexpectedly varying width or flipped and wobbly normals in response to seemingly natural hand gestures. Our ribbon brush system dramatically extends the space of ribbon geometries users can comfortably draw while enabling them to accurately predict the ribbon shape that a given hand motion produces. We achieve this by introducing a novel ruling direction computation method, enabling users to easily change ribbon ruling and normal orientation using predominantly translational controller, and thus wrist, motion. We facilitate ease-of-use by computing predictable ruling directions that smoothly change in both world and controller coordinate systems, and facilitate ease-of-learning by prioritizing ruling directions which are well-aligned with one of the controller coordinate system axes. Our comparative user studies confirm that our more general and predictable ruling computation leads to significant improvements in brush usability and effectiveness compared to all prior brushes; in a head to head comparison users preferred AdaptiBrush over the next-best brush by a margin of 2 to 1.	https://dl.acm.org/doi/abs/10.1145/3478513.3480511	Enrique Rosales, Chrystiano Araújo, Jafet Rodriguez, Nicholas Vining, Dongwook Yoon, Alla Sheffer
Aerial path planning for online real-time exploration and offline high-quality reconstruction of large-scale urban scenes	Existing approaches have shown that, through carefully planning flight trajectories, images captured by Unmanned Aerial Vehicles (UAVs) can be used to reconstruct high-quality 3D models for real environments. These approaches greatly simplify and cut the cost of large-scale urban scene reconstruction. However, to properly capture height discontinuities in urban scenes, all state-of-the-art methods require prior knowledge on scene geometry and hence, additional prepossessing steps are needed before performing the actual image acquisition flights. To address this limitation and to make urban modeling techniques even more accessible, we present algorithm that does not require any prior knowledge for the scenes. Using only captured 2D images, we estimate 3D bounding boxes for buildings on-the-fly and use them to guide online path planning for both scene exploration and building observation. Experimental results demonstrate that the aerial paths planned by our algorithm in realtime for unknown environments support reconstructing 3D models with comparable qualities and lead to shorter flight air time.	https://dl.acm.org/doi/abs/10.1145/3478513.3480491	Yilin Liu, Ruiqi Cui, Ke Xie, Minglun Gong, Hui Huang
Aesthetic-guided outward image cropping	Image cropping is a commonly used post-processing operation for adjusting the scene composition of an input photography, therefore improving its aesthetics. Existing automatic image cropping methods are all bounded by the image border, thus have very limited freedom for aesthetics improvement if the original scene composition is far from ideal, e.g. the main object is too close to the image border. In this paper, we propose a novel, aesthetic-guided method. It can go beyond the image border to create a desirable composition that is unachievable using previous cropping methods. Our method first evaluates the input image to determine how much the content of the image should be extrapolated by a field of view (FOV) evaluation model. We then synthesize the image content in the extrapolated region, and seek an optimal aesthetic crop within the expanded FOV, by jointly considering the aesthetics of the cropped view, and the local image quality of the extrapolated image content. Experimental results show that our method can generate more visually pleasing image composition in cases that are difficult for previous image cropping tools due to the border constraint, and can also automatically degrade to an inward method when high quality image extrapolation is infeasible.	https://dl.acm.org/doi/abs/10.1145/3478513.3480566	Lei Zhong, Feng-Heng Li, Hao-Zhi Huang, Yong Zhang, Shao-Ping Lu, Jue Wang
Ah les Crocodiles…	A little girl is released in the universe of a Pop-Up book tale managed by a crocodile who won't hesitate to have a discriminatory behavior against women.	https://dl.acm.org/doi/abs/10.1145/3463912.3477194	Agathe Sénéchal, Elise Debruyne, Cérine Raouraoua, William Defrance, Agatha Sip
An Abstract Drawing Method for Same Shaped but Densely Arranged Many Objects	We propose a hierarchical abstract drawing method for many 3D objects which are densely arranged and have almost the same shape. First, an abstract drawing of all the 3D objects is drawn based on a couple of the primal colors, which is one of the global properties of all the 3D objects. Then, an abstract painting is added focusing on a part of each object as local properties. Several results of abstractly rendered scenes are shown, and are confirmed the effectiveness of our method.	https://dl.acm.org/doi/abs/10.1145/3476124.3488651	Reika Yagi, Shuhei Kodama, Tokiichiro Takahashi
Anime Character Colorization using Few-shot Learning	In this paper, we propose an automatic Anime-style colorization method using only a small number of colorized reference images manually colorized by artists. To accomplish this, we introduce a few-shot patch-based learning method considering the characteristics of Anime line-drawing. To streamline the learning process, we derive optimal settings with acceptable colorization accuracy and training time for a production pipeline. We demonstrate that the proposed method helps to reduce manual labor for artists.	https://dl.acm.org/doi/abs/10.1145/3478512.3488604	Akinobu Maejima, Hiroyuki Kubo, Seitaro Shinagawa, Takuya Funatomi, Tatsuo Yotsukura, Satoshi Nakamura, Yasuhiro Mukaigawa
Associating Real Objects with Virtual Models for VR Interaction	In this paper, we present a prototype system that is capable of associating real objects with virtual models and turning the table top into imaginary virtual scenes. A user can interact with these objects when she or he is immersed in the virtual environment. To accomplish this goal, a vision-based system is developed to online recognize and track the real objects in the scene. The corresponding virtual models are retrieved based on their tactile shapes. They are displayed and moved on a head-mounted display (HMD) according to tracked object poses. The experiment demonstrates that our prototype system can find reasonable association between real and virtual objects, and users are interested in the novel interaction.	https://dl.acm.org/doi/abs/10.1145/3476124.3488654	Wan-Ting Hsu, I-Chen Lin
AutoMate: a dataset and learning approach for automatic mating of CAD assemblies	Assembly modeling is a core task of computer aided design (CAD), comprising around one third of the work in a CAD workflow. Optimizing this process therefore represents a huge opportunity in the design of a CAD system, but current research of assembly based modeling is not directly applicable to modern CAD systems because it eschews the dominant data structure of modern CAD: parametric boundary representations (BREPs). CAD assembly modeling defines assemblies as a system of pairwise constraints, called , between parts, which are defined relative to BREP topology rather than in world coordinates common to existing work. We propose SB-GCN, a representation learning scheme on BREPs that retains the topological structure of parts, and use these learned representations to predict CAD type mates. To train our system, we compiled the first large scale dataset of BREP CAD assemblies, which we are releasing along with benchmark mate prediction tasks. Finally, we demonstrate the compatibility of our model with an existing commercial CAD system by building a tool that assists users in mate creation by suggesting mate completions, with 72.2% accuracy.	https://dl.acm.org/doi/abs/10.1145/3478513.3480562	Benjamin Jones, Dalton Hildreth, Duowen Chen, Ilya Baran, Vladimir G. Kim, Adriana Schulz
Autocomplete Repetitive Stroking with Image Guidance	Image-guided drawing can compensate for the lack of skills but often requires a significant number of repetitive strokes to create textures. Existing automatic stroke synthesis methods are usually limited to predefined styles or require indirect manipulation that may break the spontaneous flow of drawing. We present a method to autocomplete repetitive short strokes during users' normal drawing process. Users can draw over a reference image as usual. At the same time, our system silently analyzes the input strokes and the reference to infer strokes that follow users' input style when certain repetition is detected. Our key idea is to jointly analyze image regions and operation history for detecting and predicting repetitions. The proposed system can reduce tedious repetitive inputs while being fully under user control.	https://dl.acm.org/doi/abs/10.1145/3478512.3488595	Yilan Chen, Kin Chung Kwan, Li-Yi Wei, Hongbo Fu
Avant Card	Ernst is figure on a postcard, which is in a card stand with many other motives. While the other cards are eagerly bought, no one is interested in his card and Ernst becomes increasingly discontented. Almost he is also bought - but then he is put back crooked again. This brings his world into imbalance and he falls out. For him a colorful journey full of different styles of representation begins - whether 2D, 3D, Real film and Stop-motion.	https://dl.acm.org/doi/abs/10.1145/3463912.3478277	Stella Raith, Josephine Roß
Awkward	A day full of socially awkward moments.	https://dl.acm.org/doi/abs/10.1145/3463912.3475842	Nata Metlukh
Barbershop: GAN-based image compositing using segmentation masks	Seamlessly blending features from multiple images is extremely challenging because of complex relationships in lighting, geometry, and partial occlusion which cause coupling between different parts of the image. Even though recent work on GANs enables synthesis of realistic hair or faces, it remains difficult to combine them into a single, coherent, and plausible image rather than a disjointed set of image patches. We present a novel solution to image blending, particularly for the problem of hairstyle transfer, based on GAN-inversion. We propose a novel latent space for image blending which is better at preserving detail and encoding spatial information, and propose a new GAN-embedding algorithm which is able to slightly modify images to conform to a common segmentation mask. Our novel representation enables the transfer of the visual properties from multiple reference images including specific details such as moles and wrinkles, and because we do image blending in a latent-space we are able to synthesize images that are coherent. Our approach avoids blending artifacts present in other approaches and finds a globally consistent image. Our results demonstrate a significant improvement over the current state of the art in a user study, with users preferring our blending solution over 95 percent of the time. Source code for the new approach is available at https://zpdesu.github.io/Barbershop.	https://dl.acm.org/doi/abs/10.1145/3478513.3480537	Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka
Barking Orders	After being bossed around by the royal guard his whole life, the Queen's corgi's luck changes when the entire British Royal family dies, leaving the corgi next in line for the throne. He takes charge of England with an iron fist, but when things get over his head, he finds himself panic-stricken, way in over his head with the future of Europe in his hands. When the guard finds him, he is able to talk the corgi down from the ledge but not without some unexpected, devastating consequences.	https://dl.acm.org/doi/abs/10.1145/3463912.3467957	Alexander Tullo
Beat	"The Story Incubated from Your heartbeats. Concept: You can share your own heartbeat with a robot, and you can feel its growth together. Main Character: A robot boy called ""Maruboro"" honest robot, who is not yet familiar with ""life"".. He has been left in an old factory quietly. One day, the viewer comes over and gives his/her heart as life to him. Viewers can project themselves into the characters and enjoy the story. Logline: The viewer gives his/her heart to Maruboro and breathes a life into him. Maruboro wants to become friends with another robot called Kakuboro, but Maruboro doesn't know how to interact with others. It makes Kakuboro angry. However, Maruboro desperately wants to make friends and he starts to think from Kakuboro's point of view. Kakuboro finally opens up his heart to Maruboro. Story: ""Beat"" is a story elaborated from your ""Heart"". Viewers can experience the work with their hearts in their hands. The heart in the animation vibrates at the same pace at viewers' heartbeat. The setting of the story is an old factory. Viewers encounter a rusted robot called Maruboro, it was absolutely static. He doesn't have a ""heart"" to move. Viewers can grant Maruboro a new heart by putting theirs on the robot. He then stands up and starts to move, expressing joy to live out all his strength. Maruboro looks a little lonely. When he finds that other robots open their hearts and connect with each other, he starts to search for friends. However, when Maruboro meets with new robots, he doesn't know how to communicate properly. He tries to open up his heart to another robot called Kakuboro. While in Maruboro's disturbance, Kakuboro dropped an important component of his factory. Kakuboro becomes angry and tries to gets rid of Maruboro. He broke Maruboro's heart accidentally. Maruboro is so depressed, but he tries to work with the viewer to search for the lost component. Finally, Maruboro finds it and give that back to Kakuboro. Kakuboro uses that to run the factory. The factory then starts to operate and sets off big fireworks. The hearts of the two robots eventually come together. Maruboro's heart starts to beat again. ""Heart"" becomes the key to move the story forward. The story aims to arouse consciousness of ""Heart"" via the growth of the robot."	https://dl.acm.org/doi/abs/10.1145/3478514.3487616	Keisuke Itoh, Hiroko Fujioka, Katsutoshi Machiba, Tetsuya Ohashi
BeeWave: Creta swarm kinetic movement using by SMA display and embedded cellular automata-based mechanism	"""Bee Wave"" is a kinetic art sculpture driven by a shaped memory alloy. It is driven by the material properties of the memory alloy, which is heated and contracted to pull the lever on the drive to pull the acrylic wings, making them sway like the wings of a bee in nature (Figure 1). Since no motor is used, it is quiet and elegant; the cell of the triangular cone is like a cellular automata, which has a mechanism to receive, store and process signals transmitted between cells and cells, and can influence the neighboring cells, from the individual triangular cone to influence the three neighboring cells, through the rule of triggering all the way to the group, like a wave cycle; and in the process of implementation into the nature of the existence of In the process of the actual work, the biological characteristics and appearance of the bionic design are incorporated, such as the adoption of a biological appearance, the mechanism of the bee to capture the hidden audio conversion behavior, the memory alloy of the shape like a living creature, and the function of the cellular automaton to transmit signals in the game of life [Conway 1970], and the above mechanisms are ""mixed"" to create this creation (Bee Wave)."	https://dl.acm.org/doi/abs/10.1145/3476124.3488635	Hsin-Fu Chen, Scottie Chih-Chieh Huang
Beyond mie theory: systematic computation of bulk scattering parameters based on microphysical wave optics	Light scattering in participating media and translucent materials is typically modeled using the radiative transfer theory. Under the assumption of independent scattering between particles, it utilizes several bulk scattering parameters to statistically characterize light-matter interactions at the macroscale. To calculate these parameters based on microscale material properties, the Lorenz-Mie theory has been considered the gold standard. In this paper, we present a generalized framework capable of systematically and rigorously computing bulk scattering parameters beyond the far-field assumption of Lorenz-Mie theory. Our technique accounts for microscale wave-optics effects such as diffraction and interference as well as interactions between nearby particles. Our framework is general, can be plugged in any renderer supporting Lorenz-Mie scattering, and allows arbitrary packing rates and particles correlation; we demonstrate this generality by computing bulk scattering parameters for a wide range of materials, including anisotropic and correlated media.	https://dl.acm.org/doi/abs/10.1145/3478513.3480543	Yu Guo, Adrian Jarabo, Shuang Zhao
Binaural audio generation via multi-task learning	We present a learning-based approach for generating binaural audio from mono audio using multi-task learning. Our formulation leverages additional information from two related tasks: the binaural audio generation task and the flipped audio classification task. Our learning model extracts spatialization features from the visual and audio input, predicts the left and right audio channels, and judges whether the left and right channels are flipped. First, we extract visual features using ResNet from the video frames. Next, we perform binaural audio generation and flipped audio classification using separate subnetworks based on visual features. Our learning method optimizes the overall loss based on the weighted sum of the losses of the two tasks. We train and evaluate our model on the FAIR-Play dataset and the YouTube-ASMR dataset. We perform quantitative and qualitative evaluations to demonstrate the benefits of our approach over prior techniques.	https://dl.acm.org/doi/abs/10.1145/3478513.3480560	Sijia Li, Shiguang Liu, Dinesh Manocha
BridgedReality: A Toolkit Connecting Physical and Virtual Spaces through Live Holographic Point Cloud Interaction	The recent emergence of point cloud streaming technologies has spawned new ways to digitally perceive and manipulate live data of users and spaces. The graphical rendering limitations prevent state-of-the-art interaction techniques from achieving segmented bare-body user input to manipulate live point cloud data. We propose BridgedReality, a toolkit that enables users to produce localized virtual effects in live scenes, without the need for an HMD nor any wearable devices or virtual controllers. Our method uses body tracking and an illusory rendering technique to achieve large scale, depth-based, real time interaction with multiple light field projection display interfaces. This toolkit circumvented time-consuming 3D object classification, and packaged multiple proximity effects in a format understandable by middle schoolers. Our work can offer a foundation for multidirectional holographic interfaces, GPU simulated interactions, teleconferencing and gaming activities, as well as live cinematic quality exhibitions.	https://dl.acm.org/doi/abs/10.1145/3476124.3488656	Mark Armstrong, Lawrence Quest, Yun Suen Pai, Kai Kunze, Kouta Minamizawa
Camera keyframing with style and control	We present a novel technique that enables 3D artists to synthesize camera motions in virtual environments following a , while enforcing user-designed camera keyframes as constraints along the sequence. To solve this constrained motion in-betweening problem, we design and train a camera motion generator from a collection of temporal cinematic features (camera and actor motions) using a conditioning on target keyframes. We further condition the generator with a to control how to perform the interpolation between the keyframes. Style codes are generated by training a second network that encodes different camera behaviors in a compact latent space, the Camera behaviors are defined as temporal correlations between actor features and camera motions and can be extracted from real or synthetic film clips. We further extend the system by incorporating a fine control of camera speed and direction via a hidden state mapping technique. We evaluate our method on two aspects: i) the capacity to synthesize style-aware camera trajectories with user defined keyframes; and ii) the capacity to ensure that in-between motions still comply with the reference camera style while satisfying the keyframe constraints. As a result, our system is the first style-aware keyframe in-betweening technique for camera control that balances style-driven automation with precise and interactive control of keyframes.	https://dl.acm.org/doi/abs/10.1145/3478513.3480533	Hongda Jiang, Marc Christie, Xi Wang, Libin Liu, Bin Wang, Baoquan Chen
Can Shadows Create a Sense of Depth to Mid-air Image?	Shadows are an important factor in the perception of object position and shape. In this study, we investigated the effect of shadows on the perception of the shape of a mid-air image by projecting shadows of different shapes onto a mid-air image that is displayed in real space. Specifically, participants viewed one oval cylinder with a shadow and one oval cylinder without a shadow individually and were forced to choose which had the greater thickness in the depth direction. As a result, we found that shadow shapes possibly changed the perception of the thickness of the mid-air image.	https://dl.acm.org/doi/abs/10.1145/3476124.3488625	Yutaro Yano, Ayami Hoshi, Naoya Koizumi
Cartoon Sliding: An MR System for Experiencing Sliding Down a Cliff	We present an MR system for experiencing the cartoon-like event of sliding down a cliff and stopping by sticking a knife, with enhancement of tactile and muscle stimuli. A player grabs a knife device and presses it against a wall device that rotates upwards in contrast to the player who virtually slides down. We demonstrate an MR content in which a player experiences sliding down the virtual cliff while handling the knife device on the rotating wall device.	https://dl.acm.org/doi/abs/10.1145/3476124.3488644	Hiroki Tsunekawa, Akihiro Matsuura
Cascaded Sobol' sampling	Rendering quality is largely influenced by the samplers used in Monte Carlo integration. Important factors include sample uniformity (e.g., low discrepancy) in the high-dimensional integration domain, sample uniformity in lower-dimensional projections, and lack of dominant structures that could result in aliasing artifacts. A widely used and successful construction is the Sobol' sequence that guarantees good high-dimensional uniformity and consequently results in faster convergence of quasi-Monte Carlo integration. We show that this sequence exhibits low uniformity and dominant structures in low-dimensional projections. These structures impair quality in the context of rendering, as they precisely occur in the 2-dimensional projections used for sampling light sources, reflectance functions, or the camera lens or sensor. We propose a new cascaded construction, which, despite dropping the sequential aspect of Sobol' samples, produces point sets exhibiting provably perfect dyadic partitioning (and therefore, excellent uniformity) in consecutive 2-dimensional projections, while preserving good high-dimensional uniformity. By optimizing the initialization parameters and performing Owen scrambling at finer levels of binary representations, we further improve over Sobol's integration convergence rate. Our method does not incur any overhead as compared to the generation of the Sobol' sequence, is compatible with Owen scrambling and can be used in rendering applications.	https://dl.acm.org/doi/abs/10.1145/3478513.3480482	Loïs Paulin, David Coeurjolly, Jean-Claude Iehl, Nicolas Bonneel, Alexander Keller, Victor Ostromoukhov
Class Balanced Sampling for the Training in GANs	Recently Top-k fake sample selection has been introduced to provide better gradients for training Generative Adversarial Networks. Since the method does not guarantee class balance of selected samples in class conditional GANs, certain classes can be completely ignored in the training. In this work, we propose class standardized critic score based sample selection which enables class balanced sample selection. Our method achieves improved FID score and Intra-FID score compared to prior Top-k selection.	https://dl.acm.org/doi/abs/10.1145/3476124.3488634	Sanghun Kim, Seungkyu Lee
CodeMiko: an interactive VTuber experience	While VTubing has grown in popularity over the last few years, CodeMiko stands out by offering high fidelity motion and facial capture and a higher degree of interactivity due to Twitch chat's ability to affect her streams. CodeMiko has created a new form of entertainment inside of Twitch by mixing gaming, live interactivity, and traditional scripted content. New interactions are created all the time, making it an incredibly exciting experience for the viewers and makes them want to keep watching. To bring CodeMiko to life, all you need is motion capture through an Xsens suit and facial capture through an iPhone live link. Her fingers are captured using Manus VR. Most of her expressivity is done through her animation blueprint and I've added blendshapes with squash and stretch to make her face feel more alive. Heightened interactivity with chat is one of the most unique aspects of the CodeMiko show. When I connect to Twitch chat, tools that I created allow the viewers to play with the character morph screen, while others boast arcade-like features. Chat can make her explode, throw balls and try to hit her, or make her fart. Chat can also visit Miko's world as a little frog, which Miko can then grab and throw around if she wants to. As an example, I have connected to chat and they will play with the character morphs live to change CodeMiko's body . Another interesting technical aspect of CodeMiko is that Miko is a mixture of motion capture and pre-recorded game animations. I've blended both so that she can switch from a fully free range of real time motion to being able to walk and run like a third person game character. Chat can then come into her world and CodeMiko can shoot them as if it were a live video game! The blending with a game mode also allows me to create interactions and reactions to things happening in her environment caused by chat. For example, when chat throws a ball at her face and it hits her, she reacts in real time with an over-the-top, pre-recorded hit animation. This opens a whole new world where I can create miniature games with mocapped characters. I've created trivia levels, a dodgeball minigame, and can't wait to make a mocapped platformer. Live-streaming on Twitch means long hours performing on a regular schedule almost every day of the week, but the audience always wants new and exciting content. I had to get very good at quickly evaluating the 3rd party content available from the Epic store and other 3rd party providers for its quality, ability to be quickly integrated into the stream, and ability to enhance Miko's world in general. This has led to all kinds of crazy new features being added like dragon eggs that need to be hatched by the audience or a car that Miko tries to drive. Miko also has an array of traditionally expensive content like entire new artified scenes and full character costume replacements, which are all enabled by building an efficient pathway in for 3rd party content. The interactivity that I've created for chat allows for them to become a part of each individual show, but also the content loop itself. Members of chat can use Twitch's built in monetization system to affect the stream in various ways. These interactions become clippable moments that viewers can then take joy in posting on social media, and oftentimes, clips from these streams will go viral. Viral clips bring in new viewers, who will then test interactivity features and create interesting moments, and these moments will go viral again. Since her debut on Twitch, CodeMiko has become more than just a streamer but an evolution of real time entertainment as we know it. This project has fully and successfully integrated a fast moving pipeline of both 3rd party and original content in order to appeal to the need for fresh content on a daily basis.	https://dl.acm.org/doi/abs/10.1145/3478511.3491309	Youna Kang
Collaborative Avatar Platform for Collective Human Expertise	In this study, we developed a system in which two users are integrated into one actual robot arm to collaborate with each other. The method of dividing the roles allows flexible movements that is not dependent on the range of movement and rotational freedom of the human arm, and enables movements that are impossible for a single person. The mixing of movements in adjustable ratio allows an expert to support a beginner to make more stable movements, and induces collaboration between users with different perspectives. Additionally, we implement tactile feedback to enable interaction between users and between users and the robot. We investigate the effects of these on usability and cognitive behavior. This system is expected to become a new method of collaboration in the cyber-physical society.	https://dl.acm.org/doi/abs/10.1145/3476122.3484841	Takayoshi Hagiwara, Takumi Katagiri, Hikari Yukawa, Itsuki Ogura, Ryohei Tanada, Takumi Nishimura, Yoshihiro Tanaka, Kouta Minamizawa
Comic Image Inpainting via Distance Transform	Inpainting techniques for natural images have progressed significantly. However, if these methods are applied to comic images, the results are not satisfactory because of very noticeable artifacts, especially around line drawings. Line drawings are challenging to inpaint because of their high-frequency components. In this paper, we propose a novel method for inpainting comic images in the distance transform domain. In this method, we first convert a line drawing into a distance image and then inpaint the distance image. By transforming line drawings into distance images, we can eventually reduce the high-frequency components, which leads to improve inpainting performance. We compared the results of our proposed method with those of the conventional methods. The results showed that the proposed method achieved 0.1% lower l1 loss, 0.5dB higher PSNR, and 0.5% higher SSIM than those of the conventional inpainting methods.	https://dl.acm.org/doi/abs/10.1145/3478512.3488607	Naoki Ono, Kiyoharu Aizawa, Yusuke Matsui
Company of Heroes 3 - cinematic trailer	The full CG trailer is an impression of the events during the occupation of the Italian territory by German troops. Building faithful models of the weapons and vehicles required the use of mapping. To give the viewer a sense of authenticity, we carried out mocap sessions and we used 3D scans of actors' silhouettes and faces to create the proper models. Every single element of the set - costumes, explosion, movement, and each facial expression was designed individually. All materials were prepared by our team. The customer was involved in the works on the script and was responsible for the final sound of the film.	https://dl.acm.org/doi/abs/10.1145/3463912.3481885	Tomasz Suwalski, Piotr Prokop
Computing sparse cones with bounded distortion for conformal parameterizations	We propose a novel method to generate sparse cone singularities with bounded distortion constraints for conformal parameterizations. It is formulated as minimizing the -norm of Gaussian curvature of vertices with hard constraints of bounding the distortion that is measured by the -norm of the log conformal factor. We use the reweighted -norm to approximate the -norm and solve each convex weighted minimization subproblem by the Douglas-Rachford (DR) splitting scheme. To quickly generate sparse cones, we modify DR splitting by weighting the -norm of the proximal mapping to force the small Gaussian curvature to quickly approach zero. Accordingly, compared with the conventional DR splitting, the modified method performs one to two orders of magnitude faster. Besides, we perform variable substitution of log conformal factors to simplify the computation process for acceleration. Our algorithm is able to bound distortion to compute sparse cone singularities, so that the resulting conformal parameterizations achieve a favorable tradeoff between the area distortion and the number of cones. We demonstrate its effectiveness and feasibility on a large number of models.	https://dl.acm.org/doi/abs/10.1145/3478513.3480526	Qing Fang, Wenqing Ouyang, Mo Li, Ligang Liu, Xiao-Ming Fu
Continuous aerial path planning for 3D urban scene reconstruction	We introduce the first drone trajectory planning algorithm, which performs (i.e., ) image acquisition along an aerial path and explicitly factors into an optimization along with scene reconstruction quality. Specifically, our method takes as input a rough 3D scene proxy and produces a drone trajectory and image capturing setup, which efficiently yields a high-quality reconstruction of the 3D scene based on three optimization objectives: one to maximize the that can be acquired along the entirety of the trajectory, another to optimize the scene capturing by maximizing the scene information that can be acquired per unit length along the aerial path, and the last one to minimize the total along the aerial path, so as to reduce the number of sharp turns. Our search scheme is based on the rapidly-exploring random tree framework, resulting in a final trajectory as a single path through the search tree. Unlike state-of-the-art works, our joint optimization for view selection and path planning is performed in a We comprehensively evaluate our method not only on benchmark virtual datasets as in existing works but also on several large-scale real urban scenes. We demonstrate that the continuous paths optimized by our method can effectively reduce onsite acquisition cost using drones, while achieving high-fidelity 3D reconstruction, compared to existing planning methods and oblique photography, a mature and popular industry solution.	https://dl.acm.org/doi/abs/10.1145/3478513.3480483	Han Zhang, Yucong Yao, Ke Xie, Chi-Wing Fu, Hao Zhang, Hui Huang
Controlling Eye Blink for Talking Face Generation via Eye Conversion	A real talking face video includes not only the movement of the mouth, but also realistic blinking details. For a computer generated talking face video, realistic eye movements are critical to overcome the uncanny valley effect. However, it remains a great challenge to introduce realistic eye movements into talking face generation systems. In this paper, we propose a two-stage system for generating talking face video with realistic controllable blinking actions. Through eye conversion and frame replacement, our architecture can ensure the controllability of the blinking motion generation. We propose an eye conversion GAN, which can convert a face image into any stages of blinking, and maintain the consistency of facial identity features. In this network, we design joint training to increase the network's ability of generating closed and half-closed eye images, which improves the authenticity of the eyes. Experiments on two popular data sets show that compared with previous work, our method can not only guarantee the authenticity of mouth movements, but also generate realistic and controllable eye blinks.	https://dl.acm.org/doi/abs/10.1145/3478512.3488610	Jiaqi Hao, Shiguang Liu, Qing Xu
Convex polyhedral meshing for robust solid modeling	We introduce a new technique to create a mesh of convex polyhedra representing the interior volume of a triangulated input surface. Our approach is particularly tolerant to defects in the input, which is allowed to self-intersect, to be non-manifold, disconnected, and to contain surface holes and gaps. We guarantee that the input surface is exactly represented as the union of polygonal facets of the output volume mesh. Thanks to our algorithm, traditionally solid modeling operations such as mesh booleans and Minkowski sums become surprisingly robust and easy to implement, even if the input has defects. Our technique leverages on the recent concept of indirect geometric predicate to provide an unprecedented combination of guaranteed robustness and speed, thus enabling the practical implementation of robust though flexible solid modeling systems. We have extensively tested our method on all the 10000 models of the Thingi10k dataset, and concluded that no existing method provides comparable robustness, precision and performances.	https://dl.acm.org/doi/abs/10.1145/3478513.3480564	Lorenzo Diazzi, Marco Attene
Cyberdream: An Interactive Rave Music Visualization in Virtual Reality	Virtual reality (VR) provides new opportunities for the design of interactive music visualizations. Exploring this area, Cyberdream is a prototype VR application realized through the author's practice-led research, which provides a journey through audio-visual environments based on the aesthetics of 1990s rave music. The project provides three audio-visual 'sound toys', which allow the user to interactively 'paint with sound', thereby facilitating creative play. Through its structural form and audio-visual sound toys, Cyberdream indicates new approaches for the design of music visualizations that harness the spatial properties of VR.	https://dl.acm.org/doi/abs/10.1145/3478514.3487609	Jonathan Weinel
DISLOCATION: VR experience	"This work represents a non-narrative VR experience with elements of interaction. It consists of two parts. The art installation and the VR work itself. The VR work talks about dislocation- the enforced departure of people from their homes, typically because of war, persecution, or natural disaster. This film takes a look at an absurd moment of disbelief and fear. It examines the internal processes that develop and offers a visual depiction of a moment in time of a person forced to fight for his life in a strange land. A moment of dislocation. The VR installation is a tent that can be typically found in refugee camps (Figure 1). It takes approximately 12m square of space. VR headset is situated in the middle of the tent. On the right side of the entrance of the tent, there is a TV mounted that is showing an animated loop that gives context to the visual treatment of the main character. Inside the tent, there is a cardboard piece with the words ""where is justice, where is humanity"" written."	https://dl.acm.org/doi/abs/10.1145/3478514.3487619	Veljko Popovic, Milivoj Popovic
Decision of Line Structure beyond Junctions Using U-Net-Based CNN for Line Drawing Rendering	This paper introduces a U-Net-based neural network to determine line structure beyond junctions more accurately than the previous work [Guo et al. 2019]. In addition to the input of the previous work, we input 3D information to our neural network. We also propose a method to generate the training dataset automatically. The rendering results by the stylized line rendering [Uchida and Saito 2020] show that our neural network improves the streams of strokes.	https://dl.acm.org/doi/abs/10.1145/3476124.3488641	Ryogo Ito, Mitsuhiro Uchida, Suguru Saito
Deep Color-Normal Residual Networks for Geometry Refinement Extracting Color Consistency and Fine Geometry	In recent years, texture mapping in 3D modeling has been remarkably improved for realistic rendering. However, small error in reconstructed 3D geometry makes serious error in texture mapping. To address the problem, most of prior methods are devoted to refinement of 3D geometry without visual clues. In this work, we refine 3D geometry based on color consistency and surface normal using a deep neural network. Our method optimizes the location of each vertex maximizing the quality of related textures.	https://dl.acm.org/doi/abs/10.1145/3476124.3488646	MinGeun Park, Seungkyu Lee
Deep3DLayout: 3D reconstruction of an indoor layout from a spherical panoramic image	Recovering the 3D shape of the bounding permanent surfaces of a room from a single image is a key component of indoor reconstruction pipelines. In this article, we introduce a novel deep learning technique capable to produce, at interactive rates, a tessellated bounding 3D surface from a single 360° image. Differently from prior solutions, we fully address the problem in 3D, significantly expanding the reconstruction space of prior solutions. A graph convolutional network directly infers the room structure as a 3D mesh by progressively deforming a graph-encoded tessellated sphere mapped to the spherical panorama, leveraging perceptual features extracted from the input image. Important 3D properties of indoor environments are exploited in our design. In particular, gravity-aligned features are actively incorporated in the graph in a projection layer that exploits the recent concept of multi head self-attention, and specialized losses guide towards plausible solutions even in presence of massive clutter and occlusions. Extensive experiments demonstrate that our approach outperforms current state of the art methods in terms of accuracy and capability to reconstruct more complex environments.	https://dl.acm.org/doi/abs/10.1145/3478513.3480480	Giovanni Pintore, Eva Almansa, Marco Agus, Enrico Gobbetti
DeepVecFont: synthesizing high-quality vector fonts via dual-modality learning	Automatic font generation based on deep learning has aroused a lot of interest in the last decade. However, only a few recently-reported approaches are capable of directly generating vector glyphs and their results are still far from satisfactory. In this paper, we propose a novel method, DeepVecFont, to effectively resolve this problem. Using our method, for the first time, visually-pleasing vector glyphs whose quality and compactness are both comparable to human-designed ones can be automatically generated. The key idea of our DeepVecFont is to adopt the techniques of image synthesis, sequence modeling and differentiable rasterization to exhaustively exploit the dual-modality information (i.e., raster images and vector outlines) of vector fonts. The highlights of this paper are threefold. First, we design a dual-modality learning strategy which utilizes both image-aspect and sequence-aspect features of fonts to synthesize vector glyphs. Second, we provide a new generative paradigm to handle unstructured data (e.g., vector glyphs) by randomly sampling plausible synthesis results to get the optimal one which is further refined under the guidance of generated structured data (e.g., glyph images). Finally, qualitative and quantitative experiments conducted on a publicly-available dataset demonstrate that our method obtains high-quality synthesis results in the applications of vector font generation and interpolation, significantly outperforming the state of the art.	https://dl.acm.org/doi/abs/10.1145/3478513.3480488	Yizhi Wang, Zhouhui Lian
Dementia Eyes: Perceiving Dementia with Augmented Reality	Dementia is a global health crisis, of which there is a need to understand the patients' perception towards improving their quality of life. We propose Dementia Eyes, a mobile AR experience that simulates common visual symptoms of senile dementia based on the known pathology and caregivers' actual experience with patients. Leveraging an iPhone and a Head-Mounted Display (HMD), we developed a real-time application which allows users to see the world from the perspective of an Alzheimer's type of dementia (AD) patient. The experience was validated by professional medical workers in Japan, and the result advocates for the efficacy of the empathy we intended to bring to them.	https://dl.acm.org/doi/abs/10.1145/3478514.3487617	Ximing Shen, Yun Suen Pai, Dai Kiuchi, Kanoko Oishi, Kehan Bao, Tomomi Aoki, Kouta Minamizawa
Depth-Aware Dynamic Projection Mapping using High-speed RGB and IR Projectors	In this paper, we propose a strong system for dynamic projection mapping (DPM). It consists of a high-speed 500-fps camera, high-speed 24-bit 947-fps RGB projector, and newly developed 8-bit 2,880-fps infrared (IR) projector. This configuration allows us to capture the depth map and project the depth-aware image at a high frame rate. We also realize 0.4-ms markerless 3D pose tracking using the depth map by leveraging small inter-frame motions under high-frame-rate capture. We exploit these captured data and apply the depth-aware DPM to an entire scene with low latency, in a setting in which tracking-based and modelless mapping are combined and performed simultaneously.	https://dl.acm.org/doi/abs/10.1145/3476122.3484843	Sora Hisaichi, Kiwamu Sumino, Kunihiro Ueda, Hidenori Kasebe, Tohru Yamashita, Takeshi Yuasa, Uwe Lippmann, Petra Aswendt, Roland Höfling, Yoshihiro Watanabe
Depths of night	Depth of Nights is created by hand-draw brush style in digital 2D animation with some 3D animation and Sand stop motion animation. We would like to create a new and significant visual style. In the storytelling, we use an interesting way to present the dream's world, the real life world and the daydreaming world. It is really an enormous challenge to plan the transitions of linking up those 3 different worlds together.	https://dl.acm.org/doi/abs/10.1145/3463912.3482608	Step Cheung
Development of a Wearable Embedded System providing Tactile and Kinesthetic Haptics Feedback for 3D Interactive Applications	Existing haptic interfaces providing both tactile and kinesthetic feedback for virtual object manipulation are still bulky, expensive and often grounded, limiting users' motion. In this work, we present a wearable, lightweight and affordable embedded system aiming to provide both tactile and kinesthetic feedback in 3D applications. We created a PCB for the circuitry and used inexpensive components. The kinesthetic feedback is provided to the user's hand through a 3D-printed exoskeleton and five servo motors placed on the back of the glove. Tactile feedback is provided to the user's hand through fifteen coin vibration motors, placed in the inner side of the hand and vibrating at three levels. The system is ideal for prototyping and could be customized, thus, making it scalable and upgradable.	https://dl.acm.org/doi/abs/10.1145/3476124.3488653	Michael Roumeliotis, Katerina Mania
Differentiable simulation	Differentiable simulation is emerging as a fundamental building block for many cutting-edge applications in computer graphics, vision and robotics, among others. This course provides an introduction to this topic and an overview of state-of-the-art methods in this context. Starting with the basics of dynamic mechanical systems, we will present a general theoretical framework for differentiable simulation, which we will specialize to rigid bodies, deformable solids, and fluids. A particular focus will be on the different alternatives for computing simulation derivatives, ranging from analytical expressions via sensitivity analysis to reverse-mode automatic differentiation. As an important step towards real-world applications, we also present extensions to non-smooth phenomena such as frictional contact. Finally, we will discuss different ways of integrating differentiable simulation into machine learning frameworks. The material covered in this course is based on the author's own works and experience, complemented by a state-of-the-art review of this young but rapidly evolving field. It will be richly illustrated, annotated, and supported by examples ranging from robotic manipulation of deformable materials to simulation-based capture of dynamic fluids. The theoretical parts will be accompanied by source code examples that will be made available to participants prior to this course.	https://dl.acm.org/doi/abs/10.1145/3476117.3483433	Stelian Coros, Miles Macklin, Bernhard Thomaszewski, Nils Thürey
Differentiable surface triangulation	Triangle meshes remain the most popular data representation for surface geometry. This ubiquitous representation is essentially a hybrid one that decouples continuous from the discrete topological Unfortunately, the combinatorial nature of the triangulation prevents taking derivatives over the space of possible meshings of any given surface. As a result, to date, mesh processing and optimization techniques have been unable to truly take advantage of modular gradient descent components of modern optimization frameworks. In this work, we present a that enables optimization for any per-vertex or per-face differentiable objective function over the space of underlying surface triangulations. Our method builds on the result that 2D triangulation can be achieved by a suitably perturbed weighted Delaunay triangulation. We translate this result into a computational algorithm by proposing a soft relaxation of the classical weighted Delaunay triangulation and optimizing over vertex weights and vertex locations. We extend the algorithm to 3D by decomposing shapes into developable sets and differentiably meshing each set with suitable boundary constraints. We demonstrate the efficacy of our method on various planar and surface meshes on a range of difficult-to-optimize objective functions. Our code can be found online: https://github.com/mrakotosaon/diff-surface-triangulation.	https://dl.acm.org/doi/abs/10.1145/3478513.3480554	Marie-Julie Rakotosaona, Noam Aigerman, Niloy J. Mitra, Maks Ovsjanikov, Paul Guerrero
Differentiable time-gated rendering	The continued advancements of time-of-flight imaging devices have enabled new imaging pipelines with numerous applications. Consequently, several forward rendering techniques capable of accurately and efficiently simulating these devices have been introduced. However, general-purpose differentiable rendering techniques that estimate derivatives of time-of-flight images are still lacking. In this paper, we introduce a new theory of differentiable time-gated rendering that enjoys the generality of differentiating with respect to arbitrary scene parameters. Our theory also allows the design of advanced Monte Carlo estimators capable of handling cameras with near-delta or discontinuous time gates. We validate our theory by comparing derivatives generated with our technique and finite differences. Further, we demonstrate the usefulness of our technique using a few proof-of-concept inverse-rendering examples that simulate several time-of-flight imaging scenarios.	https://dl.acm.org/doi/abs/10.1145/3478513.3480489	Lifan Wu, Guangyan Cai, Ravi Ramamoorthi, Shuang Zhao
Differentiable transient rendering	Recent differentiable rendering techniques have become key tools to tackle many inverse problems in graphics and vision. Existing models, however, assume steady-state light transport, i.e., infinite speed of light. While this is a safe assumption for many applications, recent advances in ultrafast imaging leverage the wealth of information that can be extracted from the exact time of flight of light. In this context, physically-based transient rendering allows to efficiently simulate and analyze light transport considering that the speed of light is indeed finite. In this paper, we introduce a novel differentiable transient rendering framework, to help bring the potential of differentiable approaches into the transient regime. To differentiate the transient path integral we need to take into account that scattering events at path vertices are no longer independent; instead, tracking the time of flight of light requires treating such scattering events at path vertices jointly as a multidimensional, evolving manifold. We thus turn to the generalized transport theorem, and introduce a novel term, which links the time-integrated contribution of a path to its light throughput, and allows us to handle discontinuities in the light and sensor functions. Last, we present results in several challenging scenarios where the time of flight of light plays an important role such as optimizing indices of refraction, non-line-of-sight tracking with nonplanar relay walls, and non-line-of-sight tracking around two corners.	https://dl.acm.org/doi/abs/10.1145/3478513.3480498	Shinyoung Yi, Donggun Kim, Kiseok Choi, Adrian Jarabo, Diego Gutierrez, Min H. Kim
DroneStick: Flying Joystick as a Novel Type of Interface	DroneStick is a novel hands-free method for smooth interaction between a human and a robotic system via one of its agents, without training and any additional handheld or wearable device or infrastructure. A flying joystick (DroneStick), being a part of a multi-robot system, is composed of a flying drone and coiled wire with a vibration motor. By pulling on the coiled wire, the operator commands certain motions of the follower robotic system. The DroneStick system does not require the user to carry any equipment before or after performing the required interaction. DroneStick provides useful feedback to the operator in the form of force transferred through the wire, translation/rotation of the flying joystick, and motor vibrations at the fingertips. Feedback allows users to interact with different forms of robotic systems intuitively. A potential application can enhance an automated 'last mile' delivery when a recipient needs to guide a delivery drone/robot gently to a spot where a parcel has to be dropped.	https://dl.acm.org/doi/abs/10.1145/3476122.3484845	Evgeny Tsykunov, Aleksey Fedoseev, Ekaterina Dorzhieva, Ruslan Agishev, Roman Ibrahimov, Derek Vasquez, Luiza Labazanova, Dzmitry Tsetserukou
Dynamic Neural Face Morphing for Visual Effects	In this work we present a machine learning approach for face morphing in videos, between two or more identities. We devise an autoencoder architecture with distinct decoders for each identity, but with an underlying learnable linear basis for their weights. Each decoder has a learnable parameter that defines the interpolating weights (or ID weights) for the basis which can successfully decode its identity. During inference, the ID weights can be interpolated to produce a range of morphing identities. Our method produces temporally consistent results and allows blending different aspects of the identities by exposing the blending weights for each layer of the decoder network. We deploy our trained models to image compositors as 2D nodes with independent controls for the blending weights. Our approach has been successfully used in production, for the aging of David Beckham in the Malaria Must Die campaign.	https://dl.acm.org/doi/abs/10.1145/3478512.3488596	Lucio Moser, Jason Selfe, Darren Hendler, Doug Roble
Dynamic and Occlusion-Robust Light Field Illumination	There is high demand for dynamic and occlusion-robust illumination to improve lighting quality for portrait photography and assembly. Multiple projectors are required for the light field to achieve such illumination. This paper proposes a dynamic and occlusion-robust illumination technique by employing a light field formed by a lens array instead of using multiple projectors. Dynamic illumination is obtained by introducing a feedback system that follows the motion of the object. The designed lens array incorporates a wide viewing angle, making the system robust against occlusion. The proposed system was evaluated through projections onto a dynamic object.	https://dl.acm.org/doi/abs/10.1145/3476124.3488624	Masahiko Yasui, Yoshihiro Watanabe, Masatoshi Ishikawa
Dynamic neural garments	A vital task of the wider digital human effort is the creation of realistic garments on digital avatars, both in the form of characteristic fold patterns and wrinkles in static frames as well as richness of garment dynamics under avatars' motion. Existing workflow of modeling, simulation, and rendering closely replicates the physics behind real garments, but is tedious and requires repeating most of the workflow under changes to characters' motion, camera angle, or garment resizing. Although data-driven solutions exist, they either focus on static scenarios or only handle dynamics of tight garments. We present a solution that, at test time, takes in body joint motion to directly produce realistic dynamic garment image sequences. Specifically, given the target joint motion sequence of an avatar, we propose to synthesize plausible dynamic garment appearance from a desired viewpoint. Technically, our solution generates a coarse garment proxy sequence, learns deep dynamic features attached to this template, and neurally renders the features to produce appearance changes such as folds, wrinkles, and silhouettes. We demonstrate generalization behavior to both unseen motion and unseen camera views. Further, our network can be fine-tuned to adopt to new body shape and/or background images. We demonstrate our method on a wide range of real and synthetic garments. We also provide comparisons against existing neural rendering and image sequence translation approaches, and report clear quantitative and qualitative improvements. Project page: http://geometry.cs.ucl.ac.uk/projects/2021/DynamicNeuralGarments/	https://dl.acm.org/doi/abs/10.1145/3478513.3480497	Meng Zhang, Tuanfeng Y. Wang, Duygu Ceylan, Niloy J. Mitra
Efficient and robust discrete conformal equivalence with boundary	We describe an efficient algorithm to compute a discrete metric with prescribed Gaussian curvature at all interior vertices and prescribed geodesic curvature along the boundary of a mesh. The metric is (discretely) conformally equivalent to the input metric. Its construction is based on theory developed in [Gu et al. 2018b] and [Springborn 2020], relying on results on hyperbolic ideal Delaunay triangulations. Generality is achieved by considering the surface's intrinsic triangulation as a degree of freedom, and particular attention is paid to the proper treatment of surface boundaries. While via a double cover approach the case with boundary can be reduced to the case without boundary quite naturally, the implied symmetry of the setting causes additional challenges related to stable Delaunay-critical configurations that we address explicitly. We furthermore explore the numerical limits of the approach and derive continuous maps from the discrete metrics.	https://dl.acm.org/doi/abs/10.1145/3478513.3480557	Marcel Campen, Ryan Capouellez, Hanxiao Shen, Leyi Zhu, Daniele Panozzo, Denis Zorin
Efficient spherical harmonic shading for separable BRDF	Spherical Harmonics (SH) are commonly and widely used in computer graphics in order to speed up the evaluation of the rendering equation. With separable BRDF, the diffuse and specular contributions are traditionally computed separately. Our first contribution is to demonstrate that there is a simple relationship between both computations, but one-way, i.e. from specular to diffuse. We show how to deduce the diffuse contribution from the specular contribution, using a single multiplication. This replaces the use of tens of multiplications for some cases up to complex rotations for other cases. Our second contribution is an efficient way to compute the SH product between an arbitrary function and a clamped cosine, much less expensive than the traditional SH triple product.	https://dl.acm.org/doi/abs/10.1145/3478512.3488597	Pierre Mézières, Mathias Paulin
Emotion Guided Speech-Driven Facial Animation	The modern deep neural network has allowed an applicable level of speech-driven facial animation, simulating natural and precise 3D animation from speech data. Regardless, many of the works show weakness in drastic emotional expression and flexibility of the animation. In this work, we introduce emotion guided speech-driven facial animation, simultaneously proceeding with classification and regression from the speech data to generate a controllable level of evident emotional expression on facial animation. Performance using our method shows reasonable expressiveness of facial emotion with controllable flexibility. Extensive experiments indicate that our method generates more expressive facial animation with controllable flexibility compared to previous approaches.	https://dl.acm.org/doi/abs/10.1145/3476124.3488649	Sewhan Chun, Daegeun Choe, Shindong Kang, Shounan An, Youngbak Jo, Insoo Oh
Ensemble denoising for Monte Carlo renderings	Various denoising methods have been proposed to clean up the noise in Monte Carlo (MC) renderings, each having different advantages, disadvantages, and applicable scenarios. In this paper, we present , an optimization-based technique that combines multiple individual MC denoisers. The combined image is modeled as a per-pixel weighted sum of output images from the individual denoisers. Computation of the optimal weights is formulated as a constrained quadratic programming problem, where we apply a dual-buffer strategy to estimate the overall MSE. We further propose an iterative solver to overcome practical issues involved in the optimization. Besides nice theoretical properties, our ensemble denoiser is demonstrated to be effective and robust, and outperforms any individual denoiser across dozens of scenes and different levels of sample rates. We also perform a comprehensive analysis on the selection of individual denoisers to be combined, providing important and practical guides for users.	https://dl.acm.org/doi/abs/10.1145/3478513.3480510	Shaokun Zheng, Fengshi Zheng, Kun Xu, Ling-Qi Yan
EpiScope: Optical Separation of Reflected Components by Rotation of Polygonal Mirror	Separating reflection components is an important task in computer graphics and vision. Episcan3D has been proposed to separate the direct and indirect reflection components in real-time. This method uses a scanning laser projector and a rolling shutter camera, so it requires unmanageably precise geometric alignment and temporal synchronization. In this paper, we propose a novel optical system that achieves the same function without imaging devices. In this method, the ray directions of projection, observation, and presentation are optically and mechanically synchronized by a rotating polygonal mirror. The direct or indirect components can be selected by a mask-based light-field filter. Especially, the selected reflection components can be seen directly by our naked eye, and there are no restrictions on image quality or delays in presentation due to the number of pixels or frame rate of the imaging system.	https://dl.acm.org/doi/abs/10.1145/3478512.3488600	Ryota Maeda, Shinsaku Hiura
Exploration of Visuo-haptic Interactions to Support Learning Leopold's Maneuvers Process in Virtual Reality	We present Leopold's Maneuvers VR, a haptic-enabled virtual reality simulation in which a user determines the size, position, and weight of a fetus within a virtual patient by palpating the patient's abdomen. Users of the application receive corresponding haptic cues (force and vibration) as they touch the fetus through the patient's torso. The physical sensations generated by the SenseGlove, paired with the immersive visuals within the virtual environment, support to create a palpation experience similar to the real Leopold's Maneuvers activity. This application addresses a need in nursing education for an immersive virtual reality experience that integrates direct hand manipulation in the learning of assessment skills.	https://dl.acm.org/doi/abs/10.1145/3478514.3487615	Soo Wan Chun, Jinsil Hwaryoung Seo, Caleb Kicklighter, Elizabeth Wells-Beede, Jack Greene, Tomas Arguello
ExtraNet: real-time extrapolated rendering for low-latency temporal supersampling	Both the frame rate and the latency are crucial to the performance of realtime rendering applications such as video games. Spatial supersampling methods, such as the Deep Learning SuperSampling (DLSS), have been proven successful at decreasing the rendering time of each frame by rendering at a lower resolution. But temporal supersampling methods that directly aim at producing more frames on the fly are still not practically available. This is mainly due to both its own computational cost and the latency introduced by interpolating frames from the future. In this paper, we present ExtraNet, an efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency. With the help of the rendered auxiliary geometry buffers of the extrapolated frame, and the temporally reliable motion vectors, we train our ExtraNet to perform two tasks simultaneously: irradiance in-painting for regions that cannot find historical correspondences, and accurate ghosting-free shading prediction for regions where temporal information is available. We present a robust hole-marking strategy to automate the classification of these tasks, as well as the data generation from a series of high-quality production-ready scenes. Finally, we use lightweight gated convolutions to enable fast inference. As a result, our ExtraNet is able to produce plausibly extrapolated frames without easily noticeable artifacts, delivering a 1.5× to near 2× increase in frame rates with minimized latency in practice.	https://dl.acm.org/doi/abs/10.1145/3478513.3480531	Jie Guo, Xihao Fu, Liqiang Lin, Hengjun Ma, Yanwen Guo, Shiqiu Liu, Ling-Qi Yan
EyelashNet: a dataset and a baseline method for eyelash matting	Eyelashes play a crucial part in the human facial structure and largely affect the facial attractiveness in modern cosmetic design. However, the appearance and structure of eyelashes can easily induce severe artifacts in high-fidelity multi-view 3D face reconstruction. Unfortunately it is highly challenging to remove eyelashes from portrait images using both traditional and learning-based matting methods due to the delicate nature of eyelashes and the lack of eyelash matting dataset. To this end, we present EyelashNet, the first eyelash matting dataset which contains 5,400 high-quality eyelash matting data captured from real world and 5,272 virtual eyelash matting data created by rendering avatars. Our work consists of a capture stage and an inference stage to automatically capture and annotate eyelashes instead of tedious manual efforts. The capture is based on a specifically-designed fluorescent labeling system. By coloring the eyelashes with a safe and invisible fluorescent substance, our system takes paired photos with colored and normal eyelashes by turning the equipped ultraviolet (UVA) flash on and off. We further correct the alignment between each pair of photos and use a novel alpha matte inference network to extract the eyelash alpha matte. As there is no prior eyelash dataset, we propose a progressive training strategy that progressively fuses captured eyelash data with virtual eyelash data to learn the latent semantics of real eyelashes. As a result, our method can accurately extract eyelash alpha mattes from fuzzy and self-shadow regions such as pupils, which is almost impossible by manual annotations. To validate the advantage of EyelashNet, we present a baseline method based on deep learning that achieves state-of-the-art eyelash matting performance with RGB portrait images as input. We also demonstrate that our work can largely benefit important real applications including high-fidelity personalized avatar and cosmetic design.	https://dl.acm.org/doi/abs/10.1145/3478513.3480540	Qinjie Xiao, Hanyuan Zhang, Zhaorui Zhang, Yiqian Wu, Luyuan Wang, Xiaogang Jin, Xinwei Jiang, Yong-Liang Yang, Tianjia Shao, Kun Zhou
Fast and accurate spherical harmonics products	Spherical Harmonics (SH) have been proven as a powerful tool for rendering, especially in real-time applications such as Precomputed Radiance Transfer (PRT). Spherical harmonics are orthonormal basis functions and are efficient in computing dot products. However, computations of triple product and multiple product operations are often the bottlenecks that prevent moderately high-frequency use of spherical harmonics. Specifically state-of-the-art methods for accurate SH triple products of order have a time complexity of ( ), which is a heavy burden for most real-time applications. Even worse, a brute-force way to compute -multiple products would take ( ) time. In this paper, we propose a fast and accurate method for spherical harmonics triple products with the time complexity of only ( ), and further extend it for computing -multiple products with the time complexity of ( +  ( )). Our key insight is to conduct the triple and multiple products in the Fourier space, in which the multiplications can be performed much more efficiently. To our knowledge, our method is theoretically the fastest for accurate spherical harmonics triple and multiple products. And in practice, we demonstrate the efficiency of our method in rendering applications including mid-frequency relighting and shadow fields.	https://dl.acm.org/doi/abs/10.1145/3478513.3480563	Hanggao Xin, Zhiqian Zhou, Di An, Ling-Qi Yan, Kun Xu, Shi-Min Hu, Shing-Tung Yau
Fast and versatile fluid-solid coupling for turbulent flow simulation	The intricate motions and complex vortical structures generated by the interaction between fluids and solids are visually fascinating. However, reproducing such a two-way coupling between thin objects and turbulent fluids numerically is notoriously challenging and computationally costly: existing approaches such as cut-cell or immersed-boundary methods have difficulty achieving physical accuracy, or even visual plausibility, of simulations involving fast-evolving flows with immersed objects of arbitrary shapes. In this paper, we propose an efficient and versatile approach for simulating two-way fluid-solid coupling within the kinetic (lattice-Boltzmann) fluid simulation framework, valid for both laminar and highly turbulent flows, and for both thick and thin objects. We introduce a novel hybrid approach to fluid-solid coupling which systematically involves a mesoscopic double-sided bounce-back scheme followed by a cut-cell velocity correction for a more robust and plausible treatment of turbulent flows near moving (thin) solids, preventing flow penetration and reducing boundary artifacts significantly. Coupled with an efficient approximation to simplify geometric computations, the whole boundary treatment method preserves the inherent massively parallel computational nature of the kinetic method. Moreover, we propose simple GPU optimizations of the core LBM algorithm which achieve an even higher computational efficiency than the state-of-the-art kinetic fluid solvers in graphics. We demonstrate the accuracy and efficacy of our two-way coupling through various challenging simulations involving a variety of rigid body solids and fluids at both high and low Reynolds numbers. Finally, comparisons to existing methods on benchmark data and real experiments further highlight the superiority of our method.	https://dl.acm.org/doi/abs/10.1145/3478513.3480493	Chaoyang Lyu, Wei Li, Mathieu Desbrun, Xiaopei Liu
Fast volume rendering with spatiotemporal reservoir resampling	Volume rendering under complex, dynamic lighting is challenging, especially if targeting real-time. To address this challenge, we extend a recent direct illumination sampling technique, spatiotemporal reservoir resampling, to multi-dimensional path space for volumetric media. By fully evaluating just a single path sample per pixel, our volumetric path tracer shows unprecedented convergence. To achieve this, we properly estimate the chosen sample's probability via approximate perfect importance sampling with spatiotemporal resampling. A key observation is recognizing that applying cheaper, biased techniques to approximate scattering along candidate paths (during resampling) does not add bias when shading. This allows us to combine transmittance evaluation techniques: cheap approximations where evaluations must occur many times for reuse, and unbiased methods for final, per-pixel evaluation. With this reformulation, we achieve low-noise, interactive volumetric path tracing with arbitrary dynamic lighting, including volumetric emission, and maintain interactive performance even on high-resolution volumes. When paired with denoising, our low-noise sampling helps preserve smaller-scale volumetric details.	https://dl.acm.org/doi/abs/10.1145/3478513.3480499	Daqi Lin, Chris Wyman, Cem Yuksel
Fencing tracking and visualization system	"We developed the ""Fencing tracking and visualization system."" It detects the tips of sabers (fencing swords) to visualize the trajectory of the sabers in real time, which doesn't require any markers but works only with the input of the images from cameras. This is the only fencing visualization technology that has been used in actual international fencing matches, such as the H.I.H. Prince Takamado Trophy JAL Presents Fencing World Cup 2019. Fencing sabre, especially its tip, moves quite fast, and its flexibility results in a large distortion in its shape. Additionally the tip is the size of only a few pixels when captured even by a 4K camera so that it is too small to detect with image recognition techniques. We developed a multi-stage deep learning network for general object detection based on YOLO v3 [Redmon and Farhadi 2017, 2018], starting from the hardware selection of a camera for analysis. Since a single camera can only cover about 8 meters, we eventually installed 24 4K cameras on the both sides of the piste to cover the entire match area and improved the robustness of the sabre tip detection. We also developed a system to estimate the 3D position of the tips from the detection results of multiple cameras. (Rhizomatiks) Planning, Creative Direction : Daito Manabe (Rhizomatiks) Planning, Technical Direction, Hardware Engineering : Motoi Ishibashi (Rhizomatiks) Software Engineering : Kyle Mc-Donald (IYOIYO), anno lab (Kisaku Tanaka, Sadam Fujioka, Nariaki Iwatani, Fumiya Funatsu), Kye Shimizu Dataset System Engineering: Tatsuya Ishii (Rhizomatiks), ZIKU Technologies, Inc. (Yoshihisa Hashimoto, Hideyuki Kasuga, Seiji Nanase, Daisetsu Ido) Dataset System Engineering : Ignis Imageworks Corp. (Tetsuya Kobayashi, Katsunori Kiuchi, Kanako Saito, Hayato Abe, Ryosuke Akazawa, Yuya Nagura, Shigeru Ohata, Ayano Takimoto, Kanami Kawamura, Yoko Konno) Visual Programming : Satoshi Horii, Futa Kera (Rhizomatiks) Videographer : Muryo Homma (Rhizomatiks) Hardware Engineering & Videographer Support : Toshitaka Mochizuki (Rhizomatiks) Hardware Engineering : Yuta Asai, Kyohei Mouri, Saki Ishikawa (Rhizomatiks) Technical Support : Shintaro Kamijyo (Rhizomatiks) Project Management : Kahori Takemura (Rhizomatiks) Project Management, Produce : Takao Inoue (Rhizomatiks) This work was conducted with assistance from Dentsu Lab Tokyo."	https://dl.acm.org/doi/abs/10.1145/3478511.3491310	Yuya Hanai, Kyle McDonald, Satoshi Horii, Futa Kera, Kisaku Tanaka, Motoi Ishibashi, Daito Manabe
Foids: bio-inspired fish simulation for generating synthetic datasets	"We present a bio-inspired fish simulation platform, which we call ""Foids"", to generate realistic synthetic datasets for an use in computer vision algorithm training. This is a first-of-its-kind synthetic dataset platform for fish, which generates all the 3D scenes just with a simulation. One of the major challenges in deep learning based computer vision is the preparation of the annotated dataset. It is already hard to collect a good quality video dataset with enough variations; moreover, it is a painful process to annotate a sufficiently large video dataset frame by frame. This is especially true when it comes to a fish dataset because it is difficult to set up a camera underwater and the number of fish (target objects) in the scene can range up to 30,000 in a fish cage on a fish farm. All of these fish need to be annotated with labels such as a bounding box or silhouette, which can take hours to complete manually, even for only a few minutes of video. We solve this challenge by introducing a realistic synthetic dataset generation platform that incorporates details of biology and ecology studied in the aquaculture field. Because it is a simulated scene, it is easy to generate the scene data with annotation labels from the 3D mesh geometry data and transformation matrix. To this end, we develop an automated fish counting system utilizing the part of synthetic dataset that shows comparable counting accuracy to human eyes, which reduces the time compared to the manual process, and reduces physical injuries sustained by the fish."	https://dl.acm.org/doi/abs/10.1145/3478513.3480520	Yuko Ishiwaka, Xiao S. Zeng, Michael Lee Eastman, Sho Kakazu, Sarah Gross, Ryosuke Mizutani, Masaki Nakada
FreeStyleGAN: free-view editable portrait rendering with the camera manifold	Current Generative Adversarial Networks (GANs) produce photorealistic renderings of portrait images. Embedding real images into the latent space of such models enables high-level image editing. While recent methods provide considerable semantic control over the (re-)generated images, they can only generate a limited set of viewpoints and cannot explicitly control the camera. Such 3D camera control is required for 3D virtual and mixed reality applications. In our solution, we use a few images of a face to perform 3D reconstruction, and we introduce the notion of the GAN camera manifold, the key element allowing us to precisely define the range of images that the GAN can reproduce in a stable manner. We train a small face-specific neural implicit representation network to map a captured face to this manifold and complement it with a warping scheme to obtain free-viewpoint novel-view synthesis. We show how our approach - due to its precise camera control - enables the integration of a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g., stereo rendering or consistent insertion of faces in synthetic 3D environments. Our solution proposes the first truly free-viewpoint rendering of realistic faces at interactive rates, using only a small number of casual photos as input, while simultaneously allowing semantic editing capabilities, such as facial expression or lighting changes.	https://dl.acm.org/doi/abs/10.1145/3478513.3480538	Thomas Leimkühler, George Drettakis
FrictionalMonolith: a monolithic optimization-based approach for granular flow with contact-aware rigid-body coupling	We propose , a monolithic pressure-friction-contact solver for more accurately, robustly, and efficiently simulating two-way interactions of rigid bodies with continuum granular materials or inviscid liquids. By carefully formulating the components of such systems within a single unified minimization problem, our solver can simultaneously handle unilateral incompressibility and implicit integration of friction for the interior of the continuum, frictional contact resolution among the rigid bodies, and mutual force exchanges between the continuum and rigid bodies. Our monolithic approach eliminates various problematic artifacts in existing weakly coupled approaches, including loss of volume in the continuum material, artificial drift and slip of the continuum at solid boundaries, interpenetrations of rigid bodies, and simulation instabilities. To efficiently handle this challenging monolithic minimization problem, we present a customized solver for the resulting quadratically constrained quadratic program that combines elements of staggered projections, augmented Lagrangian methods, inexact projected Newton, and active-set methods. We demonstrate the critical importance of a unified treatment and the effectiveness of our proposed solver in a range of practical scenarios.	https://dl.acm.org/doi/abs/10.1145/3478513.3480539	Tetsuya Takahashi, Christopher Batty
Frisson Waves: Sharing Frisson to Create Collective Empathetic Experiences for Music Performances	Frisson is a feeling and a mental experience of body reactions such as shivers, tingling skin, and goosebumps. However, this sensation is not shareable naturally with others and is rarely used in live performances. We propose Frisson Waves, a real-time system to detect, trigger and share frisson in a wave-like pattern during music performances. The system consists of a physiological sensing wristband for detecting frisson and a thermo-haptic neckband for inducing frisson. We aim to improve the connectedness of audience members and performers during music performances by sharing frisson.	https://dl.acm.org/doi/abs/10.1145/3476122.3484847	Yan He, George Chernyshov, Dingding Zheng, Jiawen Han, Ragnar Thomsen, Danny Hynds, Yuehui Yang, Yun Suen Pai, Kai Kunze, Kouta Minamizawa
GIBSON: AR/VR synchronized city walking system	GIBSON is a novel city walking system that enables distant users to walk together as if they are physically in the same city. The advancement of virtual reality technology has opened the possibility to travel around the world virtually beyond geographical limitations, but there is still room for improvement to make the experience as realistic as real travel. Unlike conventional virtual travel tools and prior multi-user collaborative XR studies, we designed our system to evoke both a sense of co-presence and a sense of being in the real space. For this purpose, we implemented two main functions: (1) function to transfer real-time audio-visual information of the surroundings and (2) function to transfer body movements of users through avatars. We also combined visual positioning system (VPS) and SLAM to align the user locations. We conducted user testing to verify the experience of cross-AR/VR city walking using GIBSON. The result suggests that our system could make people feel as if they were walking together in the city even though they are physically distanced.	https://dl.acm.org/doi/abs/10.1145/3478514.3487638	Seiichiro Takeuchi, Kyoko Hashiguchi, Yuki Homma, Kent Kajitani, Shingo Meguro
GPU Cloth Simulation Pipeline in Lightchaser Animation Studio	We present the simulation pipeline of character effects in Lightchaser Animation Studio and how we utilize GPU resources to accelerate clothing simulations. Many characters from ancient Chinese tales in our films are in complex costumes.(Figure 1) Such costumes contain five to six layers of fabrics, where several hundred thousand triangles are used to show the delicate folds of different cloth materials. At the same time, there are more than 600 similar shots bringing more than 1000 cloth simulation tasks in a film project. Therefore, the efficiency and accuracy of our cloth simulator is the key to the CFX production pipeline.	https://dl.acm.org/doi/abs/10.1145/3478512.3488616	Haowei Han, Meng Sun, Siyu Zhang, Dongying Liu, Tiantian Liu
Generalized adaptive refinement for grid-based hexahedral meshing	Due to their nice numerical properties, conforming hexahedral meshes are considered a prominent computational domain for simulation tasks. However, the automatic decomposition of a general 3D volume into a small number of hexahedral elements is very challenging. Methods that create an adaptive Cartesian grid and convert it into a conforming mesh offer superior robustness and are the only ones concretely used in the industry. Topological schemes that permit this conversion can be applied only if precise compatibility conditions among grid elements are observed. Some of these conditions are local, hence easy to formulate; others are not and are much harder to satisfy. State-of-the-art approaches fulfill these conditions by prescribing additional refinement based on special building rules for octrees. These methods operate in a restricted space of solutions and are prone to severely over-refine the input grids, creating a bottleneck in the simulation pipeline. In this article, we introduce a novel approach to transform a general adaptive grid into a new grid meeting hexmeshing criteria, without resorting to tree rules. Our key insight is that we can formulate all compatibility conditions as linear constraints in an integer programming problem by choosing the proper set of unknowns. Since we operate in a broader solution space, we are able to meet topological hexmeshing criteria at a much coarser scale than methods using octrees, also supporting generalized grids of any shape or topology. We demonstrate the superiority of our approach for both traditional grid-based hexmeshing and adaptive polycube-based hexmeshing. In all our experiments, our method never prescribed more refinement than the prior art and, in the average case, it introduced close to half the number of extra cells.	https://dl.acm.org/doi/abs/10.1145/3478513.3480508	Luca Pitzalis, Marco Livesu, Gianmarco Cherchi, Enrico Gobbetti, Riccardo Scateni
Generalized deployable elastic geodesic grids	Given a designer created free-form surface in 3d space, our method computes a grid composed of elastic elements which are completely planar and straight. Only by fixing the ends of the planar elements to appropriate locations, the 2d grid bends and approximates the given 3d surface. Our method is based purely on the notions from differential geometry of curves and surfaces and avoids any physical simulations. In particular, we introduce a well-defined elastic grid energy functional that allows identifying networks of curves that minimize the bending energy and at the same time nestle to the provided input surface well. Further, we generalize the concept of such grids to cases where the surface boundary does not need to be convex, which allows for the creation of sophisticated and visually pleasing shapes. The algorithm finally ensures that the 2d grid is perfectly planar, making the resulting gridshells inexpensive, easy to fabricate, transport, assemble, and finally also to deploy. Additionally, since the whole structure is pre-strained, it also comes with load-bearing capabilities. We evaluate our method using physical simulation and we also provide a full fabrication pipeline for desktop-size models and present multiple examples of surfaces with elliptic and hyperbolic curvature regions. Our method is meant as a tool for quick prototyping for designers, architects, and engineers since it is very fast and results can be obtained in a matter of seconds.	https://dl.acm.org/doi/abs/10.1145/3478513.3480516	Stefan Pillwein, Przemyslaw Musialski
Generalized fluid carving with fast lattice-guided seam computation	In this paper, we introduce a novel method for intelligently resizing a wide range of volumetric data including fluids. Fluid carving, the technique we build upon, only supported particle-based liquid data, and because it was based on image-based techniques, it was constrained to rectangular boundaries. We address these limitations to allow a much more versatile method for volumetric post-processing. By enclosing a region of interest in our lattice structure, users can retarget regions of a volume with non-rectangular boundaries and non-axis-aligned motion. Our approach generalizes to images, videos, liquids, meshes, and even previously unexplored domains such as fire and smoke. We also present a seam computation method that is significantly faster than the previous approach while maintaining the same level of quality, thus making our method more viable for production settings where post-processing workflows are vital.	https://dl.acm.org/doi/abs/10.1145/3478513.3480544	Sean Flynn, David Hart, Bryan Morse, Seth Holladay, Parris Egbert
Generative modelling of BRDF textures from flash images	We learn a latent space for easy capture, consistent interpolation, and efficient reproduction of visual material appearance. When users provide a photo of a stationary natural material captured under flashlight illumination, first it is converted into a latent material code. Then, in the second step, conditioned on the material code, our method produces an infinite and diverse spatial field of BRDF model parameters (diffuse albedo, normals, roughness, specular albedo) that subsequently allows rendering in complex scenes and illuminations, matching the appearance of the input photograph. Technically, we jointly embed all flash images into a latent space using a convolutional encoder, and -conditioned on these latent codes- convert random spatial fields into fields of BRDF parameters using a convolutional neural network (CNN). We condition these BRDF parameters to match the visual characteristics (statistics and spectra of visual features) of the input under matching light. A user study compares our approach favorably to previous work, even those with access to BRDF supervision. Project webpage: https://henzler.github.io/publication/neuralmaterial/.	https://dl.acm.org/doi/abs/10.1145/3478513.3480507	Philipp Henzler, Valentin Deschaintre, Niloy J. Mitra, Tobias Ritschel
Glisp: lisp-based graphic design tool	"Glisp is an open-source project to develop a design tool that aims to combine the flexibility of computational art with traditional human-centered graphic design. It started from the author's awareness of the problems with existing proprietary software as a visual artist who works on design and video production every day. The core idea is quite simple; by using the source code itself written in Lisp as a graphic data file, it brings the same extensibility to design as a programming language. With the Lisp's power of ""boot-strapping"", artists including non-programmers can program unique ways to draw graphics to fit their purpose, and become to be able to speculate unexplored styles far beyond existing cliché."	https://dl.acm.org/doi/abs/10.1145/3478511.3491312	Baku Hashimoto
GroundFlow: Multiple Flows Feedback for Enhancing Immersive Experience on the Floor in the Wet Scenes	With haptic technology, the uses can experience an enhanced immersion in virtual reality. Most haptic techniques focus on the upper body, such as the head, chest, and hands, to provide strength feedback when interacting in the virtual world. Thus, researchers have been exploring different techniques to simulate haptic feedback for walking around in virtual space, such as texture, height, vibration, shape, and resistance. However, those techniques can not provide a real wet sensation in the virtual scene. Therefore, we present GroundFlow, a water recirculation system that provides multiple flows feedback on the floor in immersive virtual reality. Our demonstration also implemented a virtual excursion that allows users to experience different water flows and their corresponding wet scenes.	https://dl.acm.org/doi/abs/10.1145/3476122.3484837	Tzu-Hua Wang, Tzu-Shan Lee, Jr-Pin Pan, Tzu-Yang Kuo, Hui-Yang Yong, Ping-Hsuan Han
Guided Image Weathering using Image-to-Image Translation	In this paper, we present a guided image weathering method that allows the user to generate the weathering process. The core of our method is a three-step method to generate textures at different time steps of the weathering process. The input texture is analyzed first to obtain the weathering degree (age map) for each pixel, then we train a conditional adversarial network to generate texture patches with diverse weathering effects. Once the training is finished, new weathering results can be generated by manipulating the age map, such as automatic interpolation and manually modified by the user.	https://dl.acm.org/doi/abs/10.1145/3478512.3488603	Li-Yu Chen, I-Chao Shen, Bing-Yu Chen
HWAuth: Handwriting-Based Socially-Inclusive Authentication	Small, local group of users who share private resources (e.g., families, university labs, business departments) usually have limited usable authentication needs. For these entities, existing authentication solutions either require excessive personal information (e.g., biometrics), do not distinguish each user (e.g., shared passwords), or lack security measures when the access key is compromised (e.g., physical keys). We propose an alternative solution by designing HWAuth: an inclusive group authentication system with a shared text that is uniquely identifiable for each user. Each user shares the same textual password, but individual handwriting styles of the text are used to distinguish each user. We evaluated the usability and security of our design through a user study with 30 participants. Our results suggest that (1) users who enter the same shared passwords are discernible from one another, and (2) that users were able to consistently login using HWAuth.	https://dl.acm.org/doi/abs/10.1145/3476124.3488638	Joon Kuy Han, Byungkon Kang, Dennis Wong
HoloBurner:: Mixed Reality Equipment for Learning Flame Color Reaction by using Aerial Imaging Display	The HoloBurner system is a mixed reality instrument of chemistry experiments for learning flame color reaction. It can provide users with the sense and enjoyment of performing experiments in a laboratory. Flame color reaction is a phenomenon which can identify an element by its unique color, with which students can learn that substances are comprised with various different elements. The HoloBurner system provide users a real feeling of operating a burner, and realistic image of flame with an aerial imaging display. Students can experience flame color reaction using the HoloBurner safely in natural way same as with real instruments.	https://dl.acm.org/doi/abs/10.1145/3476122.3484846	Toshikazu Ohshima, Kai Nishimoto
How Can I Swing Like Pro?: Golf Swing Analysis Tool for Self Training	In this work, we present an analysis tool to help golf beginners compare their swing motion to the swing motion of experts. The proposed application synchronizes videos with different swing phase timings using the latent features extracted by a neural network-based encoder and detects key frames where discrepant motions occur. We visualize synchronized image frames and 3D poses that help users recognize the differences and key factors that can be important for their swing skill improvement.	https://dl.acm.org/doi/abs/10.1145/3476124.3488645	Chen-Chieh Liao, Dong-Hyun Hwang, Hideki Koike
Human dynamics from monocular video with dynamic camera movements	We propose a new method that reconstructs 3D human motion from in-the-wild video by making full use of prior knowledge on the laws of physics. Previous studies focus on reconstructing joint angles and positions in the body local coordinate frame. Body translations and rotations in the global reference frame are partially reconstructed only when the video has a static camera view. We are interested in overcoming this static view limitation to deal with dynamic view videos. The camera may pan, tilt, and zoom to track the moving subject. Since we do not assume any limitations on camera movements, body translations and rotations from the video do not correspond to absolute positions in the reference frame. The key technical challenge is inferring body translations and rotations from a sequence of 3D full-body poses, assuming the absence of root motion. This inference is possible because human motion obeys the law of physics. Our reconstruction algorithm produces a control policy that simulates 3D human motion imitating the one in the video. Our algorithm is particularly useful for reconstructing highly dynamic movements, such as sports, dance, gymnastics, and parkour actions.	https://dl.acm.org/doi/abs/10.1145/3478513.3480504	Ri Yu, Hwangpil Park, Jehee Lee
HyperNeRF: a higher-dimensional representation for topologically varying neural radiance fields	"Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this ""hyper-space"". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between ""moments"", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub , outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at hypernerf.github.io."	https://dl.acm.org/doi/abs/10.1145/3478513.3480487	Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz
ICTree: automatic perceptual metrics for tree models	Many algorithms for virtual tree generation exist, but the visual realism of the 3D models is unknown. This problem is usually addressed by performing limited user studies or by a side-by-side visual comparison. We introduce an automated system for realism assessment of the tree model based on their perception. We conducted a user study in which 4,000 participants compared over one million pairs of images to collect subjective perceptual scores of a large dataset of virtual trees. The scores were used to train two neural-network-based predictors. A view independent ICTreeF uses the tree model's geometric features that are easy to extract from any model. The second is ICTreeI that estimates the perceived visual realism of a tree from its image. Moreover, to provide an insight into the problem, we deduce intrinsic attributes and evaluate which features make trees look like real trees. In particular, we show that branching angles, length of branches, and widths are critical for perceived realism. We also provide three datasets: carefully curated 3D tree geometries and tree skeletons with their perceptual scores, multiple views of the tree geometries with their scores, and a large dataset of images with scores suitable for training deep neural networks.	https://dl.acm.org/doi/abs/10.1145/3478513.3480519	Tomas Polasek, David Hrusa, Bedrich Benes, Martin Čadík
Indefinately:Indefinately	"This work ""indefinitely"" tells the epitome of the earth and future urban pollution. In contemporary society, high walls have been built between people, and people living in cities are basically replicas. We use VR technology to create a surreal world in VR helmets, focusing on spiritual pollution and environmental pollution. In this ambiguous world, how can people find themselves in the bustling maze of nothingness? Through the three scenes of tomb forest, acid rain wasteland and hazy village, the experimenter uses the handle to understand human diseases, disasters and pollution in the past[Ypsilanti et al., 2018].They also picked up the photo fragments of ""Lucas"" representing hope and felt the fear of being lost in the spiritual maze. In this scene, they become poor people who are lost and polluted in their souls. At the end of the scene, the experimenter found a door full of hope, picked up all the fragments of Lucas, took a group photo of Lucas and his daughter before departure, and ended the journey."	https://dl.acm.org/doi/abs/10.1145/3478514.3487626	Chenxin Zhang, Lesi Hu
Integer coordinates for intrinsic geometry processing	This paper describes a numerically robust data structure for encoding intrinsic triangulations of polyhedral surfaces. Many applications demand a correspondence between the intrinsic triangulation and the input surface, but existing data structures either rely on floating point values to encode correspondence, or do not support remeshing operations beyond basic edge flips. We instead provide an integer-based data structure that guarantees valid correspondence, even for meshes with near-degenerate elements. Our starting point is the framework of from geometric topology, which we extend to the broader set of operations needed for mesh processing (vertex insertion, edge splits, ). The resulting data structure can be used as a drop-in replacement for earlier schemes, automatically improving reliability across a wide variety of applications. As a stress test, we successfully compute an intrinsic Delaunay refinement and associated subdivision for all manifold meshes in the Thingi10k dataset. In turn, we can compute reliable and highly accurate solutions to partial differential equations even on extremely low-quality meshes.	https://dl.acm.org/doi/abs/10.1145/3478513.3480522	Mark Gillespie, Nicholas Sharp, Keenan Crane
Integration of stereoscopic laser-based geometry into 3D video using DLP Link synchronisation	Stereoscopic video projection using active shuatter glasses is a mature technology employed in multi-person immersive virtual reality environments such as CAVE systems. On the other hand, non-stereoscopic laser projectors are popular in the entertainment industry because they can display graphics at greater distances and cover larger areas. However, stereoscopic-capable laser-based vector graphics could enhance video-based immersive 3D experiences, due to their unique visual characteristics including extremely high contrast and arbitrarily extended gamut through the use of multiple laser sources. Their virtually infinite depth of field also allows for easy installation compared with video projectors, regardless of the size of the augmented space. In this work, we demonstrate a system integrating 3D laser-based vector graphics into a dynamic scene generated by a conventional stereoscopic video projection system.	https://dl.acm.org/doi/abs/10.1145/3476122.3484833	Jayson Haebich, Christian Sandor, Alvaro Cassinelli
Interactive Manga Colorization with Fast Flat Coloring	This paper proposes an interactive semi-automatic system for manga colorization. In our system, users can colorize monochrome manga images interactively by scribbling the desired colors. The proposed method creates a high quality colorized image by inputting the original monochrome image after grayscale adjustment and the flat colored image generated from the scribbles to a colorization network. Experiments show that the colorized results yielded by the proposed method are much better than existing methods.	https://dl.acm.org/doi/abs/10.1145/3476124.3488628	Delong Ouyang, Ryosuke Furuta, Yugo Shimizu, Yukinobu Taniguchi, Ryota Hinami, Shonosuke Ishiwatari
Interactive all-hex meshing via cuboid decomposition	Standard PolyCube-based hexahedral (hex) meshing methods aim to deform the input domain into an axis-aligned PolyCube volume with integer corners; if this deformation is bijective, then applying the inverse map to the voxelized PolyCube yields a valid hex mesh. A key challenge in these methods is to maintain the bijectivity of the PolyCube deformation, thus reducing the robustness of these algorithms. In this work, we present an interactive pipeline for hex meshing that sidesteps this challenge by using a new representation of PolyCubes as unions of cuboids. We begin by deforming the input tetrahedral mesh into a near-PolyCube domain whose faces are loosely aligned to the major axis directions. We then build a PolyCube by optimizing the layout of a set of cuboids with user guidance to closely fit the deformed domain. Finally, we construct an inversion-free pullback map from the voxelized PolyCube to the input domain while optimizing for mesh quality metrics. We allow extensive user control over each stage, such as editing the voxelized PolyCube, positioning surface vertices, and exploring the trade-off among competing quality metrics, while also providing automatic alternatives. We validate our method on over one hundred shapes, including models that are challenging for past PolyCube-based and frame-field-based methods. Our pipeline reliably produces hex meshes with quality on par with or better than state-of-the-art. We additionally conduct a user study with 21 participants in which the majority prefer hex meshes they make using our tool to the ones from automatic state-of-the-art methods. This demonstrates the need for intuitive interactive hex meshing tools where the user can dictate the priorities of their mesh.	https://dl.acm.org/doi/abs/10.1145/3478513.3480568	Lingxiao Li, Paul Zhang, Dmitriy Smirnov, S. Mazdak Abulnaga, Justin Solomon
Interactive cutting and tearing in projective dynamics with progressive cholesky updates	We propose a new algorithm for updating a Cholesky factorization which speeds up Projective Dynamics simulations with topological changes. Our approach addresses an important limitation of the original Projective Dynamics, i.e., that topological changes such as cutting, fracturing, or tearing require full refactorization which compromises computation speed, especially in real-time applications. Our method progressively modifies the Cholesky factor of the system matrix in the global step instead of computing it from scratch. Only a small amount of overhead is added since most of the topological changes in typical simulations are continuous and gradual. Our method is based on the update and downdate routine in CHOLMOD, but unlike recent related work, supports dynamic sizes of the system matrix and the addition of new vertices. Our approach allows us to introduce clean cuts and perform interactive remeshing. Our experiments show that our method works particularly well in simulation scenarios involving cutting, tearing, and local remeshing operations.	https://dl.acm.org/doi/abs/10.1145/3478513.3480505	Jing Li, Tiantian Liu, Ladislav Kavan, Baoquan Chen
Intuitive and efficient roof modeling for reconstruction and synthesis	We propose a novel and flexible roof modeling approach that can be used for constructing planar 3D polygon roof meshes. Our method uses a graph structure to encode roof topology and enforces the roof validity by optimizing a simple but effective planarity metric we propose. This approach is significantly more efficient than using general purpose 3D modeling tools such as 3ds Max or SketchUp, and more powerful and expressive than specialized tools such as the straight skeleton. Our optimization-based formulation is also flexible and can accommodate different styles and user preferences for roof modeling. We showcase two applications. The first application is an interactive roof editing framework that can be used for roof design or roof reconstruction from aerial images. We highlight the efficiency and generality of our approach by constructing a mesh-image paired dataset consisting of 2539 roofs. Our second application is a generative model to synthesize new roof meshes from scratch. We use our novel dataset to combine machine learning and our roof optimization techniques, by using transformers and graph convolutional networks to model roof topology, and our roof optimization methods to enforce the planarity constraint.	https://dl.acm.org/doi/abs/10.1145/3478513.3480494	Jing Ren, Biao Zhang, Bojian Wu, Jianqiang Huang, Lubin Fan, Maks Ovsjanikov, Peter Wonka
Inverse Free-form Deformation for interactive UV map editing	Free-form deformation (FFD) is useful for manual 2D texture mapping in a 2D domain. The user first places a coarse regular grid in the texture space, and then adjusts the positions of the grid points in the image space. In this paper, we consider the inverse way of this problem, namely, inverse FFD. In this problem setting, we assume that an initial image-to-texture dense mapping is already obtained by some automatic method, such as a data-driven inference. However, this initial dense mapping may not be satisfactory, so the user may want to modify it. Nonetheless, it is difficult to manually edit the dense mapping due to its huge degrees of freedom. We thus convert the dense mapping to a coarse FFD mapping to facilitate manual editing of the mapping. Inverse FFD is formulated as a least-squares optimization, so one can solve it very efficiently.	https://dl.acm.org/doi/abs/10.1145/3478512.3488614	Seung-Tak Noh, Takeo Igarashi
I♥LA: compilable markdown for linear algebra	Communicating linear algebra in written form is challenging: mathematicians must choose between writing in languages that produce well-formatted but semantically-underdefined representations such as LaTeX; or languages with well-defined semantics but notation unlike conventional math, such as C++/Eigen. In both cases, the underlying linear algebra is obfuscated by the requirements of esoteric language syntax (as in LaTeX) or awkward APIs due to language semantics (as in C++). The gap between representations results in communication challenges, including underspecified and irrepro-ducible research results, difficulty teaching math concepts underlying complex numerical code, as well as repeated, redundant, and error-prone translations from communicated linear algebra to executable code. We introduce I♥LA, a language with syntax designed to closely mimic conventionally-written linear algebra, while still ensuring an unambiguous, compilable interpretation. Inspired by Markdown, a language for writing naturally-structured plain text files that translate into valid HTML, I♥LA allows users to write linear algebra in text form and compile the same source into LaTeX, C++/Eigen, Python/NumPy/SciPy, and MATLAB, with easy extension to further math programming environments. We outline the principles of our language design and highlight design decisions that balance between readability and precise semantics, and demonstrate through case studies the ability for I♥LA to bridge the semantic gap between conventionally-written linear algebra and unambiguous interpretation in math programming environments.	https://dl.acm.org/doi/abs/10.1145/3478513.3480506	Yong Li, Shoaib Kamil, Alec Jacobson, Yotam Gingold
Joint Augmented Reality Video Analytics and Artificial Intelligence Supervision	Augmented Reality (AR) and Artificial Intelligence (AI) technologies have become ubiquitous in the public's daily lives. In the AR space, many notable industry players are engaged in bringing smaller, more ergonomic, and more powerful AR headsets and smart-glasses to the market, with the eventual goal of these devices to replace mobile phones for a wide variety of tasks. With the improvements in computing power on AR devices, a similar surge in AI-powered technologies running on these devices is also evident. In this work, we present an AR application that explores Human-AI interaction in a computer vision context by providing an integrated platform for video analytics and AI model fine-tuning.	https://dl.acm.org/doi/abs/10.1145/3476124.3488652	Benjamin Sho, Ryan Anthony De Belen, Rowan T Hughes, Tomasz Bednarz
Joint computational design of workspaces and workplans	Humans assume different production roles in a workspace. On one hand, humans design workplans to complete tasks as efficiently as possible in order to improve productivity. On the other hand, a nice workspace is essential to facilitate teamwork. In this way, workspace design and workplan design complement each other. Inspired by such observations, we propose an automatic approach to jointly design a workspace and a workplan. Taking staff properties, a space, and work equipment as input, our approach jointly optimizes a workspace and a workplan, considering performance factors such as time efficiency and congestion avoidance, as well as workload factors such as walk effort, turn effort, and workload balances. To enable exploration of design trade-offs, our approach generates a set of Pareto-optimal design solutions with strengths on different objectives, which can be adopted for different work scenarios. We apply our approach to synthesize workspaces and workplans for different workplaces such as a fast food kitchen and a supermarket. We also extend our approach to incorporate other common work considerations such as dynamic work demands and accommodating staff members with different physical capabilities. Evaluation experiments with simulations validate the efficacy of our approach for synthesizing effective workspaces and workplans.	https://dl.acm.org/doi/abs/10.1145/3478513.3480500	Yongqi Zhang, Haikun Huang, Erion Plaku, Lap-Fai Yu
Julian Tuwim: To Everyman	To Everyman is a visual rendition of Julian Tuwim's poem of the same name and a manifesto seeking to expose the techniques and tactics of political propaganda. The film uncovers the array of manipulations that paint war as necessary and make it palatable for society. This animated short shows a never-ending circle and can be seamlessly looped. It begins with a spark igniting the conflict and seems to end with a remembrance of the fallen. If played continuously, however, the story simply moves to another hotbed of tension.	https://dl.acm.org/doi/abs/10.1145/3463912.3482628	Cezary Albinski, Katarzyna Dubinska
Kaleidoscopic structured light	Full surround 3D imaging for shape acquisition is essential for generating digital replicas of real-world objects. Surrounding an object we seek to scan with a kaleidoscope, that is, a configuration of multiple planar mirrors, produces an image of the object that encodes information from a combinatorially large number of virtual viewpoints. This information is practically useful for the full surround 3D reconstruction of the object, but cannot be used directly, as we do not know what virtual viewpoint each image pixel corresponds---the pixel label. We introduce a structured light system that combines a projector and a camera with a kaleidoscope. We then prove that we can accurately determine the labels of projector and camera pixels, for arbitrary kaleidoscope configurations, using the projector-camera epipolar geometry. We use this result to show that our system can serve as a multi-view structured light system with hundreds of virtual projectors and cameras. This makes our system capable of scanning complex shapes precisely and with full coverage. We demonstrate the advantages of the kaleidoscopic structured light system by scanning objects that exhibit a large range of shapes and reflectances.	https://dl.acm.org/doi/abs/10.1145/3478513.3480524	Byeongjoo Ahn, Ioannis Gkioulekas, Aswin C. Sankaranarayanan
Keypoint-driven line drawing vectorization via PolyVector flow	Line drawing vectorization is a daily task in graphic design, computer animation, and engineering, necessary to convert raster images to a set of curves for editing and geometry processing. Despite recent progress in the area, automatic vectorization tools often produce spurious branches or incorrect connectivity around curve junctions; or smooth out sharp corners. These issues detract from the use of such vectorization tools, both from an aesthetic viewpoint and for feasibility of downstream applications (e.g., automatic coloring or inbetweening). We address these problems by introducing a novel line drawing vectorization algorithm that splits the task into three components: (1) finding keypoints, i.e., curve endpoints, junctions, and sharp corners; (2) extracting drawing topology, i.e., finding connections between keypoints; and (3) computing the geometry of those connections. We compute the optimal geometry of the connecting curves via a novel geometric flow --- --- that aligns the curves to the drawing, disambiguating directions around Y-, X-, and T-junctions. We show that our system robustly infers both the geometry and topology of detailed complex drawings. We validate our system both quantitatively and qualitatively, demonstrating that our method visually outperforms previous work.	https://dl.acm.org/doi/abs/10.1145/3478513.3480529	Ivan Puhachov, William Neveu, Edward Chien, Mikhail Bessmeltsev
LIPSYNC.AI: A.I. Driven Lips and Tongue Animations Using Articulatory Phonetic Descriptors and FACS Blendshapes	We present a solution for generating realistic lips and tongue animations, using a novel hybrid method which makes use of both the advancements in deep learning and the theory behind speech and phonetics. Our solution generates highly accurate and natural animations of the jaw, lips and tongue through the use of additional phonetic information during the neural network training, and the procedural mapping of its outputs directly to FACS [Prince et al. 2015] based blendshapes, in order to comply to animation industry standards.	https://dl.acm.org/doi/abs/10.1145/3476122.3484850	Jara Alvarez Masso, Alexandru Mihai Rogozea, Jan Medvesek, Saeid Mokaram, Yijun Yu
La Meute	After a harrassing day, Marion, a young woman in her twenties, recounts her story to a policeman, hoping to be unterstood. For her, the responsible one is part of the hounds. For him, she is the one responsible.	https://dl.acm.org/doi/abs/10.1145/3463912.3477188	Louise Cottin-Euziol, Antoine Blossier Gacic, Charline Hedreville, Agathe Moulin, Victoria Normand, Gabriel Saint-Frison
Large steps in inverse rendering of geometry	Inverse reconstruction from images is a central problem in many scientific and engineering disciplines. Recent progress on differentiable rendering has led to methods that can efficiently differentiate the full process of image formation with respect to millions of parameters to solve such problems via gradient-based optimization. At the same time, the availability of cheap derivatives does not necessarily make an inverse problem easy to solve. Mesh-based representations remain a particular source of irritation: an adverse gradient step involving vertex positions could turn parts of the mesh inside-out, introduce numerous local self-intersections, or lead to inadequate usage of the vertex budget due to distortion. These types of issues are often irrecoverable in the sense that subsequent optimization steps will further exacerbate them. In other words, the optimization lacks robustness due to an objective function with substantial non-convexity. Such robustness issues are commonly mitigated by imposing additional regularization, typically in the form of Laplacian energies that quantify and improve the smoothness of the current iterate. However, regularization introduces its own set of problems: solutions must now compromise between solving the problem and being smooth. Furthermore, gradient steps involving a Laplacian energy resemble Jacobi's iterative method for solving linear equations that is known for its exceptionally slow convergence. We propose a simple and practical alternative that casts differentiable rendering into the framework of preconditioned gradient descent. Our pre-conditioner biases gradient steps towards smooth solutions without requiring the final solution to be smooth. In contrast to Jacobi-style iteration, each gradient step propagates information among all variables, enabling convergence using fewer and larger steps. Our method is not restricted to meshes and can also accelerate the reconstruction of other representations, where smooth solutions are generally expected. We demonstrate its superior performance in the context of geometric optimization and texture reconstruction.	https://dl.acm.org/doi/abs/10.1145/3478513.3480501	Baptiste Nicolet, Alec Jacobson, Wenzel Jakob
Layered neural atlases for consistent video editing	"We present a method that decomposes, and ""unwraps"", an input video into a set of , each providing a of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image."	https://dl.acm.org/doi/abs/10.1145/3478513.3480546	Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel
Learning English to Chinese Character: Calligraphic Art Production based on Transformer	"We propose a transformer-based model to learn Square Word Calligraphy to write English words in the format of a square that resembles Chinese characters. To achieve this task, we compose a dataset by collecting the calligraphic characters created by artist Xu Bing, and labeling the position of each alphabet in the characters. Taking the input of English alphabets, we introduce a modified transformer-based model to learn the position relationship between each alphabet and predict the transformation parameters for each part to reassemble them as a Chinese character. We show the comparison results between our predicted characters and corresponding characters created by the artist to indicate our proposed model has a good performance on this task, and we also created new characters to show the ""creativity"" of our model."	https://dl.acm.org/doi/abs/10.1145/3476124.3488642	Yifan Jin, Yi Zhang, Xi Yang
Learning to cluster for rendering with many lights	We present an unbiased online Monte Carlo method for rendering with many lights. Our method adapts both the hierarchical light clustering and the sampling distribution to our collected samples. Designing such a method requires us to make clustering decisions under noisy observation, and making sure that the sampling distribution adapts to our target. Our method is based on two key ideas: a coarse-to-fine clustering scheme that can find good clustering configurations even with noisy samples, and a discrete stochastic successive approximation method that starts from a prior distribution and provably converges to a target distribution. We compare to other state-of-the-art light sampling methods, and show better results both numerically and visually.	https://dl.acm.org/doi/abs/10.1145/3478513.3480561	Yu-Chen Wang, Yu-Ting Wu, Tzu-Mao Li, Yung-Yu Chuang
Learning to reconstruct botanical trees from single images	We introduce a novel method for reconstructing the 3D geometry of botanical trees from single photographs. Faithfully reconstructing a tree from single-view sensor data is a challenging and open problem because many possible 3D trees exist that fit the tree's shape observed from a single view. We address this challenge by defining a reconstruction pipeline based on three neural networks. The networks simultaneously mask out trees in input photographs, identify a tree's species, and obtain its 3D radial bounding volume - our novel 3D representation for botanical trees. Radial bounding volumes (RBV) are used to orchestrate a procedural model primed on learned parameters to grow a tree that matches the main branching structure and the overall shape of the captured tree. While the RBV allows us to faithfully reconstruct the main branching structure, we use the procedural model's morphological constraints to generate realistic branching for the tree crown. This constraints the number of solutions of tree models for a given photograph of a tree. We show that our method reconstructs various tree species even when the trees are captured in front of complex backgrounds. Moreover, although our neural networks have been trained on synthetic data with data augmentation, we show that our pipeline performs well for real tree photographs. We evaluate the reconstructed geometries with several metrics, including leaf area index and maximum radial tree distances.	https://dl.acm.org/doi/abs/10.1145/3478513.3480525	Bosheng Li, Jacek Kałużny, Jonathan Klein, Dominik L. Michels, Wojtek Pałubicki, Bedrich Benes, Sören Pirk
Les Larmes de la Seine	"17 october 1961, ""Algerian workers"" get down the streets to manifest against the mandatory curfew imposed by the Police prefecture."	https://dl.acm.org/doi/abs/10.1145/3463912.3477191	Yanis Belaid, Eliott Benard, Nicolas Mayeur, Etienne Moulin, Hadrien Pinot, Lisa Vicente, Philippine Singer, Alice Letailleur
Light Source Selection in Primary-Sample-Space Neural Photon Sampling	This paper proposes a light source selection for photon mapping combined with recent deep-learning-based importance sampling. Although applying such neural importance sampling (NIS) to photon mapping is not difficult, a straightforward approach can sample inappropriate photons for each light source because NIS relies on the approximation of a smooth continuous probability density function on the primary sample space, whereas the light source selection follows a discrete probability distribution. To alleviate this problem, we introduce a normalizing flow conditioned by a feature vector representing the index for each light source. When the neural network for NIS is trained to sample visible photons, we achieved lower variance with the same sample budgets, compared to a previous photon sampling using Markov chain Monte Carlo.	https://dl.acm.org/doi/abs/10.1145/3476124.3488639	Yuta Tsuji, Tatsuya Yatagawa, Shigeo Morishima
Live speech portraits: real-time photorealistic talking-head animation	To the best of our knowledge, we first present a live system that generates personalized photorealistic talking-head animation only driven by audio signals at over 30 fps. Our system contains three stages. The first stage is a deep neural network that extracts deep audio features along with a manifold projection to project the features to the target person's speech space. In the second stage, we learn facial dynamics and motions from the projected audio features. The predicted motions include head poses and upper body motions, where the former is generated by an autoregressive probabilistic model which models the head pose distribution of the target person. Upper body motions are deduced from head poses. In the final stage, we generate conditional feature maps from previous predictions and send them with a candidate image set to an image-to-image translation network to synthesize photorealistic renderings. Our method generalizes well to wild audio and successfully synthesizes high-fidelity personalized facial details, e.g., wrinkles, teeth. Our method also allows explicit control of head poses. Extensive qualitative and quantitative evaluations, along with user studies, demonstrate the superiority of our method over state-of-the-art techniques.	https://dl.acm.org/doi/abs/10.1145/3478513.3480484	Yuanxun Lu, Jinxiang Chai, Xun Cao
Marco & Polo Go Round: Immersive Virtual Reality	Marco Polo Go Round is a comedic love story with a very surreal twist in which the user is invited to participate in a couple's relationship as their world literally falls apart around them. It is a narrative experience, set in one location that lasts 14 minutes. Motion-captured actors will drive detailed animated characters such that intimate and touching performances exist within a fully immersive 6DOF world.	https://dl.acm.org/doi/abs/10.1145/3478514.3487614	Benjamin Steiger Levine
Marvel's Spider-Man: Miles Morales Procedural Tools for PlayStation 5 Content Authoring	Artists at Insomniac Games created a wonderful and detailed open world, Marvel's Manhattan in autumn, on the PlayStation 4 console for Marvel's Spider-Man. This technical communication provides an overview of several procedural systems developed or improved upon for the standalone game, Marvel's Spider-Man: Miles Morales, and a snapshot of the procedural processes in use during the game's production on the PlayStation 5 console. Procedural systems allowed artists at Insomniac Games to efficiently update the setting to winter, increase visual fidelity, and propagate mark-up data across a large open world environment. The combination of procedural techniques and bespoke artistry enabled Insomniac Games to create a memorable launch title for the PlayStation 5.	https://dl.acm.org/doi/abs/10.1145/3478512.3488594	Xray Halperin
Memo for the 2021 SIGGRAPH course Computational Optimal Transport	These notes are intended to be used as a memo to recap the main definitions and results for our 2021 SIGGRAPH course. More details regarding the algorithms and their applications can be found in the book [21], while more theoretical aspects are treated in details in [23].	https://dl.acm.org/doi/abs/10.1145/3476117.3483441	Gabriel Peyré, Marco Cuturi, Justin Solomon
Midair Haptic-Optic Display with Multi-Tactile Texture based on Presenting Vibration and Pressure Sensation by Ultrasound	Reproducing the tactile texture of a real object allows humans to interact in virtual reality space with a high sense of immersion. In this study, we develop a midair haptic-optic display with multi-tactile texture using focused ultrasound. In this system, users can touch aerial 3D images with a realistic texture without wearing any devices. For rendering realistic texture, our system simultaneously presents vibration and static pressure sensation. The presentable sensation of the previous ultrasound system is limited to vibrations. The combination of the sensations can improve the reality of rendered tactile texture by ultrasound. In the demo, we presented three textures with different roughness: cloth, wooden board, and gel.	https://dl.acm.org/doi/abs/10.1145/3476122.3484849	Tao Morisaki, Masahiro Fujiwara, Yasutoshi Makino, Hiroyuki Shinoda
Modeling clothing as a separate layer for an animatable human avatar	We have recently seen great progress in building photorealistic animatable full-body codec avatars, but generating high-fidelity animation of clothing is still difficult. To address these difficulties, we propose a method to build an animatable clothed body avatar with an explicit representation of the clothing on the upper body from multi-view captured videos. We use a two-layer mesh representation to register each 3D scan separately with the body and clothing templates. In order to improve the photometric correspondence across different frames, texture alignment is then performed through inverse rendering of the clothing geometry and texture predicted by a variational autoencoder. We then train a new two-layer codec avatar with separate modeling of the upper clothing and the inner body layer. To learn the interaction between the body dynamics and clothing states, we use a temporal convolution network to predict the clothing latent code based on a sequence of input skeletal poses. We show photorealistic animation output for three different actors, and demonstrate the advantage of our clothed-body avatars over the single-layer avatars used in previous work. We also show the benefit of an explicit clothing model that allows the clothing texture to be edited in the animation output.	https://dl.acm.org/doi/abs/10.1145/3478513.3480545	Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, Chenglei Wu
Modeling flower pigmentation paterns	Although many simulation models of natural phenomena have been developed to date, little attention was given to a major contributor to the beauty of nature: the colorful patterns of flowers. We survey typical patterns and propose methods for simulating them inspired by the current understanding of the biology of floral patterning. The patterns are generated directly on geometric models of flowers, using different combinations of key mathematical models of morphogenesis: vascular patterning, positional information, reaction-diffusion, and random pattern generation. The integration of these models makes it possible to capture a wide range of the flower pigmentation patterns observed in nature.	https://dl.acm.org/doi/abs/10.1145/3478513.3480548	Lee Ringham, Andrew Owens, Mikolaj Cieslak, Lawrence D. Harder, Przemyslaw Prusinkiewicz
Monster Pets: a hotel Transylvania short	Drac's loveable, monster-sized puppy, Tinkles, has more energy than ever and just wants to play ball! Unfortunately, Drac is too busy juggling his hotel duties, so he is determined to find a monster pet companion for his huge furry friend. After a series of mismatches, Drac's plan is upended when Tinkles chooses a very unlikely companion!	https://dl.acm.org/doi/abs/10.1145/3463912.3481643	Christian Roedel, Jennifer Kluska, Derek Drymon
Monte Carlo Denoising with a Sparse Auxiliary Feature Encoder	Fast Denoising Monte Carlo path tracing is very desirable. Existing learning-based real-time methods concatenate auxiliary buffers (i.e., albedo, normal, and depth) with noisy colors as input. Such structures cannot effectively extract rich information from auxiliary buffers, however. In this work, we facilitate the U-shape kernel-prediction network with a sparse auxiliary feature encoder. Sparse convolutions can focus solely on regions whose inputs have changed and reuse the history features in other regions. With sparse convolutions, the computational complexity of the auxiliary feature encoder is reduced by 50-70% without apparent performance drops.	https://dl.acm.org/doi/abs/10.1145/3476124.3488631	Siyuan Fu, Yifan Lu, Xiao Hua Zhang, Ning Xie
Monte Carlo denoising via auxiliary feature guided self-attention	While self-attention has been successfully applied in a variety of natural language processing and computer vision tasks, its application in Monte Carlo (MC) image denoising has not yet been well explored. This paper presents a self-attention based MC denoising deep learning network based on the fact that self-attention is essentially non-local means filtering in the embedding space which makes it inherently very suitable for the denoising task. Particularly, we modify the standard self-attention mechanism to an auxiliary feature guided self-attention that considers the by-products (e.g., auxiliary feature buffers) of the MC rendering process. As a critical prerequisite to fully exploit the performance of self-attention, we design a multi-scale feature extraction stage, which provides a rich set of raw features for the later self-attention module. As self-attention poses a high computational complexity, we describe several ways that accelerate it. Ablation experiments validate the necessity and effectiveness of the above design choices. Comparison experiments show that the proposed self-attention based MC denoising method outperforms the current state-of-the-art methods.	https://dl.acm.org/doi/abs/10.1145/3478513.3480565	Jiaqi Yu, Yongwei Nie, Chengjiang Long, Wenju Xu, Qing Zhang, Guiqing Li
Motion recommendation for online character control	Reinforcement learning (RL) has been proven effective in many scenarios, including environment exploration and motion planning. However, its application in data-driven character control has produced relatively simple motion results compared to recent approaches that have used large complex motion data without RL. In this paper, we provide a real-time motion control method that can generate high-quality and complex motion results from various sets of unstructured data while retaining the advantage of using RL, which is the discovery of optimal behaviors by trial and error. We demonstrate the results for a character achieving different tasks, from simple direction control to complex avoidance of moving obstacles. Our system works equally well on biped/quadruped characters, with motion data ranging from 1 to 48 minutes, without any manual intervention. To achieve this, we exploit a finite set of discrete actions, where each action represents full-body future motion features. We first define a subset of actions that can be selected in each state and store these pieces of information in databases during the preprocessing step. The use of this subset of actions enables the effective learning of control policy even from a large set of motion data. To achieve interactive performance at run-time, we adopt a proposal network and a k-nearest neighbor action sampler.	https://dl.acm.org/doi/abs/10.1145/3478513.3480512	Kyungmin Cho, Chaelin Kim, Jungjin Park, Joonkyu Park, Junyong Noh
Multi-class inverted stippling	We introduce , a method to mimic an inversion technique used by artists when performing stippling. To this end, we extend Linde-Buzo-Gray (LBG) stippling to multi-class LBG (MLBG) stippling with multiple layers. MLBG stippling couples the layers stochastically to optimize for per-layer and overall blue-noise properties. We propose a stipple-based filling method to generate solid color backgrounds for inverting areas. Our experiments demonstrate the effectiveness of MLBG in terms of reducing overlapping and intensity accuracy. In addition, we showcase MLBG with color stippling and dynamic multi-class blue-noise sampling, which is possible due to its support for temporal coherence.	https://dl.acm.org/doi/abs/10.1145/3478513.3480534	Christoph Schulz, Kin Chung Kwan, Michael Becher, Daniel Baumgartner, Guido Reina, Oliver Deussen, Daniel Weiskopf
Multimodal Feedback Pen Shaped Interface and MR Application with Spatial Reality Display	Multimodal interface is essential to enrich the reality of drawing in the virtual 3D environment. We propose the pen shaped interface capable of providing the following multimodal feedbacks: (1) Linear motion force feedback to express contact pressure of virtual object, (2) Rotational force feedback to simulate friction of rubbing virtual surface and tangential contact force with virtual object, (3) Vibrotactile feedback, and (4) auditory feedback to express contact information and texture of virtual object. We developed Mixed Reality (MR) interaction system with pen shaped interface and Spatial Reality Display. This system will display virtual pen tip extended from the actual pen shaped interface, and user can use it to draw into virtual workspace as well as interact with virtual objects. This demonstrates the advantage of our proposal by improving the reality of virtual space interaction.	https://dl.acm.org/doi/abs/10.1145/3476122.3484834	Jun Momose, Yuta Koda, Hideki Mori, Morio Kakiuchi, Kotaro Imamura, Makoto Wakabayashi
Mum is Pouring Rain	Jane is a tenacious 8-years-old. Her mother Cécile, however, is going through a little depression. Jane thought they'd spend Christmas together, but instead she's sent to her Grandma's in the snowy countryside. Against all odds, her holidays turn out to be a real adventure. Jane meets new, unexpected friends, and learns that Life can be a feast, after all.	https://dl.acm.org/doi/abs/10.1145/3463912.3478643	Hugo de Faucompret, Ivan Zuber, Antoine Liétout
My Grandmother is an Egg	The animated short film is based on the director's grandmother's experience as a T'ung-yang-hsi. T'ung-yang-hsi is the traditional practice of pre-arranged marriage in East Asia, selling or giving a young girl to another family to be raised as a future daughter-in-law. The tradition has vanished for decades, but the patriarchal shadow still lingers. Focusing on female issues, the film aims to reflect upon women's oppression and struggle for freedom. Trying to be true to the historical context, the narration is based on interviews with the director's grandmother's children, and research in feminism in animated short films at Royal College of Art in the United Kingdom during pre-production stage. Egg is an important symbol in the film. As the metaphor of women in the film, eggs are the symbolization of the productive roles in the male-dominated society. After labor and oppression experienced by a T'ung-yang-hsi, the film reaches its climax with Hakka 'Old Mountain Song' combined with turbulent waves. Behind the rail track, there is the sea. Across the sea, therein lies freedom. Except for 2D animation with hand-painted texture, this film contains multiple experimental practices, such as animating with eggs, egg white, and egg yolk. Digital cut-out animation, and paint on old photos. This film also contains historical archives, the Household Registration Transcript during the time of Taiwan under Japanese rule. From microscopic to macroscopic, from personal witness to the general phenomenon in the society, audiences may glimpse the long past, imagine women's situation in our own times, and look forward to striving for gender equality in the future. The director hope audiences may gain fresh insight into the impact of social norms and power structures on our daily life. Egg is life per se. Eggs are fragile, but at the same time tough. My grandmother is an egg.	https://dl.acm.org/doi/abs/10.1145/3463912.3482501	Wu-Ching Chang
Nachtalb: A multisensory Neurofeedback VR-Interface	Nachtalb is an immersive interface that enables brain-to-brain interaction using multisensory feedback. With the help of the g.tec Unicorn Hybrid Black brain-computer-interface (BCI), brain-activity-data is measured and translated visually with the Oculus Quest 2, tactilely with the bHaptics TactSuit and auditorily with 3D Sound. This intends to create a feedback loop that turns brain activity from data-input into sensory output which directly influences the brain activity data-input again.	https://dl.acm.org/doi/abs/10.1145/3478514.3487621	Paul Morat, Aaron Schwerdtfeger, Frank Heidmann
NeRFactor: neural factorization of shape and reflectance under an unknown illumination	We address the problem of recovering the shape and spatially-varying reflectance of an object from multi-view images (and their camera poses) of an object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our videos, code, and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.	https://dl.acm.org/doi/abs/10.1145/3478513.3480496	Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, Jonathan T. Barron
Neural 3D holography: learning accurate wave propagation models for 3D holographic virtual and augmented reality displays	Holographic near-eye displays promise unprecedented capabilities for virtual and augmented reality (VR/AR) systems. The image quality achieved by current holographic displays, however, is limited by the wave propagation models used to simulate the physical optics. We propose a neural network-parameterized plane-to-multiplane wave propagation model that closes the gap between physics and simulation. Our model is automatically trained using camera feedback and it outperforms related techniques in 2D plane-to-plane settings by a large margin. Moreover, it is the first network-parameterized model to naturally extend to 3D settings, enabling high-quality 3D computer-generated holography using a novel phase regularization strategy of the complex-valued wave field. The efficacy of our approach is demonstrated through extensive experimental evaluation with both VR and optical see-through AR display prototypes.	https://dl.acm.org/doi/abs/10.1145/3478513.3480542	Suyeon Choi, Manu Gopakumar, Yifan Peng, Jonghyun Kim, Gordon Wetzstein
Neural actor: neural free-view synthesis of human actors with pose control	We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is developed upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as a proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high-fidelity dynamic geometry and appearance, NA leverages 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports shape control on the free-view synthesis of human actors.	https://dl.acm.org/doi/abs/10.1145/3478513.3480528	Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt
Neural frame interpolation for rendered content	The demand for creating rendered content continues to drastically grow. As it often is extremely computationally expensive and thus costly to render high-quality computer-generated images, there is a high incentive to reduce this computational burden. Recent advances in learning-based frame interpolation methods have shown exciting progress but still have not achieved the production-level quality which would be required to render fewer pixels and achieve savings in rendering times and costs. Therefore, in this paper we propose a method specifically targeted to achieve high-quality frame interpolation for rendered content. In this setting, we assume that we have full input for every -th frame in addition to auxiliary feature buffers that are cheap to evaluate (e.g. depth, normals, albedo) for every frame. We propose solutions for leveraging such auxiliary features to obtain better motion estimates, more accurate occlusion handling, and to correctly reconstruct non-linear motion between keyframes. With this, our method is able to significantly push the state-of-the-art in frame interpolation for rendered content and we are able to obtain production-level quality results.	https://dl.acm.org/doi/abs/10.1145/3478513.3480553	Karlis Martins Briedis, Abdelaziz Djelouah, Mark Meyer, Ian McGonigal, Markus Gross, Christopher Schroers
Neural marching cubes	We introduce , a approach for extracting a triangle mesh from a discretized implicit field. We base our meshing approach on Marching Cubes (MC), due to the simplicity of its input, namely a uniform grid of signed distances or occupancies, which frequently arise in surface reconstruction and from neural implicit models. However, classical MC is defined by coarse tessellation templates isolated to individual cubes. While more refined tessellations have been proposed by several MC variants, they all make heuristic assumptions, such as trilinearity, when determining the vertex positions and local mesh topologies in each cube. In principle, none of these approaches can reconstruct geometric features that reveal coherence or dependencies between nearby cubes (e.g., a ), as such information is unaccounted for, resulting in poor estimates of the true underlying implicit field. To tackle these challenges, we re-cast MC from a deep learning perspective, by designing tessellation templates more apt at preserving geometric features, and learning the vertex positions and mesh topologies from training meshes, to account for information from nearby cubes. We develop a compact per-cube parameterization to represent the output triangle mesh, while being compatible with neural processing, so that a simple 3D convolutional network can be employed for the training. We show that all topological cases in each cube that are applicable to our design can be easily derived using our representation, and the resulting tessellations can also be obtained naturally and efficiently by following a few design guidelines. In addition, our network learns local features with limited receptive fields, hence it generalizes well to new shapes and new datasets. We evaluate our neural MC approach by quantitative and qualitative comparisons to all well-known MC variants. In particular, we demonstrate the ability of our network to recover sharp features such as edges and corners, a long-standing issue of MC and its variants. Our network also reconstructs local mesh topologies more accurately than previous approaches. Code and data are available at https://github.com/czq142857/NMC.	https://dl.acm.org/doi/abs/10.1145/3478513.3480518	Zhiqin Chen, Hao Zhang
Neural radiosity	We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual, similar as in classical radiosity techniques. Traditional basis functions used in radiosity, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with diffuse and non-diffuse surfaces.	https://dl.acm.org/doi/abs/10.1145/3478513.3480569	Saeed Hadadan, Shuhong Chen, Matthias Zwicker
Nona	A grandmother's plan for a day alone is upended.	https://dl.acm.org/doi/abs/10.1145/3463912.3481046	Krissy Bailey, Louis Gonzales, Courtney Casper Kent
Occlusion Robust Part-aware Object Classification through Part Attention and Redundant Features Suppression	In recent studies, object classification with deep convolutional neural networks has shown poor generalization with occluded objects due to the large variation of occlusion situations. We propose a part-aware deep learning approach for occlusion robust object classification. To demonstrate the robustness of the method to unseen occlusion, we train our network without occluded object samples in training and test it with diverse occlusion samples. Proposed method shows improved classification performance on CIFAR10, STL10, and vehicles from PASCAL3D+ datasets.	https://dl.acm.org/doi/abs/10.1145/3476124.3488647	Sohee Kim, Seungkyu Lee
Only a Child	Only a Child is a visual poem created by over 20 animation directors under the artistic supervision of Simone Giampaolo, which gives shape and colour to the original words spoken by Severn Cullis-Suzuki at the UN Summit in Rio in 1992, a child's desperate call to action for the future of our planet. An omnibus film celebrating the environmental youth movement 30 years in the making.	https://dl.acm.org/doi/abs/10.1145/3463912.3477965	Simone Giampaolo, Gabriella de Gara
Opening the black box of mathematics for CG: SIGGRAPH ASIA 2021 course notes	Mathematics is recognized as common basis for CG technology, but sometimes used as a black box in a CG software tool. We expect better understanding of maths will conduce to not only better CG tools, but also innovative ideas for a future production pipeline. The goal of this course is to pull the trigger for the graphics people to know more about usefulness and fun of the maths behind the scenes.	https://dl.acm.org/doi/abs/10.1145/3476117.3483431	Hiroyuki Ochiai, Ken Anjyo, Ayumi Kimura
Optimizing contact-based assemblies	Modern fabrication methods have greatly simplified manufacturing of complex free-form shapes at an affordable cost, and opened up new possibilities for improving functionality and customization through automatic optimization, shape optimization in particular. However, most existing shape optimization methods focus on single parts. In this work, we focus on supporting shape optimization for assemblies, more specifically, assemblies that are held together by contact and friction. Examples of which include furniture joints, construction set assemblies, certain types of prosthetic devices and many other. To enable this optimization, we present a framework supporting robust and accurate optimization of a number of important functionals, while enforcing constraints essential for assembly functionality: weight, stress, difficulty of putting the assembly together, and how reliably it stays together. Our framework is based on smoothed formulation of elasticity equations with contact, analytically derived shape derivatives, and robust remeshing to enable large changes of shape, and at the same time, maintain accuracy. We demonstrate the improvements it can achieve for a number of computational and experimental examples.	https://dl.acm.org/doi/abs/10.1145/3478513.3480552	Davi Colli Tozoni, Yunfan Zhou, Denis Zorin
Optimizing global injectivity for constrained parameterization	Injective parameterizations of triangulated meshes are critical across applications but remain challenging to compute. Existing algorithms to find injectivity either require initialization from an injective starting state, which is currently only possible without positional constraints, or else can only prevent triangle inversion, which is insufficient to ensure injectivity. Here we present, to our knowledge, the first algorithm for recovering a globally injective parameterization from an arbitrary non-injective initial mesh subject to stationary constraints. These initial meshes can be inverted, wound about interior vertices and/or overlapping. Our algorithm in turn enables globally injective mapping for meshes with arbitrary positional constraints. Our key contribution is a new energy, called (SEA), that measures non-injectivity in a map. This energy is well-defined across both injective and non-injective maps and is smooth almost everywhere, making it readily minimizable using standard gradient-based solvers starting from a non-injective initial state. Importantly, we show that maps minimizing SEA are guaranteed to be locally injective and globally injective, in the sense that the overlapping area can be made arbitrarily small. Analyzing SEA's behavior over a new benchmark set designed to test injective mapping, we find that optimizing SEA successfully recovers globally injective maps for 85% of the benchmark and obtains locally injective maps for 90%. In contrast, state-of-the-art methods for removing triangle inversion obtain locally injective maps for less than 6% of the benchmark, and achieve global injectivity (largely by chance as prior methods are not designed to recover it) on less than 4%.	https://dl.acm.org/doi/abs/10.1145/3478513.3480556	Xingyi Du, Danny M. Kaufman, Qingnan Zhou, Shahar Z. Kovalsky, Yajie Yan, Noam Aigerman, Tao Ju
PBNS: physically based neural simulation for unsupervised garment pose space deformation	We present a methodology to automatically obtain Pose Space Deformation (PSD) basis for rigged garments through deep learning. Classical approaches rely on Physically Based Simulations (PBS) to animate clothes. These are general solutions that, given a sufficiently fine-grained discretization of space and time, can achieve highly realistic results. However, they are computationally expensive and any scene modification prompts the need of re-simulation. Linear Blend Skinning (LBS) with PSD offers a lightweight alternative to PBS, though, it needs huge volumes of data to learn proper PSD. We propose using deep learning, formulated as an implicit PBS, to un-supervisedly learn realistic cloth Pose Space Deformations in a constrained scenario: dressed humans. Furthermore, we show it is possible to train these models in an amount of time comparable to a PBS of a few sequences. To the best of our knowledge, we are the first to propose a neural simulator for cloth. While deep-based approaches in the domain are becoming a trend, these are data-hungry models. Moreover, authors often propose complex formulations to better learn wrinkles from PBS data. Supervised learning leads to physically inconsistent predictions that require collision solving to be used. Also, dependency on PBS data limits the scalability of these solutions, while their formulation hinders its applicability and compatibility. By proposing an unsupervised methodology to learn PSD for LBS models (3D animation standard), we overcome both of these drawbacks. Results obtained show cloth-consistency in the animated garments and meaningful pose-dependant folds and wrinkles. Our solution is extremely efficient, handles multiple layers of cloth, allows unsupervised outfit resizing and can be easily applied to any custom 3D avatar.	https://dl.acm.org/doi/abs/10.1145/3478513.3480479	Hugo Bertiche, Meysam Madadi, Sergio Escalera
Parallel Ping-Pong: Demonstrating Parallel Interaction through Multiple Bodies by a Single User	"We explore a ""Parallel Interaction"", where a user controls several bodies concurrently. To demonstrate this type of interaction we developed ""Parallel Ping-Pong"". The user plays ping-pong by controlling 2 robot arms with a Virtual Reality (VR) controller, and while looking from the viewpoint of one of the 2 robot arms with a Head Mounted Display (HMD). The HMD view switches between the 2 robot arms' viewpoints according to the ping-pong ball position and direction. To keep the sense of agency over the robot arms (hindered by the system latency), the robot arms calculate the balls' paths and position themselves to hit the balls back; all while integrating the controller motion. Throughout this demonstration, we investigate a real-life design of Parallel Interaction with multiple bodies by a single user."	https://dl.acm.org/doi/abs/10.1145/3476122.3484836	Kazuma Takada, Midori Kawaguchi, Yukiya Nakanishi, Akira Uehara, Mark Armstrong, Adrien Verhulst, Kouta Minamizawa, Shunichi Kasahara
Patching Non-Uniform Extraordinary Points with Sharp Features	Extending the non-uniform rational B-spline (NURBS) representation to arbitrary topology is one of the most important steps to define the iso-geometric analysis (IGA) suitable geometry. The approach needs to be NURBS-compatible and can handle non-uniform knot intervals. To achieve this goal, we present a novel patching solution which define one Bézier patch for each non-zero knot interval control grid face. The construction can reproduce the bi-cubic NURBS in the regular face and define bi-quintic Bézier patches for irregular faces. The method can also support non-uniform sharp features along the extraordinary points. Experimental results demonstrate that the new surfaces can improve the surface quality for non-uniform parameterization.	https://dl.acm.org/doi/abs/10.1145/3476124.3488626	Yi-Fei Feng, Xin Li, Chun-Ming Yuan, Li-Yong Shen
Path graphs: iterative path space filtering	To render higher quality images from the samples generated by path tracing with a low sample count, we propose a novel path reuse approach that processes a fixed collection of paths to iteratively refine and improve radiance estimates throughout the scene. Our method operates on a consisting of the union of the traced paths with additional neighbor edges inserted among clustered nearby vertices. Our approach refines the initial noisy radiance estimates via an aggregation operator, treating vertices within clusters as independent sampling techniques that can be combined using MIS. In a novel step, we also introduce a propagation operator to forward the refined estimates along the paths to successive bounces. We apply the aggregation and propagation operations to the graph iteratively, progressively refining the radiance values, converging to fixed-point radiance estimates with lower variance than the original ones. We also introduce a decorrelation (final gather) step, which uses information already in the graph and is cheap to compute, allowing us to combine the method with standard denoisers. Our approach is lightweight, in the sense that it can be easily plugged into any standard path tracer and neural final image denoiser. Furthermore, it is independent of scene complexity, as the graph size only depends on image resolution and average path depth. We demonstrate that our technique leads to realistic rendering results starting from as low as 1 path per pixel, even in complex indoor scenes dominated by multi-bounce indirect illumination.	https://dl.acm.org/doi/abs/10.1145/3478513.3480547	Xi Deng, Miloš Hašan, Nathan Carr, Zexiang Xu, Steve Marschner
Perceptual model for adaptive local shading and refresh rate	When the rendering budget is limited by power or time, it is necessary to find the combination of rendering parameters, such as resolution and refresh rate, that could deliver the best quality. Variable-rate shading (VRS), introduced in the last generations of GPUs, enables fine control of the rendering quality, in which each 16×16 image tile can be rendered with a different ratio of shader executions. We take advantage of this capability and propose a new method for adaptive control of local shading and refresh rate. The method analyzes texture content, on-screen velocities, luminance, and effective resolution and suggests the refresh rate and a VRS state map that maximizes the quality of animated content under a limited budget. The method is based on the new content-adaptive metric of judder, aliasing, and blur, which is derived from the psychophysical models of contrast sensitivity. To calibrate and validate the metric, we gather data from literature and also collect new measurements of motion quality under variable shading rates, different velocities of motion, texture content, and display capabilities, such as refresh rate, persistence, and angular resolution. The proposed metric and adaptive shading method is implemented as a game engine plugin. Our experimental validation shows a substantial increase in preference of our method over rendering with a fixed resolution and refresh rate, and an existing motion-adaptive technique.	https://dl.acm.org/doi/abs/10.1145/3478513.3480514	Akshay Jindal, Krzysztof Wolski, Karol Myszkowski, Rafał K. Mantiuk
Physical light-matter interaction in hermite-gauss space	Our purpose in this paper is two-fold: introduce a computationally-tractable decomposition of the coherence properties of light; and, present a general-purpose light-matter interaction framework for partially-coherent light. In a recent publication, Steinberg and Yan [2021] introduced a framework that generalises the classical radiometry-based light transport to physical optics. This facilitates a qualitative increase in the scope of optical phenomena that can be rendered, however with the additional expressibility comes greater analytic difficulty: This coherence of light, which is the core quantity of physical light transport, depends initially on the characteristics of the light source, and mutates on interaction with matter and propagation. Furthermore, current tools that aim to quantify the interaction of partially-coherent light with matter remain limited to specific materials and are computationally intensive. To practically represent a wide class of coherence functions, we decompose their modal content in Hermite-Gauss space and derive a set of light-matter interaction formulae, which quantify how matter scatters light and affects its coherence properties. Then, we model matter as a locally-stationary random process, generalizing the prevalent deterministic and stationary stochastic descriptions. This gives rise to a framework that is able to formulate the interaction of arbitrary partially-coherent light with a wide class of matter. Indeed, we will show that our presented formalism unifies a few of the state-of-the-art scatter and diffraction formulae into one cohesive theory. This formulae include the sourcing of partially-coherent light, scatter by rough surfaces and microgeometry, diffraction grating and interference by a layered structure.	https://dl.acm.org/doi/abs/10.1145/3478513.3480530	Shlomi Steinberg, Ling-Qi Yan
Physically-based feature line rendering	Feature lines visualize the shape and structure of 3D objects, and are an essential component of many non-photorealistic rendering styles. Existing feature line rendering methods, however, are only able to render feature lines in limited contexts, such as on immediately visible surfaces or in specular reflections. We present a novel, path-based method for feature line rendering that allows for the accurate rendering of feature lines in the presence of complex physical phenomena such as glossy reflection, depth-of-field, and dispersion. Our key insight is that feature lines can be modeled as view-dependent light sources. These light sources can be sampled as a part of , and seamlessly integrate into existing physically-based rendering methods. We illustrate the effectiveness of our method in several real-world rendering scenarios with a variety of different physical phenomena.	https://dl.acm.org/doi/abs/10.1145/3478513.3480550	Rex West
Polarimetric spatio-temporal light transport probing	Light emitted from a source into a scene can undergo complex interactions with multiple scene surfaces of different material types before being reflected towards a detector. During this transport, every surface reflection and propagation is encoded in the properties of the photons that ultimately reach the detector, including travel time, direction, intensity, wavelength and polarization. Conventional imaging systems capture intensity by integrating over all other dimensions of the incident light into a single quantity, hiding this rich scene information in these aggregate measurements. Existing methods are capable of untangling these measurements into their spatial and temporal dimensions, fueling geometric scene understanding tasks. However, examining polarimetric material properties jointly with geometric properties is an open challenge that could enable unprecedented capabilities beyond geometric scene understanding, allowing for material-dependent scene understanding and imaging through complex transport, such as macroscopic scattering. In this work, we close this gap, and propose a computational light transport imaging method that captures the spatially- and temporally-resolved complete polarimetric response of a scene, which encodes rich material properties. Our method hinges on a novel 7D tensor theory of light transport. We discover low-rank structure in the polarimetric tensor dimension and propose a data-driven rotating ellipsometry method that learns to exploit redundancy of polarimetric structure. We instantiate our theory with two imaging prototypes: spatio-polarimetric imaging and coaxial temporal-polarimetric imaging. This allows us, for the first time, to decompose scene light transport into temporal, spatial, and complete polarimetric dimensions that unveil scene properties hidden to conventional methods. We validate the applicability of our method on diverse tasks, including shape reconstruction with subsurface scattering, seeing through scattering media, untangling multi-bounce light transport, breaking metamerism with polarization, and spatio-polarimetric decomposition of crystals.	https://dl.acm.org/doi/abs/10.1145/3478513.3480517	Seung-Hwan Baek, Felix Heide
Pose with style: detail-preserving pose-guided image synthesis with conditional StyleGAN	We present an algorithm for re-rendering a person from a single image under arbitrary poses. Existing methods often have difficulties in hallucinating occluded contents photo-realistically while preserving the identity and fine details in the source image. We first learn to inpaint the correspondence field between the body surface texture and the source image with a human body symmetry prior. The inpainted correspondence field allows us to transfer/warp local features extracted from the source to the target view even under large pose changes. Directly mapping the warped local features to an RGB image using a simple CNN decoder often leads to visible artifacts. Thus, we extend the StyleGAN generator so that it takes pose as input (for controlling poses) and introduces a spatially varying modulation for the latent space using the warped local features (for controlling appearances). We show that our method compares favorably against the state-of-the-art algorithms in both quantitative evaluation and visual comparison.	https://dl.acm.org/doi/abs/10.1145/3478513.3480559	Badour Albahar, Jingwan Lu, Jimei Yang, Zhixin Shu, Eli Shechtman, Jia-Bin Huang
Practical pigment mixing for digital painting	There is a significant flaw in today's painting software: the colors do not mix like actual paints. E.g., blue and yellow make gray instead of green. This is because the software is built around the RGB representation, which models the mixing of colored lights. Paints, however, get their color from pigments, whose mixing behavior is predicted by the Kubelka-Munk model (K-M). Although it was introduced to computer graphics almost 30 years ago, the K-M model has never been adopted by painting software in practice as it would require giving up the RGB representation, growing the number of per-pixel channels substantially, and depriving the users of painting with arbitrary RGB colors. In this paper, we introduce a practical approach that enables mixing colors with K-M while keeping everything in RGB. We achieve this by establishing a latent color space, where RGB colors are represented as mixtures ofprimary pigments together with additive residuals. The latents can be manipulated with linear operations, leading to expected, plausible results. We describe the conversion between RGB and our latent representation, and show how to implement it efficiently.	https://dl.acm.org/doi/abs/10.1145/3478513.3480549	Šárka Sochorová, Ondřej Jamriška
Predicting high-resolution turbulence details in space and time	Predicting the fine and intricate details of a turbulent flow field in both space and time from a coarse input remains a major challenge despite the availability of modern machine learning tools. In this paper, we present a simple and effective dictionary-based approach to spatio-temporal upsampling of fluid simulation. We demonstrate that our neural network approach can reproduce the visual complexity of turbulent flows from spatially and temporally coarse velocity fields even when using a generic training set. Moreover, since our method generates finer spatial and/or temporal details through embarrassingly-parallel upsampling of small local patches, it can efficiently predict high-resolution turbulence details across a variety of grid resolutions. As a consequence, our method offers a whole range of applications varying from fluid flow upsampling to fluid data compression. We demonstrate the efficiency and generalizability of our method for synthesizing turbulent flows on a series of complex examples, highlighting dramatically better results in spatio-temporal upsampling and flow data compression than existing methods as assessed by both qualitative and quantitative comparisons.	https://dl.acm.org/doi/abs/10.1145/3478513.3480492	Kai Bai, Chunhao Wang, Mathieu Desbrun, Xiaopei Liu
Procedural people in Pixar's Presto: new workflows for interactive crowds	Pixar's Presto Crowd Framework (Pcf) has taken interactive crowd scalability to new heights by harnessing the power of Presto's scalable execution framework for procedurally rigged crowds. Pcf is a vectorized and highly parallelized crowd system that represents an entire crowd as an aggregate model. Prior to Pcf, Presto crowds used a model per agent, via the Mf (model framework), and only scaled to around thousand agents, even with fast GPU based linear blend skinning. Pcf CrowdPrims by contrast can scale to tens and even hundreds of thousands of agents using the same underlying data while maintaining interactive frame rates. Pcf was also designed from the ground up to import from and export to USD to make pipeline deployment seamless. Pcf Crowd Prims use procedural implementations for crowd techniques that are traditionally simulated, like Finite State Machines, making the results deterministic and pipeline friendly. Taken together, our Real-Time Live Demo will prove to the audience that they do not need to settle for the long execution times and history dependent nature of simulated crowds, and that with sufficiently a vectorized/parallelized framework, even stadium sized crowds can be generated/choreographed interactively.	https://dl.acm.org/doi/abs/10.1145/3478511.3491308	Paul Kanyuk
Project starline: a high-fidelity telepresence system	We present a real-time bidirectional communication system that lets two people, separated by distance, experience a face-to-face conversation as if they were copresent. It is the first telepresence system that is demonstrably better than 2D videoconferencing, as measured using participant ratings (e.g., presence, attentiveness, reaction-gauging, engagement), meeting recall, and observed nonverbal behaviors (e.g., head nods, eyebrow movements). This milestone is reached by maximizing audiovisual fidelity and the sense of copresence in all design elements, including physical layout, lighting, face tracking, multi-view capture, microphone array, multi-stream compression, loudspeaker output, and lenticular display. Our system achieves key 3D audiovisual cues (stereopsis, motion parallax, and spatialized audio) and enables the full range of communication cues (eye contact, hand gestures, and body language), yet does not require special glasses or body-worn microphones/headphones. The system consists of a head-tracked autostereoscopic display, high-resolution 3D capture and rendering subsystems, and network transmission using compressed color and depth video streams. Other contributions include a novel image-based geometry fusion algorithm, free-space dereverberation, and talker localization.	https://dl.acm.org/doi/abs/10.1145/3478513.3480490	Jason Lawrence, Danb Goldman, Supreeth Achar, Gregory Major Blascovich, Joseph G. Desloge, Tommy Fortes, Eric M. Gomez, Sascha Häberling, Hugues Hoppe, Andy Huibers, Claude Knaus, Brian Kuschak, Ricardo Martin-Brualla, Harris Nover, Andrew Ian Russell, Steven M. Seitz, Kevin Tong
Q-zip: singularity editing primitive for quad meshes	Singularity editing of a quadrangle mesh consists in shifting singularities around for either improving the quality of the mesh elements or canceling extraneous singularities, so as to increase mesh regularity. However, the particular structure of a quad mesh renders the exploration of allowable connectivity changes non-local and hard to automate. In this paper, we introduce a simple, principled, and general with which pairs of arbitrarily distant singularities can be efficiently displaced around a mesh through a deterministic and reversible chain of local topological operations with a minimal footprint. Dubbed Q-zip as it acts as a zipper opening up and collapsing down quad strips, our practical mesh operator for singularity editing can be easily implemented via parallel transport of a reference compass between any two irregular vertices. Batches of Q-zips performed in parallel can then be used for efficient singularity editing.	https://dl.acm.org/doi/abs/10.1145/3478513.3480523	Leman Feng, Yiying Tong, Mathieu Desbrun
Real-Time Prediction-Driven Dynamics Simulation to Mitigate Frame Time Variation	This work introduces a prediction-driven real-time dynamics method that uses a graph-based state buffer to minimize the cost of mispredictions. Our technique reduces the average time needed for dynamics computation on the main thread by running the solver pipeline on a separate thread, enabling interactive multimedia applications to increase the computational budget for graphics at no cost perceptible to the end user.	https://dl.acm.org/doi/abs/10.1145/3476124.3488633	Mackinnon Buck, Christian Eckhardt
Real-time Image-based Virtual Try-on with Measurement Garment	Virtual try-on technology that replaces a customer's wearing with arbitrary garments can significantly improve the online cloth shopping experience [anonymous 2021; Han et al. 2018; Yang et al. 2020]. In this work, we present a real-time image-based virtual try-on system composed of two parts, i.e., photo-realistic clothed person image synthesis for the customers to experience the try-on result and garment capturing for the retailers to capture the rich deformations of the target garment. Distinguished from previous image-based virtual try-on works, we formulate the problem as a supervised image-to-image translation problem using a measurement garment, and we capture the training data with a custom actuated mannequin.	https://dl.acm.org/doi/abs/10.1145/3476122.3484832	Toby Chong, I-Chao Shen, Yinfei Qian, Nobuyuki Umetani, Takeo Igarashi
RealitySketch: augmented reality sketching for real-time embedded and responsive visualizations	In this Real-Time Live, we demonstrate RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations [Suzuki et al. 2020]. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed and graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.	https://dl.acm.org/doi/abs/10.1145/3478511.3491313	Ryo Suzuki, Rubaiat Habib Kazi, Li-Yi Wei, Stephen DiVerdi, Wilmot Li, Daniel Leithinger
Recognition of Gestures over Textiles with Acoustic Signatures	We demonstrate a method capable of turning textured surfaces (in particular textile patches) into opportunistic input interfaces thanks to a machine learning model pre-trained on acoustic signals generated by scratching different fabrics. A single short audio recording is then sufficient to characterize both a gesture and the textures substrate. The sensing method does not require intervention on the fabric (no special coating, additional sensors or wires). It is passive (no acoustic or RF signal injected) and works well using regular microphones, such as those embedded in smartphones. Our prototype yields 93.86% accuracy on simultaneous texture/gesture recognition on a test matrix formed by eight textures and eight gestures as long as the microphone is close enough (e.g. under the fabric), or when the patch is attached to a solid body transmitting sound waves. Preliminary results also show that the system recognizes the manipulation of Velcro straps, zippers, or the taping or scratching of plastic cloth buttons over the air when the microphone is in personal space. This research paves the way for a fruitful collaboration between wearables researchers and fashion designers that could lead to an interaction dictionary for common textile patterns or guidelines for the design of signature-robust stitched patches not compromising aesthetic elements.	https://dl.acm.org/doi/abs/10.1145/3476122.3484835	Pui Chung Wong, Christian Sandor, Alvaro Cassinelli
Rendering of Synthetic Underwater Images Towards Restoration	In this paper, we propose to render synthetic underwater images considering revised image formation model, towards modeling restoration. Underwater images suffer from low contrast, color cast and haze due to dynamically varying properties of water, floating particles and submerged sediments. Due to this, light reaching the object, from the surface is subjected to direct scattering, backscattering and forward scattering. We propose to model this varying nature of light to render synthetic underwater images based on depth information and inherent optical properties of Jerlov water types.	https://dl.acm.org/doi/abs/10.1145/3476124.3488637	Chaitra Desai, Ramesh Ashok Tabib, Sai Sudheer Reddy, Ujwala Patil, Uma Mudenagudi
Rendering with style: combining traditional and neural approaches for high-quality face rendering	For several decades, researchers have been advancing techniques for creating and rendering 3D digital faces, where a lot of the effort has gone into geometry and appearance capture, modeling and rendering techniques. This body of research work has largely focused on facial skin, with much less attention devoted to peripheral components like hair, eyes and the interior of the mouth. As a result, even with the best technology for facial capture and rendering, in most high-end productions a lot of artist time is still spent modeling the missing components and fine-tuning the rendering parameters to combine everything into photo-real digital renders. In this work we propose to combine incomplete, high-quality renderings showing only facial skin with recent methods for neural rendering of faces, in order to automatically and seamlessly create photo-realistic full-head portrait renders from captured data without the need for artist intervention. Our method begins with traditional face rendering, where the skin is rendered with the desired appearance, expression, viewpoint, and illumination. These skin renders are then projected into the latent space of a pre-trained neural network that can generate arbitrary photo-real face images (StyleGAN2). The result is a sequence of realistic face images that match the identity and appearance of the 3D character at the skin level, but is completed naturally with synthesized hair, eyes, inner mouth and surroundings. Notably, we present the first method for projection into this latent space, allowing photo-realistic rendering and preservation of the identity of the digital human over an animated performance sequence, which can depict different expressions, lighting conditions and viewpoints. Our method can be used in new face rendering pipelines and, importantly, in other deep learning applications that require large amounts of realistic training data with ground-truth 3D geometry, appearance maps, lighting, and viewpoint.	https://dl.acm.org/doi/abs/10.1145/3478513.3480509	Prashanth Chandran, Sebastian Winberg, Gaspard Zoss, Jérémy Riviere, Markus Gross, Paulo Gotardo, Derek Bradley
Reproducing reality with a high-dynamic-range multi-focal stereo display	With well-established methods for producing photo-realistic results, the next big challenge of graphics and display technologies is to achieve perceptual realism --- producing imagery indistinguishable from real-world 3D scenes. To deliver all necessary visual cues for perceptual realism, we built a High-Dynamic-Range Multi-Focal Stereo Display that achieves high resolution, accurate color, a wide dynamic range, and most depth cues, including binocular presentation and a range of focal depth. The display and associated imaging system have been designed to capture and reproduce a small near-eye three-dimensional object and to allow for a direct comparison between virtual and real scenes. To assess our reproduction of realism and demonstrate the capability of the display and imaging system, we conducted an experiment in which the participants were asked to discriminate between a virtual object and its physical counterpart. Our results indicate that the participants can only detect the discrepancy with a probability of 0.44. With such a level of perceptual realism, our display apparatus can facilitate a range of visual experiments that require the highest fidelity of reproduction while allowing for the full control of the displayed stimuli.	https://dl.acm.org/doi/abs/10.1145/3478513.3480513	Fangcheng Zhong, Akshay Jindal, Ali Özgür Yöntem, Param Hanji, Simon J. Watt, Rafał K. Mantiuk
Repulsive surfaces	Functionals that penalize bending or stretching of a surface play a key role in geometric and scientific computing, but to date have ignored a very basic requirement: in many situations, surfaces must not pass through themselves or each other. This paper develops a numerical framework for optimization of surface geometry while avoiding (self-)collision. The starting point is the , which effectively pushes apart pairs of points that are close in space but distant along the surface. We develop a discretization of this energy for triangle meshes, and introduce a novel acceleration scheme based on a fractional Sobolev inner product. In contrast to similar schemes developed for curves, we avoid the complexity of building a multiresolution mesh hierarchy by decomposing our preconditioner into two ordinary Poisson equations, plus forward application of a fractional differential operator. We further accelerate this scheme via hierarchical approximation, and describe how to incorporate a variety of constraints (on area, volume, ). Finally, we explore how this machinery might be applied to problems in mathematical visualization, geometric modeling, and geometry processing.	https://dl.acm.org/doi/abs/10.1145/3478513.3480521	Chris Yu, Caleb Brakensiek, Henrik Schumacher, Keenan Crane
Resident Evil - Village	The animation provides a cinematic introduction, as well as an ending, to the latest game in the Resident Evil series. The film is a fairy-tale metaphor for the events from the game. The project was entirely created by Platige Image, from the initial concept and script, through character design, graphic design, and all stages of animation, to the final audio layer. The uniqueness of the project required us to adopt an unconventional work style. We did a lot of preparation related to texturing, shaders and lighting to make the final animation look like a paper cut-out with many layers. Also, to get an effect similar to a traditional shadow theater game, we decided to use stop-motion animation instead of the motion capture technique. Add to this unique mâché paper and cut-outs, from which we made the characters and scenography, and we have a dark story, which only seemed to be directed to children. Moreover, all characters in the film are original creations designed by the director and are the fairy-tale alter egos of the characters from of the cult game Resident Evil.	https://dl.acm.org/doi/abs/10.1145/3463912.3481884	Jakub Jabłoński, Piotr Prokop
Room Tilt Stick	Room Tilt Stick is an interactive head-mounted display (HMD) system that provides an illusory experience of tilting a room using a stick and of walking on an acutely inclined slope. In the proposed system, the tilting velocity of a three-dimensional modeled room in the HMD's view is correlated with the degree of force transferred to the wall or ground with a specially designed stick. Here a weight scale (i.e., a Wii Balance Board) monitors to what degree the participant's weight is rested against the wall (through a tiltable platform) or ground. After the illusory tilt of the room is completed, participants are instructed to walk on a two meter long wooden slope. The degree of physical inclination is gradually increased according to the walking distance, and the walking slope corresponds to an illusory wall in the HMD's view. The system was tested in our laboratory exhibition, and 87 out of 100 participants reported a strong sensation of the room tilting by pressing the wall or ground with the stick.	https://dl.acm.org/doi/abs/10.1145/3478514.3487613	Kousuke Motohashi, Koyo Mori, Kenri Kodaka
SKII - Kasumi Ishikawa 'VS Pressure	"The main character of 'VS Pressure' is table tennis star, Kasumi Ishikawa. In a neon-lit futuristic city, plagued by questions, she must fight the most important battle of her career - against her self-doubt and social pressure. The animation is part of an anthology featuring the stories of six exceptional female medalists from various sports. The film is highly stylized and surreal. Producing such images requires improvisation, searching for new forms, and departing from photorealistic assumptions. While cheating the laws of physics in our animated film, we had to be careful not to make it look grotesque. It was a very artistic process based on close collaboration between animators and our director. We did not use Motion Capture Motion Capture techniques. We animated the characters using a ""on twos"" technique, based on methods known from classical animation. In combination with hand-animated 2D effects such as smoke, streaks, or sparks, we created a coherent, highly stylized image."	https://dl.acm.org/doi/abs/10.1145/3463912.3480844	Damian Nenow, Artur Zicz
Self-Shape-Sensing Device with Flexible Mechanical Axes for Deformable Input Interface	We propose a novel device that is capable of sensing its own shape, structured around a flexible mechanical axis that allows for its deformation within a wider degree of freedom and enables its tangible and intuitive control by hand. This device has a 5 x 5 axis structure consisting of a spherical joint that rotates between ± 45° and a slide shaft whose length can be varied from 50 to 71 mm. We developed a signal-processing algorithm which can work on the micro controller unit (MCU) of the device and use built-in sensors to not only reconstruct deformations in real-time, but also directly use such reconstructions in 3D graphics applications. As a result, we achieved an intuitive and interactive 3D experience using volumetric displays.	https://dl.acm.org/doi/abs/10.1145/3476122.3484839	Hideki Mori, Yoshihisa Takahashi, Koichi Shiono, Hirofumi Kaneko, Hiroya Matsugami, Masaomi Nishidate
Self-Stylized Neural Painter	This work introduces Self-Stylized Neural Painter (SSNP) creating stylized artworks in a stroke-by-stroke manner. SSNP consists of digit artist, canvas, style-stroke generator (SSG). By using SSG to generate style strokes, SSNP creates different styles paintings based on the given images. We design SSG as a three-player game based on a generative adversarial network to produce pure-color strokes that are crucial for mimicking the physical strokes. Furthermore, the digital artist adjusts parameters of strokes (shape, size, transparency, and color) to reconstruct as much detailed content of the reference image as possible to improve the fidelity.	https://dl.acm.org/doi/abs/10.1145/3476124.3488617	Qian Wang, Cai Guo, Hong-Ning Dai, Ping Li
Semi-supervised video-driven facial animation transfer for production	We propose a simple algorithm for automatic transfer of facial expressions, from videos to a 3D character, as well as between distinct 3D characters through their rendered animations. Our method begins by learning a common, semantically-consistent latent representation for the different input image domains using an unsupervised image-to-image translation model. It subsequently learns, in a supervised manner, a linear mapping from the character images' encoded representation to the animation coefficients. At inference time, given the source domain (i.e., actor footage), it regresses the corresponding animation coefficients for the target character. Expressions are automatically remapped between the source and target identities despite differences in physiognomy. We show how our technique can be used in the context of markerless motion capture with controlled lighting conditions, for one actor and for multiple actors. Additionally, we show how it can be used to automatically transfer facial animation between distinct characters without consistent mesh parameterization and without engineered geometric priors. We compare our method with standard approaches used in production and with recent state-of-the-art models on single camera face tracking.	https://dl.acm.org/doi/abs/10.1145/3478513.3480515	Lucio Moser, Chinyu Chien, Mark Williams, Jose Serra, Darren Hendler, Doug Roble
Service Skills Training in Restaurants Using Virtual Reality	Currently, training in restaurants often take the form of on-the-job training (OJT), where trainees are accompanied by trainers who guide them as they serve the customers. However, it is difficult for trainers to understand the cognitive and decision-making processes of the trainees objectively, which makes OJT guidance extremely difficult. Objective measurement indices are also needed. To mitigate these issues, we developed a VR job-training system that focuses on two elements that pose challenges for the actual situation: awareness and priority-setting.	https://dl.acm.org/doi/abs/10.1145/3476124.3502251	Mai Otsuki, Takashi Okuma
Ships, splashes, and waves on a vast ocean	The simulation of large open water surface is challenging using a uniform volumetric discretization of the Navier-Stokes equations. Simulating water splashes near moving objects, which height field methods for water waves cannot capture, necessitates high resolutions. Such simulations can be carried out using the Fluid-Implicit-Particle (FLIP) method. However, the FLIP method is not efficient for the long-lasting water waves that propagate to long distances, which require sufficient depth for a correct dispersion relationship. This paper presents a new method to tackle this dilemma through an efficient hybridization of volumetric and surface-based advection-projection discretizations. We design a hybrid time-stepping algorithm that combines a FLIP domain and an adaptively remeshed Boundary Element Method (BEM) domain for the incompressible Euler equations. The resulting framework captures the detailed water splashes near moving objects with the FLIP method, and produces convincing water waves with correct dispersion relationships at modest additional costs.	https://dl.acm.org/doi/abs/10.1145/3478513.3480495	Libo Huang, Ziyin Qu, Xun Tan, Xinxin Zhang, Dominik L. Michels, Chenfanfu Jiang
Simultaneous Augmentation of Textures and Deformation Based on Dynamic Projection Mapping	In this paper, we exploit human perception characteristics and dynamic projection mapping techniques and realize overwriting of both textures and deformation of a real object. To keep the projection following a moving object and induce deformation illusion, we developed a 1000 fps projector-camera system and demonstrated augmentation of the real world. In the demonstration, the audience will see a plaster figure turning into a colorful and flabby object.	https://dl.acm.org/doi/abs/10.1145/3476122.3484838	Leo Miyashita, Kentaro Fukamizu, Masatoshi Ishikawa
Simultaneous augmentation of textures and deformation based on dynamic projection mapping	In this demonstration, we exploit human perception characteristics and dynamic projection mapping techniques and realize overwriting of both textures and deformation of a real object. To keep the projection following a moving object and induce deformation illusion, we developed a 1000 fps projector-camera system and demonstrated augmentation of the real world. In the demonstration, the audience will see a plaster figure turning into a colorful and flabby object.	https://dl.acm.org/doi/abs/10.1145/3478511.3491307	Leo Miyashita, Kentaro Fukamizu, Masatoshi Ishikawa
Skeleton2Stroke: Interactive Stroke Correspondence Editing with Pose Features	Inbetweening is an important technique for computer animations where the stroke correspondence of hand-drawn illustrations plays a significant role. Previous works typically require image vectorization and enormous computation cost to achieve this goal. In this paper, we propose an interactive method to construct stroke correspondences in character illustrations. First, we utilize a deep learning-based skeleton estimation to improve the accuracy of closed-area correspondences, which are obtained using greedy algorithm. Second, we construct stroke correspondences based on the estimated closed-area correspondences. The proposed user interface is verified by our experiment to ensure that the users can achieve high accuracy with low correction in stroke correspondence.	https://dl.acm.org/doi/abs/10.1145/3478512.3488612	Ryoma Miyauchi, Yichen Peng, Tsukasa Fukusato, Haoran Xie
SketchHairSalon: deep sketch-based hair image synthesis	Recent deep generative models allow real-time generation of hair images from sketch inputs. Existing solutions often require a user-provided binary mask to specify a target hair shape. This not only costs users extra labor but also fails to capture complicated hair boundaries. Those solutions usually encode hair structures via orientation maps, which, however, are not very effective to encode complex structures. We observe that colored hair sketches already implicitly define target hair shapes as well as hair appearance and are more flexible to depict hair structures than orientation maps. Based on these observations, we present , a two-stage framework for generating realistic hair images directly from freehand sketches depicting desired hair structure and appearance. At the first stage, we train a network to predict a hair matte from an input hair sketch, with an optional set of non-hair strokes. At the second stage, another network is trained to synthesize the structure and appearance of hair images from the input sketch and the generated matte. To make the networks in the two stages aware of long-term dependency of strokes, we apply self-attention modules to them. To train these networks, we present a new dataset containing thousands of annotated hair sketch-image pairs and corresponding hair mattes. Two efficient methods for sketch completion are proposed to automatically complete repetitive braided parts and hair strokes, respectively, thus reducing the workload of users. Based on the trained networks and the two sketch completion strategies, we build an intuitive interface to allow even novice users to design visually pleasing hair images exhibiting various hair structures and appearance via freehand sketches. The qualitative and quantitative evaluations show the advantages of the proposed system over the existing or alternative solutions.	https://dl.acm.org/doi/abs/10.1145/3478513.3480502	Chufeng Xiao, Deng Yu, Xiaoguang Han, Youyi Zheng, Hongbo Fu
Sony PCL Inc.'s virtual production	This is a demonstration of Sony PCL's virtual production, LED WALLIn-Camera VFX. Our virtual production is built with Sony's 8K Crystal LED and a cinema camera, VENICE. It achieves overwhelmingly low latency, and possible to shoot with focusing on the CG background in nearly real time. The short demo will be presented from our virtual production studio via live streaming.	https://dl.acm.org/doi/abs/10.1145/3478511.3491314	Sota Koshino
Sparse Volume Rendering using Hardware Ray Tracing and Block Walking	We propose a method to render sparse volumetric data using ray-tracing hardware efficiently. To realize this, we introduce a novel data structure, traversal algorithm, and density encoding that allows for an annotated BVH representation. In order to avoid API calls to ray tracing hardware which reduces the efficiency in the rendering, we propose the block walking for which we store information about adjacent nodes in each BVH node's corresponding field, taking advantage of the knowledge of the content layout. Doing so enables us to traverse the tree more efficiently without repeatedly accessing the spatial acceleration structure maintained by the driver. We demonstrate that our method achieves higher performance and scalability with little memory overhead, enabling interactive rendering of volumetric data.	https://dl.acm.org/doi/abs/10.1145/3478512.3488608	Mehmet Oguz Derin, Takahiro Harada, Yusuke Takeda, Yasuhiro Iba
Spatial-temporal motion control via composite cam-follower mechanisms	Motion control, both on the trajectory and timing, is crucial for mechanical automata to perform functionalities such as walking and entertaining. We present that can control their spatial-temporal motions to exactly follow trajectories and timings specified by users, and propose a computational technique to model, design, and optimize these mechanisms. The building blocks of our mechanisms are a new kind of cam-follower mechanism with a modified joint, in which the follower can perform spatial motion on a planar, cylindrical, or spherical surface controlled by the 3D cam's profile. We parameterize the geometry of these cam-follower mechanisms, formulate analytical equations to model their kinematics and dynamics, and present a method to combine multiple cam-follower mechanisms into a working mechanism. Taking this modeling as a foundation, we propose a computational approach to designing and optimizing the geometry and layout of composite cam-follower mechanisms, with an objective of performing target spatial-temporal motions driving by a small motor torque. We demonstrate the effectiveness of our technique by designing different kinds of personalized automata and showing results not attainable by conventional mechanisms.	https://dl.acm.org/doi/abs/10.1145/3478513.3480477	Yingjie Cheng, Yucheng Sun, Peng Song, Ligang Liu
SpiCa: Stereoscopic Effect Design with 3D Pottery Wheel-type Transparent Canvas	Flow effects such as flames, smoke, and liquids play an important role in activating illustrations, but drawing these effects requires artistic expertise as well as a great deal of effort. In this paper, we propose a method for adding stereoscopic flow effects to character illustrations using various shapes of 3D pottery wheel-type transparent canvases. One approach to designing a flow effect to decorate a character relies on simple curved geometry to beautify its flow in an organized composition. We extend this approach to present a drawing system—SpiCa (spinning canvas), which enables users to use transparent surface of revolution canvases to design 3D flow effects. User evaluations showed that users were able to create such effects more easily and effectively and reduce their workload with SpiCa in comparison with an existing 2D illustration tool.	https://dl.acm.org/doi/abs/10.1145/3478512.3488606	Riwano Ikeda, Issei Fujishiro
Spiral-spectral fluid simulation	We introduce a fast, expressive method for simulating fluids over radial domains, including discs, spheres, cylinders, ellipses, spheroids, and tori. We do this by generalizing the spectral approach of Laplacian Eigenfunctions, resulting in what we call fluid simulations. Starting with a set of divergence-free analytical bases for polar and spherical coordinates, we show that their singularities can be removed by introducing a set of carefully selected enrichment functions. Orthogonality is established at minimal cost, viscosity is supported analytically, and we specifically design basis functions that support scalable FFT-based reconstructions. Additionally, we present an efficient way of computing all the necessary advection tensors. Our approach applies to both three-dimensional flows as well as their surface-based, codimensional variants. We establish the completeness of our basis representation, and compare against a variety of existing solvers.	https://dl.acm.org/doi/abs/10.1145/3478513.3480536	Qiaodong Cui, Timothy Langlois, Pradeep Sen, Theodore Kim
Submarine LED: Wirelessly powered underwater display controlling its buoyancy	This work proposes the wirelessly powered underwater display that can control its position by changing its own buoyancy. The proposed display is equipped with a motor-based actuator to control its effective volume for buoyancy control. The newly proposed underwater AC wireless power transfer (WPT) function drives the power-consuming OLED display and actuation motor. This work confirmed that the proposed WPT function could deliver mW-class power even to hundreds of mm3-class volume LEDs.	https://dl.acm.org/doi/abs/10.1145/3476124.3488655	Ryo Shirai, Masanori Hashimoto
Sum-of-squares geometry processing	Geometry processing presents a variety of difficult numerical problems, each seeming to require its own tailored solution. This breadth is largely due to the expansive list of , e.g., splines, triangles, and hexahedra, joined with an ever-expanding variety of one might want to achieve with them. With the recent increase in attention toward , we can expect a variety of challenges porting existing solutions that work on triangle meshes to work on these more complex geometry types. In this paper, we present a framework for solving many core geometry processing problems on higher-order surfaces. We achieve this goal through sum-of-squares optimization, which transforms nonlinear polynomial optimization problems into sequences of convex problems whose complexity is captured by a single parameter. This allows us to solve a suite of problems on higher-order surfaces, such as continuous collision detection and closest point queries on curved patches, with only minor changes between formulations and geometries.	https://dl.acm.org/doi/abs/10.1145/3478513.3480551	Zoë Marschner, Paul Zhang, David Palmer, Justin Solomon
SuperTrack: motion tracking for physically simulated characters using supervised learning	In this paper we show how the task of motion tracking for physically simulated characters can be solved using supervised learning and optimizing a policy directly via back-propagation. To achieve this we make use of a world model trained to approximate a specific subset of the environment's transition function, effectively acting as a differentiable physics simulator through which the policy can be optimized to minimize the tracking error. Compared to popular model-free methods of physically simulated character control which primarily make use of Proximal Policy Optimization (PPO) we find direct optimization of the policy via our approach consistently achieves a higher quality of control in a shorter training time, with a reduced sensitivity to the rate of experience gathering, dataset size, and distribution.	https://dl.acm.org/doi/abs/10.1145/3478513.3480527	Levi Fussell, Kevin Bergamin, Daniel Holden
Synthesizing scene-aware virtual reality teleport graphs	We present a novel approach for synthesizing scene-aware virtual reality teleport graphs, which facilitate navigation in indoor virtual environments by suggesting desirable teleport positions. Our approach analyzes panoramic views at candidate teleport positions by extracting scene perception graphs, which encode scene perception relationships between the observer and the surrounding objects, and predict how desirable the views at these positions are. We train a graph convolutional model to predict the scene perception scores of different teleport positions. Based on such predictions, we apply an optimization approach to sample a set of desirable teleport positions while considering other navigation properties such as coverage and connectivity to synthesize a teleport graph. Using teleport graphs, users can navigate virtual environments efficaciously. We demonstrate our approach for synthesizing teleport graphs for common indoor scenes. By conducting a user study, we validate the efficacy and desirability of navigating virtual environments via the synthesized teleport graphs. We also extend our approach to cope with different constraints, user preferences, and practical scenarios.	https://dl.acm.org/doi/abs/10.1145/3478513.3480478	Changyang Li, Haikun Huang, Jyh-Ming Lien, Lap-Fai Yu
TIEboard: Developing Kids Geometric Thinking through Tangible user Interface	This research is based on the concept of computing being embedded within the tangible product that acts as both input and output device eliminating the need of traditional computers for any feedback or guidance.The idea is inspired from traditional geoboard that focuses on the age group from 5 to 8 years old. The main goal is to integrate technology seamlessly into physical manipulative and while using this product kids will be able to make complex shapes that offer kids with memorable learning experience.	https://dl.acm.org/doi/abs/10.1145/3476124.3488623	Arooj Zaidi, Junichi Yamaoka
TM-NET: deep generative networks for textured meshes	We introduce TM-NET, a novel deep generative model for synthesizing in a manner. Once trained, the network can generate novel textured meshes from scratch or predict textures for a given 3D mesh, without image guidance. Plausible and diverse textures can be generated for the same mesh part, while texture compatibility between parts in the same shape is achieved via conditional generation. Specifically, our method produces texture maps for individual shape parts, each as a deformable box, leading to a natural UV map with limited distortion. The network embeds part geometry (via a PartVAE) and part texture (via a TextureVAE) into their respective latent spaces, so as to facilitate learning texture probability distributions conditioned on geometry. We introduce a for texture generation, which can be conditioned on both part geometry and textures already generated for other parts to achieve texture compatibility. To produce high-frequency texture details, our TextureVAE operates in a high-dimensional latent space via dictionary-based vector quantization. We also exploit transparencies in the texture as an effective means to model complex shape structures including topological details. Extensive experiments demonstrate the plausibility, quality, and diversity of the textures and geometries generated by our network, while avoiding inconsistency issues that are common to novel view synthesis methods.	https://dl.acm.org/doi/abs/10.1145/3478513.3480503	Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun Lai, Hao Zhang
Tessellation-free displacement mapping for ray tracing	Displacement mapping is a powerful mechanism for adding fine to medium geometric details over a 3D surface using a 2D map encoding them. While GPU rasterization supports it through the hardware tessellation unit, ray tracing surface meshes textured with high quality displacement requires a significant amount of memory. More precisely, the input surface needs to be pre-tessellated at the displacement map resolution before being enriched with its mandatory acceleration data structure. Consequently, designing displacement maps interactively while enjoying a full physically-based rendering is often impossible, as simply tiling multiple times the map quickly saturates the graphics memory. In this work we introduce a new tessellation-free displacement mapping approach for ray tracing. Our key insight is to decouple the displacement from its base domain by mapping a displacement-specific acceleration structures directly on the mesh. As a result, our method shows low memory footprint and fast high resolution displacement rendering, making interactive displacement editing possible.	https://dl.acm.org/doi/abs/10.1145/3478513.3480535	Theo Thonat, Francois Beaune, Xin Sun, Nathan Carr, Tamy Boubekeur
Time-travel rephotography	Many historical people were only ever captured by old, faded, black and white photos, that are distorted due to the limitations of early cameras and the passage of time. This paper simulates traveling back in time with a modern camera to rephotograph famous subjects. Unlike conventional image restoration filters which apply independent operations like denoising, colorization, and superresolution, we leverage the StyleGAN2 framework to project old photos into the space of modern high-resolution photos, achieving all of these effects in a unified framework. A unique challenge with this approach is retaining the identity and pose of the subject in the original photo, while discarding the many artifacts frequently seen in low-quality antique photos. Our comparisons to current state-of-the-art restoration filters show significant improvements and compelling results for a variety of important historical people. Please go to <u>time-travel</u>l<u>-rephotography.github.io</u> for many more results.	https://dl.acm.org/doi/abs/10.1145/3478513.3480485	Xuan Luo, Xuaner (Cecilia) Zhang, Paul Yoo, Ricardo Martin-Brualla, Jason Lawrence, Steven M. Seitz
Tool-based Asymmetric Interaction for Selection in VR	Mainstream Virtual Reality (VR) devices on the market nowadays mostly use symmetric interaction design for input, yet common practice by artists suggests asymmetric interaction using different input tools in each hand could be a better alternative for 3D modeling tasks in VR. In this paper, we explore the performance and usability of a tool-based asymmetric interaction method for a 3D object selection task in VR and compare it with a symmetric interface. The symmetric VR interface uses two identical handheld controllers to select points on a sphere, while the asymmetric interface uses a handheld controller and a stylus. We conducted a user study to compare these two interfaces, and found that the asymmetric system was faster, required less workload, and was rated with better usability. We also discuss the opportunities for tool-based asymmetric input to optimize VR art workflows, and future research directions.	https://dl.acm.org/doi/abs/10.1145/3478512.3488615	Qianyuan Zou, Huidong Bai, Yuewei Zhang, Gun Lee, Fowler Allan, Billinghurst Mark
Training to Get a Drone License, Virtually	We propose a drone flight training system (DFT MR) with a novel mixed reality (MR) technique to lower the risk of drone crashes for drone flight beginners and enhance their actual feelings of drone control. Recently, virtual reality (VR) or augmented reality (AR) has been widely employed in training systems. However, the current drone training systems in AR usually lack physical feedback from wind or air resistance. In addition, the VR training environment and the real world were considered to be inequivalent. The proposed MR system integrates the advantages of both VR and AR by transmitting the high-fidelity simulated physical control feedback, such as obstacle collision and wind influence, from VR to the MR device. The system not only benefits those who have VR sickness but also provides novice drone operators virtual crash-free drones for training to get their drone licenses.	https://dl.acm.org/doi/abs/10.1145/3476124.3488630	Ming-Ru Xie, Siang-Jhih Sie, Shing-Yun Jung, Bo-Wei Dong, Pei-Hua Tsai, Neng-Hao Yu, Kuan-Wen Chen
Transflower: probabilistic autoregressive dance generation with multimodal attention	Dance requires skillful composition of complex movements that follow rhythmic, tonal and timbral features of music. Formally, generating dance conditioned on a piece of music can be expressed as a problem of modelling a high-dimensional continuous motion signal, conditioned on an audio signal. In this work we make two contributions to tackle this problem. First, we present a novel probabilistic autoregressive architecture that models the distribution over future poses with a normalizing flow conditioned on previous poses as well as music context, using a multimodal transformer encoder. Second, we introduce the currently largest 3D dance-motion dataset, obtained with a variety of motion-capture technologies, and including both professional and casual dancers. Using this dataset, we compare our new model against two baselines, via objective metrics and a user study, and show that both the ability to model a probability distribution, as well as being able to attend over a large motion and music context are necessary to produce interesting, diverse, and realistic dance that matches the music.	https://dl.acm.org/doi/abs/10.1145/3478513.3480570	Guillermo Valle-Pérez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves Oudeyer, Simon Alexanderson
Transition Motion Tensor: A Data-Driven Approach for Versatile and Controllable Agents in Physically Simulated Environments	This paper proposes the Transition Motion Tensor, a data-driven framework that creates novel and physically accurate transitions outside of the motion dataset. It enables simulated characters to adopt new motion skills efficiently and robustly without modifying existing ones. Given several physically simulated controllers specializing in different motions, the tensor serves as a temporal guideline to transition between them. Through querying the tensor for transitions that best fit user-defined preferences, we can create a unified controller capable of producing novel transitions and solving complex tasks that may require multiple motions to work coherently. We apply our framework on both quadrupeds and bipeds, perform quantitative and qualitative evaluations on transition quality, and demonstrate its capability of tackling complex motion planning problems while following user control directives.	https://dl.acm.org/doi/abs/10.1145/3478512.3488599	Jonathan Hans Soeseno, Ying-Sheng Luo, Trista Pei-chun Chen, Wei-Chao Chen
TreePartNet: neural decomposition of point clouds for 3D tree reconstruction	We present , a neural network aimed at reconstructing tree geometry from point clouds obtained by scanning real trees. Our key idea is to learn a natural exploiting the assumption that a tree comprises locally cylindrical shapes. In particular, reconstruction is a two-step process. First, two networks are used to detect priors from the point clouds. One detects semantic branching points, and the other network is trained to learn a cylindrical representation of the branches. In the second step, we apply a neural merging module to reduce the cylindrical representation to a final set of generalized cylinders combined by branches. We demonstrate results of reconstructing realistic tree geometry for a variety of input models and with varying input point quality, e.g., noise, outliers, and incompleteness. We evaluate our approach extensively by using data from both synthetic and real trees and comparing it with alternative methods.	https://dl.acm.org/doi/abs/10.1145/3478513.3480486	Yanchao Liu, Jianwei Guo, Bedrich Benes, Oliver Deussen, Xiaopeng Zhang, Hui Huang
Twenty Something	Adulting can be hard.	https://dl.acm.org/doi/abs/10.1145/3463912.3481047	Aphton Corbin, Erik Langley
Using Python scripting to enhance workflow in Autodesk Maya: coures notes	Coding empowers automation. Scripts can handle mundane and repetitive tasks in an efficient and precise manner. This course will offer will use an hands-on interactive format to walk attendees through representative scripting projects, selected to be useful for everyday workflows. It is intended to be an intermediate course. The goal is to cover provide enough information for attendees to build on later. Python scripting can automate many tasks in Maya, from running simple commands to developing plug-ins. Attendees will learn how to automate simple tasks using the magic of scripting, through four distinct projects. The course will placing objects randomly in a scene, designing custom User Interfaces (GUIs) in Maya, scripting MASH (motion graphic) networks, and scripting a leg rig, with foot-roll. By the end of the course, attendees should walk away with a solid understanding of the power Python scripting and Maya commands provide, and the the ability to build their own advance projects for Maya. This course will equip attendees with the tools, confidence, and initiative to explore more advanced scripts independently. Attendees should have programming experience, preferably in Python, but a solid grasp of the foundational programming constructs should suffice. Attendees should have Autodesk Maya, Python, and Visual Studio Code pre-loaded on their devices if they intend to follow along.	https://dl.acm.org/doi/abs/10.1145/3476117.3483437	Ann McNamara
VR social copresence with light field displays	As virtual reality (VR) devices become increasingly commonplace, asymmetric interactions between people with and without headsets are becoming more frequent. Existing video pass-through VR headsets solve one side of these asymmetric interactions by showing the user a live reconstruction of the outside world. This paper further advocates for , wherein a three-dimensional view of the user's face and eyes is presented to any number of outside viewers in a perspective-correct manner using a light field display. Tying together research in social telepresence and copresence, autostereoscopic displays, and facial capture, reverse pass-through VR enables natural eye contact and other important non-verbal cues in a wider range of interaction scenarios, providing a path to potentially increase the utility and social acceptability of VR headsets in shared and public spaces.	https://dl.acm.org/doi/abs/10.1145/3478513.3480481	Nathan Matsuda, Brian Wheelwright, Joel Hegland, Douglas Lanman
VR-Wizard: Towards an Emotion-Adaptive Experience in VR	In this research, we investigate the impact of real-time biofeedback-based emotion adaptive Virtual Reality (VR) environments on the immersiveness, game engagement, and flow state using physiological information such as Electroencephalogram (EEG), Electrodermal Activity (EDA), and Heart Rate Variability (HRV). For this, we designed VR-Wizard, a personalized emotion-adaptive VR game akin to a Harry Potter experience with an objective to collect items in the forbidden forest. The users initially train the system through a calibration process. Next, they explore the forest with adapting environmental factors based on a 'MagicMeter' indicating the user's real-time emotional states. The overall goal is to provide more personalized, immersed, and engaging emotional virtual experiences.	https://dl.acm.org/doi/abs/10.1145/3476124.3488657	Kunal Gupta, Yuewei Zhang, Yun Suen Pai, Mark Billinghurst
VRTwitch: Enabling Micro-motions in VR with Radar Sensing	Micro-motions are often difficult to incorporate in Virtual Reality (VR) while macro-motions are a popular interaction method, due to technological limitations with VR tracking methods. In this poster, we introduce VRTwitch, a forearm-mounted wearable device that is able to sense micro hand motions. VRTwitch uses an array of reconfigurable miniaturized radar sensors placed around the hand to capture subtle finger movements for gesture detection towards enhanced interaction in VR space. We created a simple interactive VR shooting game that requires precise finger motion for virtual gun manipulation as a demonstration.	https://dl.acm.org/doi/abs/10.1145/3476124.3488650	Ryo Hajika, Tamil Selvan Gunasekaran, Alaeddin Nassani, Yun Suen Pai, Mark Billinghurst
VRoid studio: a tool for making anime-like 3D characters using your imagination	We have implemented VRoid Studio, a 3D character maker that specializes in Japanese anime-like expressions. Our target is to make illustrators enable to create 3D models by themselves. Creating a 3D model is a very difficult task that involves many processes. We have defined several elements essential to express illustration-like characters, and that is textures, clothes, and hairs. We implemented several features to design them intuitively.	https://dl.acm.org/doi/abs/10.1145/3478511.3491311	Nozomi Isozaki, Shigeyoshi Ishima, Yusuke Yamada, Yutaka Obuchi, Rika Sato, Norio Shimizu
VWind: Virtual Wind Sensation to the Ear by Cross-Modal Effects of Audio-Visual, Thermal, and Vibrotactile Stimuli	In the field of virtual reality, wind displays have been researched to present wind sensation. However, since the wind displays need physical wind sources, large heat transfer mechanisms are required to produce hot wind and cold wind. We propose to present virtual wind sensation by cross-modal effects. We developed VWind, a wearable headphone-type device to give vibrotactile and thermal stimuli to the ear in addition to visual scenarios and binaural sounds. For user experience, the demonstrations are prepared to present virtual wind sensation as if users were blown to the ear or exposed to freezing winds.	https://dl.acm.org/doi/abs/10.1145/3476122.3484848	Juro Hosoi, Yuki Ban, Kenichi Ito, Shin'ichi Warisawa
Vectorized Reservoir Sampling	Reservoir sampling is becoming an essential component of realtime rendering as it enables importance resampling with limited storage. Chao's weighted random sampling algorithm is a popular choice because of its simplicity. Although it is elegant, there is a fundamental issue that many random numbers must be generated to update reservoirs. To address this issue, we modify Chao's algorithm with sample warping. We apply sample warping in two different ways and compare them. We further vectorize the modified algorithm to make reservoir sampling more useful for CPU rendering and give a couple of practical examples.	https://dl.acm.org/doi/abs/10.1145/3478512.3488602	Shinji Ogaki
Viewport-Resolution Independent Anti-Aliased Ray Marching on Interior Faces in Cube-Map Space	This paper presents a novel approach to anti-aliased ray marching by indirect shading in cube-map space. Our volume renderer firstly performs ray marching on each visible interior pixel of a maximum-resolution-limited cube map, and then resamples (usually up-scales) the cube imposter in viewport space. By this viewport-resolution-independent strategy, developers can improve both ray-marching performance and its quality of anti-aliasing when allowing larger marching strides. Moreover, our solution also covers depth-occlusion anti-aliasing for mixed mesh-volume rendering, cube-map level-of-details (LOD) optimization for a further performance boost, and multiple-volume rendering by leveraging the GPU inline ray tracing. Besides, our implementation is developer-friendly and the performance-quality tradeoff determined by the parameter configuration is easily controllable.	https://dl.acm.org/doi/abs/10.1145/3478512.3488598	Tianchen Xu, Wei Zeng, Enhua Wu
Virtual Reality Experience 'The World of Hiroshige': An Immersive Virtual Reality Experience of the World that Inspired the Works of the Japanese Artist Utagawa Hiroshige	"'The World of Hiroshige' is an immersive virtual reality experience that allows the participants not only to view the artworks of the Japanese artist Utagawa Hiroshige in 3 dimensions, but also to experience and interact with the Ukiyo or ""floating world"" that inspired the works."	https://dl.acm.org/doi/abs/10.1145/3478514.3487611	Alex Weight, Daniel Flood, Dylan Neill
Virtual Whiskers: Cheek Haptic-Based Spatial Directional Guidance in a Virtual Space	In spatial navigation, adding haptic cues to visual information lets users understand the spatial information better. Most haptic devices stimulate various body parts, while few devices target our heads that are sensitive to mechanical stimuli. This paper presents Virtual Whiskers, a spatial directional guidance technique using cheek haptics in a virtual space. We created a cheek haptic stimulation device by attaching two tiny robot arms to a Head-Mounted Display. The robot arms trace the cheek with proximity sensors to estimate the cheek surface. Target azimuthal and elevational directions are translated into a point on the cheek surface. The robot arms touch the point to present target directional cues. We demonstrate our technique in two applications.	https://dl.acm.org/doi/abs/10.1145/3478514.3487625	Fumihiko Nakamura, Adrien Verhulst, Kuniharu Sakurada, Masaaki Fukuoka, Maki Sugimoto
Visibility Enhancement for Transmissive Image using Synchronized Side-by-side Projector–Camera System	Extracting the direct light component from light transport helps to enhance the visibility of a scene. In this paper, we describe a method to improve the visibility of the target object by capturing transmissive rays without scattering rays using a synchronized projector–camera system. A rolling shutter camera and a laser raster scanning projector are placed side-by-side, and both epipolar planes are optically aligned on a screen plane which is place between the projector and camera. This paper demonstrates that our method can visualize an internal object inside diluted milk.	https://dl.acm.org/doi/abs/10.1145/3476124.3488622	Kazuto Ogiwara, Hiroyuki Kubo
Volume decomposition for two-piece rigid casting	We introduce a novel technique to automatically decompose an input object's volume into a set of parts that can be represented by two opposite height fields. Such decomposition enables the manufacturing of individual parts using two-piece reusable rigid molds. Our decomposition strategy relies on a new energy formulation that utilizes a pre-computed signal on the mesh volume representing the accessibility for a predefined set of extraction directions. Thanks to this novel formulation, our method allows for efficient optimization of a fabrication-aware partitioning of volumes in a completely automatic way. We demonstrate the efficacy of our approach by generating valid volume partitionings for a wide range of complex objects and physically reproducing several of them.	https://dl.acm.org/doi/abs/10.1145/3478513.3480555	Thomas Alderighi, Luigi Malomo, Bernd Bickel, Paolo Cignoni, Nico Pietroni
WORLD-SPACE SPATIOTEMPORAL RESERVOIR REUSE FOR RAY-TRACED GLOBAL ILLUMINATION	Path-traced global illumination of scenes with complex lighting remains particularly challenging at real-time framerates. Reservoir-based resampling methods for light sampling allow for significant noise reduction at the cost of very few shadow rays per pixel. However, current image-space approaches to reservoir reuse do not scale to sample lighting at further bounces, as is required for efficiently evaluating indirect illumination. We present a novel approach to performing reservoir-based spatiotemporal importance resampling in world space, allowing for efficient light sampling at arbitrary vertices along the eye path. Our approach caches the reservoirs of the path vertices into the cells of a hash grid built entirely on the GPU. Such a structure allows for stochastic reuse of neighboring reservoirs across space and time for efficient spatiotemporal reservoir resampling at any point in space.	https://dl.acm.org/doi/abs/10.1145/3478512.3488613	Guillaume Boissé
Weatherscapes: nowcasting heat transfer and water continuity	Due to the complex interplay of various meteorological phenomena, simulating weather is a challenging and open research problem. In this contribution, we propose a novel physics-based model that enables simulating weather at interactive rates. By considering atmosphere and pedosphere we can define the hydrologic cycle - and consequently weather - in unprecedented detail. Specifically, our model captures different warm and cold clouds, such as mammatus, hole-punch, multi-layer, and cumulonimbus clouds as well as their dynamic transitions. We also model different precipitation types, such as rain, snow, and graupel by introducing a comprehensive microphysics scheme. The Wegener-Bergeron-Findeisen process is incorporated into our Kessler-type microphysics formulation covering ice crystal growth occurring in mixed-phase clouds. Moreover, we model the water run-off from the ground surface, the infiltration into the soil, and its subsequent evaporation back to the atmosphere. We account for daily temperature changes, as well as heat transfer between pedosphere and atmosphere leading to a complex feedback loop. Our framework enables us to interactively explore various complex weather phenomena. Our results are assessed visually and validated by simulating weatherscapes for various setups covering different precipitation events and environments, by showcasing the hydrologic cycle, and by reproducing common effects such as Foehn winds. We also provide quantitative evaluations creating high-precipitation cumulonimbus clouds by prescribing atmospheric conditions based on infrared satellite observations. With our model we can generate dynamic 3D scenes of weatherscapes with high visual fidelity and even nowcast real weather conditions as simulations by streaming weather data into our framework.	https://dl.acm.org/doi/abs/10.1145/3478513.3480532	Jorge Alejandro Amador Herrera, Torsten Hädrich, Wojtek Pałubicki, Daniel T. Banuti, Sören Pirk, Dominik L. Michels
What we talk about, when we talk about story: SIGGRAPH Asia 2021 - course notes	"Story (content) is not just the domain of directors and producers...anymore. Today, it is as important for technical directors, animators, VFX creators and interactive designers whose work is essential in making come to life. This information is particularly useful when communicating with screenwriters, directors, and producers. This course answers the question, ""What is Story?"" (and you don't even have to take a course in screenwriting). Knowing the basics of story enables the crew to become collaborators with the producer and director. A director may talk about their story goals; and the crew will know what specific story benchmarks they are trying to meet. This information builds from a story being more than ""a sequence of events (acts) but can become a dramatic story that that builds from setup through resolution. Having an understanding of story structure allows one to understand a story's elements in context (i.e., theme, character, setting, conflict etc.) and their relationship to classic story structure (i.e., setup, inciting incident, rising action, climax, resolution, etc.). This information is for all whose work makes the story better, but their job is not creating the story. The following course notes have been adapted from CRC Publishers, a division of Taylor and Francis. Available on <u>Amazon</u>."	https://dl.acm.org/doi/abs/10.1145/3476117.3483430	Craig Caldwell
WizardOfVR: An Emotion-Adaptive Virtual Wizard Experience	We demonstrate WizardOfVR, a personalized emotion-adaptive Virtual Reality (VR) game akin to a Harry Potter experience, which uses using off-the-shelf physiological sensors to create a real-time biofeedback loop between a user's emotional state and an adaptive VR environment (VRE). In our demo, the user initially trains the system during a calibration process using Electroencephalogram (EEG), Electrodermal Activity (EDA), and Heart Rate Variability (HRV) physiological signals. After calibration, the user will explore a virtual forest with adapting environmental factors based on a 'SanityMeter' determined by the user's real-time emotional state. The overall goal is to provide more balanced, immersive, and optimal emotional virtual experiences.	https://dl.acm.org/doi/abs/10.1145/3478514.3487628	Kunal Gupta, Yuewei Zhang, Yun Suen Pai, Mark Billinghurst
X-Wing: Propeller-Based Force Feedback to Head in a Virtual Environment	In this demonstration, we present X-Wing, a force feedback device using ducted fans attached to a Virtual Reality (VR) Head Mounted Display (HMD). Applying haptic technologies to VR has become a standard solution to enhance the immersive experience. It is also essential for virtual interactions where user experience and performance are crucial. This research focuses on a head-based, wearable device that provides forces to the VR user using exertion force by electric thrusters with controllable strengths and directions. Our system allows the user to experience forces based on their virtual momentum and velocity. It enables a VR user to receive force feedback based on different thrust power converted into translational and rotational force for a unique VR experience such as virtual aerial simulation. We present the prototype to the attendees of SIGGRAPH Asia 2021 through a live demonstration.	https://dl.acm.org/doi/abs/10.1145/3478514.3487622	Koki Watanabe, Fumihiko Nakamura, Kuniharu Sakurada, Theophilus Teo, Maki Sugimoto
Yallah!	Beirut, 1982. As Nicolas prepares to flee his hometown, torn apart by an endless civil war, he crosses the path of Naji, a reckless teenager determined to go to the Swimming Pool. Trying to protect the young man, Nicolas finds himself pulled into a surreal race against war, all for the mere freedom of going swimming.	https://dl.acm.org/doi/abs/10.1145/3463912.3477184	Nayla Nassar, Edouard Pitula, Renaud de Saint Albin, Cécile Adant, Anaïs Sassatelli, Candice Behague
eyemR-Talk: Using Speech to Visualise Shared MR Gaze Cues	In this poster we present eyemR-Talk, a Mixed Reality (MR) collaboration system that uses speech input to trigger shared gaze visualisations between remote users. The system uses 360° panoramic video to support collaboration between a local user in the real world in an Augmented Reality (AR) view and a remote collaborator in Virtual Reality (VR). Using specific speech phrases to turn on virtual gaze visualisations, the system enables contextual speech-gaze interaction between collaborators. The overall benefit is to achieve more natural gaze awareness, leading to better communication and more effective collaboration.	https://dl.acm.org/doi/abs/10.1145/3476124.3488618	Allison Jing, Brandon Matthews, Kieran May, Thomas Clarke, Gun Lee, Mark Billinghurst
the_neuron_001: performance using brain computer interface	"In Japan, we have religious meditation training to stare at one place called ""AJIKAN."" This real-time live performance reveals the technical and cultural connection between the Brain Control Interface and AJIKAN. During the performance, the system lets the performer click a preferred location on a computer screen at which they need to stare during the performance. Then, the Brain Control Interface device from NextMind can steadily read the brain activity, and the system renders the information in real time. It connects the latest technologies and creativity in the fields, including live performance, installation, art, and XR."	https://dl.acm.org/doi/abs/10.1145/3478511.3491306	Shintaro Ono
"""Bodyreath"": An Interactive VR Installation to Experience and Understand People with Autism"	"""Bodyreath"" is an interactive virtual reality (VR) installation for understanding people with autism, which is divided into two parts, ""external"" and ""internal"". The ""external"" is the external projection, which is the artistic effect of four virtual avatars based on the characters and behaviors of autistic people. The ""internal"" is the interactive VR scenarios, allowing the audience to gain insight into the characteristics of the autistic people represented in each feature: ""Stereotyped Behavior"", ""Children from the Stars"", ""Emotional Instability"", and ""Invisibility"". Our VR installation provides an interactive narrative through body interaction and role perception, allowing the audience to transform their identity into an autistic person and perceive it from their perspective. It helps the audience to appreciate the innocence of autistic people in a different way and to understand them artistically."	https://dl.acm.org/doi/abs/10.1145/3478514.3487612	Huifeng Zhang, Shuo Yan
"""Flower and the Youth"": Virtual Narrative and Creative Methods of Oral Performance in China Intangible Cultural Heritage"	"The intangible cultural heritage of China contains many different forms of performing arts, of which oral performance is an important branch. ""Hua'er"" is the most popular folk performance sung in the Hui ethnic area of Ningxia. Based on technology of virtual reality (VR) and gesture recognition, our work proposes three design methods: interactive performance narrative, metaphorical elements and embodied cognition, applied to the VR performance, ""Flower and the Youth"". VR can provide audience with a more immersed experience that contributes to the transmission and dissemination of non-heritage performing arts. Our work provides universal design approaches to the creation of content for future intangible cultural heritage performances."	https://dl.acm.org/doi/abs/10.1145/3478514.3487623	Yu Lu, Zixiao Liu, Xuning Yan, Shuo Yan
"""Meet the Deer King"": ""Splash-Ink"" Interaction in the Innovative VR Game Based on Dunhuang Art and Culture"	"VR technologies enable 3D visualization and 3D interaction modalities, allowing the general public to get user-centered experience and explore an immersive realistic cultural environment. The art and culture of Dunhuang grotto murals is the world's art treasure. According to the traditional story of ""Deer King"" and the knowledges of preservation and restoration of the murals, we designed ""Meet the Deer King"", a VR game. In this game, we explored and designed ""splash-ink"" interaction, which derives from traditional Chinese painting style, and we applied this interaction in different scenes. The information about the original game, including its creation background, research focus, game content and implementation effects are mainly described in the supplementary materials."	https://dl.acm.org/doi/abs/10.1145/3478514.3487618	Jing Zhang, Guangwen Zou, Guangzheng Zhang
"""Once"": Multi-perspective and Interactive VR Narrative Film"	"""Once"" is an interactive virtual reality narration film based on post-war experience, which is set after a continuous war period between two countries. Current virtual reality creations about warfare have only been describing the war itself, and its expression easily causes the audience's lack of social empathy, which results in weakening the portray of anti-war consciousness. More importantly, this kind of creation just narrates in one single view of angle and performs imperfectly. Due to those issues, through the three-identity switching, subtitled guidance and stylized visualization, we create this VR narrative film, which uses the image of common people after the war as clues to express the trauma it brings."	https://dl.acm.org/doi/abs/10.1145/3478514.3487624	Zixiao Liu, Lutong Li, Shuo Yan
