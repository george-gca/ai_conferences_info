title	abstract	url	authors
"""From video game to <u>digital playground</u>"": virtual reality and augmented reality"	I would like to conclude this presentation by providing a brief summary of this course and share my vision of a playful world where the real and digital world coexist in the near future.	https://dl.acm.org/authorize?N675535	Hirofumi Motoyama, Eiji Iwata
"""The player is the star"": futuristic vision for mixed reality world developing mixed reality game - PAC IN TOWN"	"""The player is the star"" - Futuristic vision for Mixed Reality World is our (BANDAI NAMCO Studios Inc.) [Bandai Namco 2018] achievement and showing future vision for mixed reality entertainment. We developed a mixed reality game called ""PAC IN TOWN"" for mixed reality device - Microsoft HoloLens. [Microsoft HoloLens 2018]"	https://dl.acm.org/authorize?N664116	Hirofumi Motoyama
3D fabrication with universal building blocks and pyramidal shells	Augmented reality (AR) for smartphones has matured from a technology for earlier adopters, available only on select high-end phones, to one that is truly available to the general public. One of the key breakthroughs has been in methods for six degree of freedom (6DoF) tracking on phones using only the (camera and inertial sensors). 6DoF tracking is the cornerstone of smartphone AR allowing virtual content to be precisely locked on top of the real world. However, to really give users the impression of believable AR, one requires Without depth, even simple effects such as a virtual object being correctly occluded by the real-world is impossible. However, requiring a mobile depth sensor would severely restrict the access to such features. In this article, we provide a novel pipeline for mobile depth that supports a wide array of mobile phones, and uses only the existing monocular color sensor. Through several technical contributions, we provide the ability to compute low latency dense depth maps using only a single CPU core of a wide range of (medium-high) mobile phones. We demonstrate the capabilities of our approach on high-level AR applications including real-time navigation and shopping.	https://dl.acm.org/authorize?N664494	Xuelin Chen, Honghua Li, Chi-Wing Fu, Hao Zhang, Daniel Cohen-Or, Baoquan Chen
3D hair synthesis using volumetric variational autoencoders	Coarse building mass models are now routinely generated at scales ranging from individual buildings to whole cities. Such models can be abstracted from raw measurements, generated procedurally, or created manually. However, these models typically lack any meaningful geometric or texture details, making them unsuitable for direct display. We introduce the problem of automatically and realistically decorating such models by adding semantically consistent geometric details and textures. Building on the recent success of generative adversarial networks (GANs), we propose F GAN, a cascade of GANs that creates plausible details across multiple scales over large neighborhoods. The various GANs are synchronized to produce consistent style distributions over buildings and neighborhoods. We provide the user with direct control over the variability of the output. We allow him/her to interactively specify the style via images and manipulate style-adapted sliders to control style variability. We test our system on several large-scale examples. The generated outputs are qualitatively evaluated via a set of perceptual studies and are found to be realistic, semantically plausible, and consistent in style.	https://dl.acm.org/authorize?N664413	Shunsuke Saito, Liwen Hu, Chongyang Ma, Hikaru Ibayashi, Linjie Luo, Hao Li
3D touch point detection on load sensitive surface based on continuous fluctuation of a user hand	We present a load sensitive surface which provides exact contact position in 3D and a force vector of user's touch from low cost load sensors without any prior knowledge about shape of objects. Load based approach is advantageous since they provide a force vector, however, there have been mathematical ambiguity of the touch position along the line of action. To address this problem, we develop algorithm based on pseudo-inverse matrix framework, utilizing the notion that user's hands are always fluctuating slightly and unconsciously during touch interaction. In this paper, we introduce the design space of the surface, outline of the algorithm and highlighted applications. This technique to localize the touch position in physical spaces enables us to design tangible interaction with inert everyday objects and analyze user's activities happening on the surfaces.	https://dl.acm.org/authorize?N675716	Takatoshi Yoshida, Xiaoyan Shen, Tal Achituv, Hiroshi Ishii
: enhancing performance capture with real-time neural re-rendering	Markov chain Monte Carlo (MCMC) rendering utilizes a sequence of correlated path samples which is obtained by iteratively mutating the current state to the next. The efficiency of MCMC rendering depends on how well the mutation strategy is designed to adapt to the local structure of the state space. We present a novel MCMC rendering method that automatically adapts the step sizes of the mutations to the of the rendered scene. Our geometry-aware path space perturbation largely avoids tentative samples with zero contribution due to occlusion. Our method limits the mutation step size by estimating the maximum opening angle of a cone, centered around a segment of a light transport path, where no geometry obstructs visibility. This geometry-aware mutation increases the acceptance rates, while not degrading the sampling quality. As this cone estimation introduces a considerable overhead if done naively, to make our approach efficient, we discuss and analyze fast approximate methods for cone angle estimation which utilize the acceleration structure already present for the ray-geometry intersection. Our new approach, integrated into the framework of Metropolis light transport, can achieve results with lower error and less artifact in equal time compared to current path space mutation techniques.	https://dl.acm.org/authorize?N675563	Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral Agarwal, Jens Fursund, Hao Li
: reconstructing thin structures from RGBD sequences	This paper introduces a novel method for realtime portrait animation in a single photo. Our method requires only a single portrait photo and a set of facial landmarks derived from a driving source (e.g., a photo or a video sequence), and generates an animated image with rich facial details. The core of our method is a warp-guided generative model that instantly fuses various fine facial details (e.g., creases and wrinkles), which are necessary to generate a high-fidelity facial expression, onto a pre-warped image. Our method factorizes out the nonlinear geometric transformations exhibited in facial expressions by lightweight 2D warps and leaves the appearance detail synthesis to conditional generative neural networks for high-fidelity facial animation generation. We show such a factorization of geometric transformation and appearance synthesis largely helps the network better learn the high nonlinearity of the facial expression functions and also facilitates the design of the network architecture. Through extensive experiments on various portrait photos from the Internet, we show the significant efficacy of our method compared with prior arts.	https://dl.acm.org/authorize?N664435	Adarsh Kowdle, Christoph Rhemann, Sean Fanello, Andrea Tagliasacchi, Jonathan Taylor, Philip Davidson, Mingsong Dou, Kaiwen Guo, Cem Keskin, Sameh Khamis, David Kim, Danhang Tang, Vladimir Tankovich, Julien Valentin, Shahram Izadi
: surface editing by way of multi-view image processing	Deep learning systems extensively use convolution operations to process input data. Though convolution is clearly defined for structured data such as 2D images or 3D volumes, this is not true for other data types such as sparse point clouds. Previous techniques have developed approximations to convolutions for restricted conditions. Unfortunately, their applicability is limited and cannot be used for general point clouds. We propose an efficient and effective method to learn convolutions for non-uniformly sampled point clouds, as they are obtained with modern acquisition techniques. Learning is enabled by four key novelties: first, representing the convolution kernel itself as a multilayer perceptron; second, phrasing convolution as a Monte Carlo integration problem, third, using this notion to combine information from multiple samplings at different levels; and fourth using Poisson disk sampling as a scalable means of hierarchical point cloud learning. The key idea across all these contributions is to guarantee adequate consideration of the underlying non-uniform sample distribution function from a Monte Carlo perspective. To make the proposed concepts applicable to real-world tasks, we furthermore propose an efficient implementation which significantly reduces the GPU memory required during the training process. By employing our method in hierarchical network architectures we can outperform most of the state-of-the-art networks on established point cloud segmentation, classification and normal estimation benchmarks. Furthermore, in contrast to most existing approaches, we also demonstrate the robustness of our method with respect to sampling variations, even when training with uniformly sampled data only. To support the direct application of these concepts, we provide a ready-to-use TensorFlow implementation of these layers at https://github.com/viscom-ulm/MCCNN.	https://dl.acm.org/authorize?N664438	Florian Reibold, Johannes Hanika, Alisa Jung, Carsten Dachsbacher
A device for reconstructing light field data as 3D aerial image by retro-reflection	In this paper, the image photographed by the light field camera is reconstructed by the light field display, and projected in the air by Fresnel lens and an aerial imaging by retro-reflection (AIRR) technique. This allows you to face the 3D aerial image of the user in another location and perform a realistic conversation.	https://dl.acm.org/authorize?N675742	Kengo Fujii, Kazuki Shimose, Clément Trovato, Masao Nakajima, Toru Iwane, Masaki Yasugi, Hirotsugu Yamamoto
A generic visualization framework for understanding missing links in bipartite networks	The analysis of bipartite networks is critical in many application domains, such as studying gene expression in bio-informatics. One important task is missing link prediction, which infers the existence of new links based on currently observed ones. However, in practice, analysts need to utilize their domain knowledge based on the algorithm outputs in order to make sense of the results. We propose a novel visual analysis framework, MissBi, which allows for examining and understanding missing links in bipartite networks. Some initial feedback from a management school professor has demonstrated the effectiveness of the tool.	https://dl.acm.org/authorize?N675732	Jian Zhao, Fracine Chen, Patrick Chiu
A magic wand for motion capture editing and edit propagation	"This paper introduces a new method for editing character animation, by using a data-driven pose distance as a falloff to interpolate edited poses seamlessly into a motion sequence. This pose distance is defined using Green's function of the pose space Laplacian. The resulting falloff shape and timing are derived from and reflect the motion itself, replacing the need for a manually adjusted falloff spline. This data-driven region of influence is somewhat analogous to the difference between a generic 2D spline and the ""magic wand"" selection in an image editor, but applied to the animation domain. It also supports powerful non-local edit propagation in which edits are applied to all similar poses in the entire animation sequence."	https://dl.acm.org/authorize?N675668	Christopher J. Dean, J. P. Lewis, Andrew Chalmers
A radiative transfer framework for non-exponential media	We present a method that finds a of wires with to reproduce a wire sculpture design as a 3D shape abstraction. Importantly, we consider wires, which can be fabricated by a wire bending machine, to enable efficient construction of complex 3D sculptures that cannot be achieved by previous works. We call our wires , since they are as Eulerian as possible with small overlap to form the target design together. Finding such Eulerian wires is highly challenging, due to an enormous search space. After exploring a variety of optimization strategies, we formulate a population-based hybrid metaheuristic model, and design the join, bridge and split operators to refine the solution wire sets in the population. We start the exploration of each solution wire set in a bottom-up manner, and adopt an adaptive simulated annealing model to regulate the exploration. By further formulating a meta model on top to optimize the cooling schedule, and precomputing fabricable subwires, our method can efficiently find promising solutions with low wire count and overlap in one to two minutes. We demonstrate the efficiency of our method on a rich variety of wire sculptures, and physically fabricate several of them. Our results show clear improvements over other optimization alternatives in terms of solution quality, versatility, and scalability.	https://dl.acm.org/authorize?N664432	Amir Vaxman, Christian Müller, Ofir Weber
A system for acquiring, processing, and rendering panoramic light field stills for virtual reality	Designing real and virtual garments is becoming extremely demanding with rapidly changing fashion trends and increasing need for synthesizing realisticly dressed digital humans for various applications. This necessitates creating simple and effective workflows to facilitate authoring sewing patterns customized to garment and target body shapes to achieve desired looks. Traditional workflow involves a trial-and-error procedure wherein a mannequin is draped to judge the resultant folds and the sewing pattern iteratively adjusted until the desired look is achieved. This requires time and experience. Instead, we present a data-driven approach wherein the user directly indicates desired fold patterns simply by sketching while our system estimates corresponding garment and body shape parameters at interactive rates. The recovered parameters can then be further edited and the updated draped garment previewed. Technically, we achieve this via a novel shared shape space that allows the user to seamlessly specify desired characteristics across multimodal input requiring to run garment simulation at design time. We evaluate our approach qualitatively via a user study and quantitatively against test datasets, and demonstrate how our system can generate a rich quality of on-body garments targeted for a range of body shapes while achieving desired fold characteristics. Code and data are available at our project webpage.	https://dl.acm.org/authorize?N664402	Ryan S. Overbeck, Daniel Erickson, Daniel Evangelakos, Matt Pharr, Paul Debevec
A variational U-Net for motion retargeting	In this paper, we present a novel motion retargeting system by using the deep autoencoder combining the Deep Convolution Inverse Graphics Network (DC-IGN) ([Kulkarni et al. 2015]) and the U-Net ([Long et al. 2015]) to produce high-quality human motion. The retargeted motion is fully-automatically and naturally generated from the given input motion and bone length ratios. To validate the proposed motion retargeting system, we conduct several experiments and achieve more accuracy and less computational burden when compared with the conventional motion retargeting approach and other neural network architectures.	https://dl.acm.org/authorize?N675778	Hanyoung Jang, Byungjun Kwon, Moonwon Yu, Seong Uk Kim, Jongmin Kim
Accurate 3D locating and tracking of basketball players from multiple videos	With the development of pedestrian detection technologies, existing methods cannot simultaneously satisfy high-quality detection and fast calculation for practical applications, especially for accurate 3D locating and tracking of basketball players. We propose an algorithm which can robustly and automatically locate and track basketball players from multiple videos. After extracting the foregrounds, the voxels in the basketball court space are projected back to the foreground images. Occupied voxels are accumulated and smoothed based on for acceleration. Two Gaussian Mixture Models including Grouping Gaussian Mixture Model(GGMM) and Locating Gaussian Mixture Model(LGMM) are designed for continuous locating and grouping players, and a simple blob detector is employed to handle out-of-bound players. Our algorithm is insensitive to occlusions, shadows, lights and computation errors.	https://dl.acm.org/authorize?N675684	Weiliang Meng, Shibiao Xu, Er Li, Xiangyong Zeng, Xiaopeng Zhang
Adaptation of manga face representation for accurate clustering	Manga character drawing styles differ greatly among artists. To accurately cluster faces within an individual manga, we propose a method to adapt manga face representations to an individual manga. We use deep features trained for generic manga face recognition, and adapt them by deep metric learning (DML) for the target manga volume. DML uses pseudo positive and negative pairs defined by considering page and frame information. We performed experiments using a dataset comprising 104 manga volumes and found that our feature adaptation significantly improved the accuracy of manga face clustering.	https://dl.acm.org/authorize?N675782	Koki Tsubota, Toru Ogawa, Toshihiko Yamasaki, Kiyoharu Aizawa
Adaptive O-CNN: a patch-based deep representation of 3D shapes	We present a novel framework for creating Möbius-invariant subdivision operators with a simple conversion of existing linear subdivision operators. By doing so, we create a wide variety of subdivision surfaces that have properties derived from Möbius geometry; namely, reproducing spheres, circular arcs, and Möbius regularity. Our method is based on establishing a canonical form for each 1-ring in the mesh, representing the class of all 1-rings that are Möbius equivalent to that 1-ring. We perform a chosen linear subdivision operation on these canonical forms, and blend the positions contributed from adjacent 1-rings, using two novel Möbius-invariant operators, into new face and edge points. The generality of the method allows for easy coarse-to-fine mesh editing with diverse polygonal patterns, and with exact reproduction of circular and spherical features. Our operators are in closed-form and their computation is as local as the computation of the linear operators they correspond to, allowing for efficient subdivision mesh editing and optimization.	https://dl.acm.org/authorize?N664422	Peng-Shuai Wang, Chun-Yu Sun, Yang Liu, Xin Tong
Aerial path planning for urban scene reconstruction: a continuous optimization method and benchmark	We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user's body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of 10 subjects wearing 17 IMUs for validation in 64 sequences with 330 000 time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes.	https://dl.acm.org/authorize?N664498	Neil Smith, Nils Moehrle, Michael Goesele, Wolfgang Heidrich
Aerobatics control of flying creatures via self-regulated learning	Small unmanned aerial vehicles (UAVs) are ideal capturing devices for high-resolution urban 3D reconstructions using multi-view stereo. Nevertheless, practical considerations such as safety usually mean that access to the scan target is often only available for a short amount of time, especially in urban environments. It therefore becomes crucial to perform both view and path planning to minimize flight time while ensuring complete and accurate reconstructions. In this work, we address the challenge of automatic view and path planning for UAV-based aerial imaging with the goal of urban reconstruction from multi-view stereo. To this end, we develop a novel continuous optimization approach using heuristics for multi-view stereo reconstruction quality and apply it to the problem of path planning. Even for large scan areas, our method generates paths in only a few minutes, and is therefore ideally suited for deployment in the field. To evaluate our method, we introduce and describe a detailed benchmark dataset for UAV path planning in urban environments which can also be used to evaluate future research efforts on this topic. Using this dataset and both synthetic and real data, we demonstrate survey-grade urban reconstructions with ground resolutions of 1 cm or better on large areas (30 000 m ).	https://dl.acm.org/authorize?N664496	Jungdam Won, Jungnam Park, Jehee Lee
Afterglow projection: virtual information projection method to real environment using pico projector in mixed reality	Projection-based mixed reality (MR) is a method of superimposition that projects virtual information on real environment using a projector. In previous studies, a method using a pico projector as an operation device has been proposed. However, the projection range and the continuity of information presentation are limited. In this research, we replace the projection of the pico projector with a ceiling projector installed in the environment and leave it in place. In this manner, a plurality of virtual information is realized simultaneous with the continuous presentation to the real environment.	https://dl.acm.org/authorize?N675717	Shumpei Akahoshi, Mitsunori Matsushita
Alphabet collage art generation	We propose a novel method to generate alphabet collage art from a single input image by replacing the partial curves of the image with the best-matched shape of alphabet letters. The salient structure of the image is preserved, and the contour is reconstructed with letters. In our framework, we first segment the input image into regions and extract the primary curve from each letter. Second, we analyze the structure of the region contour and the curve of a letter for finding the relationship between the salient contour in the image and the structure of the glyph of letters. We propose a modified partial curve matching to generate a stylized collage result with alphabet letters.	https://dl.acm.org/authorize?N675723	Ming-Te Chi, Hao-Hsuan Tang, Chih-Kuo Yeh, Charles Morace, Hui-Nieg Chou, Shih-Syun Lin, Tong-Yee Lee
Ambient sound propagation	Over the last two decades there has been a proliferation of methods for simulating crowds of humans. As the number of different methods and their complexity increases, it becomes increasingly unrealistic to expect researchers and users to keep up with all the possible options and trade-offs. We therefore see the need for tools that can facilitate both domain experts and non-expert users of crowd simulation in making high-level decisions about the best simulation methods to use in different scenarios. In this paper, we leverage trajectory data from human crowds and machine learning techniques to learn a manifold which captures representative local navigation scenarios that humans encounter in real life. We show the applicability of this manifold in crowd research, including analyzing trends in simulation accuracy, and creating automated systems to assist in choosing an appropriate simulation method for a given scenario.	https://dl.acm.org/authorize?N664499	Zechen Zhang, Nikunj Raghuvanshi, John Snyder, Steve Marschner
An architecture for immersive interactions with an emotional character AI in VR	When entering a virtual world, the users expect an experience that feels natural. Huge progress has been made with regards to motion, vision, physical interactivity, whereas interactivity with non-playable character stays behind. This live-demo introduces a method that leads to more aware, expressive and lively agents that can answer their own needs and interact with the player. Notably, the live-demo covers the use of an emotional component and the addition of a layer of communication (speech) to allow more immersive and interactive Als in VR.	https://dl.acm.org/authorize?N675652	Gautier Boeda, Yuta Mizuno
An innovative model for animation education in Asia: massive collaborative animation projects	Massive Collaborative Animation Projects (MCAP) are a series of innovative international and intercollegiate student productions. MCAP allows students from all over the world to collaborate beyond their institutions, work with peers in other fields and cultures, and learn animation techniques by working on large-scale productions. [Joel et al., 2018] Students participate in MCAP either through taking animation courses that include MCAP as part of the curriculum or through joining MCAP as an extracurricular activity. MCAP started in 2016 and had thus far involved more than 160 students from various schools across six regions, including Mainland China, Taiwan, Japan, and South Korea. Combining findings from interviews with MCAP participants, this poster focuses on the unique experience of Asian students working on the first MCAP project, MCAP 1, which makes a traditional 3D animated short film. It discusses the challenges, benefits, and lessons gained from working with students from different cultures, inspiring novel ways in which animation education in Asia can be conducted.	https://dl.acm.org/authorize?N675781	Rebecca Yuqi Huang, Kevin Kaiting Kao, Kosuke Kumada
An interface for post-match play-by-play analysis of a fighting game based on the two players' eye movements	"In this paper, we propose an interface to support post-match play-by-play analysis of a hand-to-hand fighting game based on the two players' eye movements. In the domain of e-Sports, the ""fighting game"" genre refers to hand-to-hand combat games in which two players fight each other by manipulating their respective martial artist characters within the same game screen. An e-Sports match, like a professional chess match, is followed by analysis and commentary about the performance of the players. In this study, we constructed an interface for visualizing information about the match based on the players' eye movements to facilitate post-match play-by-play analysis and commentary. Our interface highlights commonalities and differences in the areas on the screen where the players focus their attention, as well as commonalities and differences in the direction of their eye movements."	https://dl.acm.org/authorize?N675741	Ryohei Oda, Yuto Mizumatsu, Tomoki Kajinami
An island	An Island is an immersive short film portraying the physical and emotional journey undertaken by a lone explorer as he attempts to reach the summit of a remote island. As events unfold around him, our protagonist is confronted with the joy and agony of life, love and loss, and the wonder of the natural world.	https://dl.acm.org/authorize?N675617	Rory Byrne
Appearance capture and modeling of human teeth	This paper introduces a 3D shape generative model based on deep neural networks. A new image-like ( , tensor) data representation for genus-zero 3D shapes is devised. It is based on the observation that complicated shapes can be well represented by multiple parameterizations (charts), each focusing on a different part of the shape. The new tensor data representation is used as input to Generative Adversarial Networks for the task of 3D shape generation. The 3D shape tensor representation is based on a multi-chart structure that enjoys a shape covering property and scale-translation rigidity. Scale-translation rigidity facilitates high quality 3D shape learning and guarantees unique reconstruction. The multi-chart structure uses as input a dataset of 3D shapes (with arbitrary connectivity) and a sparse correspondence between them. The output of our algorithm is a generative model that learns the shape distribution and is able to generate novel shapes, interpolate shapes, and explore the generated shape space. The effectiveness of the method is demonstrated for the task of anatomic shape generation including human body and bone (teeth) shape generation.	https://dl.acm.org/authorize?N664412	Zdravko Velinov, Marios Papas, Derek Bradley, Paulo Gotardo, Parsa Mirdehghan, Steve Marschner, Jan Novák, Thabo Beeler
Approximate convex decomposition and transfer for animated meshes	Wire art is the creation of three-dimensional sculptural art using wire strands. As the 2D projection of a 3D wire sculpture forms line drawing patterns, it is possible to craft multi-view wire sculpture art --- a static sculpture with multiple (potentially very different) interpretations when perceived at different viewpoints. Artists can effectively leverage this characteristic and produce compelling artistic effects. However, the creation of such multi-view wire sculpture is extremely time-consuming even by highly skilled artists. In this paper, we present a computational framework for automatic creation of multi-view 3D wire sculpture. Our system takes two or three user-specified line drawings and the associated viewpoints as inputs. We start with producing a sparse set of voxels via greedy selection approach such that their projections on the virtual cameras cover all the contour pixels of the input line drawings. The sparse set of voxels, however, do not necessary form one single connected component. We introduce a constrained 3D pathfinding algorithm to link isolated groups of voxels into a connected component while maintaining the similarity between the projected voxels and the line drawings. Using the reconstructed visual hull, we extract a curve skeleton and produce a collection of smooth 3D curves by fitting cubic splines and optimizing the curve deformation to best approximate the provided line drawings. We demonstrate the effectiveness of our system for creating compelling multi-view wire sculptures in both simulation and 3D physical printouts.	https://dl.acm.org/authorize?N664434	Jean-Marc Thiery, Pooran Memari, Tamy Boubekeur
Art and technology at Pixar: SIGGRAPH ASIA 2018 course notes	"As described by this now famous quote ""The art challenges technology, and the technology inspires art"", technology has always played an important part in Pixar's movie making process. This course will show how we develop and utilize new technical advancements to tell more appealing stories, using real-world examples from our latest movie . This is a direct refresher of the previous course from Siggraph Asia 2015. Since that time, Pixar's pipeline has been heavily restructured, switching its main rendering algorithm to pathtracing. We will describe how that technology has now matured, and how we were able to reintroduce complete artistic control in this new physically based world. At the same time, the pipeline kept evolving and we introduced a new stage that didn't exist before, in the form of deep compositing. Finally we'll focus on USD, OpenSubdiv and the Hydra render engine, to showcase how the whole pipeline is moving toward real-time feedback, not only for rendering, but also for many other departments such as animation, simulation and crowds."	https://dl.acm.org/authorize?N675520	Ryusuke Villemin, Chia-Chi Hu, Sonoko Konishi, Hiroaki Narita, Magnus Wrenninge, David G Yu
Art plunge: experiencing the inner worlds of famous artworks	Art Plunge is a virtual reality experience where you can get the feeling of being transported to the inner worlds of famous paintings. We have created VR-interpretations of works like Mona Lisa, Starry Night and the Birth of Venus. We explore the concept of what a painting could be in virtual reality, and how different boundaries are blurred in the process: Boundaries between our interpretation and the original painting, between technology and artistry and between now and then.	https://dl.acm.org/authorize?N664199	Martin Eklund, Martin Christensen
Art-directed costumes at pixar: design, tailoring, and simulation in production	"Costumes are an important part of character design, acting, storytelling, and visual appeal in animation. However, it is challenging to achieve art-directed natural-looking motion and detail in CG animated clothing, due to technology, workflow, and budget constraints. This course will cover Pixar's latest approach to CG costumes, from design to tailoring to simulation, and how we try to address these challenges. Our goal is to continue working towards a balance between the detail and physicality of real costumes, and the stylized artistry and movement of 2D animated clothing. Using examples from ""Incredibles 2"", ""Coco"", and other Pixar films, we will show how our artists approach the initial costume design direction, strategically plan designs to fit within time and technology constraints, and translate drawings into 3D clothing on stylized characters. Next, we will show how we create garment models using 3D and flat-panel tailoring methods, applications for common simulation parameters and settings, and robust out-of-box simulation techniques using cloth rigging and dynamic alterations. Finally, we will cover the tools used to simulate garments in shots, create appealing shapes and movement, and help Animation let the characters act with their clothing. Although Pixar uses proprietary tools, the principles can be applied to other pipelines. Along the way, we will talk about how the tailoring and simulation teams collaborate and fit in with other departments, such as Rigging, Shading, Animation, Art, and Crowds, as well as the current state of our technology and tool set. We will cover material for all levels of experience, with backgrounds ranging from artistic to technical."	https://dl.acm.org/authorize?N675521	Aimei Kutt
As-compact-as-possible vectorization for character images	It is quite time-consuming to produce high-quality compact font libraries, especially for north-east Asia language systems that contain thousands of characters. However, existing vectorization algorithms either generate results with large storage requirements, or lose most stylish details after vectorization. To solve this problem, we propose a novel data-driven vectorization algorithm for character images to make the number of control points on vectorized contours as few as possible while preserving significant details. Experimental results demonstrate that our method clearly outperforms other state-of-the-art approaches by not only preserving most stylish features but also dramatically reducing the size of vectorized fonts.	https://dl.acm.org/authorize?N675662	Zeqing Xia, Zhouhui Lian, Yingmin Tang, Jianguo Xiao
Assassin's Creed origins cinematic trailer	Egypt is faltering under the rule of inexperienced monarchs. It's a time for the silver-tongued to attempt to take control, and for the Assassin's Brotherhood to emerge and ensure that doesn't happen.	https://dl.acm.org/authorize?N675618	István Zorkóczy
Automatic dataset generation for object pose estimation	Object pose estimation based on a RGB image is essential in accomplishing many computer vision tasks, such as augmented reality and robot vision for grasping. Using structure from motion and domain randomization, we propose a method that, from a set of images, allows us to quickly generate large datasets to train a Convolutional Neural Network (ConvNet) for object pose estimation.	https://dl.acm.org/authorize?N675783	Kalenga-Bimpambu Tshilombo, Yusuke Yoshiyasu, Antonio Gabas, Kota Suzui
Automatic generation of hair motion of 3D characters following japanese anime style	"Recent Japanese animation is progressively using three-dimensional computer graphics (3DCG). However, Japanese animation created according to the Japanese traditional method called ""limited animation"" is different from photorealistic motion of 3DCG. In particular, hair motion obtained via this method is different from that obtained by physical calculation. In this study, we formulate a method of hair motion of traditional Japanese hand-drawn animation."	https://dl.acm.org/authorize?N675743	Kenji Furukawa, Susumu Nakata
Automatic generation of interactive projection mapping for leaves	Recently, the targets of projection mapping have included various objects ranging from buildings to fish. However, it is difficult to project onto dynamic and deformable objects, such as leaves. Therefore, we propose a semi-automatic system to calculate the image registrations of projections for leaves and to interactively track the projection area. We describe our results with some animated effects on various shapes of leaves (Fig. 1).	https://dl.acm.org/authorize?N675779	Tomoki Sueyoshi, Yuki Morimoto
Automatic perforation system for korean traditional painting: Dancheong	Dancheong is designed to decorate various parts of wooden buildings with beautiful and majestic colors. The painting process involves a stage called Cheoncho, where various holes are drilled into the paper to copy distinctive design patterns drawn on the paper in which the design pattern will be applied to a building part. To perform the process of Cheoncho, a craftsman punches holes one by one on the paper with a needle and repeats this action over millions of times. In order to reduce those kinds of time consuming job, we propose a system that automatically performs Cheoncho process to assist a craftsman in copying the desired pattern to the target building part with easy and accurate manner.manner.	https://dl.acm.org/authorize?N675724	Yoon-Seok Choi, Soonchul Jung, In-Su Jang, TaeWon Choi, Jin-Seo Kim
Automatic route planning for GPS art generation	In this paper, we present a novel approach for the automated route generation of global positioning system (GPS) artwork. The term GPS artwork describes the generation of drawings by leaving virtual traces on digital maps. Until now, a creation of these images has required a manual planning phase in which the artists design the route by hand. Once the route for this artwork has been planned, GPS devices have been used to track the movement. Using the presented solution, the lengthy planning phase can be significantly shortened and art creation is open to a broader public.	https://dl.acm.org/authorize?N675682	Andre Waschk, Jens Krüger
Automatic site selection of cultural venues	Cultural venues, such as libraries, theatres, cinemas and galleries, contribute to a city's tourism and economy, and enrich the cultural life of the local residents. In this paper, we propose a novel approach to automatic site selection of cultural venues in an urban area, which requires less expertise in urban planning. The two-stage approach consists of a learning stage for predicting zones as a prior constraint, and an optimisation stage for determining the number of cultural venues and their exact locations according to multiple criteria. Given an input set of urban data, our approach generates an optimal configuration of two-dimensional locations for cultural venues that complies with land use policies and provides easy access for the public. We implemented the approach using reliable methods of deep learning and stochastic optimisation, and the results demonstrate the approach's effectiveness by a comparison to their real-world counterparts.	https://dl.acm.org/authorize?N675681	Tian Feng, Tomasz Bednarz
Automatic unpaired shape deformation transfer	"Motivated by augmented and virtual reality applications such as telepresence, there has been a recent focus in performance capture of humans under motion. However, given the real-time constraint, these systems often suffer from artifacts in geometry and texture such as holes and noise in the final rendering, poor lighting, and low-resolution textures. We take the novel approach to augment such real-time performance capture systems with a deep architecture that takes a rendering from an arbitrary viewpoint, and jointly performs completion, super resolution, and denoising of the imagery in real-time. We call this approach , and our live system ""LookinGood"". Our deep architecture is trained to produce high resolution and high quality images from a coarse rendering in real-time. First, we propose a self-supervised training method that does not require manual ground-truth annotation. We contribute a specialized reconstruction error that uses semantic information to focus on relevant parts of the subject, e.g. the face. We also introduce a of the loss function that is able to discard outliers. We specifically design the system for virtual and augmented reality headsets where the consistency between the left and right eye plays a crucial role in the final user experience. Finally, we generate temporally stable results by explicitly minimizing the difference between two consecutive frames. We tested the proposed system in two different scenarios: one involving a single RGB-D sensor, and upper body reconstruction of an actor, the second consisting of full body 360° capture. Through extensive experimentation, we demonstrate how our system generalizes across unseen sequences and subjects."	https://dl.acm.org/authorize?N664444	Hongyi Xu, Espen Knoop, Stelian Coros, Moritz Bächer
Bacchus	Alex, a young woman is increasingly getting tired of adult life. Her daily routine is restricted by norms and expectations of the modern world, and human interaction has been replaced by social media. One day, she sees Bacchus, a charismatic sensual figure, who lures her into a colorful world, which is a complete opposite of her reality. In this mysterious place, you are free to follow your instincts and explore your deepest desires.	https://dl.acm.org/authorize?N675619	Rikke Alma Krogshave Planeta
Being, nothing more	"My submission for the SIGGRAPH 2018 Art Gallery relates directly to the conference theme Crossover, and more specifically, to the sub-theme of Overlap. In the call for submissions, Overlap is described as ""A state in which two organisms connect and share common ground on contexts, medium and/or aesthetics."" My project draws connections between my body and an inanimate found rock. My project entitled ""Being, nothing more"" presents the volume of my body (i.e. 5,616.188 cubic inches found using air displacement plethysmography) reimagined as a CNC milled boulder that occupies the same amount of physical space. A small rock, only a few inches across, was 3D scanned then scaled to 5,616.188 cubic inches using Meshmixer. The mesh was then modified, sliced into 1 inch layers, and 2-sided 3D CAM was developed using Fusion 360. I used a Shopbot to mill the parts from polystyrene and reassembled the large volume. The final form was sealed with a thin coat of epoxy and the piece was painted using color-shifting auto paint."	https://dl.acm.org/authorize?N675692	Jason J. Ferguson
Bend-it: design and fabrication of kinetic wire characters	With the rising interest in personalized VR and gaming experiences comes the need to create high quality 3D avatars that are both low-cost and variegated. Due to this, building dynamic avatars from a single unconstrained input image is becoming a popular application. While previous techniques that attempt this require multiple input images or rely on transferring dynamic facial appearance from a source actor, we are able to do so using only one 2D input image without any form of transfer from a source image. We achieve this using a new conditional Generative Adversarial Network design that allows fine-scale manipulation of any facial input image into a new expression while preserving its identity. Our photoreal avatar GAN (paGAN) can also synthesize the unseen mouth interior and control the eye-gaze direction of the output, as well as produce the final image from a novel viewpoint. The method is even capable of generating fully-controllable temporally stable video sequences, despite not using temporal information during training. After training, we can use our network to produce dynamic image-based avatars that are controllable on mobile devices in real time. To do this, we compute a fixed set of output images that correspond to key blendshapes, from which we extract textures in UV space. Using a subject's expression blendshapes at run-time, we can linearly blend these key textures together to achieve the desired appearance. Furthermore, we can use the mouth interior and eye textures produced by our network to synthesize on-the-fly avatar animations for those regions. Our work produces state-of-the-art quality image and video synthesis, and is the first to our knowledge that is able to generate a dynamically textured avatar with a mouth interior, all from a single image.	https://dl.acm.org/authorize?N675557	Kai-Wen Hsiao, Jia-Bin Huang, Hung-Kuo Chu
Bilby	Threatened daily by the deadly residents and harsh environment of Australia's Outback, a lonesome Bilby finds himself an unwitting protector, and unexpected friend, to a helpless (and quite adorable) baby bird.	https://dl.acm.org/authorize?N675610	Liron Topaz
Bloeistraat 11	Inseparable best friends spend their last summer holiday of childhood amusing themselves around the house. As summer progresses their bodies start to morph and shift and an awkwardness descends on their friendship. Puberty seems determined to interrupt their bond.	https://dl.acm.org/authorize?N675611	Nienke Deutz
Blur algorithms for cartoon animation	This talk introduces specific blurring algorithms tailored for cartoon animation. With our tools, the artists create various effects as shown in Figure 1: smooth transitions from color to shadow color, moody effects and cartoon motion lines.	https://dl.acm.org/authorize?N675770	Marc Salvati, Kota Ito
Book of the dead	Year 2122. After many years of failed attempts, medical science is finally able to bring back to life a cryonised patient. The researcher who performed the procedure does not understand what made this particular attempt successful. We follow the patient's recollection of her first experiences in the Afterlife, as she is sharing them with the researcher. The first area of the Afterlife looks like a forest. It is a transitional place before the place of actual death.	https://dl.acm.org/authorize?N675613	Veselin Efremov
Booxze	Two fishermen pull a strange box out of the swamp.	https://dl.acm.org/authorize?N675612	Till Sander-Titgemeyer
Bridging reinforcement learning and creativity: implementing reinforcement learning in processing	Artists are underrepresented in the reinforcement learning (RL) community due to the steep learning curve involved in in-depth understanding of RL algorithms. However, artists can play an important role in the RL community by defining innovative problems, designing creative environments, and creating novel applications. As a popular tool for artists to experiment with programming, Processing has been highly adapted by many artists as their entry point to programming. Given the popularity of Processing in the creative community, we use this tutorial as a steppingstone to bridge RL and creativity by introducing RL core concepts in Processing. The purpose of this workshop is twofold: 1) to attract more artists to the RL community by demonstrating RL demos in their familiar IDE; 2) to demystify RL problems by implementing them in a high-level language without any external libraries. Importantly, this tutorial is not about introducing a specific programming language, but will focus on how to analyze, frame, and solve RL problems.	https://dl.acm.org/authorize?N675522	Jieliang Luo, Sam Green
Brushing element fields	Aggregate elements following certain directions have a variety of applications in graphics, design, and visualization. However, authoring oriented elements in various output domains, especially in 3D, remains challenging. We propose a novel brushing system to facilitate interactive authoring of aggregate elements with diverse properties over given output domains via an element synthesis approach. To increase output quality and reduce input workload, we further propose that can automatically orient the entire elements in better alignments over the output domains according to partially user-specified strokes. The proposed system can effectively synthesize distinct types of elements within various output domains in higher quality and efficiency and offer more user friendliness than existing practices. Our method can be applied to practical applications such as graphic design, artistic collage, and aggregate modeling.	https://dl.acm.org/authorize?N675663	Chen-Yuan Hsu, Li-Yi Wei, Lihua You, Jian Jun Zhang
C	Space coordinates offer an elegant, scalable and versatile framework to propagate (multi-)scalar functions from the boundary vertices of a 3-manifold, often called a , within its volume. These generalizations of the barycentric coordinate system have progressively expanded the range of eligible cages to triangle and planar polygon surface meshes with arbitrary topology, concave regions and a spatially-varying sampling ratio, while preserving a smooth diffusion of the prescribed on-surface functions. In spite of their potential for major computer graphics applications such as freeform deformation or volume texturing, current space coordinate systems have only found a moderate impact in applications. This follows from the constraint of having only triangles in the cage most of the time, while many application scenarios favor arbitrary (non-planar) quad meshes for their ability to align the surface structure with features and to naturally cope with anisotropic sampling. In order to use space coordinates with arbitrary quad cages currently, one must triangulate them, which results in large propagation distortion. Instead, we propose a generalization of a popular coordinate system - Mean Value Coordinates - to quad and tri-quad cages, bridging the gap between high-quality coarse meshing and volume diffusion through space coordinates. Our method can process non-planar quads, comes with a closed-form solution free from global optimization and reproduces the expected behavior of Mean Value Coordinates, namely smoothness within the cage volume and continuity everywhere. As a result, we show how these coordinates compare favorably to classical space coordinates on triangulated quad cages, in particular for freeform deformation.	https://dl.acm.org/authorize?N664423	Lingjie Liu, Nenglun Chen, Duygu Ceylan, Christian Theobalt, Wenping Wang, Niloy J. Mitra
CD prayer	The compact disk (CD) used to be a symbol of the multimedia era, but it has since become the latest out-of-date media. Most people listen to music via the Internet, not physical media. But it is quite sad to forget the media that we once loved. That's why I began to make a Buddha statue. CD Prayer is a portable CD player that prays for the obsolete media. It revives a CD as a halo of Buddha and plays hi-fi music for us. CD Prayer was created by editing the 3D data shared by Yahoo JAPAN under Creative Commons license (CC-BY-3.0). The edited 3D data is also shared at Thingiverse.com, because remix, share, and tribute are an integral part of music culture and Buddhism.	https://dl.acm.org/authorize?N675693	Yuichiro Katsumoto
Canonical Möbius subdivision	In this paper, we present a novel unsupervised learning method for pixelization. Due to the difficulty in creating pixel art, preparing the paired training data for supervised learning is impractical. Instead, we propose an unsupervised learning framework to circumvent such difficulty. We leverage the dual nature of the pixelization and depixelization, and model these two tasks in the same network in a bi-directional manner with the input itself as training supervision. These two tasks are modeled as a cascaded network which consists of three stages for different purposes. transfers the input image into multi-scale grid-structured images with different aliasing effects. associated with to synthesize pixel arts with sharp edges and perceptually optimal local structures. connects the previous network and aims to recover the pixelized result to the original image. For the sake of unsupervised learning, the mirror loss is proposed to hold the reversibility of feature representations in the process. In addition, adversarial, L1, and gradient losses are involved in the network to obtain pixel arts by retaining color correctness and smoothness. We show that our technique can synthesize crisper and perceptually more appropriate pixel arts than state-of-the-art image downscaling methods. We evaluate the proposed method with extensive experiments on many images. The proposed method outperforms state-of-the-art methods in terms of visual quality and user preference.	https://dl.acm.org/authorize?N664445	Philipp Herholz, Marc Alexa
CariGANs: unpaired photo-to-caricature translation	Mimicking natural tessellation patterns is a fascinating multi-disciplinary problem. Geometric methods aiming at reproducing such partitions on surface meshes are commonly based on the Voronoi model and its variants, and are often faced with challenging issues such as metric estimation, geometric, topological complications, and most critically, parallelization. In this paper, we introduce an alternate model which may be of value for resolving these issues. We drop the assumption that regions need to be separated by lines. Instead, we regard region boundaries as narrow bands and we model the partition as a set of smooth functions layered over the surface. Given an initial set of seeds or regions, the partition emerges as the solution of a time dependent set of partial differential equations describing concurrently evolving fronts on the surface. Our solution does not require geodesic estimation, elaborate numerical solvers, or complicated bookkeeping data structures. The cost per time-iteration is dominated by the multiplication and addition of two sparse matrices. Extension of our approach in a Lloyd's algorithm fashion can be easily achieved and the extraction of the dual mesh can be conveniently preformed in parallel through matrix algebra. As our approach relies mainly on basic linear algebra kernels, it lends itself to efficient implementation on modern graphics hardware.	https://dl.acm.org/citation.cfm?id=3295681	Yaron Lipman
Chinese sea punica granatum floral pattern synthesis	"Architectural decorative painting of floral patterns on the surface of the Chinese traditional palace buildings is a sophisticated and time-consuming process. An ancient Chinese building code, titled ""Yingzao Fashi"", describes a variety of decorative floral patterns in the Song Dynasty (960--1279). It is difficult to draw those floral patterns using existing digital vector graphics software because of their complexity. We developed a floral pattern synthesis system to ease the drawing process of decorative floral patterns. In this paper, a case study is presented to demonstrate that the proposed system is used to automatically draw the traditional Sea Punica Granatum architectural decorative patterns. The user moves the mouse cursor to draw the path of a stem. Then the leaves and flowers are synthesized. In addition, collision detection is implemented to control leaf density. Vector graphics pre-loading and threading techniques are used to achieve 0.3 ms rendering speed for user interaction."	https://dl.acm.org/authorize?N675706	Shih-Hao Liu, Tung-Ju Hsieh
City of memories	In the desert lies a mysterious city, where past lives are shown within the pink dust that fills the streets. A young orphan girl is stuck here, reliving the memory of her mother. When she makes a new friend, she is faced with a choice: to keep living in the past, or to break free and leave the city together.	https://dl.acm.org/authorize?N675614	Signe Tveitan
Cocolors	There is a world in which an endless rain of dark ash fills the sky, and covers the earth. Humanity, fearing the ash that burns and melts away human flesh, has no choice but to cover themselves in protective suits, and their faces with gigantic masks.	https://dl.acm.org/authorize?N675625	Toshihisa Yokoshima
Coded skeleton: shape changing user interface with mechanical metamaterial	"We propose a design method for fabricating a novel shape-changing user interface, called ""Coded Skeleton"", by computationally integrating actuators and sensors using a mechanical metamaterial. This design method realizes the deformation of various curves using simple expansion and contraction actuators, leveraging the fact that the Coded Skeleton is flexible in one deformation mode but stiff in other. We describe the design method and structural analysis of the mechanical metamaterial that can uniquely define deformation along with outlining the creation and control method of the Coded Skeleton using this structure. Finally, we propose three applications of the Coded Skeleton."	https://dl.acm.org/authorize?N675678	Miyu Iwafune, Taisuke Ohshima, Yoichi Ochiai
Color enhancement factors to control spectral power distribution of illumination	We introduced color enhancement factors to control the spectral power distribution of illumination, which enabled us to enhance one or more colors at once while retaining the color appearance of white. In experiments, color enhancement factors corresponding to red, green, and blue were calculated using color patches of a color chart and used for controlling a sixteen-color LED lighting system. The color chart and an old wood-block print were illuminated by the modulated light from the lighting system. By changing only three parameters, each color was enhanced continuously and independently with metameric white and the color balance under daylight maintained.	https://dl.acm.org/authorize?N675784	Masaru Tsuchida, Takahito Kawanishi, Kunio Kashino
Combining deep learning algorithm with scene recognition and haptic feedback for 4D-VR cinema	Adding multiple tactile sensations is a quick and useful method of enhancing immersion in virtual reality (VR) games by providing strong feedback to players, such as kinesthesia and cutaneous feedback. Our previous work on SoEs [Chen et al. 2016] and BoEs [Han et al. 2017] has shown the potential of using haptic feedback for VR controllers that provide feedback through heat, air, vibration, and reaction force. Additionally, our another work on AoEs [Han et al. 2017] and AoEs+ [Han et al. 2018] employed integration devices to simulate two virtual environments simultaneously in a room-scale physical space with each offering multiple tactile sensations.	https://dl.acm.org/authorize?N675795	Sheng-Fu Ko, Yi-Hung Lin, Ping-Hsuan Han, Chia-Chun Chang, Chien-Hsing Chou
Construction and fabrication of reversible shape transforms	Current AR systems only track sparse geometric features but do not compute depth for all pixels. For this reason, most AR effects are pure overlays that can never be occluded by real objects. We present a novel algorithm that propagates sparse depth to every pixel in near realtime. The produced depth maps are spatio-temporally smooth but exhibit sharp discontinuities at depth edges. This enables AR effects that can fully interact with and be occluded by the real scene. Our algorithm uses a video and a sparse SLAM reconstruction as input. It starts by estimating soft depth edges from the gradient of optical flow fields. Because optical flow is unreliable near occlusions we compute forward and backward flow fields and fuse the resulting depth edges using a novel reliability measure. We then localize the depth edges by thinning and aligning them with image edges. Finally, we optimize the propagated depth smoothly but encourage discontinuities at the recovered depth edges. We present results for numerous real-world examples and demonstrate the effectiveness for several occlusion-aware AR video effects. To quantitatively evaluate our algorithm we characterize the properties that make depth maps desirable for AR applications, and present novel evaluation metrics that capture how well these are satisfied. Our results compare favorably to a set of competitive baseline algorithms in this context.	https://dl.acm.org/authorize?N664405	Shuhua Li, Ali Mahdavi-Amiri, Ruizhen Hu, Han Liu, Changqing Zou, Oliver Van Kaick, Xiuping Liu, Hui Huang, Hao Zhang
Continuous and orientation-preserving correspondences via functional maps	Reconstructing shape and reflectance properties from images is a highly under-constrained problem, and has previously been addressed by using specialized hardware to capture calibrated data or by assuming known (or highly constrained) shape or reflectance. In contrast, we demonstrate that we can recover non-Lambertian, spatially-varying BRDFs and complex geometry belonging to any arbitrary shape class, from a single RGB image captured under a combination of unknown environment illumination and flash lighting. We achieve this by training a deep neural network to regress shape and reflectance from the image. Our network is able to address this problem because of three novel contributions: first, we build a large-scale dataset of procedurally generated shapes and real-world complex SVBRDFs that approximate real world appearance well. Second, single image inverse rendering requires reasoning at multiple scales, and we propose a cascade network structure that allows this in a tractable manner. Finally, we incorporate an in-network rendering layer that aids the reconstruction task by handling global illumination effects that are important for real-world scenes. Together, these contributions allow us to tackle the entire inverse rendering problem in a holistic manner and produce state-of-the-art results on both synthetic and real data.	https://dl.acm.org/citation.cfm?id=3295682	Joseph Terran
Creating a virtual human that visualizes skin strain distribution for apparel wearing simulation	This paper describes the first step of our research collaboration to create a virtual human that simulates strain of the skin to aid the design of comfortable, ergonomic sportswear. We apply insights from sports science and computer graphics. For the former, human's kinematic properties, such as the angular rotation for each joint or the magnitude of strain on the subject's skin, play an important role in understanding and simulating human movements. Contrarily, during the creation of computer graphics characters, artists mainly focus on a plausible appearance instead of these properties. Therefore, creating such a virtual human model poses several significant interdisciplinary challenges. We demonstrate several collaborative efforts and initial research results, focusing on how to visualize human skin strain distribution of lower body of a 3D human model. The human surface model, skin strain shader, and its rig are also briefly described. Finally, we discuss our early results and future scope of our collaborative work.	https://dl.acm.org/authorize?N675690	Akinobu Maejima, Yusuke Tonuma, Yoshiyuki Kubo, Tatsuo Yotsukura, Akihiro Ozawa, Ken Kusano, Hiroto Mori, Takehiro Tagawa
"Creating an animation in 64KB: a dive into the making of ""Immersion"""	"We will present some of the techniques used to create ""H - Immersion""."	https://dl.acm.org/authorize?N675626	Julien Guertault
CreativeAI: deep learning for graphics	In computer graphics, many traditional problems are now better handled by deep-learning based data-driven methods. In applications that operate on regular 2D domains, like image processing and computational photography, deep networks are state-of-the-art, beating dedicated hand-crafted methods by significant margins. More recently, other domains such as geometry processing, animation, video processing, and physical simulations have benefited from deep learning methods as well. The massive volume of research that has emerged in just a few years is often difficult to grasp for researchers new to this area. This tutorial gives an organized overview of core theory, practice, and graphics-related applications of deep learning.	https://dl.acm.org/authorize?N675523	Niloy J. Mitra, Iasonas Kokkinos, Paul Guerrero, Nils Thuerey, Tobias Ritschel
Crowd space: a predictive crowd analysis technique	We introduce a computational solution for cost-efficient 3D fabrication using Our key idea is to employ a set of universal blocks, which can be massively prefabricated at a low cost, to quickly assemble and constitute a significant of the target object, so that only the need to be 3D printed online. We further improve the fabrication efficiency by decomposing the residual volume into a small number of printing-friendly pyramidal pieces. Computationally, we face a problem: decomposing the input object into an internal core and residual, and decomposing the residual, to fulfill a combination of objectives for efficient 3D fabrication. To this end, we formulate an optimization that jointly minimizes the residual volume, the number of pyramidal residual pieces, and the amount of support waste when printing the residual pieces. To solve the optimization in a tractable manner, we start with a maximal internal core and iteratively refine it with local cuts to minimize the cost function. Moreover, to efficiently explore the large search space, we resort to cost estimates aided by pre-computation and avoid the need to explicitly construct pyramidal decompositions for each solution candidate. Results show that our method can iteratively reduce the estimated printing time and cost, as well as the support waste, and helps to save hours of fabrication time and much material consumption.	https://dl.acm.org/authorize?N664491	Ioannis Karamouzas, Nick Sohre, Ran Hu, Stephen J. Guy
Cutting-edge VR/AR display technologies (gaze-, accommodation-, motion-aware and HDR-enabled)	Near-eye (VR/AR) displays suffer from technical, interaction as well as visual quality issues which hinder their commercial potential. This tutorial will deliver an overview of cutting-edge VR/AR display technologies, focusing on technical, interaction and perceptual issues which, if solved, will drive the next generation of display technologies. The most recent advancements in near-eye displays will be presented providing (i) correct accommodation cues, (ii) near-eye varifocal AR, (iii) high dynamic range rendition, (iv) gaze-aware capabilities, either predictive or based on eye-tracking as well as (v) motion-awareness. Future avenues for academic and industrial research related to the next generation of AR/VR display technologies will be analyzed.	https://dl.acm.org/authorize?N675524	George-Alex Koulieris, Kaan Akşit, Christian Richardt, Rafał Mantiuk
DESIA: a general framework for designing interlocking assemblies	Holographic displays have great potential to realize mixed reality by modulating the wavefront of light in a fundamental manner. As a computational display, holographic displays offer a large degree of freedom, such as focus cue generation and vision correction. However, the limited bandwidth of spatial light modulator imposes an inherent trade-off relationship between the field of view and eye-box size. Thus, we demonstrate the first practical eye-box expansion method for a holographic near-eye display. Instead of providing an intrinsic large exit-pupil, we shift the optical system's exit-pupil to cover the expanded eye-box area with pupil-tracking. For compact implementation, a pupil-shifting holographic optical element (PSHOE) is proposed that can reduce the form factor for exit-pupil shifting. A thorough analysis of the design parameters and display performance are provided. In particular, we provide a comprehensive analysis of the incorporation of the holographic optical element into a holographic display system. The influence of holographic optical elements on the intrinsic exit-pupil and pupil switching is revealed by numerical simulation and Wigner distribution function analysis.	https://dl.acm.org/authorize?N664406	Ziqi Wang, Peng Song, Mark Pauly
DT-Zheng: digital twin method for Zheng musical instrument	Zheng is one of the most representative plucked-stringed musical instruments in East Asia over three thousand years as illustrated in Figure 1. Up to now, although it is still popular, there are several factors blocking its spreading. The musical instrument is precious and not suitable to carry along. Moreover, it is hard to learn on one own.	https://dl.acm.org/authorize?N675719	Ning Xie, Xinrui Cai, Sipei Li, Yifan Lu, Mingyue Lou, Heng Tao Shen
Danswing papers	It is known that a virtual static object on a display is perceived as moving when the object, to which the particular pattern of luminance contours is given, is presented against the background with dynamic luminance change. The present study reports a novel technique in which printed objects having particular luminance contours apparently move against a background with dynamic luminance modulations. Our technique can illusorily express not only horizontal translations but also the expansion-compression and rotation of the objects. We believe that our technique can be used to highlight static objects in paper advertisements by placing the objects against a background with dynamic luminance changes.	https://dl.acm.org/authorize?N675771	Takahiro Kawabe
Data stones	A mountain and a stone: I sit atop a mountain with a pencil and paper, drawing a mountain peak across the valley. With pen and paper I attempt to understand the temporal relationships between the mountain and the river. The revealing and concealing of rock by vegetation is the exploitation of deposition in defiance of gravity and entropy. A mountain is rarely denuded, it conceals and reveals according to these ongoing geological conversations. The stone in a garden has been removed from one conversation and appropriated into another. Removed from geological processes, it becomes a projection for people unable to talk in geological time. Mundane on its own terms, the stone is appropriated into metaphysics, and contemplated like the life of a star. It is an object for speculation, not for dissection. Data stones: I offer a composition of stones, conceived in the spirit of the human/mountain, the appropriation of the mundane, and projecting onto random noise. These stones are produced procedurally from the mundane dialogue accumulated by the everyday use of instant messaging. I download every conversation I have had with one person, and sort the messages using Latent Dirichlet Allocation (LDA). The LDA gives an abstract order to sentiments and conversations that never had any order. It is another type of noise to generate another type of stone for speculation and projection. I visualise this information by making procedurally generated stones, each representing one half of the conversation. The stone is treated like a graph, where the statistical patterns my sentiments determines its shape. The stones are accompanied by a musical composition that reveals fragments of the processed conversation (composed by Tom Smith). I offer these stones for contemplation and speculation as objects of data collection, surveillance and big data processing, as well as objects of random noise.	https://dl.acm.org/authorize?N675694	Peter Nelson
Deep blending for free-viewpoint image-based rendering	For a given PDE problem, three main factors affect the accuracy of FEM solutions: basis order, mesh resolution, and mesh element quality. The first two factors are easy to control, while controlling element shape quality is a challenge, with fundamental limitations on what can be achieved. We propose to use -refinement (increasing element degree) to decouple the approximation error of the finite element method from the domain mesh quality for elliptic PDEs. Our technique produces an accurate solution even on meshes with badly shaped elements, with a slightly higher running time due to the higher cost of high-order elements. We demonstrate that it is able to automatically adapt the basis to badly shaped elements, ensuring an error consistent with high-quality meshing, without any per-mesh parameter tuning. Our construction reduces to traditional fixed-degree FEM methods on high-quality meshes with identical performance. Our construction decreases the burden on meshing algorithms, reducing the need for often expensive mesh optimization and automatically compensates for badly shaped elements, which are present due to boundary constraints or limitations of current meshing methods. By tackling mesh generation and finite element simulation jointly, we obtain a pipeline that is both more efficient and more robust than combinations of existing state of the art meshing and FEM algorithms.	https://dl.acm.org/authorize?N675564	Qingnan Fan, Jiaolong Yang, David Wipf, Baoquan Chen, Xin Tong
Deep incremental learning for efficient high-fidelity face tracking	A variety of structures in nature exhibit sparse, thin, and intricate features. It is challenging to investigate these structural characteristics using conventional numerical approaches since such features require highly refined spatial resolution to capture and therefore they incur a prohibitively high computational cost. We present a novel computational framework for high-resolution topology optimization that delivers leaps in simulation capabilities, by two orders of magnitude, from the state-of-the-art approaches. Our technique accommodates computational domains with over one billion grid voxels on a single shared-memory multiprocessor platform, allowing automated emergence of structures with both rich geometric features and exceptional mechanical performance. To achieve this, we track the evolution of thin structures and simulate its elastic deformation in a dynamic narrow-band region around high-density sites to avoid wasted computational effort on large void regions. We have also designed a mixed-precision multigrid-preconditioned iterative solver that keeps the memory footprint of the simulation to a compact size while maintaining double-precision accuracy. We have demonstrated the efficacy of the algorithm through optimizing a variety of complex structures from both natural and engineering systems.	https://dl.acm.org/authorize?N664441	Adrien Poulenard, Maks Ovsjanikov
Deep inertial poser: learning to reconstruct human pose from sparse inertial measurements in real time	We provide the first large dataset of human fixations on physical 3D objects presented in varying viewing conditions and made of different materials. Our experimental setup is carefully designed to allow for accurate calibration and measurement. We estimate a mapping from the pair of pupil positions to 3D coordinates in space and register the presented shape with the eye tracking setup. By modeling the fixated positions on 3D shapes as a probability distribution, we analysis the similarities among different conditions. The resulting data indicates that salient features depend on the viewing direction. Stable features across different viewing directions seem to be connected to semantically meaningful parts. We also show that it is possible to estimate the gaze density maps from view dependent data. The dataset provides the necessary ground truth data for computational models of human perception in 3D.	https://dl.acm.org/authorize?N664490	Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J. Black, Otmar Hilliges, Gerard Pons-Moll
Deep learning-based super-resolution for digital comics	We investigate deep learning-based super-resolution of digital comic images. Techniques effective for comic images and a new efficient model are presented.	https://dl.acm.org/authorize?N675796	Jun-Hyuk Kim, Jun-Ho Choi, Choong-Hyun Seo, Jaehyuk Chang, Jong-Seok Lee
Deep motifs and motion signatures	We study a new and elegant instance of geometric dissection of 2D shapes: dissection, which corresponds to a between two shapes where one of them can be dissected in its interior and then inverted , with hinges on the shape boundary, to reproduce the other shape, and vice versa. We call such a transform or RIOT. Since it is rare for two shapes to possess even a rough RIOT, let alone an exact one, we develop both a RIOT construction algorithm and a quick filtering mechanism to pick, from a shape collection, potential shape pairs that are likely to possess the transform. Our construction algorithm is It computes an RIOT between two given input 2D shapes, whose boundaries can undergo slight deformations, while the filtering scheme picks good inputs for the construction. Furthermore, we add properly designed hinges and connectors to the shape pieces and fabricate them using a 3D printer so that they can be played as an assembly puzzle. With many interesting and fun RIOT pairs constructed from shapes found online, we demonstrate that our method significantly expands the range of shapes to be considered for RIOT, a seemingly impossible shape transform, and offers a practical way to construct and physically realize these transforms.	https://dl.acm.org/authorize?N664492	Andreas Aristidou, Daniel Cohen-Or, Jessica K. Hodgins, Yiorgos Chrysanthou, Ariel Shamir
Deep part induction from articulated object pairs	We introduce C F , the first approach for high quality scanning of thin structures at interactive rates using a handheld RGBD camera. Thin filament-like structures are mathematically just 1D curves embedded in R , and integration-based reconstruction works best when depth sequences (from the thin structure parts) are fused using the object's (unknown) curve skeleton. Thus, using the complementary but noisy color and depth channels, C F first automatically identifies point samples on potential thin structures and groups them into , each being a group of a fixed number of aligned consecutive frames. Then, the algorithm extracts per-bundle skeleton curves using L axes, and aligns and iteratively merges the L segments from all the bundles to form the final complete curve skeleton. Thus, unlike previous methods, reconstruction happens via integration along a , i.e., the extracted curve skeleton. We extensively evaluate C F on a range of challenging examples, different scanner and calibration settings, and present high fidelity thin structure reconstructions previously just not possible from raw RGBD sequences.	https://dl.acm.org/authorize?N664414	Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, Leonidas Guibas
Deep unsupervised pixelization	Delaunay meshes (DM) are a special type of manifold triangle meshes --- where the local Delaunay condition holds everywhere --- and find important applications in digital geometry processing. This paper addresses the general DM simplification problem: given an arbitrary manifold triangle mesh with vertices and the user-specified resolution (< ), compute a Delaunay mesh with vertices that has the least Hausdorffdistance to To solve the problem, we abstract the simplification process using a 2D Cartesian grid model, in which each grid point corresponds to triangle meshes with a certain number of vertices and a simplification process is a monotonic path on the grid. We develop a novel differential-evolution-based method to compute a low-cost path, which leads to a high quality Delaunay mesh. Extensive evaluation shows that our method consistently outperforms the existing methods in terms of approximation error. In particular, our method is highly effective for small-scale CAD models and man-made objects with sharp features but less details. Moreover, our method is fully automatic and can preserve sharp features well and deal with models with multiple components, whereas the existing methods often fail.	https://dl.acm.org/authorize?N675551	Menghan Xia, Xueting Liu, Tien-Tsin Wong
DeepFocus: learned image synthesis for computational displays	Imagine taking a selfie video with your mobile phone and getting as output a 3D model of your head (face and 3D hair strands) that can be later used in VR, AR, and any other domain. State of the art hair reconstruction methods allow either a single photo (thus compromising 3D quality) or multiple views, but they require manual user interaction (manual hair segmentation and capture of fixed camera views that span full 360°). In this paper, we describe a system that can completely automatically create a reconstruction from any video (even a selfie video), and we don't require specific views, since taking your -90°, 90°, and full back views is not feasible in a selfie capture. In the core of our system, in addition to the automatization components, hair strands are estimated and deformed in 3D (rather than 2D as in state of the art) thus enabling superior results. We provide qualitative, quantitative, and Mechanical Turk human studies that support the proposed system, and show results on a diverse variety of videos (8 different celebrity videos, 9 selfie mobile videos, spanning age, gender, hair length, type, and styling).	https://dl.acm.org/authorize?N664415	Lei Xiao, Anton Kaplanyan, Alexander Fix, Matthew Chapman, Douglas Lanman
DeepLens: shallow depth of field from a single image	We propose a GPU algorithm that computes a 3 Voronoi diagram. Our algorithm is tailored for applications that solely make use of the geometry of the Voronoi cells, such as Lloyd's relaxation used in meshing, or some numerical schemes used in fluid simulations and astrophysics. Since these applications only require the geometry of the Voronoi cells, they do not need the combinatorial mesh data structure computed by the classical algorithms (Bowyer-Watson). Thus, by exploiting the specific spatial distribution of the point-sets used in this type of applications, our algorithm computes each cell independently, in parallel, based on its nearest neighbors. In addition, we show how to compute integrals over the Voronoi cells by decomposing them on the fly into tetrahedra, without needing to compute any global combinatorial information. The advantages of our algorithm is that it is fast, very simple to implement, has constant memory usage per thread and does not need any synchronization primitive. These specificities make it particularly efficient on the GPU: it gains one order of magnitude as compared to the fastest state-of-the-art multi-core CPU implementations. To ease the reproducibility of our results, the full documented source code is included in the supplemental material.	https://dl.acm.org/authorize?N675552	Minchen Li, Danny M. Kaufman, Vladimir G. Kim, Justin Solomon, Alla Sheffer
Deer calling	This string instrument generates its tone through the tailoring of strings along the unique shape of a deer's antlers. Its tone may resound the howling roar of the very lord of nature. The antlers came from a deer who was hunted down by a local hunter and bequeathed to me. In Japan, one often hears of how damaging deer can be, but they are one of god's messengers.	https://dl.acm.org/authorize?N675605	Yuto Hasebe
Demo of olfactory display with less residual odor	An olfactory display is a device which provides various scents to a user. e integration of such olfactory stimulus with conventional VR system will strongly influence human emotion and create a more immersive experience. One of the main issues accompanied by the implementations is that the odorants spread into the ambient air and it makes the user confuse what smell is presented at that time. We tried to solve this with an innovative and simple design concept of the olfactory display: installation of the air intake and inner deodorant filter. Based on the method, we include olfaction in a VR game to provide the remarkable sensation to a user with clear and various smells presentation.	https://dl.acm.org/authorize?N675750	Shingo Kato, Takamichi Nakamoto
Depth from motion for smartphone AR	We present a virtual reality display that is capable of generating a dense collection of depth/focal planes. This is achieved by driving a focus-tunable lens to sweep a range of focal lengths at a high frequency and, subsequently, tracking the focal length precisely at microsecond time resolutions using an optical module. Precise tracking of the focal length, coupled with a high-speed display, enables our lab prototype to generate 1600 focal planes per second. This enables a novel first-of-its-kind virtual reality multifocal display that is capable of resolving the vergence-accommodation conflict endemic to today's displays.	https://dl.acm.org/authorize?N664408	Julien Valentin, Adarsh Kowdle, Jonathan T. Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, Joao Afonso, Jose Pascoal, Konstantine Tsotsos, Mira Leung, Mirko Schmidt, Onur Guleryuz, Sameh Khamis, Vladimir Tankovitch, Sean Fanello, Shahram Izadi, Christoph Rhemann
Diet gamification toward chewing amount control via head mounted display	In this paper, for the purpose of preventing obesity, we developed a chewing amount control system which adds visual information and auditory information at meal. By using this system, it was suggested that the quality of chewing was improved and the sense of fullness was promoted.	https://dl.acm.org/authorize?N675718	Yuto Sugita, Keiichi Zempo, Yasumasa Ando, Yuya Kakutani, Koichi Mizutani, Naoto Wakatsuki
Differentiable Monte Carlo ray tracing through edge sampling	Transferring deformation from a source shape to a target shape is a very useful technique in computer graphics. State-of-the-art deformation transfer methods require either point-wise correspondences between source and target shapes, or pairs of deformed source and target shapes with corresponding deformations. However, in most cases, such correspondences are not available and cannot be reliably established using an automatic algorithm. Therefore, substantial user effort is needed to label the correspondences or to obtain and specify such shape sets. In this work, we propose a novel approach to automatic deformation transfer between two unpaired shape sets without correspondences. 3D deformation is represented in a high-dimensional space. To obtain a more compact and effective representation, two convolutional variational autoencoders are learned to encode source and target shapes to their latent spaces. We exploit a Generative Adversarial Network (GAN) to map deformed source shapes to deformed target shapes, both in the latent spaces, which ensures the obtained shapes from the mapping are indistinguishable from the target shapes. This is still an under-constrained problem, so we further utilize a reverse mapping from target shapes to source shapes and incorporate cycle consistency loss, i.e. applying both mappings should reverse to the input shape. This VAE-Cycle GAN (VC-GAN) architecture is used to build a reliable mapping between shape spaces. Finally, a similarity constraint is employed to ensure the mapping is consistent with visual similarity, achieved by learning a similarity neural network that takes the embedding vectors from the source and target latent spaces and predicts the light field distance between the corresponding shapes. Experimental results show that our fully automatic method is able to obtain high-quality deformation transfer results with unpaired data sets, comparable or better than existing methods where strict correspondences are required.	https://dl.acm.org/authorize?N664430	Benedikt Bitterli, Srinath Ravichandran, Thomas Müller, Magnus Wrenninge, Jan Novák, Steve Marschner, Wojciech Jarosz
Digital actor: Albert Einstein	A series of short films featuring a digital actor with the likeness of Albert Einstein. The results are an artistic interpretation of Albert Einstein reappearing in contemporary context citing some of his famous quotes. This homage to the physicist and humanist further investigates how documentary film formats can extend their horizon by meaningful inclusion of digital actors. The creation process relied on a set of specialized tools which reduced the labor effort significantly. Digital assets have been released under Creative Commons to support the ongoing effort in creating convincing digital characters. https://go.animationsinstitut.de/einstein	https://dl.acm.org/authorize?N675627	Leszek Plichta
Digital being: TV Being-005	"""DIGITAL BEING"" is an invisible and formless creature born from detritus of discarded and forgotten technologies after digital switchover. It reveals itself through an atypical movement and an interaction according to the machinery that it dominates."	https://dl.acm.org/authorize?N675606	Taezoo Park, Joohee Park
Digital reproduction of hair waving based on animator technique	Even in animation production, which was once a hand-drawn creation, it has been increased speed and reduced costs by applying NPR technology. For example, there are researches and products for generating animations of waving hairs from a single image in which a still scene of the animated characters are drawn. However, animations created by previous methods often provide a feeling of strangeness compared to the hand-drawn animations. Because, in the previous method, tequniques of experienced animators are not considered, a huge amount of time and high skill are necessary to make the generated scene closer to the hand-drawn animation. Therefore, we developed a tool that can reproduce the tequniques used by animators to discribe the waving behavior of hair, for the purpose of generating an animation without the strangeness. The parameters need for our method can be automatically determined by using a kind of genetic algorithm.	https://dl.acm.org/authorize?N675740	Naoaki Kataoka, Tomokazu Ishikawa, Yusuke Kameda, Ichiro Matsuda, Susumu Itoh
Directional heat map generation with saccade information	Heat map is an important tool for eye tracking data analysis and visualization. It is very intuitive to express the area watched by the observer, but conventional methods ignore saccade information that express gaze shifts. Based on the conventional method, this paper presents a novel heat map generation method for eye tracking data. Proposed method uses a mixed data structure of fixation points and saccades, and also adds heat map deformation calculation with saccade type data. The proposed method has advantages on indicating gaze transition path while also visualizing gaze region. Related applications will benefit from this work.	https://dl.acm.org/authorize?N675733	Yaodong Li, Zhuo Yang, Yinwei Zhan, Yongqiang Li, Gang He
Directional lightmap encoding insights	Lightmaps that respond to normal mapping are commonly used in video games. This short paper describes a novel parameterization of a standard lightmap encoding, Ambient Highlight Direction (AHD) --- a model for directional irradiance consisting of ambient and directional light --- that eliminates common interpolation artifacts. It also describes a technique for fitting the AHD model to lighting represented as spherical harmonics, where the unknown model parameters are solved in the null space of the constraint that irradiance is preserved.	https://dl.acm.org/authorize?N675679	Peter-Pike Sloan, Ari Silvennoinen
Dunhuang mural restoration using deep learning	As time goes by, the art pieces inside Dunhuang Grottoes have suffered from tremendous damage such as mural deterioration, and they are usually difficult to be repaired. Although we can achieve digital preservation by modeling the caves and preserving mural as textures in virtual environment, we still cannot have a glimpse of how the grottoes look like without damage. In this work, we propose a systematic restoration framework, which is based on Generative Adversarial Network (GAN) technique, for these high-resolution but deteriorated mural textures. The main idea is to make the machine learn the transformation between deteriorated mural textures and restored mural textures. However, the resolution of training texture images (i.e. 8192×8192) is too high to be applied with GAN technology directly due to GPU RAM limitation. Instead, our method restores a set of high-resolution yet color-inconsistent textures patch-by-patch and a set of low-resolution but color-consistent full textures, and then combines them to get the final high-resolution and color-consistent result.	https://dl.acm.org/authorize?N675680	Han-Lei Wang, Ping-Hsuan Han, Yu-Mu Chen, Kuan-Wen Chen, XinYi Lin, Ming-Sui Lee, Yi-Ping Hung
Dynamical optimal transport on discrete surfaces	We propose a workflow for spectral reproduction of paintings, which captures a painting's spectral color, invariant to illumination, and reproduces it using multi-material 3D printing. We take advantage of the current 3D printers' capabilities of combining highly concentrated inks with a large number of layers, to expand the spectral gamut of a set of inks. We use a data-driven method to both predict the spectrum of a printed ink stack and optimize for the stack layout that best matches a target spectrum. This bidirectional mapping is modeled using a pair of neural networks, which are optimized through a problem-specific multi-objective loss function. Our loss function helps find the best possible ink layout resulting in the balance between spectral reproduction and colorimetric accuracy under a multitude of illuminants. In addition, we introduce a novel spectral vector error diffusion algorithm based on combining color contoning and halftoning, which simultaneously solves the layout discretization and color quantization problems, accurately and efficiently. Our workflow outperforms the state-of-the-art models for spectral prediction and layout optimization. We demonstrate reproduction of a number of real paintings and historically important pigments using our prototype implementation that uses 10 custom inks with varying spectra and a resin-based 3D printer.	https://dl.acm.org/authorize?N675567	Christian Schumacher, Jonas Zehnder, Moritz Bächer
Eclipse	Eclipse is a 1 minute continuously looped animation in which overlapping moire patterns create a visual and sonic eclipse. The work intentionally changes depending on the format, scale and streaming limitations, and attempts to address the aesthetics of compression and how technology delimits visual and sonic experience.	https://dl.acm.org/authorize?N675607	Lee Arnold
Edible projection mapping	This installation will exhibit dynamic projection mapping on a pancake with an edible retroreflector as an optical marker. Visitors will be able to see a projected character welcoming and attracting them depends on the position of the pancake that they are holding.	https://dl.acm.org/authorize?N675751	H. Oku, M. Nomura, K. Shibahara, A. Obara
Efficient light field computation for view range expansion using viewpoint reduction	In this paper, we present an improved light field display method with wider view range. In our system, two stacked transparent LCDs are used for glasses-free light field display. They modulate the uniform backlight which enters the observer's eyes to approximate the desired light field. The patterns displayed on the LCDs are optimized according to the target light field using an algorithm based on nonnegative matrix factorization (NMF). In order to achieve wider view range and reduce the computational complexity of the pattern optimization, we utilize a small number of sampling views of the light field. To properly choose the subset of the sampling views, a stochastic sampling algorithm is adopted. The effectiveness of the proposed method is demonstrated by the experimental results, and similar light field display results can be generated with reduced sampling views.	https://dl.acm.org/authorize?N675697	Jianhong Han, Ke Wang, Jie Feng, Bingfeng Zhou
Eholo glass: electroholography glass. a lensless approach to holographic augmented reality near-eye display	We present a design and rendering method for large eye-box, fully parallax, depth of field included near-eye augmented reality (AR) display. As developments in AR progress, field of view and sense of depth are one of the most crucial factors for rendering convincing virtual objects into real environments. We propose computer generated holography (CGH) that is able to reconstruct image with real world depth of field faithfully as rendering method. Previous studies have proposed various near-eye optic design such as the use of beamsplitter and Holographic Optical Element with 4f lens system. However pure beamsplitter design suffers from the narrow field of view while 4f lens system has lens aberration as well as minimal focusing issues that leads to smaller eyebox. Having a wide field of view that matches our eyes is crucial for having an immersive experience and often narrow field of view may even leads to nausea and negative impacts on comfortability. We propose a design that utilizes a Dihedral Corner Reflector Array and a novel beamsplitter embedded optics as our eyepiece. Our primary contribution is having a reasonably large eyebox while maintaining the simple optical design as well as rendering of virtual objects with depth of field in real time without any special optics or moving parts.	https://dl.acm.org/authorize?N675698	Chun Wei Ooi, Naoya Muramatsu, Yoichi Ochiai
Enhancement techniques for human anatomy visualization	"We propose two simple and efficient visualization techniques for assisting understanding of complex three-dimensional structures like human anatomy: (1) applying Screen Space Ambient Occlusion (SSAO), Depth of Field (DoF), and Depth Cueing (DC) to an original rendering result image in real-time, and (2) adding ""caps"" to thin polygonal tube structures which results in pseudo-thick, hollow structures with a small amount of additional polygons."	https://dl.acm.org/authorize?N675734	Hirofumi Seo, Takeo Igarashi
Error estimation for many-light rendering with supersampling	Many-light rendering unifies the computation of various visual and illumination effects, which include anti-aliasing, depth of field, volumetric scattering, and subsurface scattering, into a simple direct illumination computation from many virtual point lights (VPLs). As a naive approach that sums the direct illumination from a large number of VPLs is computationally expensive, scalable methods cluster VPLs and estimate the sum by sampling a small number of VPLs for efficient computation. Although scalable methods have achieved significant speed-ups, they cannot control the error owing to clustering, resulting in noise in the rendered images. In this paper, we propose a method to improve the estimation accuracy for many-light rendering of such visual and illumination effects. We demonstrate that our method can improve the estimation accuracy for various visual and illumination effects up to 2.3 times compared with the previous method.	https://dl.acm.org/authorize?N675674	Hirokazu Sakai, Kosuke Nabata, Shinya Yasuaki, Kei Iwasaki
Evaluation of reducing three-dimensionality of movement to create 3DCG animation looks more like 2D animation	Recently, 2D animation (referring to Japanese animation in this paper) images have usually been created from 3DCG. However, even if 3DCG looks like 2D animation, the movements of the objects still seem like that of 3DCG. One of the reasons is that 3DCG movement has three-dimensionality. It was assumed that the frame rate and the change of object size according to the depth coordinates give three-dimensionality to 3DCG. Therefore, a subjective evaluation was conducted to determine whether 3DCG looks like 2D animation when the three-dimensionality caused by frame rate and the change of size according to the depth coordinates to change field of view (FoV) and projection method was decreased. As a result, it is possible to reduce the three-dimensionality, and it could be confirmed that the movement of 3DCG was more similar to that of 2D animation.	https://dl.acm.org/authorize?N675772	Kei Kitahata, Yuji Sakamoto
Eve of dust	Eve of Dust is a collaborative performance and installation between a human and a robot. The artwork draws on both the possibilities and anxieties arising from the collaboration between humans and emerging intelligent systems personified in the robot. The artwork uses a Sawyer collaborative robot, an articulated 7-jointed robot arm that somewhat resembles a snake. The robot is able to be used in close proximity to humans, unlike most industrial robots, and will stop before causing physical harm. This enables human partners to physically interact with the robot to co-create a performance of dance and music.	https://dl.acm.org/authorize?N675608	John McCormick, Adam Nash, Stephanie Hutchison
F	A common operation in geometry processing is solving symmetric and positive semi-definite systems on a subset of a mesh, with conditions for the vertices at the boundary of the region. This is commonly done by setting up the linear system for the sub-mesh, factorizing the system (potentially applying preordering to improve sparseness of the factors), and then solving by back-substitution. This approach suffers from a comparably high setup cost for each local operation. We propose to reuse factorizations defined on the full mesh to solve linear problems on sub-meshes. We show how an update on sparse matrices can be performed in a particularly efficient way to obtain the factorization of the operator on a sun-mesh significantly outperforming general factor updates and complete refactorization. We analyze the resulting speedup for a variety of situations and demonstrate that our method outperforms factorization of a new matrix by a factor of up to 10 while never being slower in our experiments.	https://dl.acm.org/authorize?N664424	Ruizhen Hu, Cheng Wen, Oliver Van Kaick, Luanmin Chen, Di Lin, Daniel Cohen-Or, Hui Huang
FIST: a fast, implicit model of the human hand with semi-anatomical structures	There are many opportunities to draw human hands in computer graphics. The motion of internal organs within a human hand has a non-negligible effect on the natural change in the appearance of the hand's surface. In this work, we propose a method for expressing this change interactively with the use of an implicit model of a human hand that has semi-anatomical structures. The model is referred to as In the model, bones are modeled anatomically based on computed tomography imaging, while soft tissues are modeled artificially. It can be controlled only by specifying the angles of the joints. The proposed method can contribute to a compelling expression of the dynamism in such hand motions as grasping, pinching, and scratching in immersive virtual reality and games.	https://dl.acm.org/authorize?N675691	Masato Nakada, Hélène Ballet, Issei Fujishiro
Fabricable eulerian wires for 3D shape abstraction	Image smoothing represents a fundamental component of many disparate computer vision and graphics applications. In this paper, we present a unified unsupervised (label-free) learning framework that facilitates generating flexible and high-quality smoothing effects by directly learning from data using deep convolutional neural networks (CNNs). The heart of the design is the training signal as a novel energy function that includes an edge-preserving regularizer which helps maintain important yet potentially vulnerable image structures, and a spatially-adaptive flattening criterion which imposes different forms of regularization onto different image regions for better smoothing quality. We implement a diverse set of image smoothing solutions employing the unified framework targeting various applications such as, image abstraction, pencil sketching, detail enhancement, texture removal and content-aware image manipulation, and obtain results comparable with or better than previous methods. Moreover, our method is extremely fast with a modern GPU (e.g, 200 fps for 1280×720 images).	https://dl.acm.org/citation.cfm?id=3295680	Jun-Yan Zhu
FacePush: experiencing pressure forces on face with HMDs	Simulated haptics is a key component to enhance immersion in virtual environments. Previous research has proposed various mechanisms to generate different haptic feedbacks. While many were deployed on limbs e.g., through wearable interfaces or handheld controllers, recent researches started to explore displaying haptics directly through HMDs. Examples include thermal [Peiris et al. 2017], vibrotactile [de Jesus Oliveira et al. 2017] and force [Gugenheimer et al. 2016] feedbacks. We present FacePush, a pulley system incorporated with a HMD to display pressure forces on user face. Unlike GyroVR [Gugenheimer et al. 2016] which enabled tangential/rotational force on user head, FacePush's pulley system creates normal force on the face region covered by the HMD. The main concept of FacePush is shifting torque provided by the two motors to a normal force pushing on face. As displayed in Figure 1. This normal force triggered by the motors pushes the HMD into user face resulting in pressure feedbacks on the face.	https://dl.acm.org/authorize?N675752	Hong-Yu Chang, Wen-Jie Tseng, Chia-En Tsai, Hsin-Yu Chen, Roshan Lalintha Peiris, Liwei Chan
Factor once: reusing cholesky factorizations on sub-meshes	Once a color image is converted to grayscale, it is a common belief that the original color cannot be fully restored, even with the state-of-the-art colorization methods. In this paper, we propose an innovative method to synthesize It is a grayscale image that can fully restore its original color. The key idea here is to encode the original color information into the synthesized grayscale, in a way that users cannot recognize any anomalies. We propose to learn and embed the color-encoding scheme via a convolutional neural network (CNN). It consists of an encoding network to convert a color image to grayscale, and a decoding network to invert the grayscale to color. We then design a loss function to ensure the trained network possesses three required properties: (a) color invertibility, (b) grayscale conformity, and (c) resistance to quantization error. We have conducted intensive quantitative experiments and user studies over a large amount of color images to validate the proposed method. Regardless of the genre and content of the color input, convincing results are obtained in all cases.	https://dl.acm.org/authorize?N664447	Paulo Gotardo, Jérémy Riviere, Derek Bradley, Abhijeet Ghosh, Thabo Beeler
Fast depth densification for occlusion-aware augmented reality	Streaming high quality rendering for virtual reality applications requires minimizing perceived latency. We introduce Shading Atlas Streaming (SAS), a novel object-space rendering framework suitable for streaming virtual reality content. SAS decouples server-side shading from client-side rendering, allowing the client to perform framerate upsampling and latency compensation autonomously for short periods of time. The shading information created by the server in object space is temporally coherent and can be efficiently compressed using standard MPEG encoding. Our results show that SAS compares favorably to previous methods for remote image-based rendering in terms of image quality and network bandwidth efficiency. SAS allows highly efficient parallel allocation in a virtualized-texture-like memory hierarchy, solving a common efficiency problem of object-space shading. With SAS, untethered virtual reality headsets can benefit from high quality rendering without paying in increased latency.	https://dl.acm.org/authorize?N664409	Aleksander Holynski, Johannes Kopf
Fast raycasting using a compound deep image for VPL range determination	The concept of using multiple deep images has been explored, under a variety of different names, as a possible acceleration approach for finding ray-geometry intersections. We leverage recent advances in deep image processing from Order Independent Transparency ( ) for fast building of a Compound Deep Image ( ) using a coherent memory format well suited for raycasting. We explore the use of a CDI and raycasting for the problem of determining distance between Virtual Point Lights ( ) and geometry for indirect lighting, with the raycasting step being a small fraction of total frametime.	https://dl.acm.org/authorize?N675685	Jesse Archer, Geoff Leach, Pyarelal Knowles
FingertipCubes: an inexpensive D.I.Y wearable for 6-DoF per fingertip pose estimation using a single RGB camera	It is natural to use our hands for interacting with a virtual world, but this remains to be widely available. The Leap Motion controller has brought 3D hand tracking to consumers, but its high cost prohibits its mass adoption, especially for users in developing countries. To facilitate mass adoption, we present a do-it-yourself wearable that has a material cost of only 1 US Dollar, and which coupled with a webcam, can provide 6-DoF(degrees of freedom) per fingertip tracking in real-time. We also propose a novel solution to the pose ambiguity problem of a single square planar fiducial marker in monocular view.	https://dl.acm.org/authorize?N675797	Ojaswi Gupta, Ramya Hebbalaguppe
First-person-view drone flying in mixed reality	This paper presents a study aimed at creating an aerial mixed reality environment by using a first-person-view drone. We develop a drone with a stereoscopic camera based on human binocular vision. Then, we present a novel mixed reality environment with bidirectional interaction between the real and virtual worlds. Our approach is effective to perceive the depth of obstacles and provides a safe and exciting environment for drone flying.	https://dl.acm.org/authorize?N675710	Dong-Hyun Kim, Yong-Guk Go, Soo-Mi Choi
FiveStar VR: shareable travel experience through multisensory stimulation to the whole body	We have developed a multisensory virtual reality system, FiveStar VR (five senses theater for VR), that enables participants to relive or share other's behavior through well-designed simultaneous stimulation to multiple modalities. FiveStar VR consists of somatosensory displays in addition to the conventional audiovisual VR setup. In the FiveStar VR, the body parts of the participant are forced to move, which is synchronized with those of an avatar in the VR space, resulting in inducing a strong perception of presence at the past walk behavior of someone else. By taking advantage of the cyclic nature of walking, the arms, the lower limbs, and the body are synchronously moved to simulate the sensation of real walking. These motion profiles do not completely follow the measured data of real walking but each gain of magnitude of the modalities is adjusted on the basis of the subjective intensity of motion impression, mainly due to the lack of the sensory suppression from the motor command of the participant. The demonstration of our exhibition booth presents a virtual trip to a tourist site, Toronto and Niagara Falls, Canada. A short-time experience of walking around the area is relived/shared by the attendee.	https://dl.acm.org/authorize?N664190	Koichi Shimizu, Gaku Sueta, Kentaro Yamaoka, Kazuki Sawamura, Yujin Suzuki, Keisuke Yoshida, Vibol Yem, Yasushi Ikei, Tomohiro Amemiya, Makoto Sato, Koichi Hirota, Michiteru Kitazaki
FlexMaps: computational design of flat flexible shells for shaping 3D objects	Single image superresolution has been a popular research topic in the last two decades and has recently received a new wave of interest due to deep neural networks. In this paper, we approach this problem from a different perspective. With respect to a downsampled low resolution image, we model a high resolution image as a combination of two components, a deterministic component and a stochastic component. The deterministic component can be recovered from the low-frequency signals in the downsampled image. The stochastic component, on the other hand, contains the signals that have little correlation with the low resolution image. We adopt two complementary methods for generating these two components. While generative adversarial networks are used for the stochastic component, deterministic component reconstruction is formulated as a regression problem solved using deep neural networks. Since the deterministic component exhibits clearer local orientations, we design novel loss functions tailored for such properties for training the deep regression network. These two methods are first applied to the entire input image to produce two distinct high-resolution images. Afterwards, these two images are fused together using another deep neural network that also performs local statistical rectification, which tries to make the local statistics of the fused image match the same local statistics of the groundtruth image. Quantitative results and a user study indicate that the proposed method outperforms existing state-of-the-art algorithms with a clear margin.	https://dl.acm.org/authorize?N675558	Chu Han, Qiang Wen, Shengfeng He, Qianshu Zhu, Yinjie Tan, Guoqiang Han, Tien-Tsin Wong
Flow zone: a cross-modal music creation VR experience to induce flow	Many people want to live a happy and fulfilling life, yet finding this positive sense of well-being is often quite a challenge. Flow is a wonderfully powerful experience that not only feels amazing in the moment but actually improves a person's general sense of well-being as well. It seems like a great answer to this problem, but unfortunately the complex concoction of parameters necessary to enter flow prevent the experience from occurring regularly. Flow Zone aims to lower the barrier to entry with its sophisticated design tailored to maximize potential for flow. VR was used as the medium to create an immersive environment that simultaneously removes distractions and focuses the player's attention on the task at hand. The enhanced immersiveness of cross-modality combined with game design elements centered around creative expression through music creates a streamlined pathway to the flow state.	https://dl.acm.org/authorize?N664191	Tanner Person, Benjamin Outram, Kouta Minamizawa
Flycon: real-time environment-independent multi-view human pose estimation with aerial vehicles	sounds arise from a massive superposition of chaotic events distributed over a large area or volume, such as waves breaking on a beach or rain hitting the ground. The directionality and loudness of these sounds as they propagate in complex 3D scenes vary with listener location, providing cues that distinguish indoors from outdoors and reveal portals and occluders. We show that ambient sources can be approximated using an ideal notion of spatio-temporal incoherence and develop a lightweight technique to capture their global propagation effects. Our approach precomputes a single FDTD simulation using a sustained source signal whose phase is randomized over frequency and source extent. It then extracts a spherical harmonic encoding of the resulting steady-state distribution of power over direction and position in the scene using an efficient flux density formulation. The resulting parameter fields are smooth and compressible, requiring only a few MB of memory per extended source. We also present a fast binaural rendering technique that exploits phase incoherence to reduce filtering cost.	https://dl.acm.org/authorize?N664497	Tobias Nägeli, Samuel Oberholzer, Silvan Plüss, Javier Alonso-Mora, Otmar Hilliges
FlyingHand: extending the range of haptic feedback on virtual hand using drone-based object recognition	This paper presents a Head Mounted Display (HMD) integrated system, that uses a drone and a virtual hand to help the users explore remote environment. The system allows the users to use hand gestures to control the drone and identify the Objects of Interest (OOI) through tactile feedback. The system uses a Convolutional Neural Network to perform object classification with the drone captured image and provides a virtual hand to realize interaction with the object. Accodingly, tactile feedback is also provided to users' hands to enhance the virtual hand body ownership. The system aims to help users assess space and objects regardless of body limitations, which could not only benefit elderly or handicapped people, but make potential contributions in environment measurement and daily life as well.	https://dl.acm.org/authorize?N675695	Tinglin Duan, Parinya Punpongsanon, Daisuke Iwai, Kosuke Sato
FrankenGAN: guided detail synthesis for building mass models using style-synchonized GANs	Many geometric quantities can be computed efficiently for convex meshes. For general meshes, methods for approximate convex decomposition have been developed that decompose a static, non-convex object into a small set of approximately convex parts. The convex hulls of those parts can then be used as a piecewise convex approximation to the original mesh. While previous work was only concerned with static meshes, we present a method for decomposing animated 3D meshes into temporally coherent approximately convex parts. Given a mesh and several training frames---that is, different spatial configurations of its vertices---we precompute an approximate convex decomposition that is independent of any specific frame. Such a decomposition can be transferred in real-time to novel, unseen frames. We apply our method to a variety of pre-animated meshes as well as a 3D character interactively controlled by a user's body pose. We further demonstrate that our method enables real-time physics simulations to interact with animated meshes.	https://dl.acm.org/authorize?N664421	Tom Kelly, Paul Guerrero, Anthony Steed, Peter Wonka, Niloy J. Mitra
Free-viewpoint synthesis over panoramic images	In the world of virtual reality, the task of constructing a playback data set to navigate through a scene has traditionally required a particularly inefficient procedure. The conventional method of taking pictures and videos with a pinhole camera model is costly due to the slow run time and memory space required. We propose a method which takes advantage of a less costly setup and improves the visual quality of the final images. This method allows users to choose the desired viewpoint, as well as whether the output should be computed as a panoramic or perspective image. This entire procedure consists of four steps: structure from motion (SfM), image rectification and depth estimation, 3D reconstruction, and view synthesis.	https://dl.acm.org/authorize?N675855	Tze-How Liew, Yueh-Chun Lai, Hong-Shiang Lin, Sun-Yu Gordon Chi, Ming Ouhyoung
Fukushima nuclear plant as a synthetic learning environment	A Virtual Reality project of the Fukushima Dai'ichi nuclear power plant was designed, modelled and programmed as a Synthetic Learning Environment (SLE) for education purposes, motivated by the fact that the disaster of March 2011 revealed much about Japan's lack of preparedness for nuclear accidents. An iterative process of design, make, share and reflect was adopted by the student developers. In Japan, the creative process is termed TKF: Tsukutte つくって; Katatte かたって; Furikaeru ふりかえる.	https://dl.acm.org/authorize?N675711	Michael Vallance, Yuto Kurashige, Takurou Magaki
GAN with autoencoder and importance sampling	Deep generative model such as generative adversarial networks (GAN) has shown impressive achievements in computer graphics applications. GAN is trained to learn the distribution of target data and is able to generate new samples similar to the original target data. However, most GAN based networks encounter mode collapse problem resulting in the generation of samples only from a single or a few modes of target data distribution. In order to address mode collapse problem, we propose to adopt autoencoder to learn target data distribution in encoded space. An importance sampling scheme is used to collect fake and real data samples in the encoded latent space and calculate the similarity of two distributions in real data space. Experimental evaluation compared to state-of-the-art method on synthetic and MNIST datasets shows the potential of our approach in reducing mode collapse problem and generating samples from diverse aspect of target data.	https://dl.acm.org/authorize?N675798	Gahye Lee, Seungkyu Lee
GPU optimization of material point methods	Microfacet-based reflection models are the most common way to represent reflection from rough surfaces. However, a major current limitation of these models is that they only account for single scattering. Unfortunately, single scattering models do not preserve energy. In this paper, we develop a microfacet BRDF for specular v-grooves that includes multiple scattering. Our approach is based on previous work by Zipin, who showed that the number of reflections inside a specular v-groove is bounded and analytically computable. Using his insight, we present a closed form solution for the BRDF and its probability density function (PDF); we also present a method for importance sampling the BRDF. As a result, our BRDF can be easily used within a path-traced rendering system such as PBRT. The model supports any microfacet distribution function, and spatially-varying surface roughness. The images produced by the model have a pleasing appearance compared to traditional single-scattering models.	https://dl.acm.org/authorize?N675561	Danhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan Taylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, Shahram Izadi, Andrea Tagliasacchi
GPU-based large-scale scientific visualization	Overview of modern GPU techniques for large-scale visualization • Focus on volume data Out-of-core techniques leveraging modern GPU features • Virtual texturing approaches Display-Aware, Remote and Web-Based Visualization	https://dl.acm.org/authorize?N675537	Johanna Beyer, Markus Hadwiger
Game AI techniques from algorithmic approach to machine learning	Since 2004, I have been developing game AI for many titles in AAA titles: • Chrome Hounds (Xbox360®) • Demon's Souls (PS3®) • Armored Core V (Xbox360®/PS3®) • Final Fantasy XIV: A Realm Reborn • Final Fantasy XV	https://dl.acm.org/authorize?N675536	Youichiro Miyake, Kazuko Manabe
Game changer	"A macho toy arcade prize is quick to judge a young girl who wants to win him, and goes on a life changing journey in attempt to stop her from winning enough tickets and taking him home. Game Changer was made to encourage a discussion around the gender coding that children are exposed to through toys and clothes. Through the film, I hope to promote the idea that toys have no gender, and that kids should be able to play with whatever toys they are interested in, not having to deal with labels like ""boys toys"" or ""girl toys""."	https://dl.acm.org/authorize?N675628	Aviv A. Mano
Games in concert: collaborative music making in virtual reality	"Over the last two years, the Games in Concert project explored the possibilities and implications of collaborative artistic creation of music in virtual reality (VR). Therefore a multiuser VR environment and three virtual instruments have been designed to create, shape and experience sound in various ways. These Are: : The artist can literally paint music in the 3D space. : The artist can add and customize sounding tree-like objects. : An external input device was used to explore the possibility to incorporate and visualize non-VR instruments in a VR space. Additionally, we built a stage setup to test the impact of a VR concert on an audience. The musicians embed the artistic content directly within the virtual environment. The spectator is free to explore their work independently, having his/her own visual and auditory perspective. Also, spectators can closely observe what the artists are creating in real-time. They are standing with them, in a manner of speaking, on the (virtual) stage. To be able to present the spectacle to a larger audience we introduced the ""Game Jockey"". Acting as the intermediary between the artists inside and the public outside the virtual environment, his in-game view and hearing are rendered on a large screen and multiple speakers, providing a visual experience with surround sound."	https://dl.acm.org/authorize?N664192	Simon Pfafff, Olav Lervik, Reto Spoerri, Eleonora Berra, Margarete Jahrmann, Martin Neukom
Generating 360 outdoor panorama dataset with reliable sun position estimation	A large dataset of outdoor panoramas with ground truth labels of sun position (SP) can be a valuable training data for learning outdoor illumination. In general, the sun position (if exists) in an outdoor panorama corresponds to the pixel with highest luminance and contrast with respect to neighbor pixels. However, both image-based estimation and manual annotation can not obtain reliable SP due to complex interplay between sun light and sky appearance. Here, we present an efficient and reliable approach to estimate a SP from an outdoor panorama with accessible metadata. Specifically, we focus on the outdoor panoramas retrieved from Google Street View and leverages built-in metadata as well as a well-established Solar Position Algorithm to propose a set of candidate SPs. Next, a custom made luminance model is used to rank each candidate and a confidence metric is computed to effectively filter out trivial cases (e.g., cloudy day, sun is occluded). We extensively evaluated the efficacy of our approach by conducting an experimental study on a dataset with over 600 panoramas.	https://dl.acm.org/authorize?N675799	Shih-Hsiu Chang, Ching-Ya Chiu, Chia-Sheng Chang, Kuo-Wei Chen, Chih-Yuan Yao, Ruen-Rone Lee, Hung-Kuo Chu
Generating anime-like face images from projected 3D models	In this paper, we propose a method that allows users to deform 3D facial model projection images with intuitive parameters for the axes of feature spaces that represent the style used for drawing faces for anime. The spaces are derived from hand-drawn and 3DCG face images with Non-negative Matrix Factorization. The results show typical anime-style features are applied successfully to 3DCG face images.	https://dl.acm.org/authorize?N675773	Keisuke Yamakawa, Suguru Saito
Generating effect animation with conditional GANs	Effect animations are used in many application, such as music videos or games. So, there are many templates of effects in softwares. However, it is difficult for an amateur to make an original animation. In this paper, we propose a deep learning based approach for generating an effect animation. This approach uses a next-frame prediction model which is conditional Generative Adversarial Networks (cGAN) [Goodfellow et al. 2014] and let users make a new animation easily by preparing for referenced effect videos. On the contrary, users can make the animation that it is difficult for even professional designers to make. The model is trained by loss between a ground-truth frame and a predicted frame, and the trained model can repeatedly predict the next frame with generated frames in order to make animation video. In experiments, we show several results and that we can partly control what frames are generated.	https://dl.acm.org/authorize?N675774	Naofumi Akimoto, Hirokatsu Kataoka, Yoshimitsu Aoki
Gesture recognition of air-tapping and its application to character input in VR space	The motion of the finger is made up of a combination of forearm part (extrinsic) muscles and hand part (intrinsic) muscles. We have created a wearable fingerless glove controller to sense sEMG(surface Electromyography) from intrinsic muscles using dry electrodes [Tsuboi et al. 2017]. Recognition of air-tapping gesture with a sensor attached to wearable finger-less glove controller is a challenging problem. In this study, we focused on motion recognition of air-tapping and performed motion recognition using CNN and evaluated its accuracy. As a result, the accuracy in intra-subject identification was 85.05%. Also, experiments are currently being conducted in anticipation of character input in VR space [Grubert et al. 2018]. Character input experiment in VR space was carried out using sEMG wearable fingerless glove controller, as a primitive experiment of the use of sEMG glove in VR space. Based on the results, we discussed the efficiency of character input using sEMG glove in VR space.	https://dl.acm.org/authorize?N675712	Mamoru Hirota, Ayumu Tsuboi, Masayuki Yokoyama, Masao Yanagisawa
Gill+Man: breathing through gills experience system	We propose the gill-breathing simulation system Gill+Man. The system presents the sense of breathing through gills like a fish. The Gill+Man system comprises three devices, namely a breath-sensing device, swallowing sense presenting device, and gill sense presenting device. These devices use simple stimulation and combine to produce the sense of having gills.	https://dl.acm.org/authorize?N675753	Izumi Mizoguchi, Takahiro Ando, Mizuki Nagano, Ryota Shijo, Sho Sakurai, Koichi Hirota, Takuya Nojima
Global-optimization-based model decomposition for support-free multi-DOF 3D printing	In 3D printing, using supporting materials as few as possible is critical for efficiency and material saving. Although both model partition methods and 3D printers with multi-DOF(degree of freedom) have been developed independently to tackle the problem, only a few existing approaches combined partition methods with multi-DOF printers. We present a global-optimization-based model decomposition method for multi-DOF 3D printers to achieve consistent printing with the least supporting materials. We solve the minimization of the surface area that need supporting where the printing sequence is determined inherently by global single-objective optimization. We first describe the printing constraints for using a multi-DOF 3D printer, then propose an optimization framework that meets the constraints. Experiments show the merits of our method.	https://dl.acm.org/authorize?N675707	Lifang Wu, Yisong Gao, Miao Yu, Meng Jian, Zechao Liu, Yupeng Guan
Global-to-local generative model for 3D shapes	We present in this paper a generic and parameter-free algorithm to efficiently build a wide variety of optical components, such as mirrors or lenses, that satisfy some light energy constraints. In all of our problems, one is given a collimated or point light source and a desired illumination after reflection or refraction and the goal is to design the geometry of a mirror or lens which transports exactly the light emitted by the source onto the target. We first propose a general framework and show that eight different optical component design problems amount to solving a equation that involves the computation of We then show that these diagrams all have the same structure and can be obtained by intersecting a 3D Power diagram with a planar or spherical domain. This allows us to propose an efficient and fully generic algorithm capable to solve these eight optical component design problems. The support of the prescribed target illumination can be a set of directions or a set of points located at a finite distance. Our solutions satisfy design constraints such as convexity or concavity. We show the effectiveness of our algorithm on simulated and fabricated examples.	https://dl.acm.org/authorize?N664429	Hao Wang, Nadav Schor, Ruizhen Hu, Haibin Huang, Daniel Cohen-Or, Hui Huang
GoThro: optical transfer of camera viewpoint using retro-transmissive optical system	Recent advances in computational photography have enabled the creation of images that contain additional attributes. However, capturing images of objects concealed behind obstructions or out of a camera's field of view is a challenge. We designed an optical transformation system that utilizes a conventional camera, a concave lens, and Micro-Mirror Array Plates (MMAPs) to enable images to be captured through small holes in walls or other obstructions. Our experimental prototype demonstrated that it was possible to capture images of the area on the other side of a wall through a 3-mm hole. Our system could be used to capture images from places difficult to position a camera, such in rubble in disaster areas.	https://dl.acm.org/authorize?N675736	Yudai Niwa, Hajime Kajita, Naoya Koizumi, Takeshi Naemura
God of War 4 TV commercial	Unit Image partnered up with BBH, Sony & Santa Monica Studio to create this tv commercial for the latest opus in the blockbuster franchise, God of War 4. The studio worked from storyboard to final execution of the film in which we follow Kratos getting a second chance at being a father for Atreus, and where they both face unexpected threats.	https://dl.acm.org/authorize?N675629	Léon Bérelle
Gourmet photography dataset for aesthetic assessment of food images	In this study, we present the Gourmet Photography Dataset (GPD), which is the first large-scale dataset for aesthetic assessment of food photographs. We collect 12,000 food images together with human-annotated labels (i.e., aesthetically positive or negative) to build this dataset. We evaluate the performance of several popular machine learning algorithms for aesthetic assessment of food images to verify the effectiveness and importance of our GPD dataset. Experimental results show that deep convolutional neural networks trained on GPD can achieve comparable performance with human experts in this task, even on unseen food photographs. Our experiments also provide insights to support further study and applications related to visual analysis of food images.	https://dl.acm.org/authorize?N675687	Kekai Sheng, Weiming Dong, Haibin Huang, Chongyang Ma, Bao-Gang Hu
Grand bassin	An afternoon at the swimming pool.	https://dl.acm.org/authorize?N675620	Héloïse Courtois, Chloé Plat Victori Jalabert, Adèle Raigneau
Grands canons	A brush makes watercolors appear on a white sheet of paper. An everyday object takes shape, drawn with precision by an artist's hand. Then two, then three, then four... Superimposed, condensed, multiplied, thousands of documentary drawings in successive series come to life on the screen, composing a veritable visual symphony of everyday objects. The accumulation, both fascinating and dizzying, takes us on a trip through time.	https://dl.acm.org/authorize?N675621	Alain Biet
HBG: humans, beasts and ghosts	Inspired by the works of the Chinese literary master Qian Zhongshu, HBG: Humans, Beasts and Ghosts is an experimental video game, a playable life simulator in which the player, assuming the role of God, facilitates the everyday being of the different worlds inhabited by humans, beasts and ghosts. The player can choose to act as an interventionist God, creating and destroying whimsically, or simply watching the worlds unfold with minimal or no interference.	https://dl.acm.org/authorize?N675609	Yuk Yiu Ip
Hair modeling from a single anime-style image	Recently, research of modeling realistic hair from a real-life portrait image has achieved extraordinary results. Modeling cartoon hair from an anime-style image, in contrast, has not drawn much attention from researchers. Yeh et al. [2015] proposed a 2.5D approach which determines a layering order and estimates hidden portions of hair segments from an anime-style image. However, their results are only suitable for a single view angle. In this work, we propose a novel approach to model cartoon hair from an anime-style image which simulates an artist's workflow and has a broader range of view angles than the method proposed by Yeh et al. [2015].	https://dl.acm.org/authorize?N675708	Charles C. Morace, Feng-Wei Wu, Chih-Kuo Yeh, Chia-Hsiang Chen, I-Cheng Yeh, Tong-Yee Lee
Hands-on: rapid interactive application prototyping for media arts and stage performance and beyond	We complement the last three editions of the course at SIGGRAPH Asia (2015, 2016) and SIGGRAPH (2017) to make it more of a hands-on nature and include OpenISS. We explore a rapid prototyping of interactive graphical applications for stage and beyond using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with various devices. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the (ISS) v2 and its OpenISS core.	https://dl.acm.org/authorize?N675538	Serguei A. Mokhov, Miao Song, Sudhir P. Mudur, Peter Grogono
Hap-link: wearable haptic device on the forearm that presents haptics sensations corresponding to the fingers	We developed a device that presents the haptic sensation of the fingertip to the forearm rather than to the fingertip as a new haptic presentation method for objects in a virtual reality environment. The device adopts a five-bar linkage mechanism and a Peltier element and presents the strength and direction of a force, vibration and the thermal sensation to the forearm. Compared with a fingertip-mounted display, it is possible to address issues of weight and size that hinder the free movement of fingers. Users can feel differences in texture and hardness/softness of objects, and experiences in the virtual reality environment are better than those without haptics cues even though haptics information is not directly presented to the fingertip.	https://dl.acm.org/authorize?N675754	Taha Moriyama, Nishi Ayaka, Takuto Nakamura, Vibol Yem, Hiroyuk Kajimoto
HapTwist: creating interactive haptic proxies in virtual reality using low-cost twistable artefacts	In recent years, virtual reality (VR) with head-mounted displays is gaining an increasing amount of attention in the consumer market, with highly realistic visual and audio contents. However, it is still challenging to use these VR devices with pre-fabricated shapes of controllers to simulate realistic haptic/kinesthetic information (i.e. the shape, the size, and the weight) of the virtual objects.	https://dl.acm.org/authorize?N664194	Kening Zhu, Taizhou Chen, Shaoyu Cai, Feng Han, Yi-Shiun Wu
Haptopus: haptic VR experience using suction mechanism embedded in head-mounted display	Along with the spread of VR experiences using low-cost head-mounted displays (HMDs), many proposals have been made to improve the VR experience by providing tactile information to the fingertips. However, attaching a device to fingertips has issues such as difficulty in attaching and detaching and hindering free movement of fingers. To address these issues, many methods have been proposed to incorporate a haptic presentation mechanism in an HMD, but only for presenting passive tactile information of the face or whole body. To present the tactile sensation of the fingertips in a configuration that can be incorporated in an HMD, we developed a skin-suction mechanism called Haptopus to simulate the pressure applied to multiple fingers. Haptopus can express the sense of fingers touching virtual objects by presenting corresponding suction pressure around the eyes.	https://dl.acm.org/authorize?N664193	Takayuki Kameoka, Yuki Kon, Hiroyuki Kajimoto
Head-tracked off-axis perspective projection improves gaze readability of 3D virtual avatars	Virtual avatars have been employed in many contexts, from simple conversational agents to communicating the internal state and intentions of large robots when interacting with humans. Rarely, however, are they employed in scenarios which require non-verbal communication of spatial information or dynamic interaction from a variety of perspectives. When presented on a flat screen, many illusions and visual artifacts interfere with such applications, which leads to a strong preference for physically-actuated heads and faces. By adjusting the perspective projection used to render 3D avatars to match a viewer's physical perspective, they could provide a useful middle ground between typical 2D/3D avatar representations, which are often ambiguous in their spatial relationships, and physically-actuated heads/faces, which can be difficult to construct or impractical to use in some environments. A user study was conducted to determine to what extent a head-tracked perspective projection scheme was able to mitigate the issues in readability of a 3D avatar's expression or gaze target compared to use of a standard perspective projection. To the authors' knowledge, this is the first user study to perform such a comparison, and the results show not only an overall improvement in viewers' accuracy when attempting to follow the avatar's gaze, but a reduction in spatial biases in predictions made from oblique viewing angles.	https://dl.acm.org/authorize?N675696	Tamas Bates, Jens Kober, Michael Gienger
Hessian-based robust ray-tracing of implicit surfaces on GPU	In recent years, the Ray Tracing of Implicit Surfaces on a GPU has been studied by many researchers. However, the existing methods have challenges that mainly includes solving for self-intersecting surfaces. General solutions for Ray Tracing suffer from the problem of false roots, and robust solutions are hard to generalize. In this paper, we present a robust algorithm based on Extended Taylor-Test Adaptive Marching Points, which allows a robust rendering of Self-Intersecting Implicit Surfaces on a GPU. We are using the Second Order Taylor Series expansion to alleviate the problem of double-roots in Self-Intersecting Implicit Surfaces. Our approach is simple to implement and is based on the Hessian Matrix of the Implicit Surface that can be attributed to the Hessian Matrix can be used to obtain second-order Taylor Series expansion for the univariate ray-equation. We compare our results using the simulated ground-truth with the smallest step-size possible with the proposed algorithm, and our proposed algorithm gives the best visual results as well as highest SSIM percentage than other approaches.	https://dl.acm.org/authorize?N675673	Jag Mohan Singh, Pankaj Wasnik, Raghavendra Ramachandra
High performance city rendering in Vulkan	City scale scenes often contain large amounts of geometry and texture that cannot altogether fit on GPU memory. Our ongoing work seek to minimise texture memory usage by streaming only view-relevant textures and to improve rendering performance using parallel opportunities offered by Vulkan, the latest generation of graphics API. Our result presents a high performance rendering of a city with streaming textures after CPU and GPU culling.	https://dl.acm.org/authorize?N675745	Alex Zhang, Kan Chen, Henry Johan, Marius Erdt
Historical streetscape simulation system that reflects changes in weather, time, and seasons	In this study, we developed a historical streetscape simulation system for local areas. In recent years, the loss or replacement of regional history and culture has become a pertinent issue in Japan owing to urbanization, depopulation, declining birthrate, and aging population. Based on the cultural property law, measures are being taken to conserve and use cultural properties depending on characteristics such as tangible cultural property, intangible cultural property, folk cultural property, monument, cultural landscape, and traditional building group. However, with changes in social environments, the historical culture of an area that was previously cultivated and conveyed through the long history of local residents has now become difficult to inherit. In particular, things that have not been designated as cultural properties get buried or lost in society. To mitigate this problem, this study focuses on the historical cultural landscape of local areas and develops a landscape simulation system for communicating and inheriting the same in an easy-to-understand manner. Attempts have previously been made to reproduce historical landscapes by real-time rendering with 3D computer graphics (CG) and virtual reality technology [Fukuda et al. 2015; Boeykens. 2011]. However, owing mainly to hardware limitations for drawing, these systems focused on single buildings; they did not reproduce city-level dynamics. Meanwhile, studies were conducted to achieve urban-scale reproductions [Dylla et al. 2008; Jacobson. 2005]. Because such large-scale developments are costly, only famous places are typically selected as target areas. In a previous study, we have already developed a historical landscape simulation system for the streetscape of the late Edo period. This system was installed as a permanent exhibition at regional museums, where it has been running stably. In the present study, we aim to extend the scale of this system in response to user requests and feedback.	https://dl.acm.org/authorize?N675794	Yasuo Kawai, Natsumi Kobayashi, Ayaka Enzaka
Holographic near-eye display with expanded eye-box	Addressing vergence-accommodation conflict in head-mounted displays (HMDs) requires resolving two interrelated problems. First, the hardware must support viewing sharp imagery over the full accommodation range of the user. Second, HMDs should accurately reproduce retinal defocus blur to correctly drive accommodation. A multitude of HMDs have been proposed, with three architectures receiving particular attention: varifocal, multifocal, and light field displays. These designs all extend depth of focus, but rely on computationally expensive rendering and optimization algorithms to reproduce accurate defocus blur (often limiting content complexity and interactive applications). To date, no unified framework has been proposed to support driving these emerging HMDs using commodity content. In this paper, we introduce , a generic, end-to-end convolutional neural network designed to efficiently solve the full range of computational tasks for accommodation-supporting HMDs. This network is demonstrated to accurately synthesize defocus blur, focal stacks, multilayer decompositions, and multiview imagery using only commonly available RGB-D images, enabling real-time, near-correct depictions of retinal blur with a broad set of accommodation-supporting HMDs.	https://dl.acm.org/authorize?N664400	Changwon Jang, Kiseung Bang, Gang Li, Byoungho Lee
Hop step sing: nozokanaide naked heart	"""Hop Step Sing!"" is a group of girls aiming to be the world's top VR idols! ""Nozokanaide Naked Heart"" is their fourth VR music video. This work makes use of the unique interactive characteristics of VR, where the user can not only watch the group singing and dancing, but also perform activities and actions with the characters---such as giving them a high-five---and enjoy the experience of being in the same space as the virtual idols!"	https://dl.acm.org/authorize?N675756	Hidekazu Iwama, Hiroshi Chida, Kenji Ishimaru
Hors piste	The two best rescue workers of the region are ready for their new mission. Despite their professionalism and their determination, it will not go as planned ...	https://dl.acm.org/authorize?N675622	Léo Brunei, Loris Cavalier, Camille Jalabert, Oscar Mallet
How to paint your rainbow	As soon as the man passes away, he goes through various surreal experiences that lead him to become a beautiful rainbow in the end. This film with full of different shapes and forms of life wouldn't provide one single conclusion to the audience. We are encouraged to watch the film numerous times and find new elements, questions and answers every time. On a side note, 'How to paint your Rainbow' is a sequel to Erick's previous film 'How to eat your Apple (2012)'.	https://dl.acm.org/authorize?N675623	Erick Oh
Human-computer interaction by voluntary vergence control	Most people can voluntarily control vergence eye movements. However, the interaction possibility of using vergence as an active input remain largely unexplored. We present a novel human-computer interaction technique which allows a user to control the depth position of an object based on voluntary vergence of the eyes. Our technique is similar to the mechanism for seeing the intended 3D image of an autostereogram, which requires cross-eyed or walleyed viewing. We invite the user to look at a visual target that is mounted on a linear motor, then consciously control the eye convergence to focus at a point in front of or behind the target. A camera is used to measure the eye convergence and control the motion of the linear motor dynamically based on the measured distance. Our technique can enhance existing eye-tracking methods by providing additional information in the depth dimension, and has great potential for hands-free interaction and assistive applications.	https://dl.acm.org/authorize?N675713	Lingyan Ruan, Bin Chen, Miu-Ling Lam
Human-marionette interaction puppetry using mechanical arm and L-shaped screen	In this paper, we propose a human-marionette interaction system to enhance the interactivity of marionette show. The proposed system is composed of a mechanical arm, an L-shaped screen, a Kinect, a computer, and audio equipment. Using gesture recognition and voice recognition, this system is designed to recognize the audience's gestures and voice to control the marionette and the audiovisual effects of stage. Our system enables audience to enjoy a personalized human-marionette interaction puppetry and transform their role into performer.	https://dl.acm.org/authorize?N675714	Mengwei Lin, Junfeng Yao, Yingying She, Chao Gao, Jin Chen
Hunting out graphic images from real images using recurrent neural network and extended principal color components	With recent graphics technology creates surprisingly realistic contents, most of such artificial creatures help immersive virtual experience. On the other hand, still human can recognize whether an observed visual information is real or graphic model. In this work, we propose a deep learning based graphic and real image classification method to hunt out a graphic image from real images. In order to employ a deep learning approach, we have built graphic-real image data set consists of around 25K images. Quantitative classification and qualitative graphic image hunting results are presented that helps interesting applications such as fake image detection or image realism enhancement.	https://dl.acm.org/authorize?N675790	Gahye Lee, Seungkyu Lee
Hybridizing education of both video games and animated films	Our animation program is a relatively small program that uses large-group projects to teach students. For 15 years, the undergraduate seniors have grouped together each year to create a single large-group animated short film, a consistently successful educational experience leading to solid foundational knowledge, successful hires, and yearly top awards. Six years ago, some of our students approached us wanting to create video games instead of films. This raised the question: can we teach both film and games without compromising the success and educational value that has come from focusing only on animated film? After some experience, our answer was yes. Though films and games have significant differences, we are still able to create film and video games within a program. Here we address how to overlap similarities and approach differences in combining the teaching of film and games.	https://dl.acm.org/authorize?N675675	Seth Holladay, Parris Egbert
I-cloth: incremental collision handling for GPU-based interactive cloth simulation	We introduce SCORES, a for Our network takes as input sets of parts from two or more source 3D shapes and a rough initial placement of the parts. It outputs an optimized part structure for the composed shape, leading to high-quality geometry construction. A unique feature of our composition network is that it is not merely learning how to connect parts. Our goal is to produce a coherent and 3D shape, despite large incompatibilities among the input parts. The network may significantly alter the geometry and structure of the input parts and a novel shape structure based on the inputs, while adding or removing parts to minimize a structure plausibility loss. We design SCORES as a network. During encoding, the input parts are recursively grouped to generate a root code. During synthesis, the root code is decoded, recursively, to produce a new, coherent part assembly. Assembled shape structures may be novel, with little global resemblance to training exemplars, yet have plausible substructures. SCORES therefore learns a hierarchical based on per-node losses. It is trained on structured shapes from ShapeNet, and is applied iteratively to reduce the plausibility loss. We show results of shape composition from multiple sources over different categories of man-made shapes and compare with state-of-the-art alternatives, demonstrating that our network can significantly expand the range of composable shapes for assembly-based modeling.	https://dl.acm.org/authorize?N664419	Min Tang, tongtong wang, Zhongyuan Liu, Ruofeng Tong, Dinesh Manocha
Image smoothing via unsupervised learning	We propose a technique to simulate granular materials that exploits the dual strengths of discrete and continuum treatments. Discrete element simulations provide unmatched levels of detail and generality, but prove excessively costly when applied to large scale systems. Continuum approaches are computationally tractable, but limited in applicability due to built-in modeling assumptions; e.g., models suitable for granular flows typically fail to capture clogging, bouncing and ballistic motion. In our hybrid approach, an oracle dynamically partitions the domain into continuum regions where safe, and discrete regions where necessary. The domains overlap along transition zones, where a Lagrangian dynamics mass-splitting coupling principle enforces agreement between the two simulation states. Enrichment and homogenization operations allow the partitions to evolve over time. This approach accurately and efficiently simulates scenarios that previously required an entirely discrete treatment.	https://dl.acm.org/authorize?N675577	Jianchao Tan, Jose Echevarria, Yotam Gingold
Inexact descent methods for elastic parameter optimization	Microfacet theory concisely models light transport over rough surfaces. Specular reflection is the result of single mirror reflections on each facet, while exact computation of multiple scattering is either neglected, or modeled using costly importance sampling techniques. Practical but accurate simulation of multiple scattering in microfacet theory thus remains an open challenge. In this work, we revisit the traditional V-groove cavity model and derive an analytical, cost-effective solution for multiple scattering in rough surfaces. Our model is made up of both real and virtual V-grooves, and allows us to calculate higher-order scattering in the microfacets in an analytical fashion. We then extend our model to include nonsymmetric grooves, allowing for additional degrees of freedom on the surface geometry, improving multiple reflections at grazing angles with backward compatibility to traditional normal distribution functions. We validate the accuracy of our model against ground-truth Monte Carlo simulations, and demonstrate its flexibility on anisotropic and textured materials. Our model is analytical, does not introduce significant cost and variance, can be seamless integrated in any rendering engine, preserves reciprocity and energy conservation, and is suitable for bidirectional methods.	https://dl.acm.org/authorize?N675560	Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter Lincoln, Adarsh Kowdle, Christoph Rhemann, Dan B Goldman, Cem Keskin, Steve Seitz, Shahram Izadi, Sean Fanello
Inheritor	"""Only the stronger will survive"", this short sentence is undeniable truth. Nowadays, when the world's population is progressively increasing, and it is leading us to intensive competition. Undoubtedly, competition is melting into our normal live like a melting pot. For example, school entrance examination, university entrance examination or seeking for a job. From all these, parents need a very high expectation from their children, and this will put a lot of pressure on them as well. To be a survivor, family members will be focusing only on the competition, but they do not pay attention to the feeling and imagination of their children With awareness of giving love without understanding becomes our inspiration to make the 3D animated short film project ""Inheritor"", the story of a boy with a warm family which is filled with love and care. On the other hand, because of over expectation from family, it makes him frustrate, unhappy and lost himself. In conclusion, we hopefully that our 3D animated short film project ""Inheritor"" is able to make the audience realize those problem. And, trying to decrease problem that leads to disregard and family conflict for the next generation so on."	https://dl.acm.org/authorize?N675624	Napatsorn Potranun
Instant transport maps on 2D grids	Relighting of human images has various applications in image synthesis. For relighting, we must infer albedo, shape, and illumination from a human portrait. Previous techniques rely on human faces for this inference, based on spherical harmonics (SH) lighting. However, because they often ignore light occlusion, inferred shapes are biased and relit images are unnaturally bright particularly at hollowed regions such as armpits, crotches, or garment wrinkles. This paper introduces the first attempt to infer light occlusion in the SH formulation directly. Based on supervised learning using convolutional neural networks (CNNs), we infer not only an albedo map, illumination but also a that encodes occlusion as nine SH coefficients per pixel. The main difficulty in this inference is the lack of training datasets compared to unlimited variations of human portraits. Surprisingly, geometric information including occlusion can be inferred plausibly even with a small dataset of synthesized human figures, by carefully preparing the dataset so that the CNNs can exploit the data coherency. Our method accomplishes more realistic relighting than the occlusion-ignored formulation.	https://dl.acm.org/authorize?N675566	Haixiang Liu, Yuanming Hu, Bo Zhu, Wojciech Matusik, Eftychios Sifakis
Integer programming for layout problems	• Simplex algorithm / interior point algorithms • Standard solvers / quite fast • Formulation is already non-trivial • Graphical Example	https://dl.acm.org/authorize?N675539	Peter Wonka
Interaction of a stereoscopic 3DCG image with motion parallax displayed in mid-air	We propose a novel system that enables a user to see stereoscopic 3DCG images in mid-air and interact with them directly as shown in Figure 1. This system displays 3DCG objects with motion parallax. Thus the user can observe them in mid-air while feeling a stereoscopic effect by the motion parallax. It is also possible to interact with the mid-air 3DCG objects by fingers. The user can move, deform and draw 3DCG objects as if they were there.	https://dl.acm.org/authorize?N675725	Mayumi Takasaki, Kyoko Ohashi, Shinji Mizuno
Interaction system with mid-air CG character that has own eyes	This paper proposes an optical system that can capture a user from the viewpoint of a mid-air CG character. The mid-air imaging system enables us to display a CG character in real space. In order for users to interact with this character, we must observe their behavior from the character's viewpoint. Therefore, we propose a method of capturing from the mid-air image position by arranging a light source display and a camera at a conjugate position using a half mirror, optically transferring them with micro-mirror array plates. The contribution of this system is capturing the full face of a user from the position of the mid-air image.	https://dl.acm.org/authorize?N675856	Kei Tsuchiya, Ayaka Sano, Naoya Koizumi
Interactive character animation by learning multi-objective control	Flying creatures in animated films often perform highly dynamic aerobatic maneuvers, which require their extreme of exercise capacity and skillful control. Designing physics-based controllers (a.k.a., control policies) for aerobatic maneuvers is very challenging because dynamic states remain in unstable equilibrium most of the time during aerobatics. Recently, Deep Reinforcement Learning (DRL) has shown its potential in constructing physics-based controllers. In this paper, we present a new concept, , which is combined with DRL to address the aerobatics control problem. The key idea of SRL is to allow the agent to take control over its own learning using an additional self-regulation policy. The policy allows the agent to regulate its goals according to the capability of the current control policy. The control and self-regulation policies are learned jointly along the progress of learning. Self-regulated learning can be viewed as building its own curriculum and seeking compromise on the goals. The effectiveness of our method is demonstrated with physically-simulated creatures performing aerobatic skills of sharp turning, rapid winding, rolling, soaring, and diving.	https://dl.acm.org/authorize?N664495	Kyungho Lee, Seyoung Lee, Jehee Lee
Interactive design and optimization of free-formed returning boomerang	"We present an interactive tool to model a returning boomerang envisioned by a user. Designing a functional and fashionable boomerang requires the computation of aerodynamics based on fluid simulation, but this computation remains difficult for interactive designs. Hence, we employ a data-driven approach [Nakamura et al. 2016; Umetani et al. 2014] by using a simple approximation instead of fluid simulation. The result shows that our interface can interactively visualize the overall 3D flight trajectory of free-formed boomerangs. We also propose an automatic assistance that maximizes two functional elements, namely (1) ""spin ability"" about an axis perpendicular to the direction of its flight and (2) ""returning ability"" to return to its throwers. By fabricating the resulting design, we can enjoy a fascinating flight of the boomerang. In addition, we conduct a user study and confirm that the proposed interface is effective for a creative boomerang design."	https://dl.acm.org/authorize?N675677	Tsukasa Fukusato, Morihiro Nakamura, Takeo Igarashi
Interactive design of periodic yarn-level cloth patterns	Object functionality is often expressed through part articulation - as when the two rigid parts of a scissor pivot against each other to perform the cutting function. Such articulations are often similar across objects within the same functional category. In this paper we explore how the observation of different articulation states provides evidence for part structure and motion of 3D objects. Our method takes as input a pair of unsegmented shapes representing two different articulation states of two functionally related objects, and induces their common parts along with their underlying rigid motion. This is a challenging setting, as we assume no prior shape structure, no prior shape category information, no consistent shape orientation, the articulation states may belong to objects of different geometry, plus we allow inputs to be noisy and partial scans, or point clouds lifted from RGB images. Our method learns a neural network architecture with three modules that respectively propose correspondences, estimate 3D deformation flows, and perform segmentation. To achieve optimal performance, our architecture alternates between correspondence, deformation flow, and segmentation prediction iteratively in an ICP-like fashion. Our results demonstrate that our method significantly outperforms state-of-the-art techniques in the task of discovering articulated parts of objects. In addition, our part induction is object-class agnostic and successfully generalizes to new and unseen objects.	https://dl.acm.org/authorize?N664417	Jonathan Leaf, Rundong Wu, Eston Schweickart, Doug L. James, Steve Marschner
Interactive modeling for craft band design	Although traditional computer-aided design (CAD) systems are mainly intended for expert users, research involving systems incorporating CG and interactive techniques that are easy to use by novices is also active. In this paper, we propose a design support system that can be used by a novice to easily design a craft band object of his or her desired pattern. We propose an algorithm that can automatically calculate geometric shapes based on rectangular parallelepipeds and cylinders according to the sizes desired by users (Fig. 1).	https://dl.acm.org/authorize?N675709	Yuki Igarashi
"Interactive visual narrative ""cloudy lady"": gaze navigation method and a prototype application"	"In this paper, a ""gaze navigation"" method for an interactive visual narrative application is proposed, and a prototype system, developed for touchscreen computer devices, such as the iPad, is described. An interactive narrative application called ""Cloudy Lady"" authored by Negar Kaghazchi has an ethic story and the user naturally follows the visual cues applied in every scene, to unravel the story by exploring the surface area using omnidirectional scrolling. (Figure 1) Unlike comic/picture books that run over many pages, or video/computer games with pre-designed paths with arrows and symbols, the story of this narrative unfolds following the natural gaze navigation and its structure, allow the user to advance the story in any phase and any direction with no restriction."	https://dl.acm.org/authorize?N675726	Negar Kaghazchi, Sachiko Kodama, Masakatsu Kaneko
Introduction to the vulkan graphics API	"""Vulcan is the god of fire including the fire of volcanoes, metalworking, and the forge in ancient Roman religion and myth. Vulcan is often depicted with a blacksmith's hammer.The Vulcanalia was the annual festival held August 23 in his honor. His Greek counterpart is Hephaestus, the god of fire and smithery. In Etruscan religion, he is identified with Sethlans."	https://dl.acm.org/authorize?N675530	Mike Bailey
Inverse elastic shell design with contact and friction	Recent advances in single-view 3D hair digitization have made the creation of high-quality CG characters scalable and accessible to end-users, enabling new forms of personalized VR and gaming experiences. To handle the complexity and variety of hair structures, most cutting-edge techniques rely on the successful retrieval of a particular hair model from a comprehensive hair database. Not only are the aforementioned data-driven methods storage intensive, but they are also prone to failure for highly unconstrained input images, complicated hairstyles, and failed face detection. Instead of using a large collection of 3D hair models directly, we propose to represent the manifold of 3D hairstyles implicitly through a compact latent space of a volumetric variational autoencoder (VAE). This deep neural network is trained with volumetric orientation field representations of 3D hair models and can synthesize new hairstyles from a compressed code. To enable end-to-end 3D hair inference, we train an additional embedding network to predict the code in the VAE latent space from any input image. Strand-level hairstyles can then be generated from the predicted volumetric representation. Our fully automatic framework does not require any ad-hoc face fitting, intermediate classification and segmentation, or hairstyle database retrieval. Our hair synthesis approach is significantly more robust and can handle a much wider variation of hairstyles than state-of-the-art data-driven hair modeling techniques with challenging inputs, including photos that are low-resolution, overexposured, or contain extreme head poses. The storage requirements are minimal and a 3D hair model can be produced from an image in a second. Our evaluations also show that successful reconstructions are possible from highly stylized cartoon images, non-human subjects, and pictures taken from behind a person. Our approach is particularly well suited for continuous and plausible hair interpolation between very different hairstyles.	https://dl.acm.org/authorize?N664416	Mickaël Ly, Romain Casati, Florence Bertails-Descoubes, Mélina Skouras, Laurence Boissieux
InverseCSG: automatic conversion of 3D models to CSG trees	Gradient-based methods are becoming increasingly important for computer graphics, machine learning, and computer vision. The ability to compute gradients is crucial to optimization, inverse problems, and deep learning. In rendering, the gradient is required with respect to variables such as camera parameters, light sources, scene geometry, or material appearance. However, computing the gradient of rendering is challenging because the rendering integral includes visibility terms that are not differentiable. Previous work on differentiable rendering has focused on approximate solutions. They often do not handle secondary effects such as shadows or global illumination, or they do not provide the gradient with respect to variables other than pixel coordinates. We introduce a general-purpose differentiable ray tracer, which, to our knowledge, is the first comprehensive solution that is able to compute derivatives of scalar functions over a rendered image with respect to arbitrary scene parameters such as camera pose, scene geometry, materials, and lighting parameters. The key to our method is a novel edge sampling algorithm that directly samples the Dirac delta functions introduced by the derivatives of the discontinuous integrand. We also develop efficient importance sampling methods based on spatial hierarchies. Our method can generate gradients in times running from seconds to minutes depending on scene complexity and desired precision. We interface our differentiable ray tracer with the deep learning library PyTorch and show prototype applications in inverse rendering and the generation of adversarial examples for neural networks.	https://dl.acm.org/authorize?N664428	Tao Du, Jeevana Priya Inala, Yewen Pu, Andrew Spielberg, Adriana Schulz, Daniela Rus, Armando Solar-Lezama, Wojciech Matusik
Invertible grayscale	This article answers an important theoretical question: How many different subdivisions of the hexahedron into tetrahedra are there? It is well known that the cube has five subdivisions into 6 tetrahedra and one subdivision into 5 tetrahedra. However, all hexahedra are not cubes and moving the vertex positions increases the number of subdivisions. Recent hexahedral dominant meshing methods try to take these configurations into account for combining tetrahedra into hexahedra, but fail to enumerate them all: they use only a set of 10 subdivisions among the 174 we found in this article. The enumeration of these 174 subdivisions of the hexahedron into tetrahedra is our combinatorial result. Each of the 174 subdivisions has between 5 and 15 tetrahedra and is actually a class of 2 to 48 equivalent instances which are identical up to vertex relabeling. We further show that exactly 171 of these subdivisions have a geometrical realization, i.e. there exist coordinates of the eight hexahedron vertices in a three-dimensional space such that the geometrical tetrahedral mesh is valid. We exhibit the tetrahedral meshes for these configurations and show in particular subdivisions of hexahedra with 15 tetrahedra that have a strictly positive Jacobian.	https://dl.acm.org/authorize?N675553	Jing Ren, Adrien Poulenard, Peter Wonka, Maks Ovsjanikov
Islands/Seom: the creating of extended existence through AR and world simulation	Islands/Seom is an interactive AR archive about creating the extended existence of the people that we admire and care for. The audience enters a room of floating cube sculptures called Islands. Each cube encapsulates the perspective, voice, and beliefs of one of the artists. Together, we preserve their ghost and transform it into a landscape. View the sculptures through our custom-made mobile app. The avatar of the artist will reanimate on top. Listen to their manifesto. Send them to a virtual world called Lacus, where they grow organically into unique ecosystems. Through a game of world-simulation, we catalyze collaboration with their persona, against the odds of distance, personalities, and culture.	https://dl.acm.org/authorize?N664105	Shih-lien Yen, Anna Libbie Grossman, Jeffrey Huang, Jungwoo Kim, Gahyae Ryu, Jihyun Her, Jinyoung Sung, Peiyu Lai
L'oiseau qui danse [best in show award]	Inspired by Visual Music, created with Trapcode Suite and animated with particles on music by Canadian band Tennyson, this project features the journey of an origami bird in a mysterious field of light.	https://dl.acm.org/authorize?N675637	Jean-Marie Marbach
Language-driven synthesis of 3D scenes from scene databases	The image processing pipeline boasts a wide variety of complex filters and effects. Translating an individual effect to operate on 3D surface geometry inevitably results in a bespoke algorithm. Instead, we propose a general-purpose back-end optimization that allows users to edit an input 3D surface by simply selecting an off-the-shelf image processing filter. We achieve this by constructing a differentiable triangle mesh renderer, with which we can changes in the image domain to the 3D mesh vertex positions. The given image processing technique is applied to the entire shape via stochastic snapshots of the shape: hence, we call our method We provide simple yet important design considerations to construct the renderer and optimization algorithms. The power of this rendering-based surface editing is demonstrated via the variety of image processing filters we apply. Each application uses an off-the-shelf implementation of an image processing method without requiring modification to the core algorithm.	https://dl.acm.org/authorize?N664427	Rui Ma, Akshay Gadi Patil, Matthew Fisher, Manyi Li, Sören Pirk, Binh-Son Hua, Sai-Kit Yeung, Xin Tong, Leonidas Guibas, Hao Zhang
Learning a shared shape space for multimodal garment design	A majority of stock 3D models in modern shape repositories are assembled with many fine-grained components. The main cause of such data form is the component-wise modeling process widely practiced by human modelers. These modeling components thus inherently reflect some function-based shape decomposition the artist had in mind during modeling. On the other hand, modeling components represent an over-segmentation since a functional part is usually modeled as a multi-component assembly. Based on these observations, we advocate that labeled segmentation of stock 3D models should not overlook the modeling components and propose a learning solution to grouping and labeling of the fine-grained components. However, directly characterizing the shape of individual components for the purpose of labeling is unreliable, since they can be arbitrarily tiny and semantically meaningless. We propose to generate part hypotheses from the components based on a hierarchical grouping strategy, and perform labeling on those part groups instead of directly on the components. Part hypotheses are mid-level elements which are more probable to carry semantic information. A multi-scale 3D convolutional neural network is trained to extract context-aware features for the hypotheses. To accomplish a labeled segmentation of the whole shape, we formulate higher-order conditional random fields (CRFs) to infer an optimal label assignment for all components. Extensive experiments demonstrate that our method achieves significantly robust labeling results on raw 3D models from public shape repositories. Our work also contributes the first benchmark for component-wise labeling.	https://dl.acm.org/authorize?N664418	Tuanfeng Y. Wang, Duygu Ceylan, Jovan Popović, Niloy J. Mitra
Learning photo enhancement by black-box model optimization data generation	We address the problem of automatic photo enhancement, in which the challenge is to determine the optimal enhancement for a given photo according to its content. For this purpose, we train a convolutional neural network to predict the best enhancement for given picture. While such machine learning techniques have shown great promise in photo enhancement, there are some limitations. One is the problem of , i.e., that it is not easy for the user to discern what has been done by a machine. In this work, we leverage existing manual photo enhancement tools as a black-box model, and predict the enhancement parameters of that model. Because the tools are designed for human use, the resulting parameters can be interpreted by their users. Another problem is the difficulty of obtaining training data. We propose generating supervised training data from high-quality professional images by randomly sampling realistic -enhancement parameters. We show that this approach allows automatic enhancement of photographs without the need for large manually labelled supervised training datasets.	https://dl.acm.org/authorize?N675664	Mayu Omiya, Edgar Simo-Serra, Satoshi Iizuka, Hiroshi Ishikawa
Learning to dress: synthesizing human dressing motion via deep reinforcement learning	We present an approach that learns to act from raw motion data for interactive character animation. Our motion generator takes a continuous stream of control inputs and generates the character's motion in an online manner. The key insight is modeling rich connections between a multitude of control objectives and a large repertoire of actions. The model is trained using Recurrent Neural Network conditioned to deal with spatiotemporal constraints and structural variabilities in human motion. We also present a new data augmentation method that allows the model to be learned even from a small to moderate amount of training data. The learning process is fully automatic if it learns the motion of a single character, and requires minimal user intervention if it deals with props and interaction between multiple characters.	https://dl.acm.org/authorize?N664484	Alexander Clegg, Wenhao Yu, Jie Tan, C. Karen Liu, Greg Turk
Learning to group and label fine-grained shape components	We introduce a learning-based method to reconstruct objects acquired in a casual handheld scanning setting with a depth camera. Our method is based on two core components. First, a deep network that provides a semantic segmentation and labeling of the frames of an input RGBD sequence. Second, an alignment and reconstruction method that employs the semantic labeling to reconstruct the acquired object from the frames. We demonstrate that the use of a semantic labeling improves the reconstructions of the objects, when compared to methods that use only the depth information of the frames. Moreover, since training a deep network requires a large amount of labeled data, a key contribution of our work is an active self-learning framework to simplify the creation of the training data. Specifically, we iteratively predict the labeling of frames with the neural network, reconstruct the object from the labeled frames, and evaluate the confidence of the labeling, to incrementally train the neural network while requiring only a small amount of user-provided annotations. We show that this method enables the creation of data for training a neural network with high accuracy, while requiring only little manual effort.	https://dl.acm.org/authorize?N664425	Xiaogang Wang, Bin Zhou, Haiyue Fang, Xiaowu Chen, Qinping Zhao, Kai Xu
Leg-jack: generation of the sensation of walking by electrical and kinesthetic stimuli to the lower limbs	We developed a neurosensory and kinesthetic stimulation system that generated a walking sensation for a seated user. An electrical stimulus was applied to Achilles' and tibialis anterior tendons with a kinesthetic stimulus generated by a lower limb device driven synchronously with an egocentric visual scene during virtual walking. The system works as a part of experience replication scheme that aims to receive other's physical activity. As a common bodily activity of humans, walking motion was focused. The evaluation experiment has shown that walking sensation was increased by each stimulation at a 1% significance level. In this demonstration, the user on a chair can feel as if he/she is walking in a haunted house. The user can move the upper body freely to look around with a virtual flashlight, however the lower body is possessed by the other. The user walks into the house despite the intention. The work gives the user a realistic experience which was not sufficiently generated with only a movie and sounds.	https://dl.acm.org/authorize?N675765	Hirofumi Kaneko, Tomohiro Amemiya, Vibol Yem, Yasushi Ikei, Koichi Hirota, Michiteru Kitazaki
Leo minor	8-year-old Leo is dying of cancer. Denying his death, his imagination carries him and his big brother far away from the hospital room. Drifting through their emotions and space, they find the strength to undergo their grief.	https://dl.acm.org/authorize?N675635	Tai Wedekind
Life sciences in virtual reality: first-year students learning as creators	Students commencing tertiary studies in life sciences over the next decade are going to arrive at universities with a more advanced digital literacy and shifting expectations of how technology supports learning. This poses a challenge to traditional approaches to tertiary science education. As the increasingly digitally literate students arrive, and the barrier to entry to Virtual Reality (VR) drops, there is an opportunity to develop new learning activities in VR. Students may learn by VR, but students can also explore complex scientific concepts by their own VR content, while developing digital and creation fluencies. We describe how first-year students explored biological science concepts with a series of learning activities based in content creation for VR. We outline the feedback students provided on these activities and how this feedback informed further development of the learning activities.	https://dl.acm.org/authorize?N675746	Christopher Hammang, Phillip Gough, Weber Liu, Eric Jiang, Pauline Ross, Jim Cook, Philip Poronnik
Light in power: a general and parameter-free algorithm for caustic design	Elastically deforming wire structures are lightweight, durable, and can be bent within minutes using CNC bending machines. We present a computational technique for the design of kinetic wire characters, tailored for fabrication on consumer-grade hardware. Our technique takes as input a network of curves or a skeletal animation, then estimates a cable-driven, compliant wire structure which matches user-selected targets or keyframes as closely as possible. To enable large localized deformations, we shape wire into functional spring-like entities at a discrete set of locations. We first detect regions where changes to local stiffness properties are needed, then insert bendable entities of varying shape and size. To avoid a discrete optimization, we first optimize stiffness properties of generic, non-fabricable entities which capture well the behavior of our bendable designs. To co-optimize stiffness properties and cable forces, we formulate an equilibrium-constrained minimization problem, safeguarding against inelastic deformations. We demonstrate our method on six fabricated examples, showcasing rich behavior including large deformations and complex, spatial motion.	https://dl.acm.org/authorize?N664431	Daniel Thul, L'ubor Ladický, Sohyeon Jeong, Marc Pollefeys
Light transport simulation in the gradient domain	Despite the wide adoption in film production and animation industry nowadays, Monte Carlo light transport simulation is still prone to producing noisy images within short rendering time. Accelerating the convergence of Monte Carlo rendering without sacrificing its accuracy is by far a challenging task. In this course, we will learn about gradient-domain light transport simulation, a recent family of techniques in physically based rendering introduced in the past five years that can accelerate traditional Monte Carlo rendering up to approximately an order of magnitude based on gradient estimation and image reconstruction. Particularly, we will introduce the fundamentals of gradient-domain rendering with gradient-domain path tracing, and then extend the discussion to gradient-domain bidirectional path tracing and photon density estimation. We also discuss volume rendering in the gradient domain before diving into advanced topics in recent state-of-the-art papers in this direction. We further discuss tips and tricks in open-source implementations of such algorithms, and provide ideas for future research directions	https://dl.acm.org/authorize?N675531	Binh-Son Hua, Adrien Gruson, Matthias Zwicker, Toshiya Hachisuka
Lillandril	Lillandril travel to kill the dragon of desolated lands.	https://dl.acm.org/authorize?N675636	Margaux Tamic
Little hero wins the masks: virtual reality creation of taiwanese classic comics	"Taiwanese comics artist Hung-Chia Yeh created martial arts comic series ""Little Hero"" (諸葛四郎) in 1958. Little Hero was later super famous in 1960s. However, Taiwanese comics culture has declined due to various historical background, which caused younger generation know little about Taiwanese comics. We did this derivative work with VR and AR technology based on one of the classic story in Little Hero called ""Little Hero Wins the Masks"" (諸葛四郎大鬥雙假面). The features of this work are: (1) Develop the technology that connect VR and interactive game table together to improve the insufficiency of sociability in VR games; (2) The feedback of handheld controller and the game card of interactive game table are designed to improve interactivity in the game (See Figure 1)."	https://dl.acm.org/authorize?N664106	Chun-Cheng Hsu, Chun-I Lee, Yu-Cheng Li
Live replay movie creation of Gran Turismo	We perform live replay movie creation using the recorded play data of Gran Turismo. Our real-time technologies enable movie editing such as authoring camerawork and adding visual effects while reproducing the race scene with high quality graphics from the play data. We also demonstrate some recent development for the future.	https://dl.acm.org/authorize?N675653	Masamichi Sugihara, Hiroki Kashiwagi, Tatsuya Matsue
Lotus: enhancing the immersive experience in virtual environment with mist-based olfactory display	With the advance of virtual reality (VR) headset and haptic technologies, users can have a great experience when they are immersed in the virtual environment (VE). A fewer of research allow users to perceive the odor from the VE simultaneously. Based on the five senses, olfaction is one of the human sense that can perceive chemical information from the environment, which is also important for recreating the VE. In the past, some research groups have shown the techniques of olfactory display. However, to create the olfactory feedback for immersive VR when the user moving around in the tracking area, where a moveable display or a lightweight portable device is required, due to the user's nose is the only human receptor for perceiving the scent. We present Lotus, a steerable mist-based olfactory display with an airflow guiding module for simulating environments with olfaction. It can provide two kinds of VEs simultaneously for enhancing the immersive experience without carrying the weighty liquid.	https://dl.acm.org/authorize?N664107	Yang-Sheng Chen, Ping-Hsuan Han, Kong-Chang Lee, Chiao-En Hsieh, Jui-Chun Hsiao, Che-Ju Hsu, Kuan-Wen Chen, Chien-Hsing Chou, Yi-Ping Hung
Luciola: a light-emitting particle moving in mid-air based on ultrasonic levitation and wireless powering	In this paper, we present an approach to realize the levitation of a small object with an embedded electronic circuit. Luciola is a light-emitting particle with a diameter of 3.5mm and a weight of 16.2mg moving in mid-air. The novelty of this paper is the ultrasonically levitated electronic object powered by resonant inductive coupling. To enable the levitation of a particle, a custom IC chip is essential in reducing the size and weight of the particle. Ths custom IC chip is designed to achieve an intermittent lighting of the LED, which increases the maximal distance between the transmitter and the receiver coils. Luciola is applied to a self-luminous pixel in a 3-dimensional (3D) mid-air display and the drawing of characters in mid-air is also demonstrated.	https://dl.acm.org/authorize?N675766	Hao Qiu, Yuki Uno, Toru Sai, Shunta Iguchi, Yota Mizutani, Takayuki Hoshi, Yoshihiro Kawahara, Yasuaki Kakehi, Makoto Takamiya
MIDAS projection: markerless and modelless dynamic projection mapping for material representation	We propose an inverse strategy for modeling thin elastic shells physically, just from the observation of their geometry. Our algorithm takes as input an arbitrary target mesh, and interprets this configuration automatically as a stable equilibrium of a shell simulator under gravity and frictional contact constraints with a given external object. Unknowns are the natural shape of the shell (i.e., its shape without external forces) and the frictional contact forces at play, while the material properties (mass density, stiffness, friction coefficients) can be freely chosen by the user. Such an inverse problem formulates as an ill-posed nonlinear system subject to conical constraints. To select and compute a plausible solution, our inverse solver proceeds in two steps. In a first step, contacts are reduced to frictionless bilateral constraints and a natural shape is retrieved using the adjoint method. The second step uses this result as an initial guess and adjusts each bilateral force so that it projects onto the admissible Coulomb friction cone, while preserving global equilibrium. To better guide minimization towards the target, these two steps are applied iteratively using a degressive regularization of the shell energy. We validate our approach on simulated examples with reference material parameters, and show that our method still converges well for material parameters lying within a reasonable range around the reference, and even in the case of arbitrary meshes that are not issued from a simulation. We finally demonstrate practical inversion results on complex shell geometries freely modeled by an artist or automatically captured from real objects, such as posed garments or soft accessories.	https://dl.acm.org/authorize?N664401	Leo Miyashita, Yoshihiro Watanabe, Masatoshi Ishikawa
MR360 interactive: playing with digital creatures in 360° videos	We present , interactive mixed reality (MR) experiences using pre-recorded and live streaming 360° videos (360-video) shown in head mounted displays. We developed the MR360 toolkit, an interface to create interactive MR content using 360-video. The toolkit detects salient lights in the 360-video and casts realistic shadows. Image based lighting is perceptually optimized to provide fast results. Real-time differential rendering obtains a composition between the virtual objects and the real-world background. We present two applications of our toolkit: a VR experience using pre-recorded 360-video, and the , an MR experience using live streaming 360-video. In both applications, participants are interacting with digital creatures with high presence in 360-video.	https://dl.acm.org/authorize?N664109	Taehyun Rhee, Andrew Chalmers, Ian Loh, Kazuki Kumagai, Kosuke Sugai, Gakuji Nomoto, Lohit Petikam, Ben Allen, Ken Anjyo
MR360 live: immersive mixed reality with live 360° video	DreamFlux presents MR360 Live, a new way to create immersive and interactive Mixed Reality applications. It blends 3D virtual objects into live streamed 360 videos in real-time, providing the illusion of interacting with objects in the video.	https://dl.acm.org/authorize?N675665	Taehyun Rhee, Ian Loh, Ben Allen, Lohit Petikam
Magic zoetrope: representation of animation by multi-layer 3D zoetrope with a semitransparent mirror	"In this research, we propose a multilayered 3D zoetrope called the ""Magic Zoetrope"", which makes it possible to animate two independent object groups concurrently and to represent various alterations in the animation, unlike a conventional 3D zoetrope. A conventional 3D zoetrope has only one object group that is illuminated by a unitary strobe light, so that the presented animation is always periodic and unchanged. Some studies [Miyashita et al. 2016; Smoot et al. 2010; Yoshida et al. 2016] and artworks, for example the Time Stratum series by Toshio Iwai, attempted to expand the range of expression of 3D zoetropes, but they did not focus on animating multiple subjects with alterations in the mutual relation between them as in the video animation."	https://dl.acm.org/authorize?N675767	Tomohiro Yokota, Tomoko Hashida
Magnetact: magnetic-sheet-based haptic interfaces for touch devices	This paper presents a rapid prototyping method of haptic interfaces for touch devices utilizing magnetic rubber sheets and conductive materials. When a magnetic sheet is thin enough, the capacitive sensor of the touch device can detect the user's finger behind the magnetic sheet due to the sheet's dielectric behavior. Furthermore, by changing the magnetic pattern of the magnetic sheet using a handy magnetizing tool, the tactile feedback can be customized within seconds. Since the construction of the interface is so simple, this method enables users to customize not only the size and shape, also the haptic feedback of the tangible interface. We demonstrated several types of interface such as buttons, sliders, switches, and cross-keys.	https://dl.acm.org/authorize?N675768	Kentaro Yasu
Manga stylized rendering in VR	The research and development department of Square Enix, the (ATD), has recently released a virtual reality (VR) adaptation of a manga. Since the very beginning of this project, its theme has been the question:	https://dl.acm.org/authorize?N675532	Julien Guertault, Élie Setbon, Pavel Martishevsky
Mayday: final chapter	We use pop-up book to present a person's life.	https://dl.acm.org/authorize?N675638	Muh Chun
Mean value coordinates for quad cages in 3D	We aim to generate high resolution shallow depth-of-field (DoF) images from a single all-in-focus image with controllable focal distance and aperture size. To achieve this, we propose a novel neural network model comprised of a depth prediction module, a lens blur module, and a guided upsampling module. All modules are differentiable and are learned from data. To train our depth prediction module, we collect a dataset of 2462 RGB-D images captured by mobile phones with a dual-lens camera, and use existing segmentation datasets to improve border prediction. We further leverage a synthetic dataset with known depth to supervise the lens blur and guided upsampling modules. The effectiveness of our system and training strategies are verified in the experiments. Our method can generate high-quality shallow DoF images at high resolution, and produces significantly fewer artifacts than the baselines and existing solutions for single image shallow DoF synthesis. Compared with the iPhone portrait mode, which is a state-of-the-art shallow DoF solution based on a dual-lens depth camera, our method generates comparable results, while allowing for greater flexibility to choose focal points and aperture size, and is not limited to one capture setup.	https://dl.acm.org/authorize?N664446	Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, Kun Zhou
Memoirs of the blind	Memoirs of the blind is an interactive installation consisting of a screen showing a black-and-white picture of a face with its eyes closed. This image at first remains still, however, when the interactor blinks, the installation detects it and takes a photo at the exact time of the blinking. Once the new face is obtained, it is processed (cropped, turned into b&w, and adjusted) and then it substitutes the displayed face.	https://dl.acm.org/authorize?N675600	Tomas Laurenzo
Miazmat	a) an attempt at approaching in an innovative way the problem of translating imagination into a work of art, and vice versa b) reconstruction, reverse engineering of whatever happened in the artist's mind c) revival of the Pure Form aesthethic in a 21st Century form d) triggering an amazing experience in the viewer - a sense of mystery of existence - metaphysical sensations	https://dl.acm.org/authorize?N675639	Klaudiusz Wesolowski
Mochitsuki: a real-object-based, interactive haptic interface	We propose an approach that breaks down the haptic experience into multiple elements and assigns optimal devices to each element, either a physical device or an electronically-controllable device. As an example of this design approach, we built a VR system that provides the experience of Mochitsuki. We designed a more durable system than the initial prototype. With this configuration, direct interactions between the user and the system are all performed by physical devices to ensure the quality of experience, while the parameters affecting the experience can be controlled by electronic devices and mechanisms.	https://dl.acm.org/authorize?N664108	Yuto Mori, Kyuma Watanabe, Masayuki Iwata, Hideki Kawai, Yasuyuki Yanagida
Modeling hair from an RGB-D camera	While computer-aided design is a major part of many modern manufacturing pipelines, the design files typically generated describe raw geometry. Lost in this representation is the procedure by which these designs were generated. In this paper, we present a method for reverse-engineering the process by which 3D models may have been generated, in the language of constructive solid geometry (CSG). Observing that CSG is a formal grammar, we formulate this inverse CSG problem as a program synthesis problem. Our solution is an algorithm that couples geometric processing with state-of-the-art program synthesis techniques. In this scheme, geometric processing is used to convert the mixed discrete and continuous domain of CSG trees to a pure discrete domain where modern program synthesizers excel. We demonstrate the efficiency and scalability of our algorithm on several different examples, including those with over 100 primitive parts. We show that our algorithm is able to find simple programs which are close to the ground truth, and demonstrate our method's applicability in mesh re-editing. Finally, we compare our method to prior state-of-the-art. We demonstrate that our algorithm dominates previous methods in terms of resulting CSG compactness and runtime, and can handle far more complex input meshes than any previous method.	https://dl.acm.org/authorize?N664410	Meng Zhang, Pan Wu, Hongzhi Wu, Yanlin Weng, Youyi Zheng, Kun Zhou
Monte Carlo convolution for learning on non-uniformly sampled point clouds	Elastic parameter optimization has revealed its importance in 3D modeling, virtual reality, and additive manufacturing in recent years. Unfortunately, it is known to be computationally expensive, especially if there are many parameters and data samples. To address this challenge, we propose to introduce the inexactness into descent methods, by iteratively solving a forward simulation step and a parameter update step in an inexact manner. The development of such inexact descent methods is centered at two questions: 1) how accurate/inaccurate can the two steps be; and 2) what is the optimal way to implement an inexact descent method. The answers to these questions are in our convergence analysis, which proves the existence of relative error thresholds for the two inexact steps to ensure the convergence. This means we can simply solve each step by a fixed number of iterations, if the iterative solver is at least linearly convergent. While the use of the inexact idea speeds up many descent methods, we specifically favor a GPU-based one powered by state-of-the-art simulation techniques. Based on this method, we study a variety of implementation issues, including backtracking line search, initialization, regularization, and multiple data samples. We demonstrate the use of our inexact method in elasticity measurement and design applications. Our experiment shows the method is fast, reliable, memory-efficient, GPU-friendly, flexible with different elastic models, scalable to a large parameter space, and parallelizable for multiple data samples.	https://dl.acm.org/authorize?N664443	Changjian Li, Hao Pan, Yang Liu, Xin Tong, Alla Sheffer, Wenping Wang
More real, less time: mimic's quest in real-time facial animation	Mimic Productions' CEO, Hermione Mitford, will present a live-stream demonstration of detailed facial animation in real-time, utilizing her photo-real 3D digital-double. The presentation will include a speech from Mitford (and her avatar) addressing Mimic's technological approach, as well as the corresponding applications for the technology. A specific focus will be placed on realism and the details of the human face.	https://dl.acm.org/authorize?N675654	Hermione Mitford
Motion regeneration using motion texture and autoencoder	Motion analysis and recognition frequently suffer from noisy motion capture data not only because of systematic noises of imaging devices but also because of motion dependent non-systematic errors such as self occlusions and motion dynamics extraction failure from visual data. In this work, we propose a motion regeneration method that extracts only statistically significant and distinct characteristics of human body motion and synthesizes a new motion data. To this end, we convert 3D human body motion to 2D motion texture that is easily applicable to well-trained deep convolutional network. An autoencoder is trained with our 2D motion textures to learn only essential characteristics of human body motion in encoded space discarding systematic noises and unexpected non-systematic errors that are nothing to do with the description of particular motion. For the verification of the effectiveness of our regenerated motion, we perform motion classification test on public body motion dataset using our Long-Short Term Memory(LSTM) based method.	https://dl.acm.org/authorize?N675791	Suekyeong Nam, Seungkyu Lee
Multi-chart generative surface modeling	We develop a new theory of volumetric light transport for media with non-exponential free-flight distributions. Recent insights from atmospheric sciences and neutron transport demonstrate that such distributions arise in the presence of correlated scatterers, which are naturally produced by processes such as cloud condensation and fractal-pattern formation. Our theory formulates a non-exponential path integral as the result of averaging stochastic classical media, and we introduce practical models to solve the resulting averaging problem efficiently. Our theory results in a generalized path integral which allows us to handle non-exponential media using the full range of Monte Carlo rendering algorithms while enriching the range of achievable appearance. We propose parametric models for controlling the statistical correlations by leveraging work on stochastic processes, and we develop a method to combine such unresolved correlations (and the resulting non-exponential free-flight behavior) with explicitly modeled macroscopic heterogeneity. This provides a powerful authoring approach where artists can freely design the shape of the attenuation profile separately from the macroscopic heterogeneous density, while our theory provides a physically consistent interpretation in terms of a path space integral. We address important considerations for graphics including reciprocity and bidirectional rendering algorithms, all in the presence of surfaces and correlated media.	https://dl.acm.org/authorize?N664420	Heli Ben-Hamu, Haggai Maron, Itay Kezurer, Gal Avineri, Yaron Lipman
Multi-directional geodesic neural networks via equivariant convolution	The Material Point Method (MPM) has been shown to facilitate effective simulations of physically complex and topologically challenging materials, with a wealth of emerging applications in computational engineering and visual computing. Borne out of the extreme importance of regularity, MPM is given attractive parallelization opportunities on high-performance modern multiprocessors. Parallelization of MPM that fully leverages computing resources presents challenges that require exploring an extensive design-space for favorable data structures and algorithms. Unlike the conceptually simple CPU parallelization, where the coarse partition of tasks can be easily applied, it takes greater effort to reach the GPU hardware saturation due to its many-core SIMT architecture. In this paper we introduce methods for addressing the computational challenges of MPM and extending the capabilities of general simulation systems based on MPM, particularly concentrating on GPU optimization. In addition to our open-source high-performance framework, we also conduct performance analyses and benchmark experiments to compare against alternative design choices which may superficially appear to be reasonable, but can suffer from suboptimal performance in practice. Our explicit and fully implicit GPU MPM solvers are further equipped with a Moving Least Squares MPM heat solver and a novel sand constitutive model to enable fast simulations of a wide range of materials. We demonstrate that more than an order of magnitude performance improvement can be achieved with our GPU solvers. Practical high-resolution examples with up to ten million particles run in less than one minute per frame.	https://dl.acm.org/citation.cfm?id=3295679	Kai Xu
Multi-view wire art	Sketch or line art colorization is a research field with significant market demand. Different from photo colorization which strongly relies on texture information, sketch colorization is more challenging as sketches may not have texture. Even worse, color, texture, and gradient have to be generated from the abstract sketch lines. In this paper, we propose a semi-automatic learning-based framework to colorize sketches with proper color, texture as well as gradient. Our framework consists of two stages. In the first drafting stage, our model guesses color regions and splashes a rich variety of colors over the sketch to obtain a color draft. In the second refinement stage, it detects the unnatural colors and artifacts, and try to fix and refine the result. Comparing to existing approaches, this two-stage design effectively divides the complex colorization task into two simpler and goal-clearer subtasks. This eases the learning and raises the quality of colorization. Our model resolves the artifacts such as water-color blurring, color distortion, and dull textures. We build an interactive software based on our model for evaluation. Users can iteratively edit and refine the colorization. We evaluate our learning model and the interactive system through an extensive user study. Statistics shows that our method outperforms the state-of-art techniques and industrial applications in several aspects including, the visual quality, the ability of user control, user experience, and other metrics.	https://dl.acm.org/authorize?N675559	Kaidi Cao, Jing Liao, Lu Yuan
Muscle action VR: to support embodied learning foundations of biomechanics in musculoskeletal system	Traditional anatomy education has struggled with teaching students muscle movements in the mindset of three-dimensional anatomical structure. We present Muscle Action VR, an embodied learning virtual reality system that allows students to explore the effects that muscles have on the body. This application was created for studying musculoskeletal structures through playful and creative engagement, while staying accurate to anatomical structures and terminology. Users learn the basics of the biomechanics of human anatomy by either moving their own body with VIVE trackers, or directly manipulating specific muscles using VIVE controllers. We believe this application contributes to teach three-dimensional spatial awareness and foundational biomechanics in anatomy education.	https://dl.acm.org/authorize?N664100	Jinsil Hwaryoung Seo, Brian Michael Smith, Michael Bruner, Austin Payne, Margaret Cook, Michelle Pine, Erica Malone, Shinjiro Sueda, Shinjiro Sueda, Ben Heymann
Narrow-band topology optimization on a sparsely populated grid	The bidirectional reflectance distribution function (BRDF) is crucial for modeling the appearance of real-world materials. In production rendering, analytic BRDF models are often used to approximate the surface appearance since they are compact and flexible. Measured BRDFs usually have a more realistic appearance, but consume much more storage and are hard to modify. In this paper, we propose a novel framework for connecting measured and analytic BRDFs. First, we develop a robust method for separating a measured BRDF into diffuse and specular components. This is commonly done in analytic models, but has been difficult previously to do explicitly for measured BRDFs. This diffuse-specular separation allows novel measured BRDF editing on the diffuse and specular parts separately. In addition, we conduct analysis on each part of the measured BRDF, and demonstrate a more intuitive and lower-dimensional PCA model than Nielsen [2015]. In fact, our measured BRDF model has the same number of parameters (8 parameters) as the commonly used analytic models, such as the GGX model. Finally, we visualize the analytic and measured BRDFs in the same space, and directly demonstrate their similarities and differences. We also design an analytic fitting algorithm for two-lobe materials, which is more robust, efficient and simple, compared to previous non-convex optimization-based analytic fitting methods.	https://dl.acm.org/authorize?N675569	Ming Gao, Xinlei Wang, Kui Wu, Andre Pradhana, Eftychios Sifakis, Cem Yuksel, Chenfanfu Jiang
Nebula III	Dennis Del Favero's collaborative research program comprises work across the fields of art, humanities, engineering and science, exploring the theorisation of interactive aesthetics and its application in intelligent visualisation systems. Nebula III explores emergent aesthetic of Georg Buchner, the 19th German dramatist and scientist. This aesthetic is characterised by its emergent dialogic spatial aesthetics, namely its approach to space as a two relationship between things where it is not simply a linear connections between entities. It foreshadows the aesthetics of Karen Barad, the contemporary theoretical physicist and philosopher who conceived of the concept of 'intra-action' to conceptualise how all spatial matter never pre-exists, but is always shaped and formed through the interactive relationships between its constitutive elements. Using this intra-active spatial aesthetic as its inspiration, Nebula III presents a 3D particle world that explores this notion of space as constructed through intra-action. Here 3D particles, while displaying elementary clustering spatial behaviour, are simultaneously single-minded and resistant to control. The user, by controlling an onscreen ball of light through the use of a tablet, can learn to assemble the particles. Gradually, depending on the user's ability to control the particles, the particles are attracted to the light, like moths to a bulb. On successfully assembling all the particles, a particle sphere emerges from the clustered particles, then dissolves into one of a series of different spatial worlds, depending on how the user interacted with the particles --- a rotating planet, a mountainscape, falling snow and a storm.	https://dl.acm.org/authorize?N675601	Dennis Del Favero
Neural network in combination with a differential evolutionary training algorithm for addressing ambiguous articulated inverse kinematic problems	Inverse kinematic systems are an important tool in many disciplines (from animated game characters to robotic structures). However, inverse kinematic problems are a challenging topic (due to their computational cost, highly non-linear nature and discontinuous, ambiguous characteristics with multiple or no-solutions). Neural networks offer a flexible computational model that is able to address these difficult inverse kinematic problems where traditional, formal techniques would be difficult or impossible In this paper, we present a solution that combines an artificial neural network and a differential evolutionary algorithm for solving inverse kinematic problems. We explore the potential advantages of neural networks for providing robust solutions to a wide range of inverse kinematic problems, particularly areas involving multiple fitness criteria, optimization, pattern and comfort factors, and function approximation. We evaluate the technique through experimentation, such as, training times, fitness criteria and quality metrics.	https://dl.acm.org/authorize?N675660	Ben Kenwright
Non-photorealistic rendering of yangzhou school painting for koi animation	Dynamic Chinese Painting is a rendering style for animations. However, the production method today which relies on the artists to generate suitable textures and sceneries is time-consuming. In this paper, we propose a system to generate a realistic appearance of Yangzhou School Chinese Painting koi (decorative carp) animation in 3D space. Our system includes repainting the original texture, enhancing the contour of the 3D input models, and adding fin-stripes to fit the target style. For the interactive part, our system not only allows users to add ripples to the scene as the user-scene interaction but also generates water streamline as the koi-scene interaction. Through our system, a new Dynamic Chinese Painting scene can be automatically generated based on the 3D input models and user-inputs.	https://dl.acm.org/authorize?N675785	Rina Savista Halim, Phillip Pan, Kuo-Wei Chen, Chih-Yuan Yao, Tong-Yee Lee
Novel structure using quasirigid folding of voxel in Ron Resch pattern	We propose a novel deformation structure that expands along the x-, y-, and z-axes simultaneously using a Ron Resch pattern. The Ron Resch pattern, devised by Ron Resch, is a folding pattern that uses a combination of triangle pattern that simultaneously expands along the x-, y-, and z-axes. This pattern shows particularly high expansion coefficient along the z-axis [Resch et al. 1974; Resch and Christiansen 1970].	https://dl.acm.org/authorize?N675700	Shingo Uzawa, Toshiharu Igarashi, Kazuki Takazawa, Nozomi Magome, Yoichi Ochiai
Object tracking-based foveated super-resolution convolutional neural network for head mounted display	Recently, the immersive virtual reality (VR) environment using the head mounted display (HMD) has attracted attention as a new growth market due to the reasonable consumer price and high accessibility compared to other VR devices. However, users feel the cognitive heterogeneity caused by low resolution images, and hence, it is difficult to use it for a long time. To solve it, transmission techniques based on image resolution conversion have studied. In this paper, we propose a novel foveated super-resolution convolutional neural network (SRCNN) for HMD using an object tracking algorithm to reduce computation load for rendering high resolution images. We implement the object tracking on the region to compensate for a frame processing speed of eye-tracking devices, relatively slow to apply the resolution conversion. SRCNN applies to cognitive regions, and typical interpolation applies to other regions to reduce the rendering cost. As a result, the computation is decreased by 90.4059%, and PSNR is higher than the conventional foveated rendering algorithm.	https://dl.acm.org/authorize?N675737	Sanghyuk Kim, Min-Woo Seo, Seung Joon Lee, Suk-Ju Kang
Ocean of oblivion	I always think thoughtfully developed and well iterated film can evoke much deeper emotion to audience than mere words. This film is my pray for victims from Sewol Ferry sink that recently happened in Korea, and also it include my wish to stop these kinds of recurring tragedies caused by money related corruptions.	https://dl.acm.org/authorize?N675630	Seong-young Kim
Oceans we make: immersive VR storytelling	Oceans We Make (OWM) is a 3-minute long immersive and interactive virtual reality (VR) experience that encourages participants to question their use of plastic. The experience blends beautiful cinematic graphics, engaging game mechanics and an emotional narrative as a novel form of VR storytelling to drive positive environmental impact.	https://dl.acm.org/authorize?N664101	Ashima Thomas, Abhi Kumar, Race Krehel, Kay Vasey, Eng Tat Khoo, Tim Marsh, Benjamin Li Junting
Oculus malus	A crazy scientist creates special glasses in order to examine some little creatures that are invisible to human eyes. These creatures may just explain a good proportion of our little daily annoyances ...	https://dl.acm.org/authorize?N675631	Félix Benicourt, Maxime Blin, Mathieu Bouzard Camille Bullet, Valentin Chotel, Adrien Kottelat, Quentin Masingarbe, Dorian Mouty
On the convergence and mode collapse of GAN	Generative adversarial network (GAN) is a powerful generative model. However, it suffers from several problems, such as convergence instability and mode collapse. To overcome these drawbacks, this paper presents a novel architecture of GAN, which consists of one generator and two different discriminators. With the fact that GAN is the analogy of a minimax game, the proposed architecture is as follows. The generator ( ) aims to produce realistic-looking samples to fool both of two discriminators. The first discriminator ( ) rewards high scores for samples from the data distribution, while the second one ( ) favors samples from the generator conversely. Specifically, the ResBlock and minibatch discrimination (MD) architectures are adopted in to improve the diversity of the samples. The leaky rectified linear unit (Leaky ReLU) and batch normalization (BN) are replaced by the scaled exponential linear unit (SELU) in to alleviate the convergence problem. A new loss function that minimizes the KL divergence is designed to better optimize the model. Extensive experiments on CIFAR-10/100 datasets demonstrate that the proposed method can effectively solve the problems of convergence and mode collapse.	https://dl.acm.org/authorize?N675688	Zhaoyu Zhang, Mengyan Li, Jun Yu
One small step	Luna is a vibrant young Chinese American girl who dreams of becoming an astronaut. From the day she witnesses a rocket launching into space on TV, Luna is driven to reach for the stars. In the big city, Luna lives with her loving father Chu, who supports her with a humble shoe repair business he runs out of his garage. As Luna grows up, she enters college, facing adversity of all kinds in pursuit of her dreams.	https://dl.acm.org/authorize?N675632	Bobby Pontillas, Andrew Chesworth
OpenISS depth camera as a near-realtime broadcast service for performing arts and beyond	is a suite of configurable tools and the open source core for , which exhibits multimodal interaction and provides a platform for artists to enhance their performance by leveraging modern day technology. It is an experimental C/C++ framework with wrappers and API for motion capture and other interactions. Recently, we extended the OpenISS by enabling SOAP and REST APIs, which brought us closer to a more flexible and scalable architecture for being available as an interactive broadcast service over the Internet to a wider audience. In April 2018, we did a realtime live streaming demonstration exhibit to a team of Ubisoft representatives along with the ISSv2 project, making as an example Microsoft Kinect-as-a-service prior to Microsoft's announcement for Kinect for Azure. During the demo, we served 43032 requests, up to 1474 requests per min and we had a total of 19 visitors. A subsequent real time-live streaming demonstration was done at the ChineseCHI workshop later in April at the CHI2018 conference as well with a live application of the service in a dance performance.	https://dl.acm.org/authorize?N675792	Jashanjot Singh, Haotao Lai, Konstantinos Psimoulis, Paul Palmieri, Inna Atanasova, Yasmine Chiter, Amirali Shirkhodaiekashani, Serguei A. Mokhov
OptCuts: joint optimization of surface cuts and parameterization	Capturing appearance often requires dense sampling in light-view space, which is often achieved in specialized, expensive hardware setups. With the aim of realizing a compact acquisition setup without multiple angular samples of light and view, we sought to leverage an alternative optical property of light, To this end, we capture a set of polarimetric images with linear polarizers in front of a single projector and camera to obtain the appearance and normals of real-world objects. We encountered two technical challenges: First, no complete polarimetric BRDF model is available for modeling mixed polarization of both specular and diffuse reflection. Second, existing polarization-based inverse rendering methods are not applicable to a single local illumination setup since they are formulated with the assumption of spherical illumination. To this end, we first present a complete polarimetric BRDF (pBRDF) model that can define mixed polarization of both specular and diffuse reflection. Second, by leveraging our pBRDF model, we propose a novel inverse-rendering method with joint optimization of pBRDF and normals to capture spatially-varying material appearance: per-material specular properties (including the refractive index, specular roughness and specular coefficient), per-pixel diffuse albedo and normals. Our method can solve the severely ill-posed inverse-rendering problem by carefully accounting for the physical relationship between polarimetric appearance and geometric properties. We demonstrate how our method overcomes limited sampling in light-view space for inverse rendering by means of polarization.	https://dl.acm.org/authorize?N675565	Hugo Lavenant, Sebastian Claici, Edward Chien, Justin Solomon
Optimal and interactive keyframe selection for motion capture	"Motion capture is increasingly used in games and movies. However, it often requires editing before it can be used. Unfortunately, editing is laborious because of the low-level representation of the data. Existing motion editing methods accomplish modest changes, but larger edits require the artist to ""re-animate"" the motion by manually selecting a subset of the frames as keyframes. In this paper, we find sets of frames that serve as keyframes for editing the motion. We formulate the problem of selecting an optimal set of keyframes as a type of shortest-path problem, and solve this problem using efficient dynamic programming. Our algorithm can simplify motion capture to around 10% of the original number of frames while retaining most of its detail. By simplifying animation with our algorithm, we realize a new approach to motion editing and stylization founded on the time-tested keyframe interface."	https://dl.acm.org/authorize?N675683	Richard Roberts, J. P. Lewis, Ken Anjyo, Jaewoo Seo, Yeongho Seol
Optimize deep super-resolution and denoising for compressed textures	Lossy GPU-native compression formats such as BCn, ETC and ASTC have been widely used to reduce video memory footprint of textures. A downloadable GPU application like a mobile 3D game should select the most suitable compression format and resolution per texture, while taking a variety of the target device's capabilities and download volume into consideration.	https://dl.acm.org/authorize?N675735	Shintaro Takemura
Out of the cradle	"""Out of the Cradle"" is a documentary employing CG, that showcases the latest academic theories pertaining to the birth of mankind. Our studio-through video game and movie development experience-has cultivated the technical expertise by which we were able to provide entertaining and stunning visuals. As with most academic studies, it can be difficult to imagine ancient settings. Based on the latest anthropological research, we added our own highly entertaining visual expressions to these theories, to create an engaging documentary, that is unlike no other. We hope you can catch a glimpse into the world of ages past through this film, contemplate on the most recent academic theories on human evolution."	https://dl.acm.org/authorize?N675633	Isamu Watamori
P	In this paper, we present an incremental learning framework for efficient and accurate facial performance tracking. Our approach is to alternate the modeling step, which takes tracked meshes and texture maps to train our deep learning-based statistical model, and the tracking step, which takes predictions of geometry and texture our model infers from measured images and optimize the predicted geometry by minimizing image, geometry and facial landmark errors. Our model extends the convolutional variational autoencoder for face tracking, and jointly learns and represents deformations and variations in geometry and texture from tracked meshes and texture maps. To accurately model variations in facial geometry and texture, we introduce the layer in the Geo-Tex VAE architecture which decomposes the facial deformation into global and local components. We train the global deformation with a fully-connected network and the local deformations with convolutional layers. Despite running this model on each frame independently - thereby enabling a high amount of parallelization - we validate that our framework achieves sub-millimeter accuracy on synthetic data and outperforms existing methods. We also qualitatively demonstrate high-fidelity, long-duration facial performance tracking on several actors.	https://dl.acm.org/authorize?N664437	Tzu-Mao Li, Miika Aittala, Frédo Durand, Jaakko Lehtinen
PPConv: polypod convolution for 3D point cloud description	3D point cloud is a collection of unordered sparse 3D points that is different from densely structured color image. Therefore, applying a fixed structure of deep learning network on 3D point cloud is a challenging task in computer vision and graphics problems. Recently, researchers have proposed deep learning methods for 3D point cloud based on data conversion or simplification. However, they lose either local 3D shape information for the simplicity of method or geometric locality for using array as an input. In this paper we propose a new convolution technique, named Polypod convolution, for 3D point cloud description that is distribution independent and maintains both local and global 3D shapes. Quantitative and qualitative evaluation results show the potential of our new network for 3D point cloud based deep learning applications.	https://dl.acm.org/authorize?N675702	Hyunsoo Song, Seungkyu Lee
PanoAnnotator: a semi-automatic tool for indoor panorama layout annotation	We present PanoAnnotator, a semi-automatic system that facilitates the annotation of 2D indoor panoramas to obtain high-quality 3D room layouts. Observing that fully-automatic methods are often restricted to a subset of indoor panoramas and generate room layouts with mediocre quality, we instead propose a hybrid method to recover high-quality room layouts by leveraging both automatic estimations and user edits. Specifically, our system first employs state-of-the-art methods to automatically extract 2D/3D features from input panorama, based on which an initial Manhattan world layout is estimated. Then, the user can further edit the layout structure via a set of intuitive operations, while the system will automatically refine the geometry according to the extracted features. The experimental results show that our automatic initialization outperforms a selected fully-automatic state-of-the-art method in producing room layouts with higher accuracy. In addition, our complete system reduces annotation time when comparing with a fully-manual tool for achieving the same high quality results.	https://dl.acm.org/authorize?N675701	Shang-Ta Yang, Chi-Han Peng, Peter Wonka, Hung-Kuo Chu
Panoramic depth reconstruction within a single shot by optimizing global sphere radii	Depth estimation in scene reconstruction remains one of the main issues in the world of virtual reality. We propose a method that uses low cost camera facilities from previous papers and their specified procedures for stereoscopic 360 imaging. However, instead of using angular disparity, we use the spherical radius for labeling depth values for scene reconstruction. The experimental results show that the reconstructed shape is less distorted by directly optimizing spherical radii than optimizing the angular disparities.	https://dl.acm.org/authorize?N675857	Po-Yao Huang, Hong-Shiang Lin, Sun-Yu Gordon Chi, Liang-Han Lin, Ming Ouhyoung
Parallel iterative solvers for real-time elastic deformations	Physics-based animation of elastic materials allows to simulate dynamic deformable objects such as fabrics, human tissue, hair, etc. Due to their complex inner mechanical behaviour, it is difficult to replicate their motions interactively and accurately at the same time. This course introduces students and practitioners to several parallel iterative techniques to tackle this problem and achieve elastic deformations in real-time. We focus on techniques for applications such as video games and interactive design, with available for physically-based animation, and where responsiveness and stability are often more important than accuracy, as long as the results are believable. The course focuses on solvers able to fully exploit the computational capabilities of modern GPU architectures, effectively solving systems of hundreds of thousands of nonlinear equations in a matter of few milliseconds. The course introduces the basic concepts concerning physics-based elastic objects, and provide an overview of the different types of numerical solvers available in the literature. Then, we show how some variants of traditional solvers can address real-time animation and assess them in terms of accuracy, robustness and performance. Practical examples are provided throughout the course, in particular how to apply the depicted solvers to Projective Dynamics and Position-based Dynamics, two recent and popular physics models for elastic materials.	https://dl.acm.org/authorize?N675533	Marco Fratarcangeli, Huamin Wang, Yin Yang
Personalizing homemade bots with plug & play AI for STEAM education	In this study, we propose a new framework for hands-on educational modules to introduce ideas in AI and robotics casually, quickly, and effectively in one package for beginners of all ages in STEAM fields. Today, courses on introductory robotics are found everywhere, from K-12 summer camps to adult continuing education. However, most of them are limited to learning basic skills on sensor-actuator interactions due to their limited time and can rarely introduce what recent exciting AI can do, such as image recognition. As a case study to demonstrate the idea of the framework, an educational module to create a toy car with a camera controlled by Raspberry Pi is introduced. Our approach uses both physical and digital environments. Participants experience running their toy cars on a physical track using a convolutional neural network (CNN) trained based on how participants drive cars in a virtual game. The tested idea can be extensible as a framework to many other examples of robotics projects and can make ideas of AI and robotics more accessible to everyone. A proposed AI model is trained to assimilate the participant's game-play style in a VR environment which will be later re-enacted by the physical robot assembled by participants. Through this approach, we intend to demonstrate the AI's ability to personalize things and hope to stimulate participants' curiosity and motivation to learn.	https://dl.acm.org/authorize?N675676	Taro Narahara, Yoshihiro Kobayashi
PhotoShape: photorealistic materials for large-scale shape collections	The visual appearance of an object can be disguised by projecting virtual shading as if overwriting the material. However, conventional projection-mapping methods depend on markers on a target or a model of the target shape, which limits the types of targets and the visual quality. In this paper, we focus on the fact that the shading of a virtual material in a virtual scene is mainly characterized by surface normals of the target, and we attempt to realize markerless and modelless projection mapping for material representation. In order to deal with various targets, including static, dynamic, rigid, soft, and fluid objects, without any interference with visible light, we measure surface normals in the infrared region in real time and project material shading with a novel high-speed texturing algorithm in screen space. Our system achieved 500-fps high-speed projection mapping of a uniform material and a tileable-textured material with millisecond-order latency, and it realized dynamic and flexible material representation for unknown objects. We also demonstrated advanced applications and showed the expressive shading performance of our technique.	https://dl.acm.org/authorize?N664407	Keunhong Park, Konstantinos Rematas, Ali Farhadi, Steven M. Seitz
Pinscreen avatars in your pocket: mobile paGAN engine and personalized gaming	We will demonstrate how a lifelike 3D avatar can be instantly built from a single selfie input image using our own team members as well as a volunteer from the audience. We will showcase some additional 3D avatars built from internet photographs, and highlight the underlying technology such as our light-weight real-time facial tracking system. Then we will show how our automated rigging system enables facial performance capture as well as full body integration. We will showcase different body customization features and other digital assets, and show various immersive applications such as 3D selfie themes, multi-player games, all running on an iPhone.	https://dl.acm.org/authorize?N675666	Koki Nagano, Liwen Hu, Lain Goldwhite
Practical HDR and wide color techniques in gran turismo SPORT	A consistent workflow is important to fully demonstrate the attractiveness of high-quality CG content, especially when such content uses high dynamic range (HDR) and wide color gamut (WCG) techniques. This course explains a practical workflow useful for both game developers and creators involved in HDR / Wide color content. Even though CG quality has significantly improved over the past years, final output quality is restricted by limitations of luminance and color gamut of conventional output devices such as televisions. Recently HDR and Wide color technology has expanded these limitations but it is problematic to output high quality HDR images on each device, because consistent interpretations of both hardware behavior and software specification is difficult. Therefore, it is necessary to carefully establish reliable standards for stable outputs on various devices. For that purpose, we need a consistent theory-based approach for each aspect of the workflow (asset collecting and editing, interchangeable formats, encoding, preview environment, verification) and rendering pipeline (lighting, tone mapping, etc.). Using reliable standards enables us to gain robust outputs with high color reproducibility and high dynamic range accuracy. This course shares a wide range of knowledge from the basics of color science to the concrete solution used in the production of Gran Turismo SPORT, a photo realistic racing game with high quality HDR images. Participants can learn about real experience in developing HDR and WCG content.	https://dl.acm.org/authorize?N675534	Hajime Uchimura, Kentaro Suzuki
Practical dynamic facial appearance modeling and acquisition	In this paper, we introduce a novel and extremely fast algorithm to compute continuous transport maps between 2D probability densities discretized on uniform grids. The core of our method is a novel iterative solver computing the optimal transport map from a grid to the uniform density in the 2D Euclidean plane. A transport map between arbitrary densities is then recovered through numerical inversion and composition. In this case, the resulting map is only approximately optimal, but it is continuous and density preserving. Our solver is derivative-free, and it converges in a few cheap iterations. We demonstrate interactive performance in various applications such as adaptive sampling, feature sensitive remeshing, and caustic design.	https://dl.acm.org/citation.cfm?id=3295678	Evangelos Kalogerakis
Pre- and post-processes for automatic colorization using a fully convolutional network	Automatic colorization is a significant task especially for Anime industry. An original trace image to be colorized contains not only outlines but also boundary contour lines of shadows and highlight areas. Unfortunately, these lines tend to decrease the consistency among all images. Thus, this paper provides a method for a cleaning pre-process of anime dataset to improve the prediction quality of a fully convolutional network, and a refinement post-process to enhance the output of the network.	https://dl.acm.org/authorize?N675747	Sophie Ramassamy, Hiroyuki Kubo, Takuya Funatomi, Daichi Ishii, Akinobu Maejima, Satoshi Nakamura, Yasuhiro Mukaigawa
Production ray tracing of feature lines	Automated feature line drawing of virtual 3D objects helps artists depict shapes and allows for creating stylistic rendering effects. High-fidelity drawing of lines that are very thin or have varying thickness and color, or lines of recursively reflected and refracted objects, is a challenging task. In this paper we describe an image-based feature detection and line drawing method that integrates naturally into a ray tracing renderer and runs as a post-process, after the pixel sampling stage. Our method supports arbitrary camera projections and surface shaders, and its performance does not dependent on the geometric complexity of the scene but on the pixel sampling rate. By leveraging various attributes stored in every pixel sample, which are typically available in production renderers, e.g. for arbitrary output variables (AOVs), feature lines of reflected and refracted objects can be obtained with relative ease. The color and width of the lines can be driven by the surface shaders, which allows for achieving a wide variety of artistic styles.	https://dl.acm.org/authorize?N675672	Shinji Ogaki, Iliyan Georgiev
Pulse	PULSE was created by six students who recently graduated from Supinfocom Rubika, a 3D animation school. During the last year of the five-year program, they had the chance to work exclusively on a short film, about 7 minutes long in full CG. The original idea was found during the 4th year at school, and they developed the idea at this point. The production itself began in September 2017 and ended in June 2018.	https://dl.acm.org/authorize?N675634	Sarah Forest, Cécile Floucat, Pauline Javelot, Juliette Gales, Thibaut Wambre, Kevin De Garidel
RFIDesk: an interactive surface for multi-touch and rich-ID stackable tangible interactions	This work introduces RFIDesk, an interactive surface that enables both multi-touch and rich-ID stackable tangible interactions. By using ultra-high frequency (UHF) radio-frequency identification (RFID) technology, the RFIDesk can effectively identify the elements of a stack. Furthermore, this system integrates capacitive multi-touch sensing based on indium tin oxide (ITO) to effectively detect touch events while preserving the interface transparency, thus enabling rich visual feedback to be displayed under the stackable objects. The interference between the two sensing technologies is resolved by applying time-division multiplexing sampling. We use a tangible tower-defense game to demonstrate the interaction possibilities of this system.	https://dl.acm.org/authorize?N675760	Meng-Ju Hsieh, Rong-Hao Liang, Jr-Ling Guo, Bing-Yu Chen
Radioactive live soundscape	To understand the effects of the nuclear accident, long-term and wide-range monitoring of the effects of nuclear radiation on animals is required in Fukushima. For monitoring such species, counting the recorded calls of animals is considered an effective method to investigate the wildlife. The sounds of singing birds, buzzing insects, swaying leaves, and trickling water in a beautiful forest implicitly imprints the diversity of organism in the forest. However, it is difficult to use information devices in the exclusion zone as these areas do not have infrastructure services and necessary to develop a monitoring system capable of operating over multiple years in unmanned conditions. The project distributes the soundscape in the zone to the public via the Internet. A microphone has been placed at the entrance to a forest in the exclusion zone (37° 28′ 04.3″ N, 140° 55′ 27.5″ E), 10 km away from the Fukushima Nuclear Power Plant. This allows us to hear the soundscape from this location in real time. This project started with a year of literature search (2011), fundraising with government approval (2012-2014), and the installation of a transmitter station with satellite Internet (2015). Finally, based on official preparations and approvals, a local electric company agreed to sign a service contract with us in an intensive feasibility survey of the all transmission facilities of the location. The final construction was completed by the end of March in 2016. This project aims to watch over the lives that will come to an end in the near future and the new lives that appearing after humans left this place on 11.3.2011. Prior to this project, we had experience in operating a similar system for more than 10 years. Therefore, it is possible to operate this project until the reactor decommissioning work has been completed until approximately 2030.	https://dl.acm.org/authorize?N675602	Hiroki Kobayashi, Hiromi Kudo, Yuta Sasaki
Rapid prototyping system using transformable and adherable PCL blocks	In this paper, we propose a rapid prototyping system with a combination of blocks that change shape and firmly adhere to each other. We focused on PCL (polycaprolactone), which is a plastic characterized by a low melting point, as a block material. PCL blocks can be transformed and bonded many times by melting them with hot water. In this research, we implemented a system that transforms external data into a block diagram and creates blocks. It enables rapid prototyping with more flexibility and stronger adhesion than ordinary block assembly.	https://dl.acm.org/authorize?N675727	Keiichiro Taniguchi, Tomoko Hashida
Rapture of the deep	Rapture of the Deep is an interactive Virtual Reality experience with eye tracking. The Experience is set in an underwater scenario using eye tracking as the main mechanism which allows the environment to react to the player's gaze and attention. In this project we worked with a retrofitted version of the HTC Vive headset with a complete Eye Tracking integration by Tobii Pro and the Tobii Pro SDK for the Unity3D Engine. Rapture of the Deep seeks to test how eye tracking technology can be employed as an attentive and invisible user interface allowing people to use reflexive and emotional behavior as a game controller.	https://dl.acm.org/authorize?N664102	Enzio Probst, Vincent Suttner, Monja Dietrich, Theres Buehler
Real-time compression and streaming of 4D performances	Real-world materials are often layered: metallic paints, biological tissues, and many more. Variation in the interface and volumetric scattering properties of the layers leads to a rich diversity of material appearances from anisotropic highlights to complex textures and relief patterns. However, simulating light-layer interactions is a challenging problem. Past analytical or numerical solutions either introduce several approximations and limitations, or rely on expensive operations on discretized BSDFs, preventing the ability to freely vary the layer properties spatially. We introduce a new unbiased layered BSDF model based on Monte Carlo simulation, whose only assumption is the layer assumption itself. Our novel position-free path formulation is fundamentally more powerful at constructing light transport paths than generic light transport algorithms applied to the special case of flat layers, since it is based on a product of solid angle instead of area measures, so does not contain the high-variance geometry terms needed in the standard formulation. We introduce two techniques for sampling the position-free path integral, a forward path tracer with next-event estimation and a full bidirectional estimator. We show a number of examples, featuring multiple layers with surface and volumetric scattering, surface and phase function anisotropy, and spatial variation in all parameters.	https://dl.acm.org/citation.cfm?id=3295684	Seungyong Lee
Real-time measurement and display system of 3D sound intensity map using optical see-through head mounted display	We propose a system of real-time measurement and visualization for three-dimensional (3D) sound field by using the optical see-through head mounted display (OSTHMD) with simultaneous localization and mapping (SLAM). By using an estimation of spatial mapping, the system achieves free movement of measurement positions in a broad area without multiple AR markers. Visualizing the 3D sound intensity of an entire room by the proposed system helps us to design the sound field within a space.	https://dl.acm.org/authorize?N675748	Yuta Kataoka, Wataru Teraoka, Yasuhiro Oikawa, Yusuke Ikeda
Real-time visual representations for mobile mixed reality remote collaboration	In this study we present a Mixed-Reality based mobile remote collaboration system that enables an expert providing real-time assistance over a physical distance. By using the Google ARCore position tracking, we can integrate the keyframes captured with one external depth sensor attached to the mobile phone as one single 3D point-cloud data set to present the local physical environment into the VR world. This captured local scene is then wirelessly streamed to the remote side for the expert to view while wearing a mobile VR headset (HTC VIVE Focus). In this case, the remote expert can immerse himself/herself in the VR scene and provide guidance just as sharing the same work environment with the local worker. In addition, the remote guidance is also streamed back to the local side as an AR cue overlaid on top of the local video see-through display. Our proposed mobile remote collaboration system supports a pair of participants performing as one remote expert guiding one local worker on some physical tasks in a more natural and efficient way in a large scale work space from a distance by simulating the face-to-face co-work experience using the Mixed-Reality technique.	https://dl.acm.org/authorize?N664103	Lei Gao, Huidong Bai, Weiping He, Mark Billinghurst, Robert W. Lindeman
Real-virtual bridge: a modular mechanism to mediate between real and virtual objects	The practical use of virtual reality (VR) is expected to be extended to a wide range of applications after its successful deployment in the entertainment sector. Currently, the domain of VR applications crosses the boundary between the real world and the virtual world. Therefore, we propose a real-virtual bridge, a conceptual model that can be used to mediate between real and virtual objects. We introduce the concept and architecture of a real-virtual bridge and describe two implementations of the bridge on smartphones and microscopes. Although concepts simlar to a real-virtual bridge exist in science fiction works, we explicitly define its architecture in this paper. We believe that a real-virtual bridge promotes emerging applications by integrating real and virtual objects.	https://dl.acm.org/authorize?N675738	Tomomi Takashina, Yuji Kokumai
Realistic AR makeup over diverse skin tones on mobile	We propose a novel approach to the application of realistic makeup over a diverse set of skin tones in mobile phones using augmented reality. The method we developed mimics the real world layering techniques and application that makeup artists use. We can accurately represent the five most commonly used materials found in commercial makeup products- Matte, Velvet, Glossy, Glitter, and Metallic. We apply skin smoothing to even out the natural skin tone and tone-mapping to further blend source and synthetic layers.	https://dl.acm.org/authorize?N675858	Bruno Evangelista, Houman Meshkin, Helen Kim, Anaelisa Aburto, Ben Max Rubinstein, Andrea Ho
Reconstruction of volumetric reflectance using spatio-sequential frequency correlation imaging	In this paper, we propose a novel pro-cam technique for reconstruction of the volumetric reflectance inside an object. The key concept is use of spatio-sequentially modulated illumination to extract only the required signal at a 3D point. We discovered an effect where a projector and camera pair with different focal lengths naturally produced a spatial frequency modulation. By combining this effect with a direct conversion technique to demodulate the signals, the resulting spatio-sequential frequency correlation enables reconstruction of the reflectance at a 3D point within the object. Experimental results based on both synthetic and real data show that the proposed method can reconstruct volumetric reflectance discretely.	https://dl.acm.org/authorize?N675686	Tsuyoshi Takatani, Takahito Aoto, Kenichiro Tanaka, Takuya Funatomi, Yasuhiro Mukaigawa
Recurrent transition networks for character locomotion	We present a novel approach, based on deep recurrent neural networks, to automatically generate transition animations given a past context of a few frames, a target character state and optionally local terrain information. The proposed Recurrent Transition Network (RTN) is trained without any gait, phase, contact or action labels. Our system produces realistic and fluid transitions that rival the quality of Motion Capture-based animations, even without any inverse-kinematics post-process. Our system could accelerate the creation of transition variations for large coverage or even replace transition nodes in a game's animation graph. The RTN also shows impressive results on a temporal super-resolution task.	https://dl.acm.org/authorize?N675661	Félix G. Harvey, Christopher Pal
Reflection	I expressed the process of one's reflection through this short film. Senses, seen as spheres in the movie, travel and pass through the flexible texture of the mind. They leave their own traces as various shapes on the texture of the mind. The traces reform and become the memories.	https://dl.acm.org/authorize?N675645	Ihsu Yoon
Relaxushion: controlling the rhythm of breathing for relaxation by overwriting somatic sensation	"In this study, we propose the method to control the rhythm of breathing for relaxation by overwriting somatic sensation. Breathing way has an essential role in controlling the state of mind and body. Thus, lots of studies tried to instruct the breathing rhythm using lights, sounds, vibrations, and so on. However, these approaches require prior training to adjust a user's breathing rhythm to the system. To improve the effectiveness of controlling the breathing rhythm, we focus on the approach of overwriting somatic sensation. We hypothesized that we could modify the breathing rhythm if users get a device's motion confused with their breathing motion. Thus, we constructed the cushion type device ""Relaxushion"" which presents the breathing motion. When we embrace this breathing cushion, we feel like putting our hands on our stomachs. A brief user study shows our method can control the rhythm of breathing without prior training and consciousness to the device."	https://dl.acm.org/authorize?N675769	Yuki Ban, Hiroyuki Karasawa, Rui Fukui, Shin' ichi Warisawa
Removing objects from videos with a few strokes	We present a system for the removal of objects from videos. As an input, the system only needs a user to draw a few strokes in at least one frame, roughly delimiting the objects to be removed. These rough masks are then automatically refined and propagated through the video. The corresponding regions are resynthesized using video inpainting techniques. Our system is able to deal with multiple, possibly crossing objects, with complex motions and with dynamic textures. This results in a computational tool that can alleviate tedious manual operations for editing high-quality videos.	https://dl.acm.org/authorize?N675689	Thuc Trinh Le, Andrés Almansa, Yann Gousseau, Simon Masnou
Research and development of augmented FPV drone racing system	Augmented FPV Drone Racing is a system that allows spectators to understand the situation of drone races easily by using augmented feedback techniques, including projection mapping and autonomous commentaries. In this project, we have been developing visualization solutions for FPV (first person view) drone racing to allow the spectators/pilots to understand easily (or intuitively) the race situation.	https://dl.acm.org/authorize?N675786	Koh Sueda, Takashi Kitada, Yushin Suzuki, Taiki Wada
Retinal HDR: HDR image projection method onto retina	The dynamic range of display is much lower than the one perceived by human eye. This problem has been studied in both aspects of photography and display [Debevec and Malik 1997; Hirsch et al. 2014].	https://dl.acm.org/authorize?N675859	Yuta Itoh, Kenta Yamamoto, Yoichi Ochiai
Reverie [best student film award]	Deep in the harsh countryside, where life fades with every breath, a boy tormented with grief battles an inescapable beast.	https://dl.acm.org/authorize?N675646	Philip Louis Piaget Rodriguez
Ring graphs in VR: exploring a new and novel method for node placement and link visibility in VR-based graph analysis	We present a new and novel graph visualization technique designed specifically for virtual reality (VR). Ring graphs organize graph nodes by categorical attributes along a ring that are placed in a sphere layout. Links between nodes are drawn within the rings using an edge bundling technique. This 3D placement of data takes advantage of the stereoscopic environment that VR offers. We conducted a user study that compared our ring visualization to a traditional node-based graph visualization and found that our ring graph method had higher usability, both in terms of accuracy in completing a set of tasks as well as lower task completion time.	https://dl.acm.org/authorize?N675749	Mikhail Sorokin, Galen Stetsyuk, Raghav Gupta, Alex Busch, College Park, Brian Russin, Celeste Lyn Paul, Samir Khuller
Robust deep residual denoising for Monte Carlo rendering	We propose a Deep Residual Learning based method that consistently outperforms both the state-of-the-art handcrafted denoisers and learning-based methods for single-image Monte Carlo denoising. Unlike the nature of existing learning-based methods which estimate the parameters and kernel weights of a filter, we map the noisy input image to its noise-free counterpart. Our method uses only three common auxiliary features (depth, normal, and albedo), and this minimal requirement on auxiliary data simplifies both the training and integration of our method into most production rendering pipelines. We have evaluated our method on unseen images produced by a different renderer. Consistently high quality denoising results are obtained in all cases. We plan to release our training dataset as we are aware that the lack of publicly available training data is currently an entry barrier of learning based denoising research for Monte Carlo rendering.	https://dl.acm.org/authorize?N675671	Kin-Ming Wong, Tien-Tsin Wong
Robust flow-guided neural prediction for sketch-based freeform surface modeling	We introduce a realtime compression architecture for 4D performance capture that is two orders of magnitude faster than current state-of-the-art techniques, yet achieves comparable visual quality and bitrate. We note how much of the algorithmic complexity in traditional 4D compression arises from the necessity to encode geometry using an model (i.e. a triangle mesh). In contrast, we propose an encoder that leverages an representation (namely a Signed Distance Function) to represent the observed geometry, as well as its changes through time. We demonstrate how SDFs, when defined over a small local region (i.e. a block), admit a low-dimensional embedding due to the innate geometric redundancies in their representation. We then propose an optimization that takes a Truncated SDF (i.e. a TSDF), such as those found in most rigid/non-rigid reconstruction pipelines, and efficiently projects each TSDF block onto the SDF latent space. This results in a collection of low entropy tuples that can be effectively quantized and symbolically encoded. On the decoder side, to avoid the typical artifacts of block-based coding, we also propose a variational optimization that compensates for quantization residuals in order to penalize unsightly discontinuities in the decompressed signal. This optimization is expressed in the SDF latent embedding, and hence can also be performed efficiently. We demonstrate our compression/decompression architecture by realizing, to the best of our knowledge, the first system for streaming a real-time captured 4D performance on consumer-level networks.	https://dl.acm.org/authorize?N675555	Wallace Lira, Chi-Wing Fu, Hao Zhang
SCORES: shape composition with recursive substructure priors	The advent of consumer depth cameras has incited the development of a new cohort of algorithms tackling challenging computer vision problems. The primary reason is that depth provides direct geometric information that is largely invariant to texture and illumination. As such, substantial progress has been made in human and object pose estimation, 3D reconstruction and simultaneous localization and mapping. Most of these algorithms naturally benefit from the ability to accurately track the pose of an object or scene of interest from one frame to the next. However, commercially available depth sensors (typically running at 30fps) can allow for large inter-frame motions to occur that make such tracking problematic. A high frame rate depth camera would thus greatly ameliorate these issues, and further increase the tractability of these computer vision problems. Nonetheless, the depth accuracy of recent systems for high-speed depth estimation [Fanello et al. 2017b] can degrade at high frame rates. This is because the active illumination employed produces a low SNR and thus a high exposure time is required to obtain a dense accurate depth image. Furthermore in the presence of rapid motion, longer exposure times produce artifacts due to motion blur, and necessitates a lower frame rate that introduces large inter-frame motion that often yield tracking failures. In contrast, this paper proposes a novel combination of hardware and software components that avoids the need to compromise between a dense accurate depth map and a high frame rate. We document the creation of a full 3D capture system for high speed and quality depth estimation, and demonstrate its advantages in a variety of tracking and reconstruction tasks. We extend the state of the art active stereo algorithm presented in Fanello et al. [2017b] by adding a space-time feature in the matching phase. We also propose a machine learning based depth refinement step that is an order of magnitude faster than traditional postprocessing methods. We quantitatively and qualitatively demonstrate the benefits of the proposed algorithms in the acquisition of geometry in motion. Our pipeline executes in 1.1ms leveraging modern GPUs and off-the-shelf cameras and illumination components. We show how the sensor can be employed in many different applications, from [non-]rigid reconstructions to hand/face tracking. Further, we show many advantages over existing state of the art depth camera technologies beyond framerate, including latency, motion artifacts, multi-path errors, and multi-sensor interference.	https://dl.acm.org/authorize?N664426	Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi, Hao Zhang
SFV: reinforcement learning of physical skills from videos	Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. We take a model-free deep reinforcement learning (deepRL) approach to automatically discovering robust dressing control policies represented by neural networks. While deepRL has demonstrated several successes in learning complex motor skills, the data-demanding nature of the learning algorithms is at odds with the computationally costly cloth simulation required by the dressing task. This paper is the first to demonstrate that, with an appropriately designed input state space and a reward function, it is possible to incorporate cloth simulation in the deepRL framework to learn a robust dressing control policy. We introduce a salient representation of haptic information to guide the dressing process and utilize it in the reward function to provide learning signals during training. In order to learn a prolonged sequence of motion involving a diverse set of manipulation skills, such as grasping the edge of the shirt or pulling on a sleeve, we find it necessary to separate the dressing task into several subtasks and learn a control policy for each subtask. We introduce a algorithm that matches the distribution of output states from one task to the input distribution for the next task in the sequence. We have used this approach to produce character controllers for several dressing tasks: putting on a t-shirt, putting on a jacket, and robot-assisted dressing of a sleeve.	https://dl.acm.org/authorize?N664483	Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, Sergey Levine
Sampling analysis using correlations for monte carlo rendering	Point patterns and stochastic structures lie at the heart of Monte Carlo based numerical integration schemes. Physically based rendering algorithms have largely benefited from these Monte Carlo based schemes that inherently solve very high dimensional light transport integrals. However, due to the underlying stochastic nature of the samples, the resultant images are corrupted with noise (unstructured aliasing or variance). This also results in bad convergence rates that prohibit using these techniques in more interactive environments (e.g. games, virtual reality). With the advent of smart rendering techniques and powerful computing units (CPUs/GPUs), it is now possible to perform physically based rendering at interactive rates. However, much is left to understand regarding the underlying sampling structures and patterns which are the primary cause of error in rendering. This course surveys the most recent state-of-the-art frameworks that are developed to better understand the impact of samples' structure on the error and its convergence during Monte Carlo integration. It provides best practices and a set of tools for easy integration of such frameworks for sampling decisions in rendering. We revisit stochastic point processes that offers a unified theory explaining stochastic structures and sampling patterns in a common principled framework. We show how this theory generalizes spectral tools developed over the years to analyze error and convergence rates, and allows for analysis of more complex point patterns with adaptive density and correlations. At the end of the course, the audience will have a comprehensive understanding of both theoretical and practical aspects of point processes that would guide them in choosing and designing sampling strategies for applications specific to Monte Carlo rendering. A codebase and web application for easy use of the introduced techniques will also be made available on https://github.com/sinbag/SamplingAnalysisWithCorrelations.	https://dl.acm.org/authorize?N675545	A. Cengiz Öztireli, Gurprit Singh
Sans gravité	SANS GRAVITÉ was created by six students who recently graduated from Supinfocom Rubika, a 3D animation school. During the last year of the five-year program, they had the chance to work exclusively on a short film, about 7 minutes long in full CG. The original idea was found during the 4th year at school, and they developed the idea at this point. The production itself began in September 2017 and ended in June 2018.	https://dl.acm.org/authorize?N675647	Charline Parisot, Jérémy Cissé, Fioretta Caterina Cosmidis, Flore Allier-Estrada, Maud Lemaître-Blanchart, Ludovic Abraham
Scientific and visual effects software integration for the visualization of a chromatophore	The scientific visualization of a chromatophore, a photosynthetic organelle, required the creation of a software integration pipeline to combine scientific software, visual effects tools, and custom camera choreography software. Furthermore, the rendering of this visualization in both fulldome and 4K3D format was done using a custom supercomputer rendering pipeline.	https://dl.acm.org/authorize?N675739	Kalina Borkiewicz, A J Christensen, Stuart Levy, Robert Patterson, Donna Cox, Jeff Carpenter
Selective guided sampling with complete light transport paths	Sketching provides an intuitive user interface for communicating free form shapes. While human observers can easily envision the shapes they intend to communicate, replicating this process algorithmically requires resolving numerous ambiguities. Existing sketch-based modeling methods resolve these ambiguities by either relying on expensive user annotations or by restricting the modeled shapes to specific narrow categories. We present an approach for modeling generic freeform 3D surfaces from sparse, expressive 2D sketches that overcomes both limitations by incorporating convolution neural networks (CNN) into the sketch processing workflow. Given a 2D sketch of a 3D surface, we use CNNs to infer the depth and normal maps representing the surface. To combat ambiguity we introduce an intermediate CNN layer that models the dense curvature direction, or flow, field of the surface, and produce an additional output confidence map along with depth and normal. The flow field guides our subsequent surface reconstruction for improved regularity; the confidence map trained unsupervised measures ambiguity and provides a robust estimator for data fitting. To reduce ambiguities in input sketches users can refine their input by providing optional depth values at sparse points and curvature hints for strokes. Our CNN is trained on a large dataset generated by rendering sketches of various 3D shapes using non-photo-realistic line rendering (NPR) method that mimics human sketching of free-form shapes. We use the CNN model to process both single- and multi-view sketches. Using our multi-view framework users progressively complete the shape by sketching in different views, generating complete closed shapes. For each new view, the modeling is assisted by partial sketches and depth cues provided by surfaces generated in earlier views. The partial surfaces are fused into a complete shape using predicted confidence levels as weights. We validate our approach, compare it with previous methods and alternative structures, and evaluate its performance with various modeling tasks. The results demonstrate our method is a new approach for efficiently modeling freeform shapes with succinct but expressive 2D sketches.	https://dl.acm.org/citation.cfm?id=3295676	Mirela Ben-Chen
Self-umbrelling turns over subjective direction of gravity	Self-umbrelling is a head-mounted display (HMD) interaction system that provides an experience approximating an out-of-body experience (OBE) involving the reversal of the subjective perception of the direction of gravity. Specifically, opening an umbrella while lying on one's back switches one's view from the first-person perspective (1PP) to a third-person perspective (3PP) originating from a position just above the supine body. In addition, the base of the 3PP moves upward every time the umbrella is opened. Thus, through the periodic action of opening the umbrella, the 1PP offers the experience of blowing something away, while the 3PP offers that of being blown away. This interaction is expected to activate a potentially hidden cognitive function related to OBEs, bringing up an important subject for designing HMD interaction between a player and an avatar.	https://dl.acm.org/authorize?N664104	Kenri Kodaka, Koyo Mori
Semantic object reconstruction via casual handheld scanning	We present a method to acquire dynamic properties of facial skin appearance, including dynamic diffuse albedo encoding blood flow, dynamic specular intensity, and per-frame high resolution normal maps for a facial performance sequence. The method reconstructs these maps from a purely passive multi-camera setup, without the need for polarization or requiring temporally multiplexed illumination. Hence, it is very well suited for integration with existing passive systems for facial performance capture. To solve this seemingly underconstrained problem, we demonstrate that albedo dynamics during a facial performance can be modeled as a combination of: (1) a static, high-resolution base albedo map, modeling full skin pigmentation; and (2) a dynamic, one-dimensional component in the CIE L*a*b* color space, which explains changes in hemoglobin concentration due to blood flow. We leverage this albedo subspace and additional constraints on appearance and surface geometry to also estimate specular reflection parameters and resolve high-resolution normal maps with unprecedented detail in a passive capture system. These constraints are built into an inverse rendering framework that minimizes the difference of the rendered face to the captured images, incorporating constraints from multiple views for every texel on the face. The presented method is the first system capable of capturing high-quality dynamic appearance maps at full resolution and video framerates, providing a major step forward in the area of facial appearance acquisition.	https://dl.acm.org/authorize?N664436	Hsueh-Ti Derek Liu, Michael Tao, Alec Jacobson
Session details: Acquiring and editing geometry via RGB (D) images	Discrete orthogonal geodesic nets (DOGs) are a quad mesh analogue of developable surfaces. In this work we study continuous deformations on these discrete objects. Our main theoretical contribution is the characterization of the shape space of DOGs for a given net connectivity. We show that generally, this space is locally a manifold of a fixed dimension, apart from a set of singularities, implying that DOGs are continuously deformable. Smooth flows can be constructed by a smooth choice of vectors on the manifold's tangent spaces, selected to minimize a desired objective function under a given metric. We show how to compute such vectors by solving a linear system, and we use our findings to devise a geometrically meaningful way to handle singular points. We base our shape space metric on a novel DOG Laplacian operator, which is proved to converge under sampling of an analytical orthogonal geodesic net. We further show how to extend the shape space of DOGs by supporting creases and curved folds and apply the developed tools in an editing system for developable surfaces that supports arbitrary bending, stretching, cutting, (curved) folds, as well as smoothing and subdivision operations.	https://dl.acm.org/citation.cfm?id=3295674	Tamy Boubekeur
Session details: Acquisition, rendering and display for virtual reality	"We describe an interactive design tool for authoring, simulating, and adjusting yarn-level patterns for knitted and woven cloth. To achieve interactive performance for notoriously slow yarn-level simulations, we propose two acceleration schemes: (a) yarn-level periodic boundary conditions that enable the restricted simulation of only small periodic patches, thereby exploiting the spatial repetition of many cloth patterns in cardinal directions, and (b) a highly parallel GPU solver for efficient yarn-level simulation of the small patch. Our system supports interactive pattern editing and simulation, and runtime modification of parameters. To adjust the amount of material used (yarn take-up) we support ""on the fly"" modification of (a) local yarn rest-length adjustments for pattern specific edits, e.g., to tighten slip stitches, and (b) global yarn length by way of a novel yarn-radius similarity transformation. We demonstrate the tool's ability to support interactive modeling, by novice users, of a wide variety of yarn-level knit and woven patterns. Finally, to validate our approach, we compare dozens of generated patterns against reference images of actual woven or knitted cloth samples, and we release this corpus of digital patterns and simulated models as a public dataset to support future comparisons."	https://dl.acm.org/citation.cfm?id=3295669	Piotr Didyk
Session details: Aerial propagation	We propose a real-time method for the infrastructure-free estimation of articulated human motion. The approach leverages a swarm of camera-equipped flying robots and optimizes the swarm's and skeletal states, which include the 3D joint positions and a set of bones. Our method allows to track the motion of human subjects, for example an athlete, over long time horizons and long distances, in challenging settings and at large scale, where fixed infrastructure approaches are not applicable. The proposed algorithm uses active infra-red markers, runs in real-time and accurately estimates robot and human pose parameters online without the need for accurately calibrated or stationary mounted cameras. Our method i) estimates a global coordinate frame for the MAV swarm, ii) jointly optimizes the human pose and relative camera positions, and iii) estimates the length of the human bones. The entire swarm is then controlled via a model predictive controller to maximize visibility of the subject from multiple viewpoints even under fast motion such as jumping or jogging. We demonstrate our method in a number of difficult scenarios including capture of long locomotion sequences at the scale of a triplex gym, in non-planar terrain, while climbing and in outdoor scenarios.	https://dl.acm.org/citation.cfm?id=3295665	Karen Liu
Session details: Beyond light transport	We propose a novel approach for performing convolution of signals on curved surfaces and show its utility in a variety of geometric deep learning applications. Key to our construction is the notion of directional functions defined on the surface, which extend the classic real-valued signals and which can be naturally convolved with with real-valued template functions. As a result, rather than trying to fix a canonical orientation or only keeping the maximal response across all alignments of a 2D template at every point of the surface, as done in previous works, we show how information across all rotations can be kept across different layers of the neural network. Our construction, which we call , or for short, allows, in particular, to propagate and relate directional information across layers and thus different regions on the shape. We first define directional convolution in the continuous setting, prove its key properties and then show how it can be implemented in practice, for shapes represented as triangle meshes. We evaluate directional convolution in a wide variety of learning scenarios ranging from classification of signals on surfaces, to shape segmentation and shape matching, where we show a significant improvement over several baselines.	https://dl.acm.org/authorize?N664439	Jocelyn Meyron, Quentin Mérigot, Boris Thibert
Session details: Capturing 4D performances	Realistic rendering with materials that exhibit high-frequency spatial variation remains a challenge, as eliminating spatial and temporal aliasing requires prohibitively high sampling rates. Recent work has made the problem more tractable, however existing methods remain prohibitively expensive when using large environmental lights and/or (correctly filtered) global illumination. We present an appearance model with explicit high-frequency micro-normal variation, and a filtering approach that scales to multi-dimensional shading integrals. By combining a novel and compact half-vector histogram scheme with a directional basis expansion, we accurately compute the integral of filtered high-frequency reflectance over large lights with angularly varying emission. Our approach is scalable, rendering images indistinguishable from ground truth at over 10× the speed of the state-of-the-art and with only 15% the memory footprint. When filtering appearance with global illumination, we outperform the state-of-the-art by ~30×.	https://dl.acm.org/authorize?N675562	Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, Gabriel Brostow
Session details: Character animation	Data-driven character animation based on motion capture can produce highly naturalistic behaviors and, when combined with physics simulation, can provide for natural procedural responses to physical perturbations, environmental changes, and morphological discrepancies. Motion capture remains the most popular source of motion data, but collecting mocap data typically requires heavily instrumented environments and actors. In this paper, we propose a method that enables physically simulated characters to learn skills from videos (SFV). Our approach, based on deep pose estimation and deep reinforcement learning, allows data-driven animation to leverage the abundance of publicly available video clips from the web, such as those from YouTube. This has the potential to enable fast and easy design of character controllers simply by querying for video recordings of the desired behavior. The resulting controllers are robust to perturbations, can be adapted to new settings, can perform basic object interactions, and can be retargeted to new morphologies via reinforcement learning. We further demonstrate that our method can predict potential human motions from still images, by forward simulation of learned controllers initialized from the observed pose. Our framework is able to learn a broad range of dynamic skills, including locomotion, acrobatics, and martial arts. (Video )	https://dl.acm.org/citation.cfm?id=3295664	Stelian Coros
Session details: Fabulously computed fashion	Recreating the appearance of humans in virtual environments for the purpose of movie, video game, or other types of production involves the acquisition of a geometric representation of the human body and its scattering parameters which express the interaction between the geometry and light propagated throughout the scene. Teeth appearance is defined not only by the light and surface interaction, but also by its internal geometry and the intra-oral environment, posing its own unique set of challenges. Therefore, we present a system specifically designed for capturing the optical properties of live human teeth such that they can be realistically re-rendered in computer graphics. We acquire our data in vivo in a conventional multiple camera and light source setup and use exact geometry segmented from intra-oral scans. To simulate the complex interaction of light in the oral cavity during inverse rendering we employ a novel pipeline based on derivative path tracing with respect to both optical properties and geometry of the inner dentin surface. The resulting estimates of the global derivatives are used to extract parameters in a joint numerical optimization. The final appearance faithfully recreates the acquired data and can be directly used in conventional path tracing frameworks for rendering virtual humans.	https://dl.acm.org/citation.cfm?id=3295670	Moritz Baecher
Session details: Faces, faces, faces	Low-distortion mapping of three-dimensional surfaces to the plane is a critical problem in geometry processing. The intrinsic distortion introduced by these UV mappings is highly dependent on the choice of surface cuts that form seamlines which break mapping continuity. Parameterization applications typically require UV maps with an application-specific upper bound on distortion to avoid mapping artifacts; at the same time they seek to reduce cut lengths to minimize discontinuity artifacts. We propose , an algorithm that jointly optimizes the parameterization and cutting of a three-dimensional mesh. OptCuts starts from an arbitrary initial embedding and a user-requested distortion bound. It requires no parameter setting and automatically seeks to minimize seam lengths subject to satisfying the distortion bound of the mapping computed using these seams. OptCuts alternates between topology and geometry update steps that consistently decrease distortion and seam length, producing a UV map with compact boundaries that strictly satisfies the distortion bound. OptCuts automatically produces high-quality, globally bijective UV maps without user intervention. While OptCuts can thus be a highly effective tool to create new mappings from scratch, we also show how it can be employed to improve pre-existing embeddings. Additionally, when semantic or other priors on seam placement are desired, OptCuts can be extended to respect these user preferences as constraints during optimization of the parameterization. We demonstrate the scalable performance of OptCuts on a wide range of challenging benchmark parameterization examples, as well as in comparisons with state-of-the-art UV methods and commercial tools.	https://dl.acm.org/authorize?N664448	Chen Cao, Menglei Chai, Oliver Woodford, Linjie Luo
Session details: Fun in geometry & fabrication	Existing online 3D shape repositories contain thousands of 3D models but lack photorealistic appearance. We present an approach to automatically assign high-quality, realistic appearance models to large scale 3D shape collections. The key idea is to jointly leverage three types of online data - shape collections, material collections, and photo collections, using the photos as reference to guide assignment of materials to shapes. By generating a large number of synthetic renderings, we train a convolutional neural network to classify materials in real photos, and employ 3D-2D alignment techniques to transfer materials to different parts of each shape model. Our system produces photorealistic, relightable, 3D shapes (PhotoShapes).	https://dl.acm.org/citation.cfm?id=3295667	Niloy Mitra
Session details: Geometry generation	Finding good global importance sampling strategies for Monte Carlo light transport is challenging. While estimators using local methods (such as BSDF sampling or next event estimation) often work well in the majority of a scene, small regions in path space can be sampled insufficiently (e.g. a reflected caustic). We propose a novel data-driven guided sampling method which selectively adapts to such problematic regions and complements the unguided estimator. It is based on complete transport paths, i.e. is able to resolve the correlation due to BSDFs and free flight distances in participating media. It is conceptually simple and places anisotropic truncated Gaussian distributions around guide paths to reconstruct a continuous probability density function (guided PDF). Guide paths are iteratively sampled from the guided as well as the unguided PDF and only recorded if they cause high variance in the current estimator. While plain Monte Carlo samples paths independently and Markov chain-based methods perturb a single current sample, we determine the reconstruction kernels by a set of neighbouring paths. This enables local exploration of the integrand without detailed balance constraints or the need for analytic derivatives. We show that our method can decompose the path space into a region that is well sampled by the unguided estimator and one that is handled by the new guided sampler. In realistic scenarios, we show 4× speedups over the unguided sampler.	https://dl.acm.org/citation.cfm?id=3295673	Richard (Hao) Zhuang
Session details: Get wired	Free-viewpoint image-based rendering (IBR) is a standing challenge. IBR methods combine warped versions of input photos to synthesize a novel view. The image quality of this combination is directly affected by geometric inaccuracies of multi-view stereo (MVS) reconstruction and by view- and image-dependent effects that produce artifacts when contributions from different input views are blended. We present a new deep learning approach to blending for IBR, in which we use held-out real image data to learn blending weights to combine input photo contributions. Our Deep Blending method requires us to address several challenges to achieve our goal of interactive free-viewpoint IBR navigation. We first need to provide sufficiently accurate geometry so the Convolutional Neural Network (CNN) can succeed in finding correct blending weights. We do this by combining two different MVS reconstructions with complementary accuracy vs. completeness tradeoffs. To tightly integrate learning in an interactive IBR system, we need to adapt our rendering algorithm to produce a fixed number of input layers that can then be blended by the CNN. We generate training data with a variety of captured scenes, using each input photo as ground truth in a held-out approach. We also design the network architecture and the training loss to provide high quality novel view synthesis, while reducing temporal flickering artifacts. Our results demonstrate free-viewpoint IBR in a wide variety of scenes, clearly surpassing previous methods in visual quality, especially when moving far from the input cameras.	https://dl.acm.org/authorize?N675556	Luigi Malomo, Jesús Pérez, Emmanuel Iarussi, Nico Pietroni, Eder Miguel, Paolo Cignoni, Bernd Bickel
Session details: How people look and move	Many analysis tasks for human motion rely on high-level similarity between sequences of motions, that are not an exact matches in joint angles, timing, or ordering of actions. Even the same movements performed by the same person can vary in duration and speed. Similar motions are characterized by similar sets of actions that appear frequently. In this paper we introduce and that are a succinct but descriptive representation of motion sequences. We first break the motion sequences to short-term movements called motion words, and then cluster the words in a high-dimensional feature space to find motifs. Hence, motifs are words that are both common and descriptive, and their distribution represents the motion sequence. To cluster words and find motifs, the challenge is to define an effective feature space, where the distances among motion words are semantically meaningful, and where variations in speed and duration are handled. To this end, we use a deep neural network to embed the motion words into feature space using a triplet loss function. To define a signature, we choose a finite set of motion-motifs, creating a bag-of-motifs representation for the sequence. Motion signatures are agnostic to movement order, speed or duration variations, and can distinguish fine-grained differences between motions of the same class. We illustrate examples of characterizing motion sequences by motifs, and for the use of motion signatures in a number of applications.	https://dl.acm.org/citation.cfm?id=3295666	Manfred Lau
Session details: Image processing	We introduce an extremely scalable and efficient yet simple palette-based image decomposition algorithm. Given an RGB image and set of palette colors, our algorithm decomposes the image into a set of additive mixing layers, each of which corresponds to a palette color applied with varying weight. Our approach is based on the geometry of images in RGBXY-space. This new geometric approach is orders of magnitude more efficient than previous work and requires no numerical optimization. We provide an implementation of the algorithm in 48 lines of Python code. We demonstrate a real-time layer decomposition tool in which users can interactively edit the palette to adjust the layers. After preprocessing, our algorithm can decompose 6 MP images into layers in 20 milliseconds.	https://dl.acm.org/authorize?N675550	Lijun Wang, Xiaohui Shen, Jianming Zhang, Oliver Wang, Zhe Lin, Chih-Yao Hsieh, Sarah Kong, Huchuan Lu
Session details: Learning geometry	Large-scale binder jetting provides a promising alternative to manual sculpting of sandstone. The weak build material, however, severely limits its use in architectural ornamentation. We propose a structural optimization that jointly optimizes an ornament's strength-to-weight ratio and balance under self-weight, thermal, wind, and live loads. To account for the difference in the tensile and compressive strength of the build material, we turn the Bresler-Pister criterion into a failure potential, measuring the distance to failure. Integrated into an XFEM-based level set formulation, we minimize this potential by changing the topology and shape of the internal structure. To deal with uncertainties in the location of live loads, and the direction of wind loads, we first estimate loads that lead to the weakest structure, then minimize the potential of failure under identified worst-case loads. With the help of first-order optimality constraints, we unify our worst-case load estimation and structural optimization into a optimization. We demonstrate applications in art, furniture design, and architectural ornamentation with three large-scale 3D printed examples.	https://dl.acm.org/authorize?N664442	Lin Gao, Jie Yang, Yi-Ling Qiao, Yu-Kun Lai, Paul L. Rosin, Weiwei Xu, Shihong Xia
Session details: Learning to compose & decompose	We present an Adaptive Octree-based Convolutional Neural Network (Adaptive O-CNN) for efficient 3D shape encoding and decoding. Different from volumetric-based or octree-based CNN methods that represent a 3D shape with voxels in the same resolution, our method represents a 3D shape adaptively with octants at different levels and models the 3D shape within each octant with a planar patch. Based on this adaptive patch-based representation, we propose an Adaptive O-CNN encoder and decoder for encoding and decoding 3D shapes. The Adaptive O-CNN encoder takes the planar patch normal and displacement as input and performs 3D convolutions only at the octants at each level, while the Adaptive O-CNN decoder infers the shape occupancy and subdivision status of octants at each level and estimates the best plane normal and displacement for each leaf octant. As a general framework for 3D shape analysis and generation, the Adaptive O-CNN not only reduces the memory and computational cost, but also offers better shape generation capability than the existing 3D-CNN approaches. We validate Adaptive O-CNN in terms of efficiency and effectiveness on different shape analysis and generation tasks, including shape classification, 3D autoencoding, shape prediction from a single image, and shape completion for noisy and incomplete point clouds.	https://dl.acm.org/citation.cfm?id=3295672	Daniel Ritchie
Session details: Low-level imaging	We propose a method for accurately simulating dissipative forces in deformable bodies when using optimization-based integrators. We represent such forces using which may be nonlinear in both positions and velocities, enabling us to model a range of dissipative effects including Coulomb friction, Rayleigh damping, and power-law dissipation. We propose a general method for incorporating dissipative forces into optimization-based time integration schemes, which hitherto have been applied almost exclusively to systems with only conservative forces. To improve accuracy and minimize artificial damping, we provide an optimization-based version of the second-order accurate TR-BDF2 integrator. Finally, we present a method for modifying arbitrary dissipation functions to conserve linear and angular momentum, allowing us to eliminate the artificial angular momentum loss caused by Rayleigh damping.	https://dl.acm.org/authorize?N675576	Lvmin Zhang, Chengze Li, Tien-Tsin Wong, Yi Ji, Chunping Liu
Session details: Mapping + transport	Capturing spatially-varying bidirectional reflectance distribution functions (SVBRDFs) of 3D objects with just a single, hand-held camera (such as an off-the-shelf smartphone or a DSLR camera) is a difficult, open problem. Previous works are either limited to planar geometry, or rely on previously scanned 3D geometry, thus limiting their practicality. There are several technical challenges that need to be overcome: First, the built-in flash of a camera is almost colocated with the lens, and at a fixed position; this severely hampers sampling procedures in the light-view space. Moreover, the near-field flash lights the object partially and unevenly. In terms of geometry, existing multiview stereo techniques assume diffuse reflectance only, which leads to overly smoothed 3D reconstructions, as we show in this paper. We present a simple yet powerful framework that removes the need for expensive, dedicated hardware, enabling practical acquisition of SVBRDF information from real-world, 3D objects with a single, off-the-shelf camera with a built-in flash. In addition, by removing the diffuse reflection assumption and leveraging instead such SVBRDF information, our method outputs high-quality 3D geometry reconstructions, including more accurate high-frequency details than state-of-the-art multiview stereo techniques. We formulate the reconstruction of SVBRDFs, shading normals, and 3D geometry as a multi-stage, iterative inverse-rendering reconstruction pipeline. Our method is also directly applicable to any existing multiview 3D reconstruction technique. We present results of captured objects with complex geometry and reflectance; we also validate our method numerically against other existing approaches that rely on dedicated hardware, additional sources of information, or both.	https://dl.acm.org/authorize?N675554	Georges Nader, Gael Guennebaud
Session details: Mixed reality	We present a system for acquiring, processing, and rendering panoramic light field still photography for display in Virtual Reality (VR). We acquire spherical light field datasets with two novel light field camera rigs designed for portable and efficient light field acquisition. We introduce a novel real-time light field reconstruction algorithm that uses a per-view geometry and a disk-based blending field. We also demonstrate how to use a light field prefiltering operation to project from a high-quality offline reconstruction model into our real-time model while suppressing artifacts. We introduce a practical approach for compressing light fields by modifying the VP9 video codec to provide high quality compression with real-time, random access decompression. We combine these components into a complete light field system offering convenient acquisition, compact file size, and high-quality rendering while generating stereo views at 90Hz on commodity VR hardware. Using our system, we built a freely available light field experience application called featuring a library of panoramic light field stills for consumer VR which has been downloaded over 15,000 times.	https://dl.acm.org/citation.cfm?id=3295668	Oliver Wang
Session details: Modeling things on (and in) your head	We introduce a novel framework for using natural language to generate and edit 3D indoor scenes, harnessing scene semantics and text-scene grounding knowledge learned from large annotated 3D scene databases. The advantage of natural language editing interfaces is strongest when performing semantic operations at the level, acting on of objects. We learn how to manipulate these sub-scenes by analyzing existing 3D scenes. We perform edits by first parsing a natural language command from the user and transforming it into a that is used to retrieve corresponding sub-scenes from the databases that match the command. We then augment this retrieved sub-scene by incorporating other objects that may be implied by the scene context. Finally, a new 3D scene is synthesized by aligning the augmented sub-scene with the user's current scene, where new objects are spliced into the environment, possibly triggering appropriate adjustments to the existing scene arrangement. A suggestive modeling interface with multiple interpretations of user commands is used to alleviate ambiguities in natural language. We conduct studies comparing our approach against both prior text-to-scene work and artist-made scenes and find that our method significantly outperforms prior work and is comparable to handmade scenes even when complex and varied natural sentences are used.	https://dl.acm.org/citation.cfm?id=3295671	Shahram Izadi
Session details: Nets, cages and meshes	We propose FlexMaps, a novel framework for fabricating smooth shapes out of flat, flexible panels with tailored mechanical properties. We start by mapping the 3D surface onto a 2D domain as in traditional UV mapping to design a set of deformable flat panels called For these panels, we design and obtain specific mechanical properties such that, once they are assembled, the static equilibrium configuration matches the desired 3D shape. FlexMaps can be fabricated from an almost rigid material, such as wood or plastic, and are made flexible in a controlled way by using computationally designed spiraling microstructures.	https://dl.acm.org/authorize?N664433	Michael Rabinovich, Tim Hoffmann, Olga Sorkine-Hornung
Session details: Optimizing structures & materials	We present two novel and complimentary approaches to measure diffraction effects in commonly found planar spatially varying holographic surfaces. Such surfaces are increasingly found in various decorative materials such as gift bags, holographic papers, clothing and security holograms, and produce impressive visual effects that have not been previously acquired for realistic rendering. Such holographic surfaces are usually manufactured with one dimensional diffraction gratings that are varying in periodicity and orientation over an entire sample in order to produce a wide range of diffraction effects such as gradients and kinematic (rotational) effects. Our proposed methods estimate these two parameters and allow an accurate reproduction of these effects in real-time. The first method simply uses a point light source to recover both the grating periodicity and orientation in the case of regular and stochastic textures. Under the assumption that the sample is made of the same repeated diffractive tile, good results can be obtained using just one to five photographs on a wide range of samples. The second method is based on polarization imaging and enables an independent high resolution measurement of the grating orientation and relative periodicity at each surface point. The method requires a minimum of four photographs for accurate results, does not assume repetition of an exemplar tile, and can even reveal minor fabrication defects. We present point light source renderings with both approaches that qualitatively match photographs, as well as real-time renderings under complex environmental illumination.	https://dl.acm.org/authorize?N675568	Guowei Yan, Wei Li, Ruigang Yang, Huamin Wang
Set-in-stone: worst-case optimization of structures weak in tension	One of the key ingredients of any physically based rendering system is a detailed specification characterizing the interaction of light and matter of all materials present in a scene, typically via the Bidirectional Reflectance Distribution Function (BRDF). Despite their utility, access to real-world BRDF datasets remains limited: this is because measurements involve scanning a four-dimensional domain at sufficient resolution, a tedious and often infeasibly time-consuming process. We propose a new parameterization that automatically adapts to the behavior of a material, warping the underlying 4D domain so that most of the volume maps to regions where the BRDF takes on non-negligible values, while irrelevant regions are strongly compressed. This adaptation only requires a brief 1D or 2D measurement of the material's retro-reflective properties. Our parameterization is unified in the sense that it combines several steps that previously required intermediate data conversions: the same mapping can simultaneously be used for BRDF acquisition, storage, and it supports efficient Monte Carlo sample generation. We observe that the above desiderata are satisfied by a core operation present in modern rendering systems, which maps uniform variates to direction samples that are proportional to an analytic BRDF. Based on this insight, we define our adaptive parameterization as an invertible, retro-reflectively driven mapping between the parametric and directional domains. We are able to create noise-free renderings of existing BRDF datasets after conversion into our representation with the added benefit that the warped data is significantly more compact, requiring 16KiB and 544KiB per spectral channel for isotropic and anisotropic specimens, respectively. Finally, we show how to modify an existing gonio-photometer to provide the needed retro-reflection measurements. Acquisition then proceeds within a 4D space that is warped by our parameterization. We demonstrate the efficacy of this scheme by acquiring the first set of spectral BRDFs of surfaces exhibiting arbitrary roughness, including anisotropy.	https://dl.acm.org/citation.cfm?id=3295683	Kun Zhou
Shading atlas streaming	Creating realistic 3D hairs that closely match the real-world inputs remains challenging. With the increasing popularity of lightweight depth cameras featured in devices such as iPhone X, Intel RealSense and DJI drones, depth cues can be very helpful in consumer applications, for example, the Animated Emoji. In this paper, we introduce a fully automatic, data-driven approach to model the hair geometry and compute a complete strand-level 3D hair model that closely resembles the input from a single RGB-D camera. Our method heavily exploits the geometric cues contained in the depth channel and leverages exemplars in a 3D hair database for high-fidelity hair synthesis. The core of our method is a local-similarity based search and synthesis algorithm that simultaneously reasons about the hair geometry, strands connectivity, strand orientation, and hair structural plausibility. We demonstrate the efficacy of our method using a variety of complex hairstyles and compare our method with prior arts.	https://dl.acm.org/authorize?N664404	Joerg H. Mueller, Philip Voglreiter, Mark Dokter, Thomas Neff, Mina Makar, Markus Steinberger, Dieter Schmalstieg
Shape and texture reconstruction for insects by using X-ray CT and focus stack imaging	Because of their variety in shapes, textures, and ecologies, insects have been an important subject of natural science for a long time. Reconstructing three dimensional (3D) shapes and textures of insects in digital format has various benefits such as deterioration free, high space efficiency and high accessibility. In addition to the scientific purpose, the highly realistic insect models are also useful for creating graphics scenes, since they are commonly seen in our daily life.	https://dl.acm.org/authorize?N675703	Wataru Ono, Hikaru Shionozaki, Takashi Ijiri, Kenji Kohiyama, Hiroya Tanaka
Shennong: taste of illusion	In ancient times, Shennong explores the forest deep in the valley to collect unknown plants with unique tastes and effects. A glamorous flower seduces Shennong and he is poisoned by tasting the petal. Shennong falls in illusion as the flower turns into a mythical beast, which torches Shennong to his last breath. In the despairing darkness, Shennong is revived by a secret friend. With the awakening superpower, he conquers the beast, breaks the illusion and acquires another valuable plant.	https://dl.acm.org/authorize?N675757	Li Mi
Simulating kimono fabrication based on the production process of Yuki-tsumugi	Yuki-tsumugi is a traditional Japanese silk fabric. In the production of this silk fabric, a splashed pattern based on a picture is first created on a sheet of exclusive grid paper. Second, piece goods is woven based on the pattern plan. Finally, a kimono is produced from the piece goods. However, estimating the appearance of changes to a kimono during each step of the production is difficult. This is due to the fact that a pattern plan is fabricated by combining basic Kasuri patterns. In addition, yarn weaving rules must be considered and the cutting of a piece goods pattern can be done in an infinite number of ways.	https://dl.acm.org/authorize?N675730	Natsuki Kagaya, Hiroshi Mori, Tomoharu Ishikawa, Kazuya Sasaki, Miyoshi Ayama, Kenji Shoji, Fubito Toyama
Simulation of bubbles with floating and rupturing effect for SPH	In this paper, we present a new SPH (Smoothed Particle Hydrodynamics) method for multi-fluid. Our idea is to extend an Implicit Incompressible SPH (IISPH) for multi-fluid using a particle density. By replacing the formulation to solve the pressure poisson equation (PPE) in IISPH with a new equation based on particle densities, it realized to simulate more stable multi-fluid which considers of higher density ratios, incompressibility, and large time step. This paper shows the simulation of bubble behaviors such as rising, merging, floating, and rupturing, using our proposed method. Additionally, we also present a method to represent a thin film generated by the floating bubbles on the liquid surface by introducting an anisotropic filter for the computation of the surface tension.	https://dl.acm.org/authorize?N675787	Hiroki Watanabe, Makoto Fujisawa, Masahiko Mikawa
Simulation of different materials texture in virtual reality through haptic gloves	This article explores the possibilities of a light and inexpensive way of doing haptic feedback through texture simulation using tactile vibration. With the popularization of virtual reality, the field of haptic feedback is in turmoil. The goal in this study is to expose a moderately realistic but cheap way of simulating haptic feedback including the texture of a surface, to propose a system accessible by the great majority.	https://dl.acm.org/authorize?N675728	Anthony Bazelle, Hugo Pourrier-Nunez, Maxime Rignault, Michael Chang
Smeat: ADMM based tools for character deformation	"Recent work on physical simulation in computer graphics has focused on energy minimization formulations of dynamics based on fast optimization methods. Because these methods can efficiently tackle a wide range of problems in the realms of both dynamic simulation and static relaxation, we adopted one such method, ADMM, and used it to implement a set of creature tools we call Smeat. We will describe this tool set and give a case study of how it was used to create character effects on ""Avengers: Infinity War""."	https://dl.acm.org/authorize?N675669	David Minor, David Corral
Social facilitation with virtual jogging companion on smartglasses	Jogging is a fundamental activity integrated various sports. It is known familiarly as one of daily exercises. Being able to do alone anytime and anywhere is important, and motivation is a key issue for initiating and maintaining such exercise. Presence of others tends to affect physical activity. It is called Social Facilitation. Individuals perform better on simple or well-rehearsed task with the social facilitation. Presence of attentive spectator makes the jogger's pace faster than no-spectator condition [Strube et al. 1981]. Furthermore, competition with a superior partner causes the jogger to get motivated and increase effort, which is called Köhler effect. Presence of others is effective for exercise; however, it is difficult to find such partner. Some virtual reality exercises have been researched to overcome this gap between individual and pair/group exercise. Software-generated virtual partners have already applied for stationary bike, plank exercise, rowing exercise with an immersive head-mounted or mere stationary display. People prefer change in their surroundings while jogging. Mueller et al. tried to realize outdoor jogging experience with a drone [Mueller and Muirhead 2015] and a distant jogger felt through his/her voice [Mueller et al. 2010]. Jogging with an autonomous robot and a runner participating from other place could connect with appropriate partner anytime and anywhere. However, whole control of the drone in crowded environment with people and buildings is difficult and voice communication gets much attention from the environment.	https://dl.acm.org/authorize?N675729	Takeo Hamada, Michiteru Kitazaki, Noboru Koshizuka
Sonaria	Follow two ever-changing creatures as they flow from one life-form to another on a vivid journey of sound and light. Sonaria's visual language is simple and abstract, designed to suggest, while the sonic language is layered and immersive, designed to answer.	https://dl.acm.org/authorize?N675758	Sara Diamond, Scot Stafford, Myles Shioda, Camille Cellucci
Sonder	For years, Finn and Natalie walked the same path. But when their time together comes to an end, Finn finds himself lost in a mysterious land. Paralyzed by the fear of moving on, he is at risk of losing himself. Through self-discovery, Finn must gather the strength to forge a new path.	https://dl.acm.org/authorize?N675648	Neth Nom
Space fusion: context-aware interaction using 3D scene parsing	Context-aware interaction (interaction varying correspondingly to scene semantics or object categories) is an important element to make mixed reality experience more immersive and realistic. However, few mixed reality applications provide this kind of interaction since it is difficult to recognize all objects in the real world scene densely as 3D. In this work, we present a 3D scene parsing system by combining semantic segmentation with visual Simultaneous Localization and Mapping (SLAM). This system can reconstruct and recognize the real indoor scene as dense point cloud with categorical labels in real-time. We also present a context-aware mixed reality application that utilizes the parsing system. Users can import their own room into the mixed reality world, and enjoy interaction with a virtual robot in their room through a head-mounted display (HMD). The virtual robot behaves correspondingly to the real object's category. Therefore, our 3D scene parsing realizes context-aware interaction.	https://dl.acm.org/authorize?N664115	Hiroyuki Yabe, Daichi Ono, Tsutomu Horikawa
Spatially augmented depth and transparency in paper materials	The human visual system uses cast shadows to judge the three-dimensional layout of an object. The purpose of this installation is to demonstrate novel visual illusions of depth and transparency for paper materials, which is induced by the conventional light projection of cast shadow patterns. Thus, this installation focuses on perceptual rather than technical aspects. Illuminating a target object, the spatial vicinity of the object is darkened to produce the visual impression of a shadow of the object. By controlling the blurriness of the cast shadow and/or spatial distance between the object and its shadow, a perceptual change in the layout of the object is induced. The audience can interactively enjoy visual experiences wherein objects and letters on a paper perceptually float in the air. We also demonstrate that it is possible to edit the material appearance of a real object by manipulating the shape of the shadow; an opaque colored paper appear to be a transparent color film floating in the air.	https://dl.acm.org/authorize?N675788	Takahiro Kawabe
Spatially augmented depth and transparency in paper materials	The human visual system uses cast shadows to judge the three-dimensional layout of an object. The purpose of this installation is to demonstrate novel visual illusions of depth and transparency for paper materials, which is induced by the conventional light projection of cast shadow patterns. Thus, this installation focuses on perceptual rather than technical aspects. Illuminating a target object, the spatial vicinity of the object is darkened to produce the visual impression of a shadow of the object. By controlling the blurriness of the cast shadow and/or spatial distance between the object and its shadow, a perceptual change in the layout of the object is induced. The audience can interactively enjoy visual experiences wherein objects and letters on a paper perceptually float in the air. We also demonstrate that it is possible to edit the material appearance of a real object by manipulating the shape of the shadow; an opaque colored paper appear to be a transparent color film floating in the air.	https://dl.acm.org/authorize?N675788	Takahiro Kawabe
Spatially augmented depth and transparency in paper materials	The human visual system uses cast shadows to judge the three-dimensional layout of an object. The purpose of this installation is to demonstrate novel visual illusions of depth and transparency for paper materials, which is induced by the conventional light projection of cast shadow patterns. Thus, this installation focuses on perceptual rather than technical aspects. Illuminating a target object, the spatial vicinity of the object is darkened to produce the visual impression of a shadow of the object. By controlling the blurriness of the cast shadow and/or spatial distance between the object and its shadow, a perceptual change in the layout of the object is induced. The audience can interactively enjoy visual experiences wherein objects and letters on a paper perceptually float in the air. We also demonstrate that it is possible to edit the material appearance of a real object by manipulating the shape of the shadow; an opaque colored paper appear to be a transparent color film floating in the air.	https://dl.acm.org/authorize?N675761	Takahiro Kawabe
Spatially augmented depth and transparency in paper materials	The human visual system uses cast shadows to judge the three-dimensional layout of an object. The purpose of this installation is to demonstrate novel visual illusions of depth and transparency for paper materials, which is induced by the conventional light projection of cast shadow patterns. Thus, this installation focuses on perceptual rather than technical aspects. Illuminating a target object, the spatial vicinity of the object is darkened to produce the visual impression of a shadow of the object. By controlling the blurriness of the cast shadow and/or spatial distance between the object and its shadow, a perceptual change in the layout of the object is induced. The audience can interactively enjoy visual experiences wherein objects and letters on a paper perceptually float in the air. We also demonstrate that it is possible to edit the material appearance of a real object by manipulating the shape of the shadow; an opaque colored paper appear to be a transparent color film floating in the air.	https://dl.acm.org/authorize?N675761	Takahiro Kawabe
Spatially augmented depth and transparency in paper materials	The human visual system uses cast shadows to judge the three-dimensional layout of an object. The purpose of this installation is to demonstrate novel visual illusions of depth and transparency for paper materials, which is induced by the conventional light projection of cast shadow patterns. Thus, this installation focuses on perceptual rather than technical aspects. Illuminating a target object, the spatial vicinity of the object is darkened to produce the visual impression of a shadow of the object. By controlling the blurriness of the cast shadow and/or spatial distance between the object and its shadow, a perceptual change in the layout of the object is induced. The audience can interactively enjoy visual experiences wherein objects and letters on a paper perceptually float in the air. We also demonstrate that it is possible to edit the material appearance of a real object by manipulating the shape of the shadow; an opaque colored paper appear to be a transparent color film floating in the air.	https://dl.acm.org/authorize?N675788	Takahiro Kawabe
Spatially augmented depth and transparency in paper materials	The human visual system uses cast shadows to judge the three-dimensional layout of an object. The purpose of this installation is to demonstrate novel visual illusions of depth and transparency for paper materials, which is induced by the conventional light projection of cast shadow patterns. Thus, this installation focuses on perceptual rather than technical aspects. Illuminating a target object, the spatial vicinity of the object is darkened to produce the visual impression of a shadow of the object. By controlling the blurriness of the cast shadow and/or spatial distance between the object and its shadow, a perceptual change in the layout of the object is induced. The audience can interactively enjoy visual experiences wherein objects and letters on a paper perceptually float in the air. We also demonstrate that it is possible to edit the material appearance of a real object by manipulating the shape of the shadow; an opaque colored paper appear to be a transparent color film floating in the air.	https://dl.acm.org/authorize?N675788	Takahiro Kawabe
Spatially augmented depth and transparency in paper materials	The human visual system uses cast shadows to judge the three-dimensional layout of an object. The purpose of this installation is to demonstrate novel visual illusions of depth and transparency for paper materials, which is induced by the conventional light projection of cast shadow patterns. Thus, this installation focuses on perceptual rather than technical aspects. Illuminating a target object, the spatial vicinity of the object is darkened to produce the visual impression of a shadow of the object. By controlling the blurriness of the cast shadow and/or spatial distance between the object and its shadow, a perceptual change in the layout of the object is induced. The audience can interactively enjoy visual experiences wherein objects and letters on a paper perceptually float in the air. We also demonstrate that it is possible to edit the material appearance of a real object by manipulating the shape of the shadow; an opaque colored paper appear to be a transparent color film floating in the air.	https://dl.acm.org/authorize?N675761	Takahiro Kawabe
Spatially augmented depth and transparency in paper materials	The human visual system uses cast shadows to judge the three-dimensional layout of an object. The purpose of this installation is to demonstrate novel visual illusions of depth and transparency for paper materials, which is induced by the conventional light projection of cast shadow patterns. Thus, this installation focuses on perceptual rather than technical aspects. Illuminating a target object, the spatial vicinity of the object is darkened to produce the visual impression of a shadow of the object. By controlling the blurriness of the cast shadow and/or spatial distance between the object and its shadow, a perceptual change in the layout of the object is induced. The audience can interactively enjoy visual experiences wherein objects and letters on a paper perceptually float in the air. We also demonstrate that it is possible to edit the material appearance of a real object by manipulating the shape of the shadow; an opaque colored paper appear to be a transparent color film floating in the air.	https://dl.acm.org/authorize?N675761	Takahiro Kawabe
Spectral rendering of fluorescence using importance sampling	Spectral rendering is necessary for rendering a scene with fluorescence, because fluorescence is a strongly wavelength dependent phenomenon. We propose a method for rendering fluorescence under global illumination environment efficiently by using importance sampling of wavelength considering both spectra of fluorescent materials and light sources.	https://dl.acm.org/authorize?N675789	Wataru Yamamoto, Bisser Raytchev, Toru Tamaki, Kazufumi Kaneda
Spider-Man IG-impostors: cityscapes and beyond	"Spider-Man's traversal though Manhattan in the video game <u>Marvel's Spider-Man</u> (2018) for Sony's PS4 platform allows the player to climb any building and jump off any structure while rotating the view 360 degrees at 30 frames per second and displayed at 4k resolution. Flat card ""billboard"" style impostor systems could not represent the city environment at the desired quality so Insomniac Games developed the 3D IG-Impostor system to represent the mid to distant cityscape for Marvel's Manhattan in an efficient and persistent cache. This environment data cache was then available for multi-view rendering used by other systems within the Insomniac Engine. There are no 2D impostors in <u>Marvel's Spider-Man.</u>"	https://dl.acm.org/authorize?N675670	Xray Halperin, David Santiago, Abdul Bezrati
St(r)ay	"""St(r)ay"" was inspired by my father's dementia, during which he failed to recognize his friends and relatives and even lost his self-awareness. During his illness, I witnessed a person who once had the same complex personality, emotions, love, hate, and life experiences as you and I, drawn gradually by the illness into an empty soul, which prompted me to create an animation completely from a dementia patient's point of view. Using the patient's point of view to tell the story, I tried to capture my father's mood when he stepped unconsciously into the whirlpool of dementia, and explore the value of affection and life."	https://dl.acm.org/authorize?N675649	Chiang Yao
Stabilized real-time face tracking via a learned dynamic rigidity prior	We propose a technique for interpolating between probability distributions on discrete surfaces, based on the theory of optimal transport. Unlike previous attempts that use linear programming, our method is based on a dynamical formulation of quadratic optimal transport proposed for flat domains by Benamou and Brenier [2000], adapted to discrete surfaces. Our structure-preserving construction yields a Riemannian metric on the (finite-dimensional) space of probability distributions on a discrete surface, which translates the so-called Otto calculus to discrete language. From a practical perspective, our technique provides a smooth interpolation between distributions on discrete surfaces with less diffusion than state-of-the-art algorithms involving entropic regularization. Beyond interpolation, we show how our discrete notion of optimal transport extends to other tasks, such as distribution-valued Dirichlet problems and time integration of gradient flows.	https://dl.acm.org/authorize?N664440	Pedro Hermosilla, Tobias Ritschel, Pere-Pau Vázquez, Àlvar Vinacua, Timo Ropinski
Study on background optical flow cancellation using cyclically updated camera posture information	To build efficient processing scheme of moving object detection function using camera images, we propose a method that evaluates acceleration tolerance of camera's motion. The motion of camera should cause background optical flow inside camera images that prevents detecting actually moving object. For effective background optical flow cancellation, precise motion of the camera is required. The motion of the camera is estimated from position and attitude of a vehicle that mounts the camera. In a general measurement subsystem, these kinds of information are fetched in a certain cycle that is independent of moving object detection process. Gap between the fetch cycle and the process cycle can affect background optical flow cancellation result. In this paper, we analyze relationship between the gaps of cycles and allowed motion acceleration of the camera, and then verify the result on the actual moving object detection system.	https://dl.acm.org/authorize?N675793	Yoshibumi Fukuda, Kazuya Sugimoto
Stuffed	STUFFED was created by six students who recently graduated from Supinfocom Rubika, a 3D animation school. During the last year of the five-year program, they had the chance to work exclusively on a short film, about 7 minutes long in full CG. The original idea was found during the 4th year at school, and they developed the idea at this point. The production itself began in September 2017 and ended in June 2018.	https://dl.acm.org/authorize?N675640	Élise Simoulin, Édouard Heutte, Clotilde Bonnotte, Anna Komaromi, Marisa Di Vora Peixoto, Helena Bastioni
Synthesis and rendering of seamless and non-repetitive 4D texture variations for measured optical material properties	We have lifted the one weakness of an existing fully automatic acquisition system for spatially varying optical material behavior of real object surfaces. While its expression of spatially varying material behavior with spherical dependence on incoming light as 4D texture (ABTF material model) allows flexible mapping on arbitrary 3D geometries, photo-realistic rendering and interaction in real-time, this very method of texture-like representation exposed it to common problems of texturing, striking in two levels. First, non-seamless textures create visible border artifacts. Second, even a perfectly seamless texture causes repetition artifacts due to side-by-side distribution in large numbers over the 3D surface. We solved both problems through our novel texture synthesis that generates a set of seamless texture variations randomly distributed on the surface at shading time. When compared to regular 2D textures, the inter-dimensional coherence of the 4D ABTF material model poses entirely new challenges to texture synthesis, which includes maintaining the consistency of material behavior throughout the space spanned by the spatial image domain and the angular illumination hemisphere. In addition, we tackle the increased memory consumption caused by the numerous variations through a fitting scheme specifically designed to reconstruct the most prominent effects captured in the material model.	https://dl.acm.org/authorize?N675699	Martin Ritz, Simon Breitfelder, Pedro Santos, Arjan Kuijper, Dieter Fellner
TactGAN: vibrotactile designing driven by GAN-based automatic generation	In this study, we propose the vibrotactile feedback designing system using GAN (Generative Adversarial Network)-based vibrotactile signal generator (TactGAN). Preparing appropriate vibrotactile signals for applications is difficult and takes much time because we need recording or directly hand tuning signals if the required signals do not exist in the database of vibrotactile stimuli. To solve these problems, TactGAN can generate signals presenting specific tactile impression based on user-defined parameters. It can also automatically generate signals presenting the tactile impression of images. It realizes the rapid designing of vibrotactile signals for application with such feedback. Users can experience the rapid designing process of the vibrotactile stimuli for specific user interfaces or specific contents on applications. TactGAN enables us to apply various vibrotactile stimuli to UI components like buttons using material kinds or tactile words, and to attach textures with vibrotactile feedback to the 3D model.	https://dl.acm.org/authorize?N675762	Yuki Ban, Yusuke Ujitoko
Tactile microcosm of ALife: interaction with artificial life by aerial mixed reality display	This study aims to propose a new educational tool for interacting with artificial life (ALife). The proposed system consists of an ALife behavior simulation and a tactile aerial display with a combination of aerial imaging by retro-reflection and a force-feedback device. The content is mid-air interaction with a simulated school of fish-like ALife. The behavior is based on a modified BOIDs algorithm with a predator-prey relationship between different species. A user can enjoy interaction with the fish through aerial imaging and haptic feeling. The user can feel the activity of ALife as a force field using haptics.	https://dl.acm.org/authorize?N675720	Toshikazu Ohshima, Tsukasa Sumizono
Tales of wedding rings VR	"This project was created to answer the question ""what if you can step inside your favorite manga story?"" and with the aim to achieve a narrative experience that is uniquely Japanese, using a visual grammar that is only possible for VR."	https://dl.acm.org/authorize?N675759	Kaei Sou
Tangible interaction with 3D printed modular robots through multi-channel sensors	Tangible interaction with customized products integrating sensors and actuators recently grows into an interdisciplinary research area in computer graphics and human-robot interaction (e.g., [Groeger et al. 2016; Yu et al. 2018]). In this paper, we introduce tangible interaction into 3D printed modular robots. Our user study demonstrates that interacting with our robots can effectively improve human spatial ability, which plays an important role during a person's development in science, technology, engineering or math (STEM).	https://dl.acm.org/authorize?N675721	Minjing Yu, Yong-Jin Liu, Guozhen Zhao, Charlie C. L. Wang
Tangible projection mapping: dynamic appearance augmenting of objects in hands	We propose a technique of dynamic appearance augmentation by projection to object in user's hands, named Tangible Projection Mapping. This technique allows users to hold a target object freely, and augments appearance of that in various postures by user's manipulation. In addition, Tangible Projection Mapping has the potential to contribute to widespread use of dynamic projection mapping because of using only off-the-shelf devices. In our demonstration, any textures and movies can be projected on the objects of various shapes. It can provide a deep sense of unity with the attractively enhanced object in hands.	https://dl.acm.org/authorize?N675763	Yuki Morikubo, Eugene San Lorenzo, Daiki Miyazaki, Naoki Hashimoto
Tentacle flora: lifelike robotic sculpture	The Tentacle Flora is a robotic sculpture inspired by a vision of a colony of the sea anemone growing on the coral. A shape-memory alloy actuator is used as tentacles and is composed of a BioMetal Fiber such that it can bend in three directions. The top of the actuator glows softly mimicking a bioluminescent organism using a full colored LED. The Tentacle Flora induces the beauty, wonder, and existence of living sea anemones in the depths of the ocean.	https://dl.acm.org/authorize?N675722	Akira Nakayasu
That torch is now in our hands	"On February 28, 2016, Donald Trump gave his first address to Congress. He sent a clear message that enlightens his Presidential philosophy: ""Each American generation passes the torch of truth, liberty and justice --- in an unbroken chain all the way down to the present. That torch is now in our hands. And we will use it to light up the world."" ""That Torch is Now in Our Hands"" is an interactive installation made of 2 parts: - A model of the Statue of Liberty holding the torch. This torch is the main light of the space, up to 5 public address speakers surround the statue (the number commensurate to the exhibition space size). The loudspeakers diffuse the excerpts of the U.S. President talk. The statue light reacts to the sound: the light intensity is stronger when nobody is speaking, and it becomes lower when Trump's voice is heard. The higher the voice, the lower the light. The statue is like a bird, covered with feathers. We understand quickly that the feathers are just glued with tar. In an extended version, we can imagine 5 fans making feathers flying like a tornado around the statue, full Trump's address: https://youtu.be/3ZxsuGEL6yE (after 7')"	https://dl.acm.org/authorize?N675603	Maurice Benayoun, Xiao Wang
The Ising model: blink and polyptic	"Blinkand ""Polyptelic"" are software generated animations that consist of a matrix of cells that transition between 2 states, taking stock of their neighbors' actions. The process goes back and forth between states of stability where all cells try to be like their neighbors, and states of transition where the cells know their neighbors' state but cant decide if they should be like them or not. A certain degree of deviance is programmed so that some of the cells resist or differ from the general status quo of the collection. The status of the overall image is dynamically determined computationally in realtime and therefore guarantees a continuous stream of behavior based on the interaction of the cells."	https://dl.acm.org/authorize?N675604	George Legrady
The bolt connection	THE BOLT CONNECTION was created by six students who recently graduated from Supinfocom Rubika, a 3D animation school. During the last year of the five-year program, they had the chance to work exclusively on a short film, about 7 minutes long in full CG. The original idea was found during the 4th year at school, and they developed the idea at this point. The production itself began in September 2017 and ended in June 2018.	https://dl.acm.org/authorize?N675641	Nicolas Lebas, Claire Cartier, Mathilde Dourdy, Thibault Grunenberger, Maurine Lecerf, Shih-Hui Pan
The living wall display: physical augmentation of interactive content using an autonomous mobile display	The Living Wall Display displays interactive content on a mobile wall screen that moves in concert with content animation. To augment the interaction experience, the display dynamically changes its position and orientation, responding to the content animation triggered by user interactions. We implement three proof of concept prototypes that represent pseudo force impact of the interactive content using physical screen movement. Pilot studies show that the Living Wall augments content expressiveness, and increases the sense of presence of the screen content.	https://dl.acm.org/authorize?N675764	Yuki Onishi, Yoshiki Kudo, Kazuki Takashima, Anthony Tang, Yoshifumi Kitamura
The need 4 speed in real-time dense visual tracking	Despite the popularity of real-time monocular face tracking systems in many successful applications, one overlooked problem with these systems is rigid instability. It occurs when the input facial motion can be explained by either head pose change or facial expression change, creating ambiguities that often lead to jittery and unstable rigid head poses under large expressions. Existing rigid stabilization methods either employ a heavy anatomically-motivated approach that are unsuitable for real-time applications, or utilize heuristic-based rules that can be problematic under certain expressions. We propose the first rigid stabilization method for real-time monocular face tracking using a dynamic rigidity prior learned from realistic datasets. The prior is defined on a region-based face model and provides dynamic region-based adaptivity for rigid pose optimization during real-time performance. We introduce an effective offline training scheme to learn the dynamic rigidity prior by optimizing the convergence of the rigid pose optimization to the ground-truth poses in the training data. Our real-time face tracking system is an optimization framework that alternates between rigid pose optimization and expression optimization. To ensure tracking accuracy, we combine both robust, drift-free facial landmarks and dense optical flow into the optimization objectives. We evaluate our system extensively against state-of-the-art monocular face tracking systems and achieve significant improvement in tracking accuracy on the high-quality face tracking benchmark. Our system can improve facial-performance-based applications such as facial animation retargeting and virtual face makeup with accurate expression and stable pose. We further validate the dynamic rigidity prior by comparing it against other variants on the tracking accuracy.	https://dl.acm.org/citation.cfm?id=3295675	Toshiya Hachisuka
The power of real-time collaborative filmmaking 2	PocketStudio is designed to allow filmmakers to easily create, play and stream 3D animation sequences in real-time using real-time collaborative editing, a unified workflow and other real-time technologies, such as augmented reality.	https://dl.acm.org/authorize?N675667	Jean-Colas Prunier, Stephane Tayeb
The shape space of discrete orthogonal geodesic nets	"Facial caricature is an art form of drawing faces in an exaggerated way to convey humor or sarcasm. In this paper, we propose the first Generative Adversarial Network (GAN) for unpaired photo-to-caricature translation, which we call ""CariGANs"". It explicitly models geometric exaggeration and appearance stylization using two components: , which only models the geometry-to-geometry transformation from face photos to caricatures, and , which transfers the style appearance from caricatures to face photos without any geometry deformation. In this way, a difficult cross-domain translation problem is decoupled into two easier tasks. The perceptual study shows that caricatures generated by our are closer to the hand-drawn ones, and at the same time better persevere the identity, compared to state-of-the-art methods. Moreover, our allow users to control the shape exaggeration degree and change the color/texture style by tuning the parameters or giving an example caricature."	https://dl.acm.org/citation.cfm?id=3295677	Hao Li
The stained club	THE STAINED CLUB was created by six students who recently graduated from Supinfocom Rubika, a 3D animation school. During the last year of the five-year program, they had the chance to work exclusively on a short film, about 7 minutes long in full CG. The original idea was found during the 4th year at school, and they developed the idea at this point. The production itself began in September 2017 and ended in June 2018.	https://dl.acm.org/authorize?N675642	Mélanie Lopez, Simon Boucly, Marie Ciesielski, Alice Jaunet, Chan Stéphie Peang, Béatrice Viguier
Towards multifocal displays with dense focal stacks	We present an incremental collision handling algorithm for GPU-based interactive cloth simulation. Our approach exploits the spatial and temporal coherence between successive iterations of an optimization-based solver for collision response computation. We present an incremental continuous collision detection algorithm that keeps track of deforming vertices and combine it with spatial hashing. We use a non-linear GPU-based impact zone solver to resolve the penetrations. We combine our collision handling algorithm with implicit integration to use large time steps. Our overall algorithm, I-Cloth, can simulate complex cloth deformation with a few hundred thousand vertices at 2 - 8 frames per second on a commodity GPU. We highlight its performance on different benchmarks and observe up to 7 - 10X speedup over prior algorithms.	https://dl.acm.org/authorize?N664403	Jen-Hao Rick Chang, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan
Tracking the gaze on objects in 3D: how do people really look at the bunny?	Interlocking assemblies have a long history in the design of puzzles, furniture, architecture, and other complex geometric structures. The key defining property of interlocking assemblies is that all component parts are immobilized by their geometric arrangement, preventing the assembly from falling apart. Computer graphics research has recently contributed design tools that allow creating new interlocking assemblies. However, these tools focus on specific kinds of assemblies and explore only a limited space of interlocking configurations, which restricts their applicability for design. In this paper, we propose a new general framework for designing interlocking assemblies. The core idea is to represent part relationships with a family of base and leverage efficient graph analysis tools to compute an interlocking arrangement of parts. This avoids the exponential complexity of brute-force search. Our algorithm iteratively constructs the geometry of assembly components, taking advantage of all existing blocking relations for constructing successive parts. As a result, our approach supports a wider range of assembly forms compared to previous methods and provides significantly more design flexibility. We show that our framework facilitates efficient design of complex interlocking assemblies, including new solutions that cannot be achieved by state of the art approaches.	https://dl.acm.org/authorize?N664493	Xi Wang, Sebastian Koch, Kenneth Holmqvist, Marc Alexa
Trajectile command	This is a quick overview of Trajectile Command: A free virtual reality arcade game that runs in a web browser.	https://dl.acm.org/authorize?N664117	Adam Twite
Transforming medical education and training with VR using M.A.G.E.S.	In this work, we propose a novel VR s/w system aiming to disrupt the healthcare training industry with the first Psychomotor Virtual Reality (VR) Surgical Training solution. Our system generates a fail-safe, realistic environment for surgeons to master and extend their skills in an affordable and portable solution. We deliver an educational tool for orthopedic surgeries to enhance the learning procedure with gamification elements, advanced interactability and cooperative features in an immersive VR operating theater. Our methodology transforms medical training to a cost-effective, easily and broadly accessible process. We also propose a fully customizable SDK platform able to generate educational VR simulations with minimal adaptations. The latter is accomplished by prototyping the learning pipeline into structured, independent and reusable segments, which are used to generate more complex behaviors. Our architecture supports all current and forthcoming VR HMDs and standard 3D content generation.	https://dl.acm.org/authorize?N675850	George Papagiannakis, Nick Lydatakis, Steve Kateros, Stelios Georgiou, Paul Zikas
Transitory project: an interactive artistic digital installation based on an artificial intelligence	We create a digital and artistic area that aims to illustrate and humanize, through visuals, sounds and words, the way in which westerners of the 21st century meet in the digital age. Based on an artificial intelligence, the work interacts directly with the viewer by analyzing data such as its position, its proximity to other people located in the area, etc.	https://dl.acm.org/authorize?N675780	Maël Crespin-Pommier, Baptiste Olivier, Antoine Demière, Antonin Leuret, Alain Lioret
Translucent surface detection by raycasting through multiple depth images	Time-of-flight depth camera assumes that target scene consists of opaque surfaces. Therefore, any translucent object causing multipath problem in depth calculation cannot be appropriately reconstructed. If we are able to detect translucent surface under time-of-flight principle, depth map obtained from opaque region will be more reliable. Translucent surface can be recovered by separate approach afterhand. In this paper, we propose a translucent surface detection method from multiple depth images obtained at different viewpoints. First, multiple depth maps are registered in a 3D space based on camera transformations. Our method classifies surfaces into three types: opaque, translucent, and not determined surface. Raycasting through registered depth maps investigates overlapped surfaces identifying respective surface types. Experimental evaluation on both synthetic 3D models and real translucent object shows promising translucent surface detection results.	https://dl.acm.org/authorize?N675704	Seungpyo Ha, Seungkyu Lee
TuVe: a flexible display with a tube	Ordinary displays, e.g., liquid crystal displays (LCD), can only provide two-dimensional information. Expressions and interactions with such displays are limited to the surface. On the other hand, many research studies on novel display systems have been proposed to tackle such limitations and provide three-dimensional information. A display system made up of a tube with fluids could be one of the solutions. A tube with fluids inside of it can take various forms according to the environment, therefore making it possible to project information onto various surface shapes. Dietz et al. proposed a kit for prototyping of user interfaces with fluids[Dietz 2014]. They discussed components e.g. pumps, tubes, and accessories for tubing for users can constitute fluidic user interfaces easily. Popp proposed an art piece, [Popp 2011], which consists of several tubes, two-phase flows inside of them, and pumps that control the flows for each tube. However, there is no discussion about changing the control method according to the tube taking various shapes. In this case, information could only be presented in a known simple shape, and users cannot change the shape of the surface. In order to tackle these issues, we propose a novel tube display , that consists of a tube and fluids, while offering a dynamical shape-changing display with computer vision based calibration.	https://dl.acm.org/authorize?N675775	Yuki Inoue, Yuichi Itoh, Takao Onoye
USD and scene interoperability: demystifying the state of the art	We want to start from scratch and explain what USD is and isn't.	https://dl.acm.org/authorize?N675546	Paul Kanyuk, Stephen Gustafson
Undiscovered	"""Undiscovered"" follows Sasquatch on his quest for a good photo of himself. His attempts to jump into a photo before scaring away the hikers who are taking them are thwarted by the usual suspects---food in his teeth, closed eyes, mussed-up hair, an out-of-focus shot-resulting in deleted photos every time."	https://dl.acm.org/authorize?N675643	Sara Litzenberger
Untitled (head piece)	Combining video in a CRT TV with physical movement to represent a state of human existence. Unlike Brâncuşi's Sleeping Muse which aims to create the most ideal form, the imperfect form of human body placing in an artificial container reveals the condition of our contemporary existence.	https://dl.acm.org/authorize?N675615	Wing Hong Tung
VarioLight: hybrid dynamic projection mapping using high-speed projector and optical axis controller	Projection mapping have attracted much attention in recent years and many researches have tried to expand it to dynamic objects. However, it has some problems in the range of motion, the resolution, and the kinds of object that can be projected. In this paper, we propose a method for realizing dynamic projection mapping by combining a high-speed/low-latency projector and a mirror-based high-speed optical axis controller. Based on high-speed visual feedback with multiple dot markers, dynamic projection mapping onto rotating/deforming objects moving in a wide range, with almost-imperceptible delay, has become possible.	https://dl.acm.org/authorize?N675776	Yuri Mikawa, Tomohiro Sueishi, Yoshihiro Watanabe, Masatoshi Ishikawa
Vector graphics editing with interweaving and penetrating	Decorative patterns are common in traditional Chinese architectures as shown in Figure 1. However, scalable vector graphics (SVG) is not capable of representing interweaving and penetrating patterns. In this paper, we develop a web-based vector graphics interweaving and penetrating editing system. We propose a data structure to dealing with interweaving and penetrating, allowing users to assign depth value for each edge of a polygon. As a result, when we click on a polygon and move it to interweave with another one, the intersecting edge is calculated using linear interpolation of the depth values. In contrast, the conventional SVG format arranges layers to separate two polygons for interweaving and penetrating. In other words, users need to split a polygon into multiple polygons and assigning them to different layers to achieve interweaving and penetrating. As shown in Figure 2, the proposed system handles the interweaving and penetrating problem intuitively and maintains the topology of the polygons. After finish editing, the proposed system allows the user to save the drawing in both standard SVG format and the proposed augmented depth value format for future editing.	https://dl.acm.org/authorize?N675715	Yu-Lin Chao, Tung-Ju Hsieh, Pei-Ying Chiang
Vermin [jury special award]	Set in a contemporary society of mice and rats, Hubert, a young observing rat, recites slam poetry full of hope within his head as he passes by individualistic strangers in the Parisian metro. Hubert's poetry remains optimistic and persistent, despite the harsh reality of the world he lives in.	https://dl.acm.org/authorize?N675644	Jérémie Becquer
Video to fully automatic 3D hair model	We introduce a generative model for 3D man-made shapes. The presented method takes a global-to-local (G2L) approach. An adversarial network (GAN) is built first to construct the overall structure of the shape, segmented and labeled into parts. A novel conditional auto-encoder (AE) is then augmented to act as a part-level refiner. The GAN, associated with additional local discriminators and quality losses, synthesizes a voxel-based model, and assigns the voxels with part labels that are represented in separate channels. The AE is trained to amend the initial synthesis of the parts, yielding more plausible part geometries. We also introduce new means to measure and evaluate the performance of an adversarial generative model. We demonstrate that our global-to-local generative model produces significantly better results than a plain three-dimensional GAN, in terms of both their shape variety and the distribution with respect to the training data.	https://dl.acm.org/authorize?N664411	Shu Liang, Xiufeng Huang, Xianyu Meng, Kunyao Chen, Linda G. Shapiro, Ira Kemelmacher-Shlizerman
Visual analytics of big networks: novel approaches for exploring complex networks in big data	"Four ""Paradigms"" of Science • Empirical Science • Theoretical Science • Computational Science • Data Science"	https://dl.acm.org/authorize?N675547	Daniel Filonik, Tomasz Bednarz
Visualization system for tsunami evacuation behavior	To design disaster prevention plans during a tsunami, it is necessary to achieve consensus through collaboration between local governments and residents in terms of evacuation sites and evacuation routes. In this study, we implement a system that simulates evacuee behavior during a tsunami using massive agents and develop free and easily accessible tools that can be used by local governments and residents for disaster prevention.	https://dl.acm.org/authorize?N675705	Yasuo Kawai, Yurie Kaizu, Shusei Yoshida
Vulkan rendering framework for mobile multimedia	Vulkan is the most recent graphics rendering API, designed mainly for the benefit of gaming and highly intensive workloads where the CPU becomes the bottleneck. While Vulkan is quite amenable to games and other heavy rendering tasks, its use is often discouraged for 2D applications rendering and editing videos with animated effects. In this paper, we present the first Vulkan-based animation and effects engine for mobile video rendering to test these claims. We compare our solution with the preloaded video editor applications in mobile phones. We see a significant improvement in all regards, with a 30 FPS playback for 4K videos achieved using 30% less memory and 20% less power.	https://dl.acm.org/authorize?N675731	Mahak Gambhir, Swati Panda, Shaik Jani Basha
Warp-guided GANs for single-photo facial animation	We propose a method for efficiently computing orientation-preserving and approximately continuous correspondences between non-rigid shapes, using the functional maps framework. We first show how orientation preservation can be formulated directly in the functional (spectral) domain without using landmark or region correspondences and without relying on external symmetry information. This allows us to obtain functional maps that promote orientation preservation, even when using descriptors, that are invariant to orientation changes. We then show how higher quality, approximately continuous and bijective pointwise correspondences can be obtained from initial functional maps by introducing a novel refinement technique that aims to simultaneously improve the maps both in the spectral and spatial domains. This leads to a general pipeline for computing correspondences between shapes that results in high-quality maps, while admitting an efficient optimization scheme. We show through extensive evaluation that our approach improves upon state-of-the-art results on challenging isometric and non-isometric correspondence benchmarks according to both measures of continuity and coverage as well as producing semantically meaningful correspondences as measured by the distance to ground truth maps.	https://dl.acm.org/authorize?N664449	Chenglei Wu, Takaaki Shiratori, Yaser Sheikh
Wild love	Alan and Beverly, away hiking on a romantic getaway, are tenderly in love. Unfortunately, in the middle of their blinding happiness, they provoke - without even noticing it - the death of a marmot. Against an army of enraged and bloodthirsty rodents, Alan and Beverly's perfect love won't be of any use ... While on a romantic getaway, Alan and Beverly cause a fatal accident. This crime won't remain unpunished ...	https://dl.acm.org/authorize?N675755	Paul Autric, Quentin Camus Maryka Laudet, Léa Georges, Zoé Sottiaux, Corentin Yvergniaux
Xpression: mobile real-time facial expression transfer	We developed Xpression, a mobile application which allows user to reenact faces from images and videos with only RGB camera on mobile devices. It transfers facial expression from source user to target user. Unlike other reenactment researches, our method works on video as well as still images and requires only mobile device. Our application is freely available to the public on iOS App Store.	https://dl.acm.org/authorize?N675777	Toby Long Hin Chong, Issay Yoshida
dongSpace: a wide-area mixed reality multiplayer game system	dongSpace is a wide area multiplayer interactive game system based on mixed reality technology. The system uses a head-mounted display to track user's posture, allowing user to perform a wide range of movement in more than 2,000 square meters of outdoor environments. The laptop that the user carries with him will render the game screen in real time. The computer performs computational rendering to provide high-definition, large-angle-of-view content display. The system also provides users with a hand-held simulation gun with tracking that can be used to shoot or interact with virtual environment objects. The system integrates display devices, tracking devices, interactive devices, computers and other devices, which breaks through the limitations of traditional MR systems.	https://dl.acm.org/authorize?N675744	Yihua Bao, Dong Li, Dongdong Weng, Mo Su
paGAN: real-time avatars using dynamic textures	We propose a novel discrete scheme for simulating viscous thin films at real-time frame rates. Our scheme is based on a new formulation of the gradient flow approach, that leads to a discretization based on that are easily computable on the GPU. Our approach has physical fidelity, as the total mass is guaranteed to be preserved, an appropriate discrete energy is controlled, and the film height is guaranteed to be non-negative at all times. In addition, and unlike all existing methods for thin films simulation, it is fast enough to allow realtime interaction with the flow, for designing initial conditions and controlling the forces during the simulation.	https://dl.acm.org/authorize?N675575	Weifeng Ge, Bingchen Gong, Yizhou Yu
