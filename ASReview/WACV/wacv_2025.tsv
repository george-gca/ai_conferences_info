title	abstract	url	authors
360PanT: Training-Free Text-Driven 360-Degree Panorama-to-Panorama Translation	Preserving boundary continuity in the translation of 360-degree panoramas remains a significant challenge for existing text-driven image-to-image translation methods. These methods often produce visually jarring discontinuities at the translated panorama's boundaries disrupting the immersive experience. To address this issue we propose 360PanT a training-free approach to text-based 360-degree panorama-to-panorama translation with boundary continuity. Our 360PanT achieves seamless translations through two key components: boundary continuity encoding and seamless tiling translation with spatial control. Firstly the boundary continuity encoding embeds critical boundary continuity information of the input 360-degree panorama into the noisy latent representation by constructing an extended input image. Secondly leveraging this embedded noisy latent representation and guided by a target prompt the seamless tiling translation with spatial control enables the generation of a translated image with identical left and right halves while adhering to the extended input's structure and semantic layout. This process ensures a final translated 360-degree panorama with seamless boundary continuity. Experimental results on both real-world and synthesized datasets demonstrate the effectiveness of our 360PanT in translating 360-degree panoramas. Code is available at https://github.com/littlewhitesea/360PanT	https://openaccess.thecvf.com//content/WACV2025/html/Wang_360PanT_Training-Free_Text-Driven_360-Degree_Panorama-to-Panorama_Translation_WACV_2025_paper.html	Hai Wang, Jing-Hao Xue
3D Edge Sketch from Multiview Images	The semantic reconstruction of a scene relies in part on the curvilinear structure inherent in images. The recovery of curvilinear structure is not only key to the representation of objects via ridges and other object curves but is also critical to the reconstruction from texture-poor images which lack a sufficient number of features. Prior methods advocate for the recovery of curve segments from images and reconstructing these into an organized collection of 3D curve segments often referred to as the 3D curve sketch which serves as the basis for further reconstruction of curves and surfaces. Observing that the process of edge grouping can lead to fictitious curves or missing veridical groupings this paper advocates for a reconstruction of curvilinear structure directly from image edges in the form of a 3D edge sketch. The multiview reconstruction of edges faces significant combinatorial challenges which are effectively addressed in this paper. We demonstrate through experiments that the 3D edge sketch recovers a vast majority of the curvilinear structure and is a reliable substrate from which 3D curves can be constructed.	https://openaccess.thecvf.com//content/WACV2025/html/Zheng_3D_Edge_Sketch_from_Multiview_Images_WACV_2025_paper.html	Yilin Zheng, Chiang-Heng Chien, Ricardo Fabbri, Benjamin Kimia
3D Part Segmentation via Geometric Aggregation of 2D Visual Features	Supervised 3D part segmentation models are tailored for a fixed set of objects and parts limiting their transferability to open-set real-world scenarios. Recent works have explored vision-language models (VLMs) as a promising alternative using multi-view rendering and textual prompting to identify object parts. However naively applying VLMs in this context introduces several drawbacks such as the need for meticulous prompt engineering and fails to leverage the 3D geometric structure of objects. To address these limitations we propose COPS a COmprehensive model for Parts Segmentation that blends the semantics extracted from visual concepts and 3D geometry to effectively identify object parts. COPS renders a point cloud from multiple viewpoints extracts 2D features projects them back to 3D and uses a novel geometric-aware feature aggregation procedure to ensure spatial and semantic consistency. Finally it clusters points into parts and labels them. We demonstrate that COPS is efficient scalable and achieves zero-shot state-of-the-art performance across five datasets covering synthetic and real-world data texture-less and coloured objects as well as rigid and non-rigid shapes. The code is available at https://3d-cops.github.io.	https://openaccess.thecvf.com//content/WACV2025/html/Garosi_3D_Part_Segmentation_via_Geometric_Aggregation_of_2D_Visual_Features_WACV_2025_paper.html	Marco Garosi, Riccardo Tedoldi, Davide Boscaini, Massimiliano Mancini, Nicu Sebe, Fabio Poiesi
3D Shape Completion using Multi-Resolution Spectral Encoding	Reconstruction of intricate local patterns and large missing regions during 3D shape completion has the contradictory requirements of computation over a wider context and operations for finer detail restoration. To this end we propose a multi-resolution spectral encoding based 3D shape completion approach to work on truncated Signed Distance Field (SDF) based shape representations. Our novelty lies in judiciously integrating multi-resolution 3D convolutional blocks that encode the input shape and a spectral module (SM) that captures the shape-wide context thus addressing the contradictory requirements. SM acts on the features extracted from both partial input scans and shape priors using the multi-resolution convolutional blocks. Our SM contains a 3D convolutional block placed between fast Fourier transform (FFT) and inverse FFT operations which results in the expansion of the receptive field for the appropriate context computation. Our approach has an attention-based encoder-decoder architecture where the encoding of a partial scan is acted upon by shape prior encodings to produce attention maps. These attention maps are leveraged differently in pretraining and in the later training and inference stages of our approach to produce the reconstructed 3D shape. A surface gradient-based loss function is used in addition to the L1 loss both in the pretraining and training stages for emphasizing the differences in minute details. These along with an attention refinement operation often leads to complete reconstruction while restoring finer details. Experiments using standard synthetic and real datasets demonstrate the superiority of our approach over the state-of-the-art.	https://openaccess.thecvf.com//content/WACV2025/html/Deka_3D_Shape_Completion_using_Multi-Resolution_Spectral_Encoding_WACV_2025_paper.html	Pallabjyoti Deka, Saumik Bhattacharya, Debashis Sen, Prabir Kumar Biswas
3D Synthesis for Architectural Design	We introduce a 3D synthesis method for architectural design to allow for the efficient generation of diverse and realistic building designs. In spite of advances in 3D synthesis current off-the-shelf 3D synthesis techniques are inappropriate for architectural design: they are trained primarily on isolated objects have limited diversity blend building facades with background and produce overly complex geometry that is difficult to edit or manipulate a major issue in an iterative design process. We propose an alternative pipeline that integrates auto-generated coarse models with segment-wise texture inpainting and semantics-based editing resulting in diverse style-consistent and shape-precise designs. We show through qualitative and quantitative experiments that our pipeline generates more diverse visually appealing architectures with clean geometries without the need for any extensive training.	https://openaccess.thecvf.com//content/WACV2025/html/Tsai_3D_Synthesis_for_Architectural_Design_WACV_2025_paper.html	I-Ting Tsai, Bharath Hariharan
3D Understanding of Deformable Linear Objects: Datasets and Transferability Benchmark	Deformable linear objects are commonly found in our daily lives. Understanding them visually can be challenging even for humans as the same object can become entangled and look completely different. Examples of deformable linear objects include blood vessels and wiring harnesses which are crucial for the proper functioning of systems like the human body and vehicles. Recently some studies have focused on 2D image segmentation of wires. However there are no point cloud datasets available for studying 3D deformable linear objects which are more complex and challenging. To address this gap we introduce two point cloud datasets PointWire and PointVessel generated using our proposed semi-automatic pipeline. We evaluated state-of-the-art methods on these large-scale 3D deformable linear object benchmarks. Additionally we analyzed the generalization capabilities of these methods through transferability experiments on the PointWire and PointVessel datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Zagar_3D_Understanding_of_Deformable_Linear_Objects_Datasets_and_Transferability_Benchmark_WACV_2025_paper.html	Bare Luka Å½agar, Mingyu Liu, Tim Hertel, Ekim Yurtsever, Alois C. Knoll
@BENCH: Benchmarking Vision-Language Models for Human-Centered Assistive Technology	As Vision-Language Models (VLMs) advance human-centered Assistive Technologies (ATs) for helping People with Visual Impairments (PVIs) are evolving into generalists capable of performing multiple tasks simultaneously. However benchmarking VLMs for ATs remains under-explored. To bridge this gap we first create a novel AT benchmark (@BENCH). Guided by a pre-design user study with PVIs our benchmark includes the five most crucial vision-language tasks: Panoptic Segmentation Depth Estimation Optical Character Recognition (OCR) Image Captioning and Visual Question Answering (VQA). Besides we propose a novel AT model (@MODEL) that addresses all tasks simultaneously and can be expanded to more assistive functions for helping PVIs. Our framework exhibits outstanding performance across tasks by integrating multi-modal information and it offers PVIs a more comprehensive assistance. Extensive experiments prove the effectiveness and generalizability of our framework.	https://openaccess.thecvf.com//content/WACV2025/html/Jiang_BENCH_Benchmarking_Vision-Language_Models_for_Human-Centered_Assistive_Technology_WACV_2025_paper.html	Xin Jiang, Junwei Zheng, Ruiping Liu, Jiahang Li, Jiaming Zhang, Sven Matthiesen, Rainer Stiefelhagen
A 0-Shot Self-Attention Mechanism for Accelerated Diagonal Attention	The ability of Transformers to process longer sequences has led to unprecedented levels of generalization in visual tasks. However the complexity of Transformers is dominated by the quadratic cost associated with the computation of the attention blocks posing a bottleneck that impedes the scaling of sequence length and the realization of more advanced AI solutions. We propose and explore the hypothesis that the self-attention mechanism exhibits regularities that can be exploited to enhance performance and achieve linear-cost attention without significant loss of effectiveness. Specifically we investigate the attention matrix of Visual Transformers to identify and leverage these regularities in order to simplify the computation process. The resulting procedure significantly reduces the computational cost of Transformers by directly reducing attention block complexity. Moreover the designed procedure is 0-shot self-supervised thus it requires no retraining additional data or parameters as all Transformer parameters remain unchanged. Consequently the proposed method can be seamlessly applied to pre-trained Visual Transformers without the need for retraining. Experiments conducted on a series of Vision Transformers pre-trainedon ImageNet-1K dataset demonstrate the effectiveness of our proposed approach.	https://openaccess.thecvf.com//content/WACV2025/html/Viti_A_0-Shot_Self-Attention_Mechanism_for_Accelerated_Diagonal_Attention_WACV_2025_paper.html	Mario Viti, Nadiya Shvai, Arcadi Llanza, Amir Nakib
A Conflict-Guided Evidential Multimodal Fusion for Semantic Segmentation	This article presents a novel and robust approach to semantic segmentation based on the fusion of different image modalities (conventional and non-conventional images). The robustness of fusion methods and their ability to tolerate sensor failures are crucial challenges for their deployment in real-world environments. It is essential to develop unique fusion models that can operate even in the absence of certain modalities during inference. However current fusion methods have a strong dependence on the RGB branch resulting in significant performance losses in case of its unavailability. To address this issue we propose ECoLaF (Evidential Conflict-guided Late Fusion) a late fusion method based on Dempster-Shafer theory. This method adaptively reduces the output of each modality according to their conflicts before fusing them. Experimental results show that our approach outperforms state-of-the-art methods in terms of robustness on the MCubeS and DeLiVER datasets especially when the RGB sensor is not operational. This study offers new perspectives for improving the robustness of semantic segmentation in multimodal contexts. Code is available at https://github.com/deregnaucourtlucas/ECoLaF.	https://openaccess.thecvf.com//content/WACV2025/html/Deregnaucourt_A_Conflict-Guided_Evidential_Multimodal_Fusion_for_Semantic_Segmentation_WACV_2025_paper.html	Lucas Deregnaucourt, Hind Laghmara, Alexis Lechervy, Samia Ainouz
A Conic Transformation Approach for Solving the Perspective-Three-Point Problem	We propose a conic transformation method to solve the Perspective-Three-Point (P3P) problem. In contrast to the current state-of-the-art solvers which formulate the P3P problem by intersecting two conics and constructing a degenerate conic to find the intersection our approach builds upon a new formulation based on a transformation that maps the two conics to a new coordinate system where one of the conics becomes a standard parabola in a canonical form. This enables expressing one variable in terms of the other variable and as a consequence substantially simplifies the problem of finding the conic intersection. Moreover the polynomial coefficients are fast to compute and we only need to determine the real-valued intersection points which avoids the requirement of using computationally expensive complex arithmetic. While the current state-of-the-art methods reduce the conic intersection problem to solving a univariate cubic equation our approach despite resulting in a quartic equation is still faster thanks to this new simplified formulation. Extensive evaluations demonstrate that our method achieves higher speed while maintaining robustness and stability comparable to state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_A_Conic_Transformation_Approach_for_Solving_the_Perspective-Three-Point_Problem_WACV_2025_paper.html	Haidong Wu, Snehal Bhayani, Janne HeikkilÃ¤
A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization	Large text-to-image models have revolutionized the ability to generate imagery using natural language. However particularly unique or personal visual concepts such as pets and furniture will not be captured by the original model. This has led to interest in how to personalize a text-to-image model. Despite significant progress this task remains a formidable challenge particularly in preserving the subject's identity. Most researchers attempt to address this issue by modifying model architectures. These methods are capable of keeping the subject structure and color but fail to preserve identity details. Towards this issue our approach takes a data-centric perspective. We introduce a novel regularization dataset generation strategy on both the text and image level. This strategy enables the model to preserve fine details of the desired subjects such as text and logos. Our method is architecture-agnostic and can be flexibly applied on various text-to-image models. We show on established benchmarks that our data-centric approach forms the new state of the art in terms of identity preservation and text alignment.	https://openaccess.thecvf.com//content/WACV2025/html/He_A_Data_Perspective_on_Enhanced_Identity_Preservation_for_Diffusion_Personalization_WACV_2025_paper.html	Xingzhe He, Zhiwen Cao, Nick Kolkin, Lantao Yu, Kun Wan, Helge Rhodin, Ratheesh Kalarot
A Generic Vehicle-to-Sensor Calibration Framework	In autonomous driving systems online vehicle-to-sensor (v2s) calibration is a critical component for ensuring safe perception-based control. Since sensor pose may shift during the life-time of a vehicle online calibration is essential to maintain safe driving conditions. To this end this paper introduces Epipoles as a 3D Directional Compass (E3DC) a sensor-agnostic v2s online calibration method. Leveraging the nonholonomic nature of vehicles a hand-eye constraint between the vehicle and the sensor naturally emerges. Consequently we require only the sensor's data to determine the v2s extrinsic rotation. More specifically since we only require sensor odometry estimates to perform v2s calibration E3DC can leverage off-the-shelf odometry estimation pipelines. This offers vast flexibility and wide applicability as the odometry estimation pipeline can be tailored to the specific sensor type and driving environment. We demonstrate that our method is robust and achieves state-of-the-art performance on both the KITTI dataset and a new dataset which will be made publicly available. To the best of our knowledge this is the first v2s calibration dataset for autonomous driving scenarios.	https://openaccess.thecvf.com//content/WACV2025/html/Hu_A_Generic_Vehicle-to-Sensor_Calibration_Framework_WACV_2025_paper.html	Sumin Hu, Youngmin Yoo, Jeeseong Kim, Changsoo Lim, Doohyun Cho, Bongnam Kang
A Mamba-Based Siamese Network for Remote Sensing Change Detection	Change detection in remote sensing images is an essential tool for analyzing a region at different times. It finds varied applications in monitoring environmental changes man-made changes as well as corresponding decision-making and prediction of future trends. Deep learning methods like Convolutional Neural Networks (CNNs) and Transformers have achieved remarkable success in detecting significant changes given two images at different times. In this paper we propose a Mamba-based Change Detector (M-CD) that segments out the regions of interest even better. Mamba-based architectures demonstrate linear-time training capabilities and an improved receptive field over transformers. Our experiments on four widely used change detection datasets demonstrate significant improvements over existing state-of-the-art (SOTA) methods. Code: https://github.com/JayParanjape/M-CD	https://openaccess.thecvf.com//content/WACV2025/html/Paranjape_A_Mamba-Based_Siamese_Network_for_Remote_Sensing_Change_Detection_WACV_2025_paper.html	Jay N. Paranjape, Celso de Melo, Vishal M. Patel
A Multi-Task Supervised Compression Model for Split Computing	Split computing ( split learning) is a promising approach to deep learning models for resource-constrained edge computing systems where weak sensor (mobile) devices are wirelessly connected to stronger edge servers through channels with limited communication capacity. State-of-the-art work on split computing presents methods for single tasks such as image classification object detection or semantic segmentation. The application of existing methods to multi-task problems degrades model accuracy and/or significantly increase runtime latency. In this study we propose Ladon the first multi-task-head supervised compression model for multi-task split computing. Experimental results show that the multi-task supervised compression model either outperformed or rivaled strong lightweight baseline models in terms of predictive performance for ILSVRC 2012 COCO 2017 and PASCAL VOC 2012 datasets while learning compressed representations at its early layers. Furthermore our models reduced end-to-end latency (by up to 95.4%) and energy consumption of mobile devices (by up to 88.2%) in multi-task split computing scenarios.	https://openaccess.thecvf.com//content/WACV2025/html/Matsubara_A_Multi-Task_Supervised_Compression_Model_for_Split_Computing_WACV_2025_paper.html	Yoshitomo Matsubara, Matteo Mendula, Marco Levorato
A New Benchmark and Baseline for Real-Time High-Resolution Image Inpainting on Edge Devices	Existing image inpainting methods have shown impressive completion results for low-resolution images. However most of these algorithms fail at high resolutions and require powerful hardware limiting their deployment on edge devices. Motivated by this we propose the first baseline for REal-Time High-resolution image INpainting on Edge Devices (RETHINED) that is able to inpaint at ultra-high-resolution and can run in real-time (30ms) in a wide variety of mobile devices. A simple yet effective novel method formed by a lightweight Convolutional Neural Network (CNN) to recover structure followed by a resolution-agnostic patch replacement mechanism to provide detailed texture. Specially our pipeline leverages the structural capacity of CNN and the high-level detail of patch-based methods which is a key component for high-resolution image inpainting. To demonstrate the real application of our method we conduct an extensive analysis on various mobile-friendly devices and demonstrate similar inpainting performance while being 100 times faster than existing state-of-the-art methods. Furthemore we release DF8K-Inpainting the first free-from mask UHD inpainting dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Ortega_A_New_Benchmark_and_Baseline_for_Real-Time_High-Resolution_Image_Inpainting_WACV_2025_paper.html	Marcelo SÃ¡nchez Ortega, Gil Triginer Garces, Coloma Ballester, Ignacio Sarasua, Lara Raad
A Novel Perspective for Multi-Modal Multi-Label Skin Lesion Classification	The efficacy of deep learning-based Computer-Aided Diagnosis (CAD) methods for skin diseases relies on analyzing multiple data modalities (i.e. clinical+dermoscopic images and patient metadata) and addressing the challenges of multi-label classification. Current approaches tend to rely on limited multi-modal techniques and treat the multi-label problem as a multiple multi-class problem overlooking issues related to imbalanced learning and multi-label correlation. This paper introduces the innovative Skin Lesion Classifier utilizing a Multi-modal Multi-label TransFormer-based model (SkinM2Former). For multi-modal analysis we introduce the Tri-Modal Cross-attention Transformer (TMCT) that fuses the three image and metadata modalities at various feature levels of a transformer encoder. For multi-label classification we introduce a multi-head attention (MHA) module to learn multi-label correlations complemented by an optimisation that handles multi-label and imbalanced learning problems. SkinM2Former achieves a mean average accuracy of 77.27% and a mean diagnostic accuracy of 77.85% on the public Derm7pt dataset outperforming state-of-the-art (SOTA) methods.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_A_Novel_Perspective_for_Multi-Modal_Multi-Label_Skin_Lesion_Classification_WACV_2025_paper.html	Yuan Zhang, Yutong Xie, Hu Wang, Jodie C Avery, M Louise Hull, Gustavo Carneiro
A Parametric Approach to Adversarial Augmentation for Cross-Domain Iris Presentation Attack Detection	Iris-based biometric systems are vulnerable to presentation attacks (PAs) where adversaries present physical artifacts (e.g. printed iris images textured contact lenses) to defeat the system. This has led to the development of various presentation attack detection (PAD) algorithms which typically perform well in intra-domain settings. However they often struggle to generalize effectively in cross-domain scenarios where training and testing employ different sensors PA instruments and datasets. In this work we use adversarial training samples of both bonafide irides and PAs to improve the cross-domain performance of a PAD classifier. The novelty of our approach lies in leveraging transformation parameters from classical data augmentation schemes (e.g. translation rotation) to generate adversarial samples. We achieve this through a convolutional autoencoder ADV-GEN that inputs original training samples along with a set of geometric and photometric transformations. The transformation parameters act as regularization variables guiding ADV-GEN to generate adversarial samples in a constrained search space. Experiments conducted on the LivDet-Iris 2017 database comprising four datasets and the LivDet-Iris 2020 dataset demonstrate the efficacy of our proposed method. The code is available at https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD.	https://openaccess.thecvf.com//content/WACV2025/html/Pal_A_Parametric_Approach_to_Adversarial_Augmentation_for_Cross-Domain_Iris_Presentation_WACV_2025_paper.html	Debasmita Pal, Redwan Sony, Arun Ross
A Pipeline and NIR-Enhanced Dataset for Parking Lot Segmentation	"Discussions of minimum parking requirement policies often include maps of parking lots which are time-consuming to construct manually. Open-source datasets for such parking lots are scarce particularly for US cities. This paper introduces the idea of using Near-Infrared (NIR) channels as input and several post-processing techniques to improve the prediction of off-street surface parking lots using satellite imagery. We constructed two datasets with 12617 image-mask pairs each: one with 3-channel (RGB) and another with 4-channel (RGB + NIR). The datasets were used to train five deep learning models (OneFormer Mask2Former SegFormer DeepLabV3 and FCN) for semantic segmentation classifying images to differentiate between parking and non-parking pixels. Our results demonstrate that the NIR channel improved accuracy because parking lots are often surrounded by grass--even though the NIR channel needed to be upsampled from a lower resolution. Post-processing including eliminating erroneous ""holes"" simplifying edges and removing road and building footprints further improved the accuracy. Best model OneFormer trained on 4-channel input and paired with post-processing techniques achieves a mean Intersection over Union (mIoU) of 84.9% and a pixel-wise accuracy of 96.3%."	https://openaccess.thecvf.com//content/WACV2025/html/Qiam_A_Pipeline_and_NIR-Enhanced_Dataset_for_Parking_Lot_Segmentation_WACV_2025_paper.html	Shirin Qiam, Saipraneeth Devunuri, Lewis J. Lehe
A Rapid Test for Accuracy and Bias of Face Recognition Technology	Measuring the accuracy of face recognition (FR) systems is essential for improving performance and ensuring responsible use. Accuracy is typically estimated using large annotated datasets which are costly and difficult to obtain. We propose a novel method for 1:1 face verification that benchmarks FR systems quickly and without manual annotation starting from approximate labels (e.g. from web search results). Unlike previous methods for training set label cleaning ours leverages the embedding representation of the models being evaluated achieving high accuracy in smaller-sized test datasets. Our approach reliably estimates FR accuracy and ranking significantly reducing the time and cost of manual labeling. We also introduce the first public benchmark of five FR cloud services revealing demographic biases particularly lower accuracy for Asian women. Our rapid test method can democratize FR testing promoting scrutiny and responsible use of the technology. Our method is provided as a publicly accessible tool at https://github.com/caltechvisionlab/frt-rapid-test.	https://openaccess.thecvf.com//content/WACV2025/html/Knott_A_Rapid_Test_for_Accuracy_and_Bias_of_Face_Recognition_WACV_2025_paper.html	Manuel Knott, Ignacio Serna, Ethan Mann, Pietro Perona
A Realistic Protocol for Evaluation of Weakly Supervised Object Localization	Weakly Supervised Object Localization (WSOL) allows training deep learning models for classification and localization (LOC) using only global class-level labels. The absence of bounding box (bbox) supervision during training raises challenges in the literature for hyper-parameter tuning model selection and evaluation. WSOL methods rely on a validation set with bbox annotations for model selection and a test set with bbox annotations for threshold estimation for producing bboxes from localization maps. This approach however is not aligned with the WSOL setting as these annotations are typically unavailable in real-world scenarios. Our initial empirical analysis shows a significant decline in LOC performance when model selection and threshold estimation rely solely on class labels and the image itself respectively compared to using manual bbox annotations. This highlights the importance of incorporating bbox labels for optimal model performance. In this paper a new WSOL evaluation protocol is proposed that provides LOC information without the need for manual bbox annotations. In particular we generated noisy pseudo-boxes from a pretrained off-the-shelf region proposal method such as Selective Search CLIP and RPN for model selection. These bboxes are also employed to estimate the threshold from LOC maps circumventing the need for test-set bbox annotations. Our experiments with several WSOL methods on challenging natural and medical image datasets show that using the proposed pseudo-bboxes for validation facilitates the model selection and threshold estimation with LOC performance comparable to models selected using GT bboxes on the validation set and threshold estimation on the test set. It also outperforms models selected using class-level labels and then dynamically thresholded with only LOC maps.	https://openaccess.thecvf.com//content/WACV2025/html/Murtaza_A_Realistic_Protocol_for_Evaluation_of_Weakly_Supervised_Object_Localization_WACV_2025_paper.html	Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Eric Granger
A Reality Check on Pre-training for Exemplar-free Class-Incremental Learning	Exemplar-free class-incremental learning (EFCIL) aims to classify streaming data without storing examples from the past. Recent EFCIL works suggest that (i) models pre-trained with large amounts of data should be used to initialize learning (ii) self-supervised learned transformers generalize better than supervised convolutional models (iii) adding generated data to the pre-training dataset can improve incremental accuracy. In this article we question the above assertions by comprehensively evaluating various initial training strategies combined with four EFCIL algorithms using four large-scale datasets. Our results indicate that: (i) pre-trained models are preferable when the domain of the incremental classification task is well represented in the pre-training datasets but training with initial data remains useful when the domain shift is significant (ii) supervised convolutional networks remain competitive particularly when improving representation transferability using data augmentation or a projector (iii) adding classes from an external dataset to train the initial model boosts performance when the initial set of classes is small but has a limited effect otherwise (iv) additional classes generated with a diffusion model are not necessarily more useful than a well-chosen set of ImageNet classes to improve model transferability. We provide a nuanced analysis of these results and formulate recommendations to facilitate the practical adoption of EFCIL algorithms.	https://openaccess.thecvf.com//content/WACV2025/html/Feillet_A_Reality_Check_on_Pre-training_for_Exemplar-free_Class-Incremental_Learning_WACV_2025_paper.html	Eva Feillet, Adrian Popescu, CÃ©line Hudelot
A Recipe for Geometry-Aware 3D Mesh Transformers	Utilizing patch-based transformers for unstructured geometric data such as polygon meshes presents significant challenges primarily due to the absence of a canonical ordering and variations in input sizes. Prior approaches to handling 3D meshes and point clouds have either relied on computationally intensive node-level tokens for large objects or resorted to resampling to standardize patch size. Moreover these methods generally lack a geometry-aware stable Structural Embedding (SE) often depending on simplistic absolute SEs such as 3D coordinates which compromise isometry invariance essential for tasks like semantic segmentation. In our study we meticulously examine the various components of a geometry-aware 3D mesh transformer from tokenization to structural encoding assessing the contribution of each. Initially we introduce a spectral-preserving tokenization rooted in algebraic multi-grid methods. Subsequently we detail an approach for embedding features at the patch level accommodating patches with variable node counts. Through comparative analyses against a baseline model employing simple point-wise Multi-Layer Perceptrons (MLP) our research highlights critical insights: 1) the importance of structural and positional embeddings facilitated by heat diffusion in general 3D mesh transformers; 2) the effectiveness of novel components such as geodesic masking and feature interaction via cross-attention in enhancing learning; and 3) the superior performance and efficiency of our proposed methods in challenging segmentation and classification tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Farazi_A_Recipe_for_Geometry-Aware_3D_Mesh_Transformers_WACV_2025_paper.html	Mohammad Farazi, Yalin Wang
A Regional-Level Resource-Saving Model for Winter Road Surface Snow Detection in Extreme Weathers	Achieving timely and accurate snow detection on road surfaces in extreme weather conditions is vital for both transportation and computer vision applications. However conventional object detection models particularly those designed for small targets fall short in addressing the challenge that is posed by special regional-level multi-scale recognition task. To this end an end-to-end precise and swift road surface snow detection architecture termed the Resource-Saving Snow Detect Model (RSSD) that includes a multidimensional directional attention mechanism is proposed. In this model we designed three dedicated modules namely Multi-dimensional Bidirectional Attention Module (MDBA) Split-EMA-Convolution (SEC) and Equal Split Convolution (ESC) to address the essential feature extraction and fusion tasks in snow detection. MDBA is able to promote lateral interaction and comprehensive feature fusion across scales while SEC can not only enhance feature extraction for regional awareness but also reduces computational load making it efficient under minimal computational power consumption. ESC preserves feature height fusion while significantly reducing computational costs thereby enhancing the real-time detection capability of the model. In experimental evaluations conducted with data collected by in-vehicle cameras from various roads in the United States and Canada the results demonstrate higher detection accuracy and speed compared to the latest Transformer-based real-time object detection methods and other exiting methods in the literature. Furthermore we validated the model's performance and data sensitivity through semi-supervised learning with 50000 unlabeled images. This research holds significant implications for winter road traffic and provides valuable insights for similar computer vision tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Zhou_A_Regional-Level_Resource-Saving_Model_for_Winter_Road_Surface_Snow_Detection_WACV_2025_paper.html	Xinhao Zhou, Tong Wang, Zhaodong Liu, Hao Wei, Guangyuan Pan
A Semantically Impactful Image Manipulation Dataset: Characterizing Image Manipulations using Semantic Significance	We investigate how to characterize semantic significance (SS) in detecting image manipulations (IMD) for media forensics. We introduce the Characterization of Semantic Impact for IMD (CSI-IMD) dataset which focuses on localizing and evaluating the semantic impact of image manipulations to counter advanced generative techniques. Our evaluation of 10 state-of-the-art IMD and localization methods on CSI-IMD reveals key insights. Unlike existing datasets CSI-IMD provides detailed semantic annotations beyond traditional manipulation masks aiding in the development of new defensive strategies. The dataset features manipulations from advanced generation methods offering various levels of semantic significance. It is divided into two parts: a gold-standard set of 1000 manually annotated manipulations with high-quality control and an extended set of 500000 automated manipulations for large-scale training and analysis. We also propose a new SS-focused task to assess the impact of semantically targeted manipulations. Our experiments show that current IMD methods struggle with manipulations created using stable diffusion with TruFor and Cat-Net performing the best among those tested. The CSI-IMD dataset will become available at https://github.com/csiimd/csiimd.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_A_Semantically_Impactful_Image_Manipulation_Dataset_Characterizing_Image_Manipulations_using_WACV_2025_paper.html	Yuwei Chen, Ming-Ching Chang, Mattias Kirchner, Zhenfei Zhang, Xin Li, Arslan Basharat, Anthony Hoogs
A Simple-but-Effective Baseline for Training-Free Class-Agnostic Counting	Class-Agnostic Counting (CAC) seeks to accurately count objects in a given image with only a few reference examples. While previous methods achieving this relied on additional training recent efforts have shown that it's possible to accomplish this without training by utilizing pre-existing foundation models particularly the Segment Anything Model (SAM) for counting via instance-level segmentation. Although promising current training-free methods still lag behind their training-based counterparts in terms of performance. In this research we present a straightforward training-free solution that effectively bridges this performance gap serving as a strong baseline. The primary contribution of our work lies in the discovery of four key technologies that can enhance performance. Specifically we suggest employing a superpixel algorithm to generate more precise initial point prompts utilizing an image encoder with richer semantic knowledge to replace the SAM encoder for representing candidate objects and adopting a multiscale mechanism and a transductive prototype scheme to update the representation of reference examples. By combining these four technologies our approach achieves significant improvements over existing training-free methods and delivers performance on par with training-based ones.	https://openaccess.thecvf.com//content/WACV2025/html/Lin_A_Simple-but-Effective_Baseline_for_Training-Free_Class-Agnostic_Counting_WACV_2025_paper.html	Yuhao Lin, Haiming Xu, Lingqiao Liu, Javen Qinfeng Shi
A Spatio-Temporal Representation Learning as an Alternative to Traditional Glosses in Sign Language Translation and Production	This work addresses the challenges associated with the use of glosses in both Sign Language Translation (SLT) and Sign Language Production (SLP). While the glosses have long been used as a bridge between sign language and spoken language they come with two major limitations that impede the advancement of sign language systems. First annotating the glosses is a labor-intensive and time-consuming process which limits the scalability of datasets. Second the glosses oversimplify sign language by stripping away its spatio-temporal dynamics reducing complex signs to basic labels and missing the subtle movements essential for precise interpretation. To address these limitations we introduce Universal Gloss-level Representation (UniGloR) a framework designed to capture the spatio-temporal features inherent in sign language providing a more dynamic and detailed alternative to the use of the glosses. The core idea of UniGloR is simple yet effective: We derive dense spatio-temporal representations from sign keypoint sequences using self-supervised learning and seamlessly integrate them into SLT and SLP tasks. Our experiments in a keypoint-based setting demonstrate that UniGloR either outperforms or matches the performance of previous SLT and SLP methods on two widely-used datasets: PHOENIX14T and How2Sign. Code is available at https://github.com/eddie-euijun-hwang/UniGloR.	https://openaccess.thecvf.com//content/WACV2025/html/Hwang_A_Spatio-Temporal_Representation_Learning_as_an_Alternative_to_Traditional_Glosses_WACV_2025_paper.html	Eui Jun Hwang, Sukmin Cho, Huije Lee, Youngwoo Yoon, Jong C. Park
A Two-Head Loss Function for Deep Average-K Classification	Average-K classification is an alternative to top-K classification in which the number of labels returned varies with the ambiguity of the input image but must average to K over all the samples. A simple method to solve this task is to threshold the softmax output of a model trained with the cross-entropy loss. This approach is theoretically proven to be asymptotically consistent but it is not guaranteed to be optimal for a finite set of samples. In this paper we propose a new loss function based on a multi-label classification head in addition to the classical softmax. This second head is trained using pseudo-labels generated by thresholding the softmax head while guaranteeing that K classes are returned on average. We show that this approach allows the model to better capture ambiguities between classes and as a result to return more consistent sets of possible classes. Experiments on two datasets from the literature demonstrate that our approach outperforms the softmax baseline as well as several other loss functions more generally designed for weakly supervised multi-label classification. The gains are larger the higher the uncertainty especially for classes with few samples.	https://openaccess.thecvf.com//content/WACV2025/html/Garcin_A_Two-Head_Loss_Function_for_Deep_Average-K_Classification_WACV_2025_paper.html	Camille Garcin, Maximilien Servajean, Alexis Joly, Joseph Salmon
A Versatile and Differentiable Hand-Object Interaction Representation	Synthesizing accurate hands-object interactions (HOI) is critical for applications in Computer Vision Augmented Reality (AR) and Mixed Reality (MR). Despite recent advances the accuracy of reconstructed or generated HOI leaves room for refinement. Some techniques have improved the accuracy of dense correspondences by shifting focus from generating explicit contacts to using rich HOI fields. Still they lack full differentiability or continuity and are tailored to specific tasks. In contrast we present a Coarse Hand-Object Interaction Representation (CHOIR) a novel versatile and fully differentiable field for HOI modelling. CHOIR leverages discrete unsigned distances for continuous shape and pose encoding alongside multivariate Gaussian distributions to represent dense contact maps with few parameters. To demonstrate the versatility of CHOIR we design JointDiffusion a diffusion model to learn a grasp distribution conditioned on noisy hand-object interactions or only object geometries for both refinement and synthesis applications. We demonstrate JointDiffusion's improvements over the SOTA in both applications: it increases the contact F1 score by 5% for refinement and decreases the sim. displacement by 46% for synthesis. Our experiments show that JointDiffusion with CHOIR yields superior contact accuracy and physical realism compared to SOTA methods designed for specific tasks. Project page: https://theomorales.com/CHOIR	https://openaccess.thecvf.com//content/WACV2025/html/Morales_A_Versatile_and_Differentiable_Hand-Object_Interaction_Representation_WACV_2025_paper.html	ThÃ©o Morales, Omid Taheri, Gerard Lacey
A Video is Worth 10000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval	Existing long video retrieval systems are trained and tested in the paragraph-to-video retrieval regime where every long video is described by a single long paragraph. This neglects the richness and variety of possible valid descriptions of a video which could range anywhere from moment-by-moment detail to a single phrase summary. To provide a more thorough evaluation of the capabilities of long video retrieval systems we propose a pipeline that leverages state-of-the-art large language models to carefully generate a diverse set of synthetic captions for long videos. We validate this pipeline's fidelity via rigorous human inspection. We use synthetic captions from this pipeline to perform a benchmark of a representative set of video language models using long video datasets and show that the models struggle on shorter captions. We show that finetuning on this data can both mitigate these issues (+2.8% R@1 over SOTA on ActivityNet with diverse captions) and even improve performance on standard paragraph-to-video retrieval (+1.0% R@1 on ActivityNet). We also use synthetic data from our pipeline as query expansion in the zero-shot setting (+3.4% R@1 on ActivityNet). We derive insights by analyzing failure cases for retrieval with short captions.	https://openaccess.thecvf.com//content/WACV2025/html/Gwilliam_A_Video_is_Worth_10000_Words_Training_and_Benchmarking_with_WACV_2025_paper.html	Matthew Gwilliam, Michael Cogswell, Meng Ye, Karan Sikka, Abhinav Shrivastava, Ajay Divakaran
AC-IND: Sparse CT Reconstruction Based on Attenuation Coefficient Estimation and Implicit Neural Distribution	Computed tomography (CT) reconstruction plays a crucial role in industrial nondestructive testing and medical diagnosis. Sparse view CT reconstruction aims to reconstruct high-quality CT images while only using a small number of projections which helps to improve the detection speed of industrial assembly lines and is also meaningful for reducing radiation in medical scenarios. Sparse CT reconstruction methods based on implicit neural representations (INRs) have recently shown promising performance but still produce artifacts because of the difficulty of obtaining useful prior information. In this work we incorporate a powerful prior: the total number of material categories of objects. To utilize the prior we design AC-IND a self-supervised method based on Attenuation Coefficient Estimation and Implicit Neural Distribution. Specifically our method first transforms the traditional INR from scalar mapping to probability distribution mapping. Then we design a compact attenuation coefficient estimator initialized with values from a rough reconstruction and fast segmentation. Finally our algorithm finishes the CT reconstruction by jointly optimizing the estimator and the generated distribution. Through experiments we find that our method not only outperforms the comparative methods in sparse CT reconstruction but also can automatically generate semantic segmentation maps. Code is available at https://github.com/AIIAAI/AC-IND.	https://openaccess.thecvf.com//content/WACV2025/html/Xie_AC-IND_Sparse_CT_Reconstruction_Based_on_Attenuation_Coefficient_Estimation_and_WACV_2025_paper.html	Wangduo Xie, Richard Schoonhoven, Tristan van Leeuwen, Matthew B. Blaschko
ACE: Action Concept Enhancement of Video-Language Models in Procedural Videos	Vision-language models (VLMs) are capable of recognizing unseen actions. However existing VLMs lack intrinsic understanding of procedural action concepts. Hence they overfit to fixed labels and are not invariant to unseen action synonyms. To address this we propose a simple fine-tuning technique Action Concept Enhancement (ACE) to improve the robustness and concept understanding of VLMs in procedural action classification. ACE continually incorporates augmented action synonyms and negatives in an auxiliary classification loss by stochastically replacing fixed labels during training. This creates new combinations of action labels over the course of fine-tuning and prevents overfitting to fixed action representations. We show the enhanced concept understanding of our VLM by visualizing the alignment of encoded embeddings of unseen action synonyms in the embedding space. Our experiments on the ATA IKEA and GTEA datasets demonstrate the efficacy of ACE in domains of cooking and assembly leading to significant improvements in zero-shot action classification while maintaining competitive performance on seen actions.	https://openaccess.thecvf.com//content/WACV2025/html/Ghoddoosian_ACE_Action_Concept_Enhancement_of_Video-Language_Models_in_Procedural_Videos_WACV_2025_paper.html	Reza Ghoddoosian, Nakul Agarwal, Isht Dwivedi, Behzad Dariush
ACE: Anatomically Consistent Embeddings in Composition and Decomposition	Medical images acquired from standardized protocols show consistent macroscopic or microscopic anatomical structures and these structures consist of composable/decomposable organs and tissues but existing self-supervised learning (SSL) methods do not appreciate such composable/decomposable structure attributes inherent to medical images. To overcome this limitation this paper introduces a novel SSL approach called ACE to learn anatomically consistent embedding via composition and decomposition with two key branches: (1) global consistency capturing discriminative macro-structures via extracting global features; (2) local consistency learning fine-grained anatomical details from composable/decomposable patch features via corresponding matrix matching. Experimental results across 6 datasets and 2 backbones evaluated in few-shot learning fine-tuning and property analysis show ACE's superior robustness transferability and clinical potential. The innovations of our ACE lie in grid-wise image cropping leveraging the intrinsic properties of compositionality and decompositionality of medical images bridging the semantic gap from high-level pathologies to low-level tissue anomalies and providing a new SSL method for medical imaging.	https://openaccess.thecvf.com//content/WACV2025/html/Zhou_ACE_Anatomically_Consistent_Embeddings_in_Composition_and_Decomposition_WACV_2025_paper.html	Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael Gotway, Jianming Liang
AH-OCDA: Amplitude-Based Curriculum Learning and Hopfield Segmentation Model for Open Compound Domain Adaptation	Open compound domain adaptation (OCDA) is a practical domain adaptation problem that consists of a source domain target compound domain and unseen open domain. In this problem the absence of domain labels and pixel-level segmentation labels for both compound and open domains poses challenges to the direct application of existing domain adaptation and generalization methods. To address this issue we propose Amplitude-based curriculum learning and a Hopfield segmentation model for Open Compound Domain Adaptation (AH-OCDA). Our method comprises two complementary components: 1) amplitude-based curriculum learning and 2) Hopfield segmentation model. Without prior knowledge of target domains within the compound domains amplitude-based curriculum learning gradually induces the semantic segmentation model to adapt from the near-source compound domain to the far-source compound domain by ranking unlabeled compound domain images through Fast Fourier Transform (FFT). Additionally the Hopfield segmentation model maps segmentation feature distributions from arbitrary domains to the feature distributions of the source domain. AH-OCDA achieves state-of-the-art performance on two OCDA benchmarks and extended open domains demonstrating its adaptability to continuously changing compound domains and unseen open domains.	https://openaccess.thecvf.com//content/WACV2025/html/Choi_AH-OCDA_Amplitude-Based_Curriculum_Learning_and_Hopfield_Segmentation_Model_for_Open_WACV_2025_paper.html	Jaehyun Choi, Junwon Ko, Dong-Jae Lee, Junmo Kim
AIC3DOD: Advancing Indoor Class-Incremental 3D Object Detection with Point Transformer Architecture and Room Layout Constraints	Over the recent years there has been a growing interest in class-incremental 3D object detection based on point clouds. However the current state-of-the-art (SOTA) methods still fall short of practical adoption mainly due to two key observations. Firstly existing SOTA methods are limited by the capability of feature representation from the object detection model. Secondly these methods overlook the importance of incorporating prior information or geometry constraints which are crucial elements for 3D point cloud tasks. In this study we strive to enhance the performance of class-incremental 3D object detection for indoor scenes by proposing AIC3DOD - Advancing Indoor Class-incremental 3D Object Detection using the point transformer architecture with room layout constraints. Our approach employs a transformer architecture in our detection model and optimizes the class incremental step in the transformer architecture. Besides AIC3DOD incorporates additional prior information namely room layout to impose physical constraints on detected objects thereby enhancing overall object detection performance. Extensive experimental results on the ScanNet dataset demonstrate the effectiveness of our approach showcasing our superior performance compared to other SOTA methods in the class-incremental 3D object detection task.	https://openaccess.thecvf.com//content/WACV2025/html/Cheng_AIC3DOD_Advancing_Indoor_Class-Incremental_3D_Object_Detection_with_Point_Transformer_WACV_2025_paper.html	Zhongyao Cheng, Fang Wu, Peisheng Qian, Ziyuan Zhao, Xulei Yang
AIDE: Improving 3D Open-Vocabulary Semantic Segmentation by Aligned Vision-Language Learning	3D open-vocabulary semantic segmentation aims at recognizing countless categories beyond the limited set of annotations used in traditional settings. Due to the lack of large-scale 3D-vision-language segmentation data instead of training models from scratch the current solutions distill knowledge from pre-trained 2D vision-language models (VLMs) into 3D models. However this distillation is supervised by misaligned 3D-scene-image-to-text data pairs consequently leading to suboptimal performance. Moreover as 2D VLMs are trained on 2D datasets text encoders of VLMs which serve as the bridge between 3D models and an unbounded set of categories lack 3D semantics. In this paper to address these issues and improve generalization performance we propose an AlIgned 3D Open-Vocabulary SEmantic Segmentation framework called AIDE with two novel modules. To collect high-quality and well-aligned 3D-scene-image-to-text pairs our CLIP-rewarded alignment module (i) generates diverse captions of multi-view images of 3D scenes to capture details by varying the temperatures and then (ii) samples captions based on their similarity to corresponding images for rich and accurate associations. Next to adapt 2D VLMs to 3D contexts our adaptive segmentation module introduces (iii) trainable tokens within the input space and each layer of the text encoder while freezing the text encoder to avoid catastrophic forgetting. Extensive experiments show that AIDE outperforms previous methods by a large margin on three representative benchmarks demonstrating its effectiveness.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_AIDE_Improving_3D_Open-Vocabulary_Semantic_Segmentation_by_Aligned_Vision-Language_Learning_WACV_2025_paper.html	Yimu Wang, Krzysztof Czarnecki
ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only	3D object detection plays a crucial role in various applications such as autonomous vehicles robotics and augmented reality. However training 3D detectors requires a costly precise annotation which is a hindrance to scaling annotation to large datasets. To address this challenge we propose a weakly supervised 3D annotator that relies solely on 2D bounding box annotations from images along with size priors. One major problem is that supervising a 3D detection model using only 2D boxes is not reliable due to ambiguities between different 3D poses and their identical 2D projection. We introduce a simple yet effective and generic solution: we build 3D proxy objects with annotations by construction and add them to the training dataset. Our method requires only size priors to adapt to new classes. To better align 2D supervision with 3D detection our method ensures depth invariance with a novel expression of the 2D losses. Finally to detect more challenging instances our annotator follows an offline pseudo-labelling scheme which gradually improves its 3D pseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our method not only performs on-par or above previous works on the Car category but also achieves performance close to fully supervised methods on more challenging classes. We further demonstrate the effectiveness and robustness of our method by being the first to experiment on the more challenging nuScenes dataset. We additionally propose a setting where weak labels are obtained from a 2D detector pre-trained on MS-COCO instead of human annotations. The code is available at https://github.com/CEA-LIST/ALPI	https://openaccess.thecvf.com//content/WACV2025/html/Lahlali_ALPI_Auto-Labeller_with_Proxy_Injection_for_3D_Object_Detection_using_WACV_2025_paper.html	Saad Lahlali, Nicolas Granger, Herve Le Borgne, Quoc-Cuong Pham
ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic Reconstruction	We propose an online 3D semantic segmentation method that incrementally reconstructs a 3D semantic map from a stream of RGB-D frames. Unlike offline methods ours is directly applicable to scenarios with real-time constraints such as robotics or mixed reality. To overcome the inherent challenges of online methods we make two main contributions. First to effectively extract information from the input RGB-D video stream we jointly estimate geometry and semantic labels per frame in 3D. A key focus of our approach is to reason about semantic entities both in the 2D input and the local 3D domain to leverage differences in spatial context and network architectures. Our method predicts 2D features using an off-the-shelf segmentation network. The extracted 2D features are refined by a lightweight 3D network to enable reasoning about the local 3D structure. Second to efficiently deal with an infinite stream of input RGBD frames a subsequent network serves as a temporal expert predicting the incremental scene updates by leveraging 2D 3D and past information in a learned manner. These updates are then integrated into a global scene representation. Using these main contributions our method can enable scenarios with real-time constraints and can scale to arbitrary scene sizes by processing and updating the scene only in a local region defined by the new measurement. Our experiments demonstrate improved results compared to existing online methods that purely operate in local regions and show that complementary sources of information can boost the performance. We provide a thorough ablation study on the benefits of different architectural as well as algorithmic design decisions. Our method yields competitive results on the popular ScanNet benchmark and SceneNN dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Weder_ALSTER_A_Local_Spatio-Temporal_Expert_for_Online_3D_Semantic_Reconstruction_WACV_2025_paper.html	Silvan Weder, Francis Engelmann, Johannes L. SchÃ¶nberger, Akihito Seki, Marc Pollefeys, Martin R. Oswald
AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for Unsupervised Surgical Instrument Segmentation	Surgical instrument segmentation (SIS) is pivotal for robotic-assisted minimally invasive surgery assisting surgeons by identifying surgical instruments in endoscopic video frames. Recent unsupervised surgical instrument segmentation (USIS) methods primarily rely on pseudo-labels derived from low-level features such as color and optical flow but these methods show limited effectiveness and generalizability in complex and unseen endoscopic scenarios. In this work we propose a label-free unsupervised model featuring a novel module named Multi-View Normalized Cutter (m-NCutter). Different from previous USIS works our model is trained using a graph-cutting loss function that leverages patch affinities for supervision eliminating the need for pseudo-labels. The framework adaptively determines which affinities from which levels should be prioritized. Therefore the low- and high-level features and their affinities are effectively integrated to train a label-free unsupervised model showing superior effectiveness and generalization ability. We conduct comprehensive experiments across multiple SIS datasets to validate our approach's state-of-the-art (SOTA) performance robustness and exceptional potential as a pre-trained model. Our code is released at https://github.com/MingyuShengSMY/AMNCutter.	https://openaccess.thecvf.com//content/WACV2025/html/Sheng_AMNCutter_Affinity-Attention-Guided_Multi-View_Normalized_Cutter_for_Unsupervised_Surgical_Instrument_Segmentation_WACV_2025_paper.html	Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
AMP-ViT: Optimizing Vision Transformer Efficiency with Adaptive Mixed-Precision Post-Training Quantization	Vision transformers (ViTs) have revolutionized computer vision but face significant challenges due to their high computational and memory demands. Existing post-training quantization methods struggle to maintain performance at low bit-widths due to activation asymmetry and reliance on manual configurations. To overcome these challenges we introduce SymAlign to address activation asymmetry and reduce clamping loss. Additionally we propose AutoScale an automatic and data-driven mechanism that adapts to variant activations. We incorporate the above-mentioned techniques and propose an adaptive mixed-precision post-training quantization framework for vision transformers (AMP-ViT). Our comprehensive approach addresses asymmetry variant distribution and uneven sensitivities making it the first to tackle these challenges thoroughly. Our experiments on ViT DeiT and Swin demonstrate significant accuracy improvements compared with SOTA on the ImageNet dataset. Specifically our proposed methods achieve accuracy improvements ranging from 0.90% to 23.35% on 4-bit ViTs with single-precision and from 3.82% to 78.14% on 5-bit fully quantized ViTs with mixed-precision.	https://openaccess.thecvf.com//content/WACV2025/html/Tai_AMP-ViT_Optimizing_Vision_Transformer_Efficiency_with_Adaptive_Mixed-Precision_Post-Training_Quantization_WACV_2025_paper.html	Yu-Shan Tai, An-Yeu Wu
ANTHROPOS-V: Benchmarking the Novel Task of Crowd Volume Estimation	We introduce the novel task of Crowd Volume Estimation (CVE) defined as the process of estimating the collective body volume of crowds using only RGB images. Besides event management and public safety CVE can be instrumental in approximating body weight unlocking weight-sensitive applications such as infrastructure stress assessment and assuring even weight balance. We propose the first benchmark for CVE comprising ANTHROPOS-V a synthetic photorealistic video dataset featuring crowds in diverse urban environments. Its annotations include each person's volume SMPL shape parameters and keypoints. Also we explore metrics pertinent to CVE define baseline models adapted from Human Mesh Recovery and Crowd Counting domains and propose a CVE-specific methodology that surpasses baselines. Although synthetic the weights and heights of individuals are aligned with the real-world population distribution across genders and they transfer to the downstream task of CVE from real images. Benchmark and code are available at github.com/colloroneluca/Crowd-Volume-Estimation.	https://openaccess.thecvf.com//content/WACV2025/html/Collorone_ANTHROPOS-V_Benchmarking_the_Novel_Task_of_Crowd_Volume_Estimation_WACV_2025_paper.html	Luca Collorone, Stefano Darrigo, Massimiliano Pappa, Guido M. Damely di Melendugno, Giovanni Ficarra, Fabio Galasso
ARD-VAE: A Statistical Formulation to Find the Relevant Latent Dimensions of Variational Autoencoders	The variational autoencoder (VAE) is a popular deep latent-variable model (DLVM) due to its simple yet effective formulation for modeling the data distribution. Moreover optimizing the VAE objective function is more manageable than other DLVMs. The bottleneck dimension of the VAE is a crucial design choice and it has strong ramifications for the model's performance such as finding the hidden explanatory factors of a dataset using the representations learned by the VAE. However the size of the latent dimension of the VAE is often treated as a hyperparameter estimated empirically through trial and error. To this end we propose a statistical formulation to discover the relevant latent factors required for modeling a dataset. In this work we use a hierarchical prior in the latent space that estimates the variance of the latent axes using the encoded data which identifies the relevant latent dimensions. For this we replace the fixed prior in the VAE objective function with a hierarchical prior keeping the remainder of the formulation unchanged. We call the proposed method the automatic relevancy detection in the variational autoencoder (ARD-VAE). We demonstrate the efficacy of the ARD-VAE on multiple benchmark datasets in finding the relevant latent dimensions and their effect on different evaluation metrics such as FID score and disentanglement analysis.	https://openaccess.thecvf.com//content/WACV2025/html/Saha_ARD-VAE_A_Statistical_Formulation_to_Find_the_Relevant_Latent_Dimensions_WACV_2025_paper.html	Surojit Saha, Sarang Joshi, Ross Whitaker
ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for 3D Scene Stylization	The radiance fields style transfer is an emerging field that has recently gained popularity as a means of 3D scene stylization thanks to the outstanding performance of neural radiance fields in 3D reconstruction and view synthesis. We highlight a research gap in radiance fields style transfer the need for sufficient perceptual controllability motivated by the existing concept in the 2D image style transfer. In this paper we present ARF-Plus a unique 3D neural style transfer framework offering manageable control over perceptual factors to systematically explore the perceptual controllability in 3D scene stylization. Four distinct types of controls - color preservation control (style pattern) scale control spatial (selective stylization area) control and depth enhancement control - come with our proposed novel loss functions and strategies seamlessly integrated into this framework. This unlocks a realm of limitless possibilities allowing customized modifications of stylization effects and flexible merging of the strengths of different styles ultimately enabling the creation of novel and eye-catching stylistic effects on 3D scenes.	https://openaccess.thecvf.com//content/WACV2025/html/Li_ARF-Plus_Controlling_Perceptual_Factors_in_Artistic_Radiance_Fields_for_3D_WACV_2025_paper.html	Wenzhao Li, Tianhao Wu, Fangcheng Zhong, A. Cengiz Oztireli
ARTIST: Improving the Generation of Text-Rich Images with Disentangled Diffusion Models and Large Language Models	Diffusion models have demonstrated exceptional capabilities in generating a broad spectrum of visual content yet their proficiency in rendering text is still limited: they often generate inaccurate characters or words that fail to blend well with the underlying image. To address these shortcomings we introduce a novel framework named ARTIST which incorporates a dedicated textual diffusion model to focus on the learning of text structures specifically. Initially we pretrain this textual model to capture the intricacies of text representation. Subsequently we finetune a visual diffusion model enabling it to assimilate textual structure information from the pretrained textual model. This disentangled architecture design and training strategy significantly enhance the text rendering ability of the diffusion models for text-rich image generation. Additionally we leverage the capabilities of pretrained large language models to interpret user intentions better contributing to improved generation quality. Empirical results on the MARIO-Eval benchmark underscore the effectiveness of the proposed method showing an improvement of up to 15% in various metrics.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_ARTIST_Improving_the_Generation_of_Text-Rich_Images_with_Disentangled_Diffusion_WACV_2025_paper.html	Jianyi Zhang, Yufan Zhou, Jiuxiang Gu, Curtis Wigington, Tong Yu, Yiran Chen, Tong Sun, Ruiyi Zhang
ARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media Damage	Accurately detecting and classifying damage in analogue media such as paintings photographs textiles mosaics and frescoes is essential for cultural heritage preservation. While machine learning models excel in correcting degradation if the damage operator is known a priori we show that they fail to robustly predict where the damage is even after supervised training; thus reliable damage detection remains a challenge. Motivated by this we introduce ARTeFACT a dataset for damage detection in diverse types analogue media with over 11000 annotations covering 15 kinds of damage across various subjects media and historical provenance. Furthermore we contribute human-verified text prompts describing the semantic contents of the images and derive additional textual descriptions of the annotated damage. We evaluate CNN Transformer diffusion-based segmentation models and foundation vision models in zero-shot supervised unsupervised and text-guided settings revealing their limitations in generalising across media types. Our dataset is available at https://daniela997.github.io/ARTeFACT/ as the first-of-its kind benchmark for analogue media damage detection and restoration.	https://openaccess.thecvf.com//content/WACV2025/html/Ivanova_ARTeFACT_Benchmarking_Segmentation_Models_on_Diverse_Analogue_Media_Damage_WACV_2025_paper.html	Daniela Ivanova, Marco Aversa, Paul Henderson, John Williamson
Achieving Byzantine-Resilient Federated Learning via Layer-Adaptive Sparsified Model Aggregation	Federated Learning (FL) enables multiple clients to train a model collaboratively without sharing their local data. Yet the FL system is vulnerable to well-designed Byzantine attacks which aim to disrupt the model training process by uploading malicious model updates. Existing robust aggregation rule-based defense methods overlook the diversity of magnitude and direction across different layers of the model updates resulting in limited robustness performance particularly in non-IID settings. To address these challenges we propose the Layer-Adaptive Sparsified Model Aggregation (LASA) approach which combines pre-aggregation sparsification with layer-wise adaptive aggregation to improve robustness. Specifically LASA includes a pre-aggregation sparsification module that sparsifies updates from each client before aggregation reducing the impact of malicious parameters and minimizing the interference from less important parameters for the subsequent filtering process. Based on sparsified updates a layer-wise adaptive filter then adaptively selects benign layers using both magnitude and direction metrics across all clients for aggregation. We provide a detailed theoretical robustness analysis of LASA and a resilience analysis of the FL integrated with LASA. Extensive experiments are conducted on various IID and non-IID datasets. The numerical results demonstrate the effectiveness of LASA. Code is available at https://github.com/JiiahaoXU/LASA.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Achieving_Byzantine-Resilient_Federated_Learning_via_Layer-Adaptive_Sparsified_Model_Aggregation_WACV_2025_paper.html	Jiahao Xu, Zikai Zhang, Rui Hu
ActionDiffusion: An Action-Aware Diffusion Model for Procedure Planning in Instructional Videos	We present ActionDiffusion - a novel diffusion model for procedure planning in instructional videos that is the first to take temporal inter-dependencies between actions into account. Our approach is in stark contrast to existing methods that fail to exploit the rich information content available in the particular order in which actions are performed. Our method unifies the learning of temporal dependencies between actions and denoising of the action plan in the diffusion process by projecting the action information into the noise space. This is achieved 1) by adding action embeddings in the noise masks in the noise-adding phase and 2) by introducing an attention mechanism in the noise prediction network to learn the correlations between different action steps. We report extensive experiments on three instructional video benchmark datasets (CrossTask Coin and NIV) and show that our method outperforms previous state-of-the-art methods on all metrics on CrossTask and NIV and all metrics except accuracy on Coin dataset. We show that by adding action embeddings into the noise mask the diffusion model can better learn action temporal dependencies and increase the performances on procedure planning. Codes are available at https://www.collaborative-ai.org/publications/shi25_wacv/	https://openaccess.thecvf.com//content/WACV2025/html/Shi_ActionDiffusion_An_Action-Aware_Diffusion_Model_for_Procedure_Planning_in_Instructional_WACV_2025_paper.html	Lei Shi, Paul-Christian BÃ¼rkner, Andreas Bulling
Active Event Alignment for Monocular Distance Estimation	Event cameras provide a natural and data efficient representation of visual information motivating novel computational strategies towards extracting visual information. Inspired by the biological vision system we propose a behavior driven approach for object-wise distance estimation from event camera data. This behavior-driven method mimics how biological systems like the human eye stabilize their view based on object distance: distant objects require minimal compensatory rotation to stay in focus while nearby objects demand greater adjustments to maintain alignment. This adaptive strategy leverages natural stabilization behaviors to estimate relative distances effectively. Unlike traditional vision algorithms that estimate depth across the entire image our approach targets local depth estimation within a specific region of interest. By aligning events within a small region we estimate the angular velocity required to stabilize the image motion. We demonstrate that under certain assumptions the compensatory rotational flow is inversely proportional to the object's distance. The proposed approach achieves new state-of-the-art accuracy in distance estimation - a performance gain of 16% on EVIMO2. EVIMO2 event sequences comprise complex camera motion and substantial variance in depth of static real world scenes. Code: https://github.com/pbideau/AAEDepth.	https://openaccess.thecvf.com//content/WACV2025/html/Cai_Active_Event_Alignment_for_Monocular_Distance_Estimation_WACV_2025_paper.html	Nan Cai, Pia Bideau
Active Learning for Image Segmentation with Binary User Feedback	Deep learning algorithms have depicted commendable performance in a variety of computer vision applications. However training a robust deep neural network necessitates a large amount of labeled training data which is time-consuming and labor-intensive to acquire. This problem is even more serious for an application like image segmentation as the human oracle has to hand-annotate each and every pixel in a given training image which is extremely laborious. Active learning algorithms automatically identify the salient and exemplar samples from large amounts of unlabeled data and tremendously reduce human annotation effort in inducing a machine learning model. In this paper we propose a novel active learning algorithm for image segmentation with the goal of further reducing the labeling burden on the human oracles. Our framework identifies a batch of informative images together with a list of semantic classes for each and the human annotator merely needs to answer whether a given semantic class is present or absent in a given image. To the best of our knowledge this is the first research effort to develop an active learning framework for image segmentation which poses only binary (yes/no) queries to the users. We pose the image and class selection as a constrained optimization problem and derive a linear programming relaxation to select a batch of (image-class) pairs which are maximally informative to the underlying deep neural network. Our extensive empirical studies on three challenging datasets corroborate the potential of our method in substantially reducing human annotation effort for real-world image segmentation applications.	https://openaccess.thecvf.com//content/WACV2025/html/Goswami_Active_Learning_for_Image_Segmentation_with_Binary_User_Feedback_WACV_2025_paper.html	Debanjan Goswami, Shayok Chakraborty
Active Learning for Vision Language Models	Pre-trained vision-language models (VLMs) like CLIP have demonstrated impressive zero-shot performance on a wide range of downstream computer vision tasks. However there still exists a considerable performance gap between these models and a supervised deep model trained on a downstream dataset. To bridge this gap we propose a novel active learning (AL) framework that enhances the zero-shot classification performance of VLMs by selecting only a few informative samples from the unlabeled data for annotation during training. To achieve this our approach first calibrates the predicted entropy of VLMs and then utilizes a combination of self-uncertainty and neighbor-aware uncertainty to calculate a reliable uncertainty measure for active sample selection. Our extensive experiments show that the proposed approach outperforms existing AL approaches on several image classification datasets and significantly enhances the zero-shot performance of VLMs.	https://openaccess.thecvf.com//content/WACV2025/html/Safaei_Active_Learning_for_Vision_Language_Models_WACV_2025_paper.html	Bardia Safaei, Vishal M. Patel
Active Learning with Context Sampling and One-vs-Rest Entropy for Semantic Segmentation	Multi-class semantic segmentation remains a cornerstone challenge in computer vision. Yet dataset creation remains excessively demanding in time and effort especially for specialized domains. Active Learning (AL) mitigates this challenge by selecting data points for annotation strategically. However existing patch-based AL methods often overlook boundary pixels' critical information essential for accurate segmentation. We present OREAL a novel patch-based AL method designed for multi-class semantic segmentation. OREAL enhances boundary detection by employing maximum aggregation of pixel-wise uncertainty scores. Additionally we introduce one-vs-rest entropy a novel uncertainty score function that computes class-wise uncertainties while achieving implicit class balancing during dataset creation. Comprehensive experiments across diverse datasets and model architectures validate our hypothesis.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_Active_Learning_with_Context_Sampling_and_One-vs-Rest_Entropy_for_Semantic_WACV_2025_paper.html	Fei Wu, Pablo MÃ¡rquez Neila, Hedyeh Rafii-Tari, Raphael Sznitman
AdQuestA: Knowledge-Guided Visual Question Answer Framework for Advertisements	In the rapidly evolving landscape of digital marketing effective customer engagement through advertisements is crucial for brands. Thus computational understanding of ads is pivotal for recommendation authoring and customer behaviour simulation. Despite advancements in knowledge-guided visual-question-answering (VQA) models existing frameworks often lack domain-specific responses and suffer from a dearth of benchmark datasets for advertisements. To address this gap we introduce ADVQA the first dataset for ad-related VQA sourced from Facebook and X (twitter) which facilitates further research in ad comprehension. It comprises open-ended questions and detailed context obtained automatically from web articles. Moreover we present AdQuestA a novel multimodal framework for knowledge-guided open-ended question-answering tailored to advertisements. AdQuestA leverages a Retrieval Augmented Generation (RAG) to obtain question-aware ad context as explicit knowledge and image-grounded implicit knowledge effectively exploiting inherent relationships for reasoning. Extensive experiments corroborate its efficacy yielding state-of-the-art performance on the ADVQA dataset even surpassing 10X larger models such as GPT-4 on this task. Our framework not only enhances understanding of ad content but also advances the broader landscape of knowledge-guided VQA models.	https://openaccess.thecvf.com//content/WACV2025/html/Choudhary_AdQuestA_Knowledge-Guided_Visual_Question_Answer_Framework_for_Advertisements_WACV_2025_paper.html	Neha Choudhary, Poonam Goyal, Devashish Siwatch, Atharva Chandak, Harsh Mahajan, Varun Khurana, Yaman Kumar
Ad^2mix: Adversarial and Adaptive Mixup for Unsupervised Domain Adaptation	Transformer has recently gained tremendous popularity in unsupervised domain adaptation tasks due to its superior generalization ability. State-of-the-art methods leverage mixup to build an intermediate domain to reduce domain gap. However such strategy becomes less effective when the domain gap becomes large as the domain gap between intermediate domain and source domain is not minimized and the constructed intermediate domain is non-informative. How to address the adaptation problem when domain gap becomes large is an important research problem in domain adaptation. In this paper we propose an adversarial and adaptive mixup (Ad^2mix) framework which gradually aligns the intermediate domain towards source domain to fully unleash the potential of both the transformer architecture and mixup to address the large domain gap problem. Specifically we formulate a general framework for intermediate domain learning with mixup. We propose adversarial mixup with a specially designed mixup alike adversarial adaptation operation to reduce the domain gap between the intermediate domain and source domain. To construct an informative intermediate domain unlike existing methods which utilize a Beta distribution to generate mixup coefficients to interpolate source and target data we adaptively assign mixup coefficient for each target data instance based on their transferability and discriminativity information. Our framework creates a natural curriculum of intermediate domains from near source domain to near target domain for gradual adaptation. Extensive experimental studies and evaluations on three public domain adaptation benchmark datasets and one medical domain adaptation task demonstrate the superiority of our framework.	https://openaccess.thecvf.com//content/WACV2025/html/Zhu_Ad2mix_Adversarial_and_Adaptive_Mixup_for_Unsupervised_Domain_Adaptation_WACV_2025_paper.html	Lei Zhu, Yanyu Xu, Yong Liu, Rick Siow Mong Goh, Xinxing Xu
Ada-VE: Training-Free Consistent Video Editing using Adaptive Motion Prior	Video-to-video synthesis poses significant challenges in maintaining character consistency smooth temporal transitions and preserving visual quality during fast motion. While recent fully cross-frame self-attention mechanisms have improved character consistency across multiple frames they come with high computational costs and often include redundant operations especially for videos with higher frame rates. To address these inefficiencies we propose an adaptive motion-guided cross-frame attention mechanism that selectively reduces redundant computations. This enables a greater number of cross-frame attentions over more frames within the same computational budget thereby enhancing both video quality and temporal coherence. Our method leverages optical flow to focus on moving regions while sparsely attending to stationary areas allowing for the joint editing of more frames without increasing computational demands. Traditional frame interpolation techniques struggle with motion blur and flickering in intermediate frames which compromises visual fidelity. To mitigate this we introduce KV-caching for jointly edited frames reusing keys and values across intermediate frames to preserve visual quality and maintain temporal consistency throughout the video. With our adaptive cross-frame self-attention approach we achieve a threefold increase in the number of keyframes processed compared to existing methods all within the same computational budget as fully cross-frame attention baselines. This results in significant improvements in prediction accuracy and temporal consistency outperforming state-of-the-art approaches. Code is made publicly available at https://github.com/tanvir-utexas/AdaVE/tree/main.	https://openaccess.thecvf.com//content/WACV2025/html/Mahmud_Ada-VE_Training-Free_Consistent_Video_Editing_using_Adaptive_Motion_Prior_WACV_2025_paper.html	Tanvir Mahmud, Mustafa Munir, Radu Marculescu, Diana Marculescu
AdaPrefix++: Integrating Adapters Prefixes and Hypernetwork for Continual Learning	Continual learning allows systems to continuously learn and adapt to the tasks in an evolving real-world environment without forgetting previous tasks. Developing deep learning models that can continually learn over a sequence of tasks is challenging. We propose a novel method AdaPrefix which addresses this and empowers continual learning capability in pretrained large models (PLMs). AdaPrefix provide a continual learning method for transformer-based deep learning models by appropriately integrating the parameter-efficient methods adapters and prefixes. AdaPrefix is an effective approach for smaller PLMs and achieves better results than state-of-the-art approaches. We further improve upon AdaPrefix by proposing AdaPrefix++ enabling knowledge transfer across the tasks. It leverages hypernetworks to generate prefixes and continually learns the hypernetwork parameters to facilitate knowledge transfer. AdaPrefix++ has a smaller parameter growth compared to AdaPrefix and is more effective and valuable for continual learning in PLMs. We performed several experiments on various benchmark datasets to demonstrate the performance of our approach for different PLMs and continual learning scenarios.	https://openaccess.thecvf.com//content/WACV2025/html/Adhikari_AdaPrefix_Integrating_Adapters_Prefixes_and_Hypernetwork_for_Continual_Learning_WACV_2025_paper.html	Sayanta Adhikari, Dupati Srikar Chandra, P. K. Srijith, Pankaj Wasnik, Naoyuki Oneo
Adaptive Deviation Learning for Visual Anomaly Detection with Data Contamination	Visual anomaly detection targets to detect images that notably differ from normal pattern and it has found extensive application in identifying defective parts within the manufacturing industry. These anomaly detection paradigms predominantly focus on training detection models using only clean unlabeled normal samples assuming an absence of contamination; a condition often unmet in real-world scenarios. The performance of these methods significantly depends on the quality of the data and usually decreases when exposed to noise. We introduce a systematic adaptive method that employs deviation learning to compute anomaly scores end-to-end while addressing data contamination by assigning relative importance to the weights of individual instances. In this approach the anomaly scores for normal instances are designed to approximate scalar scores obtained from the known prior distribution. Meanwhile anomaly scores for anomaly examples are adjusted to exhibit statistically significant deviations from these reference scores. Our approach incorporates a constrained optimization problem within the deviation learning framework to update instance weights resolving this problem for each mini-batch. Comprehensive experiments on the MVTec and VisA benchmark datasets indicate that our proposed method surpasses competing techniques and exhibits both stability and robustness in the presence of data contamination.	https://openaccess.thecvf.com//content/WACV2025/html/Das_Adaptive_Deviation_Learning_for_Visual_Anomaly_Detection_with_Data_Contamination_WACV_2025_paper.html	Anindya Sundar Das, Guansong Pang, Monowar Bhuyan
Adaptive and Temporally Consistent Gaussian Surfels for Multi-View Dynamic Reconstruction	3D Gaussian Splatting has recently achieved notable success in novel view synthesis for dynamic scenes and geometry reconstruction in static scenes. Building on these advancements early methods have been developed for dynamic surface reconstruction by globally optimizing entire sequences. However reconstructing dynamic scenes with significant topology changes emerging or disappearing objects and rapid movements remains a substantial challenge particularly for long sequences. To address these issues we propose AT-GS a novel method for reconstructing high-quality dynamic surfaces from multi-view videos through per-frame incremental optimization. To avoid local minima across frames we introduce a unified and adaptive gradient-aware densification strategy that integrates the strengths of conventional cloning and splitting techniques. Additionally we reduce temporal jittering in dynamic surfaces by ensuring consistency in curvature maps across consecutive frames. Our method achieves superior accuracy and temporal coherence in dynamic surface reconstruction delivering high-fidelity space-time novel view synthesis even in complex and challenging scenes. Extensive experiments on diverse multi-view video datasets demonstrate the effectiveness of our approach showing clear advantages over baseline methods.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_Adaptive_and_Temporally_Consistent_Gaussian_Surfels_for_Multi-View_Dynamic_Reconstruction_WACV_2025_paper.html	Decai Chen, Brianne Oberson, Ingo Feldmann, Oliver Schreer, Anna Hilsmann, Peter Eisert
Advancing Chart Question Answering with Robust Chart Component Recognition	Chart comprehension presents significant challenges for machine learning models due to the diverse and intricate shapes of charts. Existing multimodal methods often overlook these visual features or fail to integrate them effectively for Chart Question Answering. To address this we introduce ChartFormer a unified framework that enhances chart component recognition by accurately identifying and classifying components such as bars lines pies titles legends and axes. Additionally we propose a novel Question-guided Deformable Co-Attention (QDCAt) mechanism which fuses chart features encoded by ChartFormer with the given question leveraging the question's guidance to ground the correct answer. Extensive experiments demonstrate a 3.2% improvement in mAP over the baselines for chart component recognition. For ChartQA and OpenCQA tasks our approach achieves improvements of 15.4% in accuracy and 0.8 in BLEU score respectively underscoring the robustness of our solution for detailed visual data interpretation across various applications. The source code and dataset are publicly available at https://github.com/VT-NLP/chartQA.	https://openaccess.thecvf.com//content/WACV2025/html/Zheng_Advancing_Chart_Question_Answering_with_Robust_Chart_Component_Recognition_WACV_2025_paper.html	Hanwen Zheng, Sijia Wang, Chris Thomas, Lifu Huang
Advancing Weight and Channel Sparsification with Enhanced Saliency	"Pruning aims to accelerate and compress models by removing redundant parameters identified by specifically designed importance scores which are usually imperfect. This removal is irreversible often leading to subpar performance in pruned models. Dynamic sparse training while attempting to adjust sparse structures during training for continual reassessment and refinement has several limitations including criterion inconsistency between pruning and growth unsuitability for structured sparsity and short-sighted growth strategies. Our paper introduces an efficient innovative paradigm to enhance a given importance criterion for either unstructured or structured sparsity. Our method separates the model into an active structure for exploitation and an exploration space for potential updates. During exploitation we optimize the active structure whereas in exploration we reevaluate and reintegrate parameters from the exploration space through a pruning and growing step consistently guided by the same given importance criterion. To prepare for exploration we briefly ""reactivate"" all parameters in the exploration space and train them for a few iterations while keeping the active part frozen offering a preview of the potential performance gains from reintegrating these parameters. We show on various datasets and configurations that existing importance criterion even simple as magnitude can be enhanced with ours to achieve state-of-the-art performance and training cost reductions. Notably on ImageNet with ResNet50 ours achieves an +1.3 increase in Top-1 accuracy over prior art at 90% ERKsparsity. Compared with the SOTA latency pruning method HALP we reduced its training cost by over 70% while attaining a faster and more accurate pruned model."	https://openaccess.thecvf.com//content/WACV2025/html/Sun_Advancing_Weight_and_Channel_Sparsification_with_Enhanced_Saliency_WACV_2025_paper.html	Xinglong Sun, Maying Shen, Hongxu Yin, Lei Mao, Pavlo Molchanov, Jose M. Alvarez
Adversarial Attention Deficit: Fooling Deformable Vision Transformers with Collaborative Adversarial Patches	Deformable vision transformers reduce the expensive quadratic time-complexity of attention modeling by using sparse attention structures making it possible to use transformers in large-scale vision applications such as multi-view vision systems. We show that existing adversarial attacks against conventional vision transformers do not transfer to deformable transformers primarily due to the data-dependent dynamic nature of sparse attention. In this work we present for the first time adversarial attacks against deformable vision transformers by getting control of their attention-inferring module. We develop a novel collaborative attack where a source patch manipulates attention to point to a target patch containing the adversarial noise which fools the model. We observe that our attack alters less than 1% of the patched area in the input field completely disrupting object detection and resulting in 0% AP in single-view object detection using MS COCO and 0% MODA in multi-view object detection using Wildtrack.	https://openaccess.thecvf.com//content/WACV2025/html/Alam_Adversarial_Attention_Deficit_Fooling_Deformable_Vision_Transformers_with_Collaborative_Adversarial_WACV_2025_paper.html	Quazi Mishkatul Alam, Bilel Tarchoun, Ihsen Alouani, Nael Abu-Ghazaleh
Adversarial Learning Based Knowledge Distillation on 3D Point Clouds	The significant improvements in point cloud representation learning have increased its applicability in many real-life applications resulting in the need for lightweight better-performing models. One widely proposed efficient method is knowledge distillation where a lightweight model uses knowledge from large models. Very few works exist on distilling the knowledge for point clouds. Most of the work focuses on cross-modal-based approaches that make the method expensive to train. This paper proposes PointKAD an adversarial knowledge distillation framework for point cloud-based tasks. PointKAD includes adversarial feature distillation and response distillation with the help of discriminators to extract and distill the representation of feature maps and logits. We conduct extensive experimental studies on both synthetic (ModelNet40) and real (ScanObjectNN) datasets to show that PointKAD achieves state-of-the-art results compared to the existing knowledge distillation methods for point cloud classification. Additionally we present results on the part segmentation task highlighting the efficacy of the PointKAD framework. Our experiments further reveal that PointKAD is capable of transferring knowledge across different tasks and datasets showcasing its versatility. Furthermore we demonstrate that PointKAD can be applied to a cross-modal training setup achieving competitive performance with cross-modal-based point cloud methods for classification.	https://openaccess.thecvf.com//content/WACV2025/html/J_Adversarial_Learning_Based_Knowledge_Distillation_on_3D_Point_Clouds_WACV_2025_paper.html	Sanjay S J, Akash J, Sreehari Rajan, Dimple A Shajahan, Charu Sharma
Aerial Mirage: Unmasking Hallucinations in Large Vision Language Models	Drones excel at capturing aerial-view images especially in human unreachable areas. Automatically interpreting and describing these images enables decision-making easier without the need to review the images extensively. Traditional image captioning models struggle with aerial imagery due to diverse orientations perspectives and unclear objects. Integrating the capabilities of Large Vision Language Models (LVLMs) with drone images can improve description utility benefiting strategic missions like surveillance search and rescue etc. However the lack of image-caption datasets for drone imagery poses a significant challenge for training and evaluating drone image captioning. To address this gap we contribute the first Aerial-view Image Captioning dataset (AeroCaps) containing four captions per image. Another major hurdle for the task is the hallucinatory nature of LVLMs. To this end we perform the first extensive analysis of hallucinations on aerial imagery by two SOTA LVLMs - LLaVA and InstructBLIP on our proposed dataset and VisDrone. We explore the reasons behind such hallucinations. We release the LVLM-generated image captions along with our hallucination-labelled annotations as the Labelled Illusion Dataset (LID) for further research. Additionally we review how effective advanced LLMs like GPT-4 are in evaluating the degree of hallucinations made by other LVLMs like LLaVA.	https://openaccess.thecvf.com//content/WACV2025/html/Basak_Aerial_Mirage_Unmasking_Hallucinations_in_Large_Vision_Language_Models_WACV_2025_paper.html	Debolena Basak, Soham Bhatt, Sahith Kanduri, Maunendra Sankar Desarkar
Aggregated Attributions for Explanatory Analysis of 3D Segmentation Models	Analysis of 3D segmentation models especially in the context of medical imaging is often limited to segmentation performance metrics that overlook the crucial aspect of explainability and bias. Currently effectively explaining these models with saliency maps is challenging due to the high dimensions of input images multiplied by the ever-growing number of segmented class labels. To this end we introduce Agg^2Exp a methodology for aggregating fine-grained voxel attributions of the segmentation model's predictions. Unlike classical explanation methods that primarily focus on the local feature attribution Agg^2Exp enables a more comprehensive global view on the importance of predicted segments in 3D images. Our benchmarking experiments show that gradient-based voxel attributions are more faithful to the model's predictions than perturbation-based explanations. As a concrete use-case we apply Agg^2Exp to discover knowledge acquired by the Swin UNEt TRansformer model trained on the TotalSegmentator v2 dataset for segmenting anatomical structures in computed tomography medical images. Agg^2Exp facilitates the explanatory analysis of large segmentation models beyond their predictive performance. The source code is publicly available at https://github.com/MI2DataLab/agg2exp.	https://openaccess.thecvf.com//content/WACV2025/html/Chrabaszcz_Aggregated_Attributions_for_Explanatory_Analysis_of_3D_Segmentation_Models_WACV_2025_paper.html	Maciej Chrabaszcz, Hubert Baniecki, Piotr Komorowski, Szymon Plotka, Przemyslaw Biecek
AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning	Significant progress has been made in advancing large multimodal conversational models (LMMs) capitalizing on vast repositories of image-text data available online. Despite this progress these models often encounter substantial domain gaps hindering their ability to engage in complex conversations across new domains. Recent efforts have aimed to mitigate this issue albeit relying on domain-specific image-text data to curate instruction-tuning data. However many domains such as agriculture lack such vision-language data. In this work we propose an approach to construct instruction-tuning data that harnesses vision-only data for the agriculture domain. We utilize diverse agricultural datasets spanning multiple domains curate class-specific information and employ large language models (LLMs) to construct an expert-tuning set resulting in a 70k AgroInstruct. Subsequently we expert-tuned and created AgroGPT an efficient LMM that can hold complex agriculture-related conversations and provide useful insights. We also develop AgroEvals for evaluation and compare AgroGPT's performance with large open and closed-source models. AgroGPT excels at identifying fine-grained agricultural concepts can act as an agriculture expert and provides helpful information for multimodal agriculture questions. The code datasets and models are available at https://github.com/awaisrauf/agroGPT.	https://openaccess.thecvf.com//content/WACV2025/html/Awais_AgroGPT_Efficient_Agricultural_Vision-Language_Model_with_Expert_Tuning_WACV_2025_paper.html	Muhammad Awais, Ali Husain Salem Abdulla Alharthi, Amandeep Kumar, Hisham Cholakkal, Rao Muhammad Anwer
Agtech Framework for Cranberry-Ripening Analysis using Vision Foundation Models	Agricultural domains are being transformed by recent advances in AI and computer vision that support quantitative visual evaluation. Using aerial and ground imaging over a time series we develop a framework for characterizing the ripening process of cranberry crops a crucial component for precision agriculture tasks such as comparing crop breeds (high-throughput phenotyping) and detecting disease. Using drone imaging we capture images from 20 waypoints across multiple bogs and using ground-based imaging (hand-held camera) we image same bog patch using fixed fiducial markers. Both imaging methods are repeated to gather a multi-week time series spanning the entire growing season. Aerial imaging provides multiple samples to compute a distribution of albedo values. Ground imaging enables tracking of individual berries for a detailed view of berry appearance changes. Using vision transformers (ViT) for feature detection after segmentation we extract a high dimensional feature descriptor of berry appearance. Interpretability of appearance is critical for plant biologists and cranberry growers to support crop breeding decisions (e.g. comparison of berry varieties from breeding programs). For interpretability we create a 2D manifold of cranberry appearance by using a UMAP dimensionality reduction on ViT features. This projection enables quantification of ripening paths and a useful metric of ripening rate. We demonstrate the comparison of four cranberry varieties based on our ripening assessments. This work is the first of its kind and has future impact for cranberries and for other crops including wine grapes olives blueberries and maize. Aerial and ground datasets are made publicly available.	https://openaccess.thecvf.com//content/WACV2025/html/Johnson_Agtech_Framework_for_Cranberry-Ripening_Analysis_using_Vision_Foundation_Models_WACV_2025_paper.html	Faith Johnson, Ryan Meegan, Jack Lowry, Peter Oudemans, Kristin Dana
AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image Models	We consider the problem of customizing text-to-image diffusion models with user-supplied reference images. Given new prompts the existing methods can capture the key concept from the reference images but fail to align the generated image with the prompt. In this work we seek to address this key issue by proposing new methods that can easily be used in conjunction with existing customization methods that optimize the embeddings and weights at various intermediate stages of the text encoding process before being fed into the noise prediction model of a text-to-image diffusion model. The first contribution of this paper is a dissection of the various stages of the text encoding process leading up to the conditioning vector for text-to-image models. We take a holistic view of existing customization methods and notice that key and value outputs from this process differs substantially from their corresponding baseline (non-customized) models. While this difference does not impact the concept being customized it leads to other parts of the generated image not being aligned with the prompt. Further we also observe that these keys and values allow independent control various aspects of the final generation enabling semantic manipulation of the output. Taken together the features spanning these keys and values serve as the basis for our next contribution where we fix the aforementioned issues with existing methods. We propose a new post-processing algorithm AlignIT that infuses the keys and values for the concept of interest while ensuring the keys and values for all other tokens in the input prompt are unchanged. Our proposed method can be plugged in directly to existing customization methods leading to a substantial performance improvement in the alignment of the final result with the input prompt while retaining the customization quality. We conduct extensive experiments across various different customization methods and a wide variety of reference images and show consistent improvements both qualitatively and quantitatively.	https://openaccess.thecvf.com//content/WACV2025/html/Agarwal_AlignIT_Enhancing_Prompt_Alignment_in_Customization_of_Text-to-Image_Models_WACV_2025_paper.html	Aishwarya Agarwal, Srikrishna Karanam, Balaji Vasan Srinivasan
All-in-One Image Compression and Restoration	Visual images corrupted by various types and levels of degradations are commonly encountered in practical image compression. However most existing image compression methods are tailored for clean images therefore struggling to achieve satisfying results on these images. Joint compression and restoration methods typically focus on a single type of degradation and fail to address a variety of degradations in practice. To this end we propose a unified framework for all-in-one image compression and restoration which incorporates the image restoration capability against various degradations into the process of image compression. The key challenges involve distinguishing authentic image content from degradations and flexibly eliminating various degradations without prior knowledge. Specifically the proposed framework approaches these challenges from two perspectives: ie content information aggregation and degradation representation aggregation. Extensive experiments demonstrate the following merits of our model: 1) superior rate-distortion (RD) performance on various degraded inputs while preserving the performance on clean data; 2) strong generalization ability to real-world and unseen scenarios; 3) higher computing efficiency over compared methods. Our code is available at https://github.com/ZeldaM1/All-in-one.	https://openaccess.thecvf.com//content/WACV2025/html/Zeng_All-in-One_Image_Compression_and_Restoration_WACV_2025_paper.html	Huimin Zeng, Jiacheng Li, Ziqiang Zheng, Zhiwei Xiong
An Encoder-Agnostic Weakly Supervised Method for Describing Textures	Recent advances in Large Language Models (LLMs) have enabled the semantic description of textures in natural language aiming to capture them in richer detail. However most methods are confined to either depending on supervised training with pairs of images and manually annotated visual attributes that most texture datasets lack or using Vision-Language Models (VLMs) such as CLIP. In this paper we develop an encoder-agnostic Weakly supervised Texture Description Generator (WTDG) that employs a novel Scaled Ranked Kullback-Leibler divergence (SR-KL) loss between image and text modalities. Within the SR-KL loss formulation we leverage category information which is always available as ground-truths for all benchmark texture recognition datasets. We further extend our proposed WTDG to assist in texture recognition by using its generated texture descriptions. Thus we develop a multimodal framework called Tex^2 which is adept at simultaneous generation of texture description and recognition. Our approach exhibits promising performance in describing and recognizing textures on benchmark datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Mao_An_Encoder-Agnostic_Weakly_Supervised_Method_for_Describing_Textures_WACV_2025_paper.html	Shangbo Mao, Deepu Rajan
An Image is Worth Multiple Words: Multi-Attribute Inversion for Constrained Text-to-Image Synthesis	We consider the problem of constraining diffusion model outputs with a user-supplied reference image. Our key objective is to extract multiple attributes (e.g. color object layout style) from this single reference image and then generate new samples and novel compositions with them. We first perform an extensive attribute distribution analysis that leads to the discovery of an extended conditioning space consisting of multiple textual conditions. These textual conditions vary both per-layer of the U-Net as well as per-timestep of the denoising process. We observe that although the extended conditioning space provides greater control over different attributes of the generated image often a subset of these attributes are captured in the same set of U-Net layers and/or across same denoising timesteps. For instance color and style are captured across same U-Net layers whereas layout and color are captured across same timestep stages. Existing works in multi-attribute constrained image generation proposed extending textual inversion by learning per-layer or per-timestep tokens but they suffer from attribute entanglement for the reasons described above. To address the aforementioned gap we introduce our second contribution where we design a new multi-attribute textual inversion algorithm MATTE with associated disentanglement-enhancing regularization losses that operates jointly across both layer and timestep dimensions and explicitly leads to four disentangled tokens (color style layout and object). We conduct extensive qualitative and quantitative evaluations to demonstrate the effectiveness of the proposed approach.	https://openaccess.thecvf.com//content/WACV2025/html/Agarwal_An_Image_is_Worth_Multiple_Words_Multi-Attribute_Inversion_for_Constrained_WACV_2025_paper.html	Aishwarya Agarwal, Srikrishna Karanam, Tripti Shukla, Balaji Vasan Srinivasan
An Investigation on LLMs' Visual Understanding Ability using SVG for Image-Text Bridging	Large language models (LLMs) have made significant advancements in natural language understanding. However through that enormous semantic representation that the LLM has learnt is it somehow possible for it to understand images as well? This work investigates this question. To enable the LLM to process images we convert them into a representation given by Scalable Vector Graphics (SVG). To study what the LLM can do with this XML-based textual description of images we test the LLM on three broad computer vision tasks: (i) visual reasoning and question answering (ii) image classification under distribution shift few-shot learning and (iii) generating new images using visual prompting. Even though we do not naturally associate LLMs with any visual understanding capabilities our results indicate that the LLM can often do a decent job in many of these tasks potentially opening new avenues for research into LLMs' ability to understand image data. Our code data and models can be found here https://github.com/mu-cai/svg-llm.	https://openaccess.thecvf.com//content/WACV2025/html/Cai_An_Investigation_on_LLMs_Visual_Understanding_Ability_using_SVG_for_WACV_2025_paper.html	Mu Cai, Zeyi Huang, Yuheng Li, Utkarsh Ojha, Haohan Wang, Yong Jae Lee
Analyzing and Improving the Skin Tone Consistency and Bias in Implicit 3D Relightable Face Generators	With the advances in generative adversarial networks (GANs) and neural rendering 3D relightable face generation has received significant attention. Among the existing methods a particularly successful technique uses an implicit lighting representation and generates relit images through the product of synthesized albedo and light-dependent shading images. While this approach produces high-quality results with intricate shading details it often has difficulty producing relit images with consistent skin tones particularly when the lighting condition is extracted from images of individuals with dark skin. Additionally this technique is biased towards producing albedo images with lighter skin tones. Our main observation is that this problem is rooted in the biased spherical harmonics (SH) coefficients used during training. Following this observation we conduct an analysis and demonstrate that the bias appears not only in band 0 (DC term) but also in the other bands of the estimated SH coefficients. We then propose a simple but effective strategy to mitigate the problem. Specifically we normalize the SH coefficients by their DC term to eliminate the inherent magnitude bias while statistically align the coefficients in the other bands to alleviate the directional bias. We also propose a scaling strategy to match the distribution of illumination magnitude in the generated images with the training data. Through extensive experiments we demonstrate the effectiveness of our solution in increasing the skin tone consistency and mitigating bias.	https://openaccess.thecvf.com//content/WACV2025/html/Zeng_Analyzing_and_Improving_the_Skin_Tone_Consistency_and_Bias_in_WACV_2025_paper.html	Libing Zeng, Nima Khademi Kalantari
Anchored Diffusion for Video Face Reenactment	Video generation has drawn significant interest recently pushing the development of large-scale models capable of producing realistic videos with coherent motion. Due to memory constraints these models typically generate short video segments that are then combined into long videos. The merging process poses a significant challenge as it requires ensuring smooth transitions and overall consistency. In this paper we introduce Anchored Diffusion a novel method for synthesizing relatively long and seamless videos. We extend Diffusion Transformers (DiTs) to incorporate temporal information creating our sequence-DiT (sDiT) model for generating short video segments. Unlike previous works we train our model on video sequences with random non-uniform temporal spacing and incorporate temporal information via external guidance increasing flexibility and allowing it to capture both short and long-term relationships. Furthermore during inference we leverage the transformer architecture to modify the diffusion process generating a batch of non-uniform sequences anchored to a common frame ensuring consistency regardless of temporal distance. To demonstrate our method we focus on face reenactment a task of transforming the action from the driving video to the source face. Through comprehensive experiments we show our approach outperforms current techniques in producing longer consistent high-quality videos while offering editing capabilities.	https://openaccess.thecvf.com//content/WACV2025/html/Kligvasser_Anchored_Diffusion_for_Video_Face_Reenactment_WACV_2025_paper.html	Idan Kligvasser, Regev Cohen, George Leifman, Ehud Rivlin, Michael Elad
Anomaly Detection for People with Visual Impairments using an Egocentric 360-Degree Camera	Recent advancements in computer vision have led to a renewed interest in developing assistive technologies for individuals with visual impairments. Although extensive research has been conducted in the field of computer vision-based assistive technologies most of the focus has been on understanding contexts in images rather than addressing their physical safety and security concerns. To address this challenge we propose the first step towards detecting anomalous situations for visually impaired people by observing their entire surroundings using an egocentric 360-degree camera. We first introduce a novel egocentric 360-degree video dataset called VIEW360 (Visually Impaired Equipped with Wearable 360-degree camera) which contains abnormal activities that visually impaired individuals may encounter such as shoulder surfing and pickpocketing. Furthermore we propose a new architecture called the FDPN (Frame and Direction Prediction Network) which facilitates frame-level prediction of abnormal events and identifying of their directions. Finally we evaluate our approach on our VIEW360 dataset and the publicly available UCF-Crime and Shanghaitech datasets demonstrating state-of-the-art performance.	https://openaccess.thecvf.com//content/WACV2025/html/Song_Anomaly_Detection_for_People_with_Visual_Impairments_using_an_Egocentric_WACV_2025_paper.html	Inpyo Song, Sanghyeon Lee, Minjun Joo, Jangwon Lee
AnomalyDINO: Boosting Patch-Based Few-Shot Anomaly Detection with DINOv2	Recent advances in multimodal foundation models have set new standards in few-shot anomaly detection. This paper explores whether high-quality visual features alone are sufficient to rival existing state-of-the-art vision-language models. We affirm this by adapting DINOv2 for one-shot and few-shot anomaly detection with a focus on industrial applications. We show that this approach does not only rival existing techniques but can even outmatch them in many settings. Our proposed vision-only approach AnomalyDINO follows the well-established patch-level deep nearest neighbor paradigm and enables both image-level anomaly prediction and pixel-level anomaly segmentation. The approach is methodologically simple and training-free and thus does not require any additional data for fine-tuning or meta-learning. Despite its simplicity AnomalyDINO achieves state-of-the-art results in one- and few-shot anomaly detection (e.g. pushing the one-shot performance on MVTec-AD from an AUROC of 93.1% to 96.6%). The reduced overhead coupled with its outstanding few-shot performance makes AnomalyDINO a strong candidate for fast deployment e.g. in industrial contexts.	https://openaccess.thecvf.com//content/WACV2025/html/Damm_AnomalyDINO_Boosting_Patch-Based_Few-Shot_Anomaly_Detection_with_DINOv2_WACV_2025_paper.html	Simon Damm, Mike Laszkiewicz, Johannes Lederer, Asja Fischer
AnonyNoise: Anonymizing Event Data with Smart Noise to Outsmart Re-Identification and Preserve Privacy	The increasing capabilities of deep neural networks for re-identification combined with the rise in public surveillance in recent years pose a substantial threat to individual privacy. Event cameras were initially considered as a promising solution since their output is sparse and therefore difficult for humans to interpret. However recent advances in deep learning proof that neural networks are able to reconstruct high-quality grayscale images and re-identify individuals using data from event cameras. In our paper we contribute a crucial ethical discussion on data privacy and present the first event anonymization pipeline to prevent re-identification not only by humans but also by neural networks. Our method effectively introduces learnable data-dependent noise to cover personally identifiable information in raw event data reducing attackers' re-identification capabilities by up to 60% while maintaining substantial information for the performing of downstream tasks. Moreover our anonymization generalizes well on unseen data and is robust against image reconstruction and inversion attacks. Code: https://github.com/dfki-av/AnonyNoise	https://openaccess.thecvf.com//content/WACV2025/html/Bendig_AnonyNoise_Anonymizing_Event_Data_with_Smart_Noise_to_Outsmart_Re-Identification_WACV_2025_paper.html	Katharina Bendig, RenÃ© Schuster, Nicole Thiemer, Karen Joisten, Didier Stricker
Are Exemplar-Based Class Incremental Learning Models Victim of Black-Box Poison Attacks?	Class Incremental Learning (CIL) models are designed to continuously learn new classes without forgetting previously learned ones often relying on an exemplar set to retain a portion of knowledge from previously learned classes. However their vulnerability to adversarial attacks under novel and unexplored conditions remains unstudied. In this work we are the first to evaluate the robustness of exemplar-based CIL models using a non-overlapping dataset where the dataset is independent of the training and test sets of the target model. We propose and implement a novel black-box attack framework targeting the exemplar set of class incremental learning models using zero-overlapping data. Specifically we focus on scenarios where the target model provides only hard-label predictions without interactive access. Our experimental evaluation covers a range of exemplar-based incremental learning algorithms different surrogate models and black-box attack options. Our findings reveal significant vulnerabilities in exemplar-based CIL models to poisoning-based attacks using a non-overlapping dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Perla_Are_Exemplar-Based_Class_Incremental_Learning_Models_Victim_of_Black-Box_Poison_WACV_2025_paper.html	Neeresh Kumar Perla, Md. Iqbal Hossain, Afia Sajeeda, Ming Shao
Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance	Neural Networks (NNs) have surpassed human accuracy in image classification on ImageNet yet they often lack robustness against image corruption i.e. corruption robustness with such robustness being seemingly effortless for human perception. In this paper we propose visually-continuous corruption robustness (VCR) - an extension of corruption robustness to allow assessing it over the wide and continuous range of changes that correspond to the human perceptive quality (i.e. from the original image to the full distortion of all perceived visual information) along with two novel human-aware metrics for NN evaluation. To compare VCR of NNs with human perception we conducted extensive experiments on 14 commonly used image corruptions with 7718 human participants and state-of-the-art robust NN models with different training objectives (e.g. standard adversarial corruption robustness) different architectures (e.g. convolution NNs vision transformers) and different amounts of training data augmentation. Our study showed that: 1) assessing robustness against continuous corruption can reveal insufficient robustness undetected by existing benchmarks; as a result 2) the gap between NN and human robustness is larger than previously known; and finally 3) some image corruptions have a similar impact on human perception offering opportunities for more cost-effective robustness assessments.	https://openaccess.thecvf.com//content/WACV2025/html/Shen_Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks_Relative_to_Human_WACV_2025_paper.html	Huakun Shen, Boyue Hu, Krzysztof Czarnecki, Lina Marsso, Marsha Chechik
Assessing the Quality of 3D Reconstruction in the Absence of Ground Truth: Application to a Multimodal Archaeological Dataset	This paper proposes a new dataset of archaeological artefacts for evaluating 3D reconstruction methods and questions the notion of ground truth. Indeed 3D reconstruction of archaeological objects can be carried out using either scanners or photographic methods. It turns out that multi-view stereo (MVS) faithfully reconstructs the overall shape of an object on a par with a hand-held scanner while calibrated photometric stereo (CPS) reveals relief details. The restitution of low- and high-frequencies is therefore the prerogative of distinct methods which indicates that the ground truth and the metric used for evaluation should be chosen in view of the target frequencies. This observation led us to combine MVS and CPS using MVS to calibrate the illumination used by CPS. We demonstrate on our dataset of archaeological objects that this original 3D reconstruction method indeed combines the advantages of MVS and CPS. Our proposed dataset can be accessed here: https://github.com/BenjaminCoupry/the-MAD-project.	https://openaccess.thecvf.com//content/WACV2025/html/Coupry_Assessing_the_Quality_of_3D_Reconstruction_in_the_Absence_of_WACV_2025_paper.html	Benjamin Coupry, Baptiste Brument, Antoine Laurent, Jean MÃ©lou, Yvain QuÃ©au, Jean-Denis Durou
Attention-Based Class-Conditioned Alignment for Multi-Source Domain Adaptation of Object Detectors	Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modality information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment yet it suffers from error accumulation caused by noisy pseudo-labels that can negatively affect adaptation with imbalanced data. To overcome these limitations we propose an attention-based class-conditioned alignment method for MSDA designed to align instances of each object category across domains. In particular an attention module combined with an adversarial domain classifier allows learning domain-invariant and class-specific instance representations. Experimental results on multiple benchmarking MSDA datasets indicate that our method outperforms state-of-the-art methods and exhibits robustness to class imbalance achieved through a conceptually simple class-conditioning strategy. Our code is available at: https://github.com/imatif17/ACIA.	https://openaccess.thecvf.com//content/WACV2025/html/Belal_Attention-Based_Class-Conditioned_Alignment_for_Multi-Source_Domain_Adaptation_of_Object_Detectors_WACV_2025_paper.html	Atif Belal, Akhil Meethal, Francisco Perdigon Romero, Marco Pedersoli, Eric Granger
Attention-Guided Masked Autoencoders for Learning Image Representations	Masked autoencoders (MAEs) have established themselves as a powerful pre-training method for computer vision tasks. While vanilla MAEs put equal emphasis on reconstructing the individual parts of the image we propose to inform the reconstruction process through an attention-guided loss function. By leveraging advances in unsupervised object discovery we obtain an attention map of the scene which we employ in the loss function to put increased emphasis on reconstructing relevant objects. Thus we incentivize the model to learn improved representations of the scene for a variety of tasks. Our evaluations show that our pre-trained models produce off-the-shelf representations more effective than the vanilla MAE for such tasks demonstrated by improved linear probing and k-NN classification results on several benchmarks while at the same time making ViTs more robust against varying backgrounds and changes in texture.	https://openaccess.thecvf.com//content/WACV2025/html/Sick_Attention-Guided_Masked_Autoencoders_for_Learning_Image_Representations_WACV_2025_paper.html	Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski
Attribute Diffusion: Diffusion Driven Diverse Attribute Editing	Image attribute editing is a widely researched area fueled by the recent advancements in deep generative models. Existing methods treat semantic attributes as binary and do not allow the user to generate multiple variations of the attribute edits. This limits the applications of editing methods in the real world e.g. exploring multiple eyeglass variations on an e-commerce platform. In this work we present a technique to generate a collection of diverse attribute edits and a principled way to explore them. Generation and controlled exploration of attribute variations is challenging as it requires fine control over the attribute styles while preserving other attributes and the identity of the subject. Capitalizing on the attribute disentanglement property of the latent spaces of pretrained GANs we represent the attribute edits in this space. Next we train a diffusion model to model these latent directions of edits. We propose a coarse-to-fine sampling strategy to explore these variations in a controlled manner. Extensive experiments on various datasets establish the effectiveness and generalization of the proposed approach for the generation and controlled exploration of diverse attribute edits. Code is available at - rishubhpar.github.io/attributediffusion	https://openaccess.thecvf.com//content/WACV2025/html/Parihar_Attribute_Diffusion_Diffusion_Driven_Diverse_Attribute_Editing_WACV_2025_paper.html	Rishubh Parihar, Prasanna Balaji, Raghav Magazine, Sarthak Vora, Varun Jampani, Venkatesh Babu Radhakrishnan
AutoProSAM: Automated Prompting SAM for 3D Multi-Organ Segmentation	Segment Anything Model (SAM) is one of the pioneering prompt-based foundation models for image segmentation and has been rapidly adopted for various medical imaging applications. However in clinical settings creating effective prompts is notably challenging and time-consuming requiring the expertise of domain specialists such as physicians. This requirement significantly diminishes SAM's primary advantage--its interactive capability with end users--in medical applications. Moreover recent studies have indicated that SAM originally designed for 2D natural images performs suboptimally on 3D medical image segmentation tasks. This subpar performance is attributed to the domain gaps between natural and medical images and the disparities in spatial arrangements between 2D and 3D images particularly in multi-organ segmentation applications. To overcome these challenges we present a novel technique termed AutoProSAM. This method automates 3D multi-organ CT-based segmentation by leveraging SAM's foundational model capabilities without relying on domain experts for prompts. The approach utilizes parameter-efficient adaptation techniques to adapt SAM for 3D medical imagery and incorporates an effective automatic prompt learning paradigm specific to this domain. By eliminating the need for manual prompts it enhances SAM's capabilities for 3D medical image segmentation and achieves state-of-the-art (SOTA) performance in CT-based multi-organ segmentation tasks. The code is in this link.	https://openaccess.thecvf.com//content/WACV2025/html/Li_AutoProSAM_Automated_Prompting_SAM_for_3D_Multi-Organ_Segmentation_WACV_2025_paper.html	Chengyin Li, Rafi Ibn Sultan, Prashant Khanduri, Yao Qiang, Chetty Indrin, Dongxiao Zhu
Automated Evaluation of Large Vision-Language Models on Self-Driving Corner Cases	Large Vision-Language Models (LVLMs) have received widespread attentions for advancing the interpretable self-driving. Existing evaluations of LVLMs primarily focus on multi-faceted capabilities in natural circumstances lacking automated and quantifiable assessment for self-driving let alone the severe road corner cases. In this work we propose CODA-LM the very first benchmark for the automatic evaluation of LVLMs for self-driving corner cases. We adopt a hierarchical data structure and prompt powerful LVLMs to analyze complex driving scenes and generate high-quality pre-annotations for the human annotators while for LVLM evaluation we show that using the text-only large language models (LLMs) as judges reveals even better alignment with human preferences than the LVLM judges. Moreover with our CODA-LM we build CODA-VLM a new driving LVLM surpassing all open-sourced counterparts on CODA-LM. Our CODA-VLM performs comparably with GPT-4V even surpassing GPT-4V by +21.42% on the regional perception task. We hope CODA-LM can become the catalyst to promote interpretable self-driving empowered by LVLMs.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_Automated_Evaluation_of_Large_Vision-Language_Models_on_Self-Driving_Corner_Cases_WACV_2025_paper.html	Kai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, Dit-Yan Yeung, Huchuan Lu, Xu Jia
Automated Patient Positioning with Learned 3D Hand Gestures	Positioning patients for scanning and interventional procedures is a critical task that requires high precision and accuracy. The conventional workflow involves manually adjusting the patient support to align the center of the target body part with the laser projector or other guiding devices. This process is not only time-consuming but also prone to inaccuracies. In this work we propose an automated patient positioning system that utilizes a camera to detect specific hand gestures from technicians allowing users to indicate the target patient region to the system and initiate automated positioning. Our approach relies on a novel multi-stage pipeline to recognize and interpret the technicians' gestures translating them into precise motions of medical devices. We evaluate our proposed pipeline during actual MRI scanning procedures using RGB-Depth cameras to capture the process. Results show that our system achieves accurate and precise patient positioning with minimal technician intervention. Furthermore we validate our method on HaGRID a large-scale hand gesture dataset demonstrating its effectiveness in hand detection and gesture recognition.	https://openaccess.thecvf.com//content/WACV2025/html/Gao_Automated_Patient_Positioning_with_Learned_3D_Hand_Gestures_WACV_2025_paper.html	Zhongpai Gao, Abhishek Sharma, Meng Zheng, Benjamin Planche, Terrence Chen, Ziyan Wu
Autoregressive Adaptive Hypergraph Transformer for Skeleton-Based Activity Recognition	Extracting multiscale contextual information and higher-order correlations among skeleton sequences using Graph Convolutional Networks (GCNs) alone is inadequate for effective action classification. Hypergraph convolution addresses the above issues but cannot harness the long-range dependencies. The transformer proves to be effective in capturing these dependencies and making complex contextual features accessible. We propose an Autoregressive Adaptive HyperGraph Transformer (AutoregAd-HGformer) model for in-phase (autoregressive and discrete) and out-phase (adaptive) hypergraph generation. The vector quantized in-phase hypergraph equipped with powerful autoregressive learned priors produces a more robust and informative representation suitable for hyperedge formation. The out-phase hypergraph generator provides a model-agnostic hyperedge learning technique to align the attributes with input skeleton embedding. The hybrid (supervised and unsupervised) learning in AutoregAd-HGformer explores the action-dependent feature along spatial temporal and channel dimensions. The extensive experimental results and ablation study indicate the superiority of our model over state-of-the-art hypergraph architectures on the NTU RGB+D NTU RGB+D 120 and NW-UCLA datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Ray_Autoregressive_Adaptive_Hypergraph_Transformer_for_Skeleton-Based_Activity_Recognition_WACV_2025_paper.html	Abhisek Ray, Ayush Raj, Maheshkumar H. Kolekar
BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction using Neural Radiance Fields	Reconstruction of deformable scenes from endoscopic videos is important for many applications such as intraoperative navigation surgical visual perception and robotic surgery. It is a foundational requirement for realizing autonomous robotic interventions for minimally invasive surgery. However previous approaches in this domain have been limited by their modular nature and are confined to specific camera and scene settings. Our work adopts the Neural Radiance Fields (NeRF) approach to learning 3D implicit representations of scenes that are both dynamic and deformable over time and furthermore with unknown camera poses. This work removes the constraints of known camera poses and overcomes the drawbacks of the state-of-the-art unstructured dynamic scene reconstruction technique which relies on the static part of the scene for accurate reconstruction. Through several experimental datasets we demonstrate the versatility of our proposed model to adapt to diverse camera and scene settings and show its promise for both current and future robotic surgical systems.	https://openaccess.thecvf.com//content/WACV2025/html/Saha_BASED_Bundle-Adjusting_Surgical_Endoscopic_Dynamic_Video_Reconstruction_using_Neural_Radiance_WACV_2025_paper.html	Shreya Saha, Zekai Liang, Shan Lin, Jingpei Lu, Michael Yip, Sainan Liu
BIV-Priv-Seg: Locating Private Content in Images Taken by People with Visual Impairments	Individuals who are blind or have low vision (BLV) are at a heightened risk of sharing private information if they share photographs they have taken. To facilitate developing technologies that can help them preserve privacy we introduce BIV-Priv-Seg the first localization dataset originating from people with visual impairments that shows private content. It contains 1028 images with segmentation annotations for 16 private object categories. We first characterize BIV-Priv-Seg and then evaluate modern models' performance for locating private content in the dataset. We find modern models struggle most with locating private objects that are not salient small and lack text as well as recognizing when private content is absent from an image. We facilitate future extensions by sharing our new dataset with the evaluation server at https://vizwiz.org/tasks-and-datasets/object-localization/.	https://openaccess.thecvf.com//content/WACV2025/html/Tseng_BIV-Priv-Seg_Locating_Private_Content_in_Images_Taken_by_People_with_WACV_2025_paper.html	Yu-Yun Tseng, Tanusree Sharma, Lotus Zhang, Abigale Stangl, Leah Findlater, Yang Wang, Danna Gurari
Background-Aware Moment Detection for Video Moment Retrieval	Video moment retrieval (VMR) identifies a specific moment in an untrimmed video for a given natural language query. This task is prone to suffer the weak alignment problem innate in video datasets. Due to the ambiguity a query does not fully cover the relevant details of the corresponding moment or the moment may contain misaligned and irrelevant frames potentially limiting further performance gains. To tackle this problem we propose a background-aware moment detection transformer (BM-DETR). Our model adopts a contrastive approach carefully utilizing the negative queries matched to other moments in the video. Specifically our model learns to predict the target moment from the joint probability of each frame given the positive query and the complement of negative queries. This leads to effective use of the surrounding background improving moment sensitivity and enhancing overall alignments in videos. Extensive experiments on four benchmarks demonstrate the effectiveness of our approach. Our code is available at https://github.com/minjoong507/BM-DETR.	https://openaccess.thecvf.com//content/WACV2025/html/Jung_Background-Aware_Moment_Detection_for_Video_Moment_Retrieval_WACV_2025_paper.html	Minjoon Jung, Youwon Jang, Seongho Choi, Joochan Kim, Jin-Hwa Kim, Byoung-Tak Zhang
Balancing Shared and Task-Specific Representations: A Hybrid Approach to Depth-Aware Video Panoptic Segmentation	In this work we present Multiformer a novel approach to depth-aware video panoptic segmentation (DVPS) based on the mask transformer paradigm. Our method learns object representations that are shared across segmentation monocular depth estimation and object tracking subtasks. In contrast to recent unified approaches that progressively refine a common object representation we propose a hybrid method using task-specific branches within each decoder block ultimately fusing them into a shared representation at the block interfaces. Extensive experiments on the Cityscapes-DVPS and SemKITTI-DVPS datasets demonstrate that Multiformer achieves state-of-the-art performance across all DVPS metrics outperforming previous methods by substantial margins. With a ResNet-50 backbone Multiformer surpasses the previous best result by 3.0 DVPQ points while also improving depth estimation accuracy. Using a Swin-B backbone Multiformer further improves performance by 4.0 DVPQ points. Multiformer also provides valuable insights into the design of multi-task decoder architectures.	https://openaccess.thecvf.com//content/WACV2025/html/Stolle_Balancing_Shared_and_Task-Specific_Representations_A_Hybrid_Approach_to_Depth-Aware_WACV_2025_paper.html	Kurt H.W. Stolle
Bandit Based Attention Mechanism in Vision Transformers	Vision Transformers (ViT) have demonstrated remarkable performance on many computer vision tasks. However their high computational cost and quadratic complexity pose challenges for deployment in resource-constrained environments. The core of Vision Transformers is the self-attention mechanism which aggregates information from different image regions or patches. In a conventional ViT processing involves attention to all patches creating a substantial computational bottleneck and extended training times. We hypothesize that applying soft attention to all patches may be unnecessary and instead focusing on relevant and significant patches (hard attention) would be sufficient. To address this we introduce a module within the Vision Transformer that allows the attention mechanism to selectively process only the essential patches. We propose a novel bandit-based attention mechanism that leverages the idea of exploration and exploitation. The extensive experimentation across various datasets illustrates that the proposed bandit attention-based ViT not only achieves superior performance compared to the existing state-of-the-art vision transformer models but also results in greater throughput and lower computational time in the training as well as the inference. The code is publicly available at https://github.com/aquorio15/bandit wacv	https://openaccess.thecvf.com//content/WACV2025/html/Chowdhury_Bandit_Based_Attention_Mechanism_in_Vision_Transformers_WACV_2025_paper.html	Amartya Roy Chowdhury, Raghuram Bharadwaj Diddigi, Prabuchandran K J, Achyut Mani Tripathi
Bandwidth-Efficient Communication Modelling for Autonomous Vehicle Collaborative Perception	Accurate perception systems are crucial for intelligent vehicle motion planning and control. Cooperative perception can significantly enhance the perception capabilities of autonomous vehicles by sharing information beyond the field of view of individual vehicles. However most recent studies on cooperative perception have overlooked the bandwidth limitations of information sharing. To address this issue this paper presents a novel multi-agent cooperative 3D object detection framework to improve the detection accuracy of connected autonomous vehicles (CAVs) in bandwidth-constrained wireless communication environments. The framework includes an attention-based communication mechanism to select optimal communication partners and a foreground-background separation strategy to prioritize key information. The effectiveness of the proposed framework is evaluated on three benchmark datasets CARLA-3D V2XSet and OPV2V demonstrating the state-of-the-art performance-bandwidth trade-off.	https://openaccess.thecvf.com//content/WACV2025/html/Jin_Bandwidth-Efficient_Communication_Modelling_for_Autonomous_Vehicle_Collaborative_Perception_WACV_2025_paper.html	Dinghao Jin, Yuan Zeng, Yi Gong
Bayesian Optimal Latent Projection for Noisy Image Restoration	In recent years image restoration using large-scale latent diffusion generative models (DGM) has attracted increasing attention and achieved significant progress. Most of these latent DGM-based image restoration methods require predicting the original clean image in each iteration which is then used to estimate the image for the next iteration. However these predicted original clean images are often inaccurate leading to errors in the subsequent image estimation. In other words there is a significant deviation between the final sampling restoration trajectory and the ground truth trajectory. The purpose of this paper is to narrow the gap between these two trajectories and enhance the performance of image restoration. We propose the Bayesian Optimal Latent Projection (BOLP) algorithm which identifies the optimal random noise within the Gaussian distribution to iteratively correct the estimated image at each step thereby minimizing the distance to the ground truth image. Experiments in deblurring super-resolution and inpainting on FFHQ and ImageNet datasets demonstrate that the BOLP outperforms the previously established best algorithms and sets a new state of the art.	https://openaccess.thecvf.com//content/WACV2025/html/Shi_Bayesian_Optimal_Latent_Projection_for_Noisy_Image_Restoration_WACV_2025_paper.html	Ziqiang Shi, Rujie Liu, Jun Takahashi, Takuma Yamamoto
BeautyBank: Encoding Facial Makeup in Latent Space	The advancement of makeup transfer editing and image encoding has demonstrated their effectiveness and superior quality. However existing makeup works primarily focus on low-dimensional features such as color distributions and patterns limiting their versatillity across a wide range of makeup applications. Futhermore existing high-dimensional latent encoding methods mainly target global features such as structure and style and are less effective for tasks that require detailed attention to local color and pattern features of makeup. To overcome these limitations we propose BeautyBank a novel makeup encoder that disentangles pattern features of bare and makeup faces. Our method encodes makeup features into a high-dimensional space preserving essential details necessary for makeup reconstruction and broadening the scope of potential makeup research applications. We also propose a Progressive Makeup Tuning (PMT) strategy specifically designed to enhance the preservation of detailed makeup features while preventing the inclusion of irrelevant attributes. We further explore novel makeup applications including facial image generation with makeup injection and makeup similarity measure. Extensive empirical experiments validate that our method offers superior task adaptability and holds significant potential for widespread application in various makeup-related fields. Furthermore to address the lack of large-scale high-quality paired makeup datasets in the field we constructed the Bare-Makeup Synthesis Dataset (BMS) comprising 324000 pairs of 512x512 pixel images of bare and makeup-enhanced faces.	https://openaccess.thecvf.com//content/WACV2025/html/Lu_BeautyBank_Encoding_Facial_Makeup_in_Latent_Space_WACV_2025_paper.html	Qianwen Lu, Xingchao Yang, Takafumi Taketomi
Benchmarking VLMs' Reasoning About Persuasive Atypical Images	Vision language models (VLMs) have shown strong zero-shot generalization across various tasks especially when integrated with large language models (LLMs). However their ability to comprehend rhetorical and persuasive visual media such as advertisements remains understudied. Ads often employ atypical imagery using surprising object juxtapositions to convey shared properties. For example Fig. 1 (e) shows a beer with a feather-like texture. This requires advanced reasoning to deduce that this atypical representation signifies the beer's lightness. We introduce three novel tasks Multi-label Atypicality Classification Atypicality Statement Retrieval and Atypical Object Recognition to benchmark VLMs' understanding of atypicality in persuasive images. We evaluate how well VLMs use atypicality to infer an ad's message and test their reasoning abilities by employing semantically challenging negatives. Finally we pioneer atypicality-aware verbalization by extracting comprehensive image descriptions sensitive to atypical elements. Findings reveal that: (1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple effective strategies can extract atypicality-aware information leading to comprehensive image verbalization; (3) atypicality aids persuasive ad understanding. Code and data is available at aysanaghazadeh.github.io/PersuasiveAdVLMBenchmark/	https://openaccess.thecvf.com//content/WACV2025/html/Malakouti_Benchmarking_VLMs_Reasoning_About_Persuasive_Atypical_Images_WACV_2025_paper.html	Sina Malakouti, Aysan Aghazadeh, Ashmit Khandelwal, Adriana Kovashka
Beta Sampling is All You Need: Efficient Image Generation Strategy for Diffusion Models using Stepwise Spectral Analysis	Generative diffusion models have emerged as a powerful tool for high-quality image synthesis yet their iterative nature demands significant computational resources. This paper proposes an efficient time step sampling method based on an image spectral analysis of the diffusion process aimed at optimizing the denoising process. Instead of the traditional uniform distribution-based time step sampling we introduce a Beta distribution-like sampling technique that prioritizes critical steps in the early and late stages of the process. Our hypothesis is that certain steps exhibit significant changes in image content while others contribute minimally. We validated our approach using Fourier transforms to measure frequency response changes at each step revealing substantial low-frequency changes early on and high-frequency adjustments later. Experiments with ADM and Stable Diffusion demonstrated that our Beta Sampling method consistently outperforms uniform sampling achieving better FID and IS scores and offers competitive efficiency relative to state-of-the-art methods like AutoDiffusion. This work provides a practical framework for enhancing diffusion model efficiency by focusing computational resources on the most impactful steps with potential for further optimization and broader application.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Beta_Sampling_is_All_You_Need_Efficient_Image_Generation_Strategy_WACV_2025_paper.html	Haeil Lee, Hansang Lee, Seoyeon Gye, Junmo Kim
Beyond Boxes: Mask-Guided Spatio-Temporal Feature Aggregation for Video Object Detection	The primary challenge in Video Object Detection (VOD) is effectively exploiting temporal information to enhance object representations. Traditional strategies such as aggregating region proposals often suffer from feature variance due to the inclusion of background information. We introduce a novel instance mask-based feature aggregation approach significantly refining this process and deepening the understanding of object dynamics across video frames. We present FAIM a new VOD method that enhances temporal Feature Aggregation by leveraging Instance Mask features. In particular we propose the lightweight Instance Feature Extraction Module (IFEM) to learn instance mask features and the Temporal Instance Classification Aggregation Module (TICAM) to aggregate instance mask and classification features across video frames. Using YOLOX as a base detector FAIM achieves 87.9% mAP on the ImageNet VID dataset at 33 FPS on a single 2080Ti GPU setting a new benchmark for the speed-accuracy trade-off. Additional experiments on multiple datasets validate that our approach is robust method-agnostic and effective in multi-object tracking demonstrating its broader applicability to video understanding tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Hashmi_Beyond_Boxes_Mask-Guided_Spatio-Temporal_Feature_Aggregation_for_Video_Object_Detection_WACV_2025_paper.html	Khurram Azeem Hashmi, Talha Uddin Sheikh, Didier Stricker, Muhammad Zeshan Afzal
Beyond Grids: Exploring Elastic Input Sampling for Vision Transformers	Vision transformers have excelled in various computer vision tasks but mostly rely on rigid input sampling using a fixed-size grid of patches. It limits their applicability in real-world problems such as active visual exploration where patches have various scales and positions. Our paper addresses this limitation by formalizing the concept of input elasticity for vision transformers and introducing an evaluation protocol for measuring this elasticity. Moreover we propose modifications to the transformer architecture and training regime which increase its elasticity. Through extensive experimentation we spotlight opportunities and challenges associated with such architecture.	https://openaccess.thecvf.com//content/WACV2025/html/Pardyl_Beyond_Grids_Exploring_Elastic_Input_Sampling_for_Vision_Transformers_WACV_2025_paper.html	Adam Pardyl, Grzegorz Kurzejamski, Jan Olszewski, Tomasz Trzcinski, Bartosz Zielinski
Beyond Spatial Explanations: Explainable Face Recognition in the Frequency Domain	The need for more transparent face recognition (FR) along with other visual-based decision-making systems has recently attracted more attention in research society and industry. The reasons why two face images are matched or not matched by a deep learning-based face recognition system are not obvious due to the high number of parameters and the complexity of the models. However it is important for users operators and developers to ensure trust and accountability of the system and to analyze drawbacks such as biased behavior. While many previous works use spatial semantic maps to highlight the regions that have a significant influence on the decision of the face recognition system frequency components which are also considered by CNNs are neglected. In this work we take a step forward and investigate explainable face recognition in the unexplored frequency domain. This makes this work the first to propose explainability of verification-based decisions in the frequency domain thus explaining the relative influence of the frequency components of each input toward the obtained outcome. To achieve this we manipulate face images in the spatial frequency domain and investigate the impact on verification outcomes. In extensive quantitative experiments along with investigating two special scenarios cases cross-resolution FR and morphing attacks (the latter in supplementary material) we observe the applicability of our proposed frequency-based explanations.	https://openaccess.thecvf.com//content/WACV2025/html/Huber_Beyond_Spatial_Explanations_Explainable_Face_Recognition_in_the_Frequency_Domain_WACV_2025_paper.html	Marco Huber, Naser Damer
Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification	A key challenge in visible-infrared person re-identification (V-I ReID) is training a backbone model capable of effectively addressing the significant discrepancies across modalities. State-of-the-art methods that generate a single intermediate bridging domain are often less effective as this generated domain may not adequately capture sufficient common discriminant information. This paper introduces Bidirectional Multi-step Domain Generalization (BMDG) a novel approach for unifying feature representations across diverse modalities. BMDG creates multiple virtual intermediate domains by learning and aligning body part features extracted from both I and V modalities. In particular our method aims to minimize the cross-modal gap in two steps. First BMDG aligns modalities in the feature space by learning shared and modality-invariant body part prototypes from V and I images. Then it generalizes the feature representation by applying bidirectional multi-step learning which progressively refines feature representations in each step and incorporates more prototypes from both modalities. Based on these prototypes multiple bridging steps enhance the feature representation. Experiments conducted on V-I ReID datasets indicate that our BMDG approach can outperform state-of-the-art part-based and intermediate generation methods and can be integrated into other part-based methods to enhance their V-I ReID performance.	https://openaccess.thecvf.com//content/WACV2025/html/Alehdaghi_Bidirectional_Multi-Step_Domain_Generalization_for_Visible-Infrared_Person_Re-Identification_WACV_2025_paper.html	Mahdi Alehdaghi, Pourya Shamsolmoali, Rafael M. O. Cruz, Eric Granger
BioNet and NeFF: Crop Biomass Prediction from Point Clouds to Drone Imagery	Crop biomass offers crucial insights into plant health and yield making it essential for crop science farming systems and agricultural research. However current measurement methods which are labor-intensive destructive and imprecise hinder large-scale quantification of this trait. To address this limitation we present a biomass prediction network (BioNet) designed for adaptation across different data modalities including point clouds and drone imagery. Our BioNet utilizing a sparse 3D convolutional neural network (CNN) and a transformer-based prediction module processes point clouds and other 3D data representations to predict biomass. To further extend BioNet for drone imagery we integrate a neural feature field (NeFF) module enabling 3D structure reconstruction and the transformation of 2D semantic features from vision foundation models into the corresponding 3D surfaces. For the point cloud modality BioNet demonstrates superior performance on two public datasets with an approximate 6.1% relative improvement (RI) over the state-of-the-art. In the RGB image modality the combination of BioNet and NeFF achieves a 7.9% RI. Additionally the NeFF-based approach utilizes inexpensive portable drone-mounted cameras providing a scalable solution for large field applications.	https://openaccess.thecvf.com//content/WACV2025/html/Li_BioNet_and_NeFF_Crop_Biomass_Prediction_from_Point_Clouds_to_WACV_2025_paper.html	Xuesong Li, Zeeshan Hayder, Ali Zia, Connor Cassidy, Shiming Liu, Warwick Stiller, Eric Stone, Warren Conaty, Lars Petersson, Vivien Rolland
BioPose: Biomechanically-Accurate 3D Pose Estimation from Monocular Videos	Recent advancements in 3D human pose estimation from single-camera images and videos have relied on parametric models like SMPL. However these models oversimplify anatomical structures limiting their accuracy in capturing true joint locations and movements which reduces their applicability in biomechanics healthcare and robotics. Biomechanically accurate pose estimation on the other hand typically requires costly marker-based motion capture systems and optimization techniques in specialized labs. To bridge this gap we propose BioPose a novel learning-based framework for predicting biomechanically accurate 3D human pose directly from monocular videos. BioPose includes three key components: a multi-query human mesh recovery model (MQ-HMR) a neural inverse kinematics (NeurIK) model and a 2D-informed pose refinement technique. MQ-HMR leverages a multi-query deformable transformer to extract multi-scale fine-grained image features enabling precise human mesh recovery. NeurIK treats the mesh vertices as virtual markers applying a spatial-temporal network to regress biomechanically accurate 3D poses under anatomical constraints. To further improve 3D pose estimations a 2D-informed refinement step optimizes the query tokens during inference by aligning the 3D structure with 2D pose observations. Experiments on benchmark datasets demonstrate that BioPose significantly outperforms state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Koleini_BioPose_Biomechanically-Accurate_3D_Pose_Estimation_from_Monocular_Videos_WACV_2025_paper.html	Farnoosh Koleini, Muhammad Usama Saleem, Pu Wang, Hongfei Xue, Ahmed Helmy, Abbey Fenwick
Bit-Flip Induced Latency Attacks in Object Detection	Deep learning and computer vision have experienced significant advancements particularly in critical applications such as autonomous driving and real-time surveillance where object detection (OD) plays a pivotal role. Ensuring the accuracy and speed of these systems is paramount to prevent accidents or failures. Recently latency-based attacks have emerged as a new threat driven by the essential need for real-time performance in various applications. These attacks target model responsiveness to disrupt system performance without necessarily compromising accuracy. Our preliminary experiments show that introducing just a few bit flips to key parameters in OD models can significantly increase latency degrading performance. Meanwhile recent advancements in memory-based attacks such as Row Hammer demonstrate the ability to conveniently introduce bit flips at desired locations without physical hardware interaction. Based on the observations we propose a novel attack on OD models that leverages row-hammer to introduce bit-flips via side channels targeting the non-maximum suppression (NMS) filter and significantly increasing latency. Unlike previous methods that modify input data our technique ensures efficiency by minimizing bit-flips through critical path exploitation and achieves practical applicability with only a subset of validation data. Experiments across various datasets and models validate our approach demonstrating latency increases up to 71.6 ms (20.4x) with just 31 bit-flips.	https://openaccess.thecvf.com//content/WACV2025/html/Sistla_Bit-Flip_Induced_Latency_Attacks_in_Object_Detection_WACV_2025_paper.html	Manojna Sistla, Yu Wen, Aamir Bader Shah, Chenpei Huang, Lening Wang, Xuqing Wu, Jiefu Chen, Miao Pan, Xin Fu
Blind Image Deblurring with FFT-ReLU Sparsity Prior	Blind image deblurring is the process of recovering a sharp image from a blurred one without prior knowledge about the blur kernel. It is a small data problem since the key challenge lies in estimating the unknown degrees of blur from a single image or limited data instead of learning from large datasets. The solution depends heavily on developing algorithms that effectively model the image degradation process. We introduce a method that leverages a prior which targets the blur kernel to achieve effective deblurring across a wide range of image types. In our extensive empirical analysis our algorithm achieves results that are competitive with the state-of-the-art blind image deblurring algorithms and it offers up to two times faster inference making it a highly efficient solution.	https://openaccess.thecvf.com//content/WACV2025/html/Al_Radi_Blind_Image_Deblurring_with_FFT-ReLU_Sparsity_Prior_WACV_2025_paper.html	Abdul Mohaimen Al Radi, Prothito Shovon Majumder, Md. Mosaddek Khan
Boosting Diffusion Guidance via Learning Degradation-Aware Models for Blind Super Resolution	Recently diffusion-based blind super-resolution (SR) methods have shown great ability to generate high-resolution images with abundant high-frequency detail but the detail is often achieved at the expense of fidelity. Meanwhile another line of research focusing on rectifying the reverse process of diffusion models (i.e. diffusion guidance) has demonstrated the power to generate high-fidelity results for non-blind SR. However these methods rely on known degradation kernels making them difficult to apply to blind SR. To address these issues we introduce degradation-aware models that can be integrated into the diffusion guidance framework eliminating the need to know degradation kernels. Additionally we propose two novel techniques--input perturbation and guidance scalar--to further improve our performance. Extensive experimental results show that our proposed method has superior performance over state-of-the-art methods on blind SR benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Lu_Boosting_Diffusion_Guidance_via_Learning_Degradation-Aware_Models_for_Blind_Super_WACV_2025_paper.html	Shao-Hao Lu, Ren Wang, Ching-Chun Huang, Wei-Chen Chiu
Boosting Semi-Supervised Video Action Detection with Temporal Context	This paper studies semi-supervised learning of video action detection (VAD) which assumes that only a small portion of training videos are labeled and the others remain unlabeled. The existing semi-supervised methods for VAD mainly focus on leveraging spatial context of unlabeled video lacking its exploration of temporal context. To resolve this we present a novel semi-supervised learning framework that effectively incorporates spatio-temporal context during training. We first introduce a new augmentation strategy called temporal cross-view augmentation to achieve robust representation across clips depicting the same action but not aligned on the time axis. We also propose a new context fusion method called global-local context fusion that effectively utilizes the spatio-temporal context of videos to enhances the features of each frame by incorporating those of other frames within a clip; this method aids in actively leveraging spatio-temporal context of video leading to significant performance improvement. Our framework was evaluated on UCF101-24 and JHMDB-21 where it outperformed all existing methods in every evaluation setting.	https://openaccess.thecvf.com//content/WACV2025/html/Kwon_Boosting_Semi-Supervised_Video_Action_Detection_with_Temporal_Context_WACV_2025_paper.html	Donghyeon Kwon, Inho Kim, Suha Kwak
Breaking the Frame: Visual Place Recognition by Overlap Prediction	Visual place recognition methods struggle with occlusion and partial visual overlaps. We propose a novel visual place recognition approach based on overlap prediction called VOP shifting from traditional reliance on global image similarities and local features to image overlap prediction. VOP proceeds co-visible image sections by obtaining patch-level embeddings using a Vision Transformer backbone and establishing patch-to-patch correspondences without requiring expensive feature detection and matching. Our approach uses a voting mechanism to assess overlap scores for potential database images. It provides a nuanced image retrieval metric in challenging scenarios. Experimental results show that VOP leads to more accurate relative pose estimation and localization results on the retrieved image pairs than state-of-the-art baselines on a number of large-scale real-world indoor and outdoor benchmarks. The code is available at https://github.com/weitong8591/vop.git.	https://openaccess.thecvf.com//content/WACV2025/html/Wei_Breaking_the_Frame_Visual_Place_Recognition_by_Overlap_Prediction_WACV_2025_paper.html	Tong Wei, Philipp Lindenberger, JirÃ­ Matas, Daniel Barath
BroadTrack: Broadcast Camera Tracking for Soccer	Camera calibration and localization sometimes simply named camera calibration enables many applications in the context of soccer broadcasting for instance regarding the interpretation and analysis of the game or the insertion of augmented reality graphics for storytelling or refereeing purposes. To contribute to such applications the research community has typically focused on single-view calibration methods leveraging the near-omnipresence of soccer field markings in wide-angle broadcast views but leaving all temporal aspects if considered at all to general-purpose tracking or filtering techniques. Only a few contributions have been made to leverage any domain-specific knowledge for this tracking task and as a result there lacks a truly performant and off-the-shelf camera tracking system tailored for soccer broadcasting specifically for elevated tripod-mounted cameras around the stadium. In this work we present such a system capable of addressing the task of soccer broadcast camera tracking efficiently robustly and accurately outperforming by far the most precise methods of the state-of-the-art. By combining the available open-source soccer field detectors with carefully designed camera and tripod models our tracking system BroadTrack halves the mean reprojection error rate and gains more than 15% in terms of Jaccard index for camera calibration on the SoccerNet dataset. Furthermore as the SoccerNet dataset videos are relatively short (30 seconds) we also present qualitative results on a 20-minute broadcast clip to showcase the robustness and the soundness of our system.	https://openaccess.thecvf.com//content/WACV2025/html/Magera_BroadTrack_Broadcast_Camera_Tracking_for_Soccer_WACV_2025_paper.html	Floriane Magera, Thomas Hoyoux, Olivier Barnich, Marc Van Droogenbroeck
CACE: Sim-to-Real Indoor 3D Semantic Segmentation via Context-Aware Augmentation and Consistency Enforcement	Indoor 3D domain adaptation for semantic segmentation is an understudied task. The first unsupervised sim-to-real benchmark was only proposed recently. Existing methods try to modify the source domain data by simulating the occlusion and noise pattern of the target domain. However this methodology unrealistically demands a clear definition of the real-world data patterns and is highly dependent on the simulation quality. In this paper we propose a novel adaptation framework via Context-aware Augmentation and Consistency Enforcement (CACE). Our CACE framework consists of two modules a space and context-aware augmentation module that is invariant of target data pattern and domain gaps and a carefully designed self-supervision module that maximizes the utility of the augmented data. Our CACE surpasses the state-of-the-art method by over 6% on the indoor 3D sim-to-real benchmark 3D-FRONT - ScanNet.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_CACE_Sim-to-Real_Indoor_3D_Semantic_Segmentation_via_Context-Aware_Augmentation_and_WACV_2025_paper.html	Tsung-Yu Chen, Luyu Yang, Tzu-Yu Chuang, Shang-Hong Lai
CAMEL: Confidence-Aware Multi-Task Ensemble Learning with Spatial Information for Retina OCT Image Classification and Segmentation	Precise retina Optical Coherence Tomography (OCT) image classification and segmentation are important for diagnosing various retinal diseases and identifying specific regions. Alongside comprehensive lesion identification reducing the predictive uncertainty of models is crucial for improving reliability in clinical retinal practice. However existing methods have primarily focused on a limited set of regions identified in OCT images and have often faced challenges due to aleatoric and epistemic uncertainty. To address these issues we propose CAMEL (Confidence-Aware Multi-task Ensemble Learning) a novel framework designed to reduce task-specific uncertainty in multi-task learning. CAMEL achieves this by estimating model confidence at both pixel and image levels and leveraging confidence-aware ensemble learning to minimize the uncertainty inherent in single-model predictions. CAMEL demonstrates state-of-the-art performance on a comprehensive retinal OCT image dataset containing annotations for nine distinct retinal regions and nine retinal diseases. Furthermore extensive experiments highlight the clinical utility of CAMEL especially in scenarios with minimal regions significant class imbalances and diverse regions and diseases. Our code is publicly available at: https://github.com/DSAIL-SKKU/CAMEL.	https://openaccess.thecvf.com//content/WACV2025/html/Jung_CAMEL_Confidence-Aware_Multi-Task_Ensemble_Learning_with_Spatial_Information_for_Retina_WACV_2025_paper.html	Juho Jung, Migyeong Yang, Hyunseon Won, Jiwon Kim, Jeong Mo Han, Joon Seo Hwang, Daniel Duck-Jin Hwang, Jinyoung Han
CAMS: Convolution and Attention-Free Mamba-Based Cardiac Image Segmentation	Convolutional Neural Networks (CNNs) and Transformer-based self-attention models have become the standard for medical image segmentation. This paper demonstrates that convolution and self-attention while widely used are not the only effective methods for segmentation. Breaking with convention we present a Convolution and self-attention-free Mamba-based semantic Segmentation Network named CAMS-Net. Specifically we design a Mamba-based Channel Aggregator and Spatial Aggregator which are applied independently in each encoder-decoder stage. The Channel Aggregator extracts information across different channels and the Spatial Aggregator learns features across different spatial locations. We also propose a Linearly Interconnected Factorized Mamba (LIFM) block to reduce the computational complexity of a Mamba block and to enhance its decision function by introducing a non-linearity between two factorized Mamba blocks. Our model outperforms the existing state-of-the-art CNN self-attention and Mamba-based methods on CMR and M&Ms-2 Cardiac segmentation datasets showing how this innovative convolution and self-attention-free method can inspire further research beyond CNN and Transformer paradigms achieving linear complexity and reducing the number of parameters. Source code and pre-trained models are available at: https://github.com/kabbas570/CAMS-Net.	https://openaccess.thecvf.com//content/WACV2025/html/Khan_CAMS_Convolution_and_Attention-Free_Mamba-Based_Cardiac_Image_Segmentation_WACV_2025_paper.html	Abbas Khan, Muhammad Asad, Martin Benning, Caroline Roney, Gregory Slabaugh
CATALOG: A Camera Trap Language-Guided Contrastive Learning Model	Foundation Models (FMs) have been successful in various computer vision tasks like image classification object detection and image segmentation. However these tasks remain challenging when these models are tested on datasets with different distributions from the training dataset a problem known as domain shift. This is especially problematic for recognizing animal species in camera-trap images where we have variability in factors like lighting camoulage and occlusions. In this paper we propose the Camera Trap Language-guided Contrastive Learning (CATALOG) model to address these issues. Our approach combines multiple FMs to extract visual and textual features from camera-trap data and uses a contrastive loss function to train the model. We evaluate CATALOG on two benchmark datasets and show that it outperforms previous state-of-the-art methods in camera-trap image recognition especially when the training and testing data have different animal species or come from different geographical areas. Our approach demonstrates the potential of using FMs in combination with multi-modal fusion and contrastive learning for addressing domain shifts in camera-trap image recognition. The code of CATALOG is publicly available at https://github.com/Julian075/CATALOG.	https://openaccess.thecvf.com//content/WACV2025/html/Santamaria_CATALOG_A_Camera_Trap_Language-Guided_Contrastive_Learning_Model_WACV_2025_paper.html	Julian D. Santamaria, Claudia Isaza, Jhony H. Giraldo
CCASeg: Decoding Multi-Scale Context with Convolutional Cross-Attention for Semantic Segmentation	Capturing multi-scale context within feature maps is crucial for semantic segmentation. With the success of the Vision Transformer (ViT) recent models have been designed with transformer decoders to capture it. However these models face limitations in utilizing diverse contextual information due to the inherent nature of the attention mechanism and structural constraints. Typically multi-head attention which leads to similar receptive fields for each token feture is achieved at the expense of significantly increased computational cost. The nature of the structure can cause inconsistent combination of the information across different levels. To address this issue in this paper we propose a novel and effective decoding scheme CCASeg which is based on convolutional cross-attention (CCA). The proposed CCA along with the decoding structure is devised not only to capture both local and global context through convolutional kernels of various sizes but also to achieve high efficiency by effective utilization of the cheap convolution operations. Moreover the decoding structure which ensures the successive combination of information across various levels facilitates understanding of diverse contexts. Consequently this novel decoding scheme enables feature maps to effectively learn the relationships between objects of different sizes. In this way our proposed CCASeg outperforms previous state-of-the-art methods on popular semantic segmentation benchmarks including ADE20K Cityscapes COCO-stuff and iSAID.	https://openaccess.thecvf.com//content/WACV2025/html/Yoo_CCASeg_Decoding_Multi-Scale_Context_with_Convolutional_Cross-Attention_for_Semantic_Segmentation_WACV_2025_paper.html	Jiwon Yoo, Dami Ko, Gyeonghwan Kim
CE-VAE: Capsule Enhanced Variational AutoEncoderfor Underwater Image Enhancement	Unmanned underwater image analysis for marine monitoring faces two key challenges: (i) degraded image quality due to light attenuation and (ii) hardware storage constraints limiting high-resolution image collection. Existing methods primarily address image enhancement with approaches that hinge on storing the full-size input. In contrast we introduce the Capsule Enhanced Variational AutoEncoder (CE-VAE) a novel architecture designed to efficiently compress and enhance degraded underwater images. Our attention-aware image encoder can project the input image onto a latent space representation while being able to run online on a remote device. The only information that needs to be stored on the device or sent to a beacon is a compressed representation. There is a dual-decoder module that performs offline full-size enhanced image generation. One branch reconstructs spatial details from the compressed latent space while the second branch utilizes a capsule-clustering layer to capture entity-level structures and complex spatial relationships. This parallel decoding strategy enables the model to balance fine-detail preservation with context-aware enhancements. CE-VAE achieves state-of-the-art performance in underwater image enhancement on six benchmark datasets providing up to 3x higher compression efficiency than existing approaches. Code available at https://github.com/iN1k1/ce-vae-underwater-image-enhancement.	https://openaccess.thecvf.com//content/WACV2025/html/Pucci_CE-VAE_Capsule_Enhanced_Variational_AutoEncoderfor_Underwater_Image_Enhancement_WACV_2025_paper.html	Rita Pucci, Niki Martinel
CEMIL: Contextual Attention Based Efficient Weakly Supervised Approach for Histopathology Image Classification	"Multiple Instance Learning (MIL) has shown potential for analyzing Whole Slide Images (WSIs) in digital pathology but it faces challenges related to redundant information learning and generalization due to limited supervision and the computational complexity of Gigapixel WSIs. Many MIL-based methods apply a small weight matrix to all WSI patches. In this study we focus on developing computationally efficient models that improve MIL-based WSI classification by processing fewer patches while improving performance. We propose an attention-based approach using knowledge distillation where a compute-intensive ""instructor"" model analyzes all WSI patches to train a resource-efficient ""learner"" model which considers only a subset of patches. Comprehensive evaluations on four cancer subtype datasets--TCGA-BRCA TCGA-NSCLC TCGA-RCC and PANDA--demonstrate that an ""observe-everything"" instructor can effectively train an ""observe-minimally"" learner network. Overall our proposed learner network enhances performance by 4% compared to the state-of-the-art while reducing inference time by 45% and FLOPs by approximately 88%."	https://openaccess.thecvf.com//content/WACV2025/html/Rahman_CEMIL_Contextual_Attention_Based_Efficient_Weakly_Supervised_Approach_for_Histopathology_WACV_2025_paper.html	Tawsifur Rahman, Alexander S. Baras, Rama Chellappa
CIRCOD: Co-Saliency Inspired Referring Camouflaged Object Discovery	Camouflaged object detection (COD) the task of identifying objects concealed within their surroundings is often quite challenging due to the similarity that exists between the foreground and background. By incorporating an additional referring image where the target object is clearly visible we can leverage the similarities between the two images to detect the camouflaged object. In this paper we propose a novel problem setup: referring camouflaged object discovery (RCOD). In RCOD segmentation occurs only when the object in the referring image is also present in the camouflaged image; otherwise a blank mask is returned. This setup is particularly valuable when searching for specific camouflaged objects. Current COD methods are often generic leading to numerous false positives in applications focused on specific objects. To address this we introduce a new framework called Co-Saliency Inspired Referring Camouflaged Object Discovery (CIRCOD). Our approach consists of two main components: Co-Saliency-Aware Image Transformation (CAIT) and Co-Salient Object Discovery (CSOD). The CAIT module reduces the appearance and structural variations between the camouflaged and referring images while the CSOD module utilizes the similarities between them to segment the camouflaged object provided the images are semantically similar. Covering all semantic categories in current COD benchmark datasets we collected over 1000 referring images to validate our approach. Our extensive experiments demonstrate the effectiveness of our method and show that it achieves superior results compared to existing methods. Code is available at https://github.com/avigupta2798/CIRCOD/	https://openaccess.thecvf.com//content/WACV2025/html/Gupta_CIRCOD_Co-Saliency_Inspired_Referring_Camouflaged_Object_Discovery_WACV_2025_paper.html	Avi Gupta, Koteswar Rao Jerripothula, Tammam Tillo
CISOL: An Open and Extensible Dataset for Table Structure Recognition in the Construction Industry	Reproducibility and replicability are critical pillars of empirical research particularly in machine learning where they depend not only on the availability of models but also on the datasets used to train and evaluate those models. In this paper we introduce the Construction Industry Steel Ordering List (CISOL) dataset which was developed with a focus on transparency to ensure reproducibility replicability and extensibility. CISOL provides a valuable new research resource and highlights the importance of having diverse datasets even in niche application domains such as table extraction in civil engineering. CISOL is unique in that it contains real-world civil engineering documents from industry making it a distinctive contribution to the field. The dataset contains more than 120000 annotated instances in over 800 document images positioning it as a medium-sized dataset that provides a robust foundation for Table Structure Recognition (TSR) and Table Detection (TD) tasks. Benchmarking results show that CISOL achieves 67.22 mAP@0.5:0.95:0.05 using the YOLOv8 model outperforming the TSR-specific TATR model. This highlights the effectiveness of CISOL as a benchmark for advancing TSR especially in specialized domains.	https://openaccess.thecvf.com//content/WACV2025/html/Tschirschwitz_CISOL_An_Open_and_Extensible_Dataset_for_Table_Structure_Recognition_WACV_2025_paper.html	David Tschirschwitz, Volker Rodehorst
CL-Cross VQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering	Visual Question Answering (VQA) systems witnessed a significant advance in recent years due to the development of large-scale Vision-Language Pre-trained Models (VLPMs). As the application scenario and user demand change over time an advanced VQA system is expected to be capable of continuously expanding its knowledge and capabilities over time not only to handle new tasks (i.e. new question types or visual scenes) but also to answer questions in new specialized domains without forgetting previously acquired knowledge and skills. Existing works studying CL on VQA tasks primarily consider answer- and question-type incremental learning or scene- and function-incremental learning whereas how VQA systems perform when they encounter new domains and increasing user demands has not been studied. Motivated by this we introduce CL-CrossVQA a rigorous Continual Learning benchmark for Cross-domain Visual Question Answering through which we conduct extensive experiments on 4 VLPMs 5 CL approaches and 5 VQA datasets from different domains. In addition by probing the forgetting phenomenon of the intermediate layers we provide insights into how model architecture affects CL performance why CL approaches can help mitigate forgetting in VLPMs and how to design CL approaches suitable for VLPMs in this challenging continual learning environment. To facilitate future work on developing an advanced All-in-One VQA system we will release our datasets and code.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_CL-Cross_VQA_A_Continual_Learning_Benchmark_for_Cross-Domain_Visual_Question_WACV_2025_paper.html	Yao Zhang, Haokun Chen, Ahmed Frikha, Denis Krompass, Gengyuan Zhang, Jindong Gu, Volker Tresp
CLASS: Conditional Latent Architecture for Search and Synthesis of Design Layouts	We propose CLASS; a novel unified model for the synthesis and search for design layouts two tasks that are often handled separately by prior works. We propose to learn a compact and coherent latent feature of a layout supporting joint search and synthesis. This allows various operations such style-conditioned layout generation latent space manipulation and provides seamless integration of search and synthesis for an effective design workflow. We train CLASS with a dual decoder: a new transformer-based layout-conditioned decoder and a CNN-based raster decoder. The latent-conditioned decoder explicitly conditions upon a latent vector while generating a layout in an auto-regressive fashion. We train CLASS under variational framework which in conjunction with a raster-decoder enhances the latent representation improving both generation and retrieval performances. We show the effectiveness of CLASS on the RICO and PubLayNet benchmarks and demonstrate that CLASS is capable of high-quality synthesis from scratch as well as performing self-completion interpolation project between design layouts whilst achieving close to or better than state-of-the-art search performance.	https://openaccess.thecvf.com//content/WACV2025/html/Manandhar_CLASS_Conditional_Latent_Architecture_for_Search_and_Synthesis_of_Design_WACV_2025_paper.html	Dipu Manandhar, Paul Guerrero, Zhaowen Wang, John Collomosse
CLFace: A Scalable and Resource-Efficient Continual Learning Framework for Lifelong Face Recognition	An important aspect of deploying face recognition (FR) algorithms in real-world applications is their ability to learn new face identities from a continuous data stream. However the online training of existing deep neural network-based FR algorithms which are pre-trained offline on large-scale stationary datasets encounter two major challenges: 1) catastrophic forgetting of previously learned identities and 2) the need to store past data for complete retraining from scratch leading to significant storage constraints and privacy concerns. In this paper we introduce CLFace a continual learning framework designed to preserve and incrementally extend the learned knowledge. CLFace eliminates the classification layer resulting in a resource-efficient FR model that remains fixed throughout lifelong learning and provides label-free supervision to a student model making it suitable for open-set face recognition during incremental steps. We introduce an objective function that employs feature-level distillation to reduce drift between feature maps of the student and teacher models across multiple stages. Additionally it incorporates a geometry-preserving distillation scheme to maintain the orientation of the teacher model's feature embedding. Furthermore a contrastive knowledge distillation is incorporated to continually enhance the discriminative power of the feature representation by matching similarities between new identities. Experiments on several benchmark FR datasets demonstrate that CLFace outperforms baseline approaches and state-of-the-art methods on unseen identities using both in-domain and out-of-domain datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Hasan_CLFace_A_Scalable_and_Resource-Efficient_Continual_Learning_Framework_for_Lifelong_WACV_2025_paper.html	Mahedi Hasan, Shoaib Meraj Sami, Nasser Nasrabadi
CLIP-Fusion: A Spatio-Temporal Quality Metric for Frame Interpolation	Video frame interpolation is an ill-posed problem and a wide variety of methods have been proposed ranging from more traditional computer vision strategies to the most recent developments with neural network models. While there are many methods to interpolate video frames quality assessment regarding the resulting artifacts from these methods remains dependent on off-the-shelf methods. Although these methods can make accurate quality predictions for many visual artifacts such as compression blurring and banding their performance is mediocre for video frame interpolation artifacts due to the unique spatio-temporal qualities of such artifacts. To address this we aim to leverage semantic feature extraction capabilities of the pre-trained visual backbone of CLIP. Specifically we adapt its multi-scale approach to our feature extraction network and combine it with the spatio-temporal attention mechanism of the Video Swin Transformer. This allows our model to detect interpolation-related artifacts across frames and predict the relevant differential mean opinion score. Our model outperforms existing state-of-the-art quality metrics for assessing the quality of interpolated frames in both full-reference and no-reference settings.	https://openaccess.thecvf.com//content/WACV2025/html/Cokmez_CLIP-Fusion_A_Spatio-Temporal_Quality_Metric_for_Frame_Interpolation_WACV_2025_paper.html	Goksel Mert ÃÃ¶kmez, Yang Zhang, Christopher Schroers, TunÃ§ Ozan Aydin
CLIPArTT: Adaptation of CLIP to New Domains at Test Time	Pre-trained vision-language models (VLMs) exemplified by CLIP demonstrate remarkable adaptability across zero-shot classification tasks without additional training. However their performance diminishes in the presence of domain shifts. In this study we introduce CLIP Adaptation duRing Test-Time (CLIPArTT) a fully test-time adaptation (TTA) approach for CLIP which involves automatic text prompts construction during inference for their use as text supervision. Our method employs a unique minimally invasive text prompt tuning process wherein multiple predicted classes are aggregated into a single new text prompt used as pseudo label to re-classify inputs in a transductive manner. Additionally we pioneer the standardization of TTA benchmarks (e.g. TENT) in the realm of VLMs. Our findings demonstrate that without requiring additional transformations nor new trainable modules CLIPArTT enhances performance dynamically across non-corrupted datasets such as CIFAR-100 corrupted datasets like CIFAR-100-C and ImageNet-C alongside synthetic datasets such as VisDA-C. This research underscores the potential for improving VLMs' adaptability through novel test-time strategies offering insights for robust performance across varied datasets and environments. The code can be found at: https://github.com/dosowiechi/CLIPArTT.git	https://openaccess.thecvf.com//content/WACV2025/html/Hakim_CLIPArTT_Adaptation_of_CLIP_to_New_Domains_at_Test_Time_WACV_2025_paper.html	Gustavo A Vargas Hakim, David Osowiechi, Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers
CLIPScope: Enhancing Zero-Shot OOD Detection with Bayesian Scoring	Detection of out-of-distribution (OOD) samples is crucial for safe real-world deployment of machine learning models. Recent advances in vision language foundation models have made them capable of detecting OOD samples without requiring in-distribution (ID) images. However these zero-shot methods often underperform as they do not adequately consider ID class likelihoods in their detection confidence scoring. Hence we introduce CLIPScope a zero-shot OOD detection approach that normalizes the confidence score of a sample by class likelihoods akin to a Bayesian posterior update. Furthermore CLIPScope incorporates a novel strategy to mine OOD classes from a large lexical database. It selects class labels that are farthest and nearest to ID classes in terms of CLIP embedding distance to maximize coverage of OOD samples. We conduct extensive ablation studies and empirical evaluations demonstrating state of the art performance of CLIPScope across various OOD detection benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Fu_CLIPScope_Enhancing_Zero-Shot_OOD_Detection_with_Bayesian_Scoring_WACV_2025_paper.html	Hao Fu, Naman Patel, Prashanth Krishnamurthy, Farshad khorrami
CLIPping Imbalances: A Novel Evaluation Baseline and PEARL Dataset for Pedestrian Attribute Recognition	Pedestrian Attribute Recognition (PAR) serves as a fundamental task in computer vision and is crucial for upgrading security systems. It helps in precisely identifying and characterizing various attributes of pedestrians. However current PAR datasets have certain issues in representing a wide range of attributes correctly which makes the existing PAR methods less effective in real-world scenarios. Addressing this limitation this paper introduces PEARL a comprehensive dataset comprising of diverse pedestrian images annotated with 146 attributes. These samples have been sourced from surveillance videos across twelve countries. This paper also formulates an image-based PAR using language-image fusion strategy and utilizes CLIP as a new evaluation baseline. Specifically we leverage textual information by transforming sets of attributes into meaningful sentences. Addressing the inherent data imbalance in PAR we provide three types of prompt settings to optimize the training of the CLIP model. Our evaluation encompasses a thorough assessment of the proposed baseline model across various datasets including PEARL dataset as well as established PAR benchmarks such as PA100K RAP and PETA.	https://openaccess.thecvf.com//content/WACV2025/html/Vijay_CLIPping_Imbalances_A_Novel_Evaluation_Baseline_and_PEARL_Dataset_for_WACV_2025_paper.html	Kamalakar Vijay, Lalit Lohani, Kamakshya Prasad Nayak, Debi Prosad Dogra, Heeseung Choi, Hyungjoo Jung, Ig-Jae Kim
CM3T: Framework for Efficient Multimodal Learning for Inhomogeneous Interaction Datasets	Challenges in cross-learning involve inhomogeneous or even inadequate amount of training data and lack of resources for retraining large pretrained models. Inspired by transfer learning techniques in NLP adapters and prefix tuning this paper presents a new model-agnostic plugin architecture for cross-learning called CM3T that allows transformer-based models to be able adapt to new or missing information. We introduce two adapter blocks: multi-head vision adapters for transfer learning and cross-attention adapters for multimodal learning. Training becomes substantially efficient as the backbone and other plugins do not need to be finetuned along with these additions. Experiments and ablation studies on three datasets - Epic-Kitchens-100 MPIIGroupInteraction and UDIVA v0.5 - show the efficacy of this framework on different recording settings and tasks. With only 12.8% trainable parameters compared to the backbone to process video input and 22.3% trainable parameters for two additional modalities we achieve comparable and even better results than the state-of-the-art. Compared to similar methods our work achieves this result without any specific requirements for training or pretraining and is a step towards bridging the gap between a general model and specific practical applications in the field of video classification.	https://openaccess.thecvf.com//content/WACV2025/html/Agrawal_CM3T_Framework_for_Efficient_Multimodal_Learning_for_Inhomogeneous_Interaction_Datasets_WACV_2025_paper.html	Tanay Agrawal, Mohammed Guermal, Michal Balazia, Francois Bremond
COSNet: A Novel Semantic Segmentation Network using Enhanced Boundaries in Cluttered Scenes	Automated waste recycling aims to efficiently separate the recyclable objects from the waste by employing vision-based systems. However the presence of varying shaped objects having different material types makes it a challenging problem especially in cluttered environments. Existing segmentation methods perform reasonably on many semantic segmentation datasets by employing multi-contextual representations however their performance is degraded when utilized for waste object segmentation in cluttered scenarios. In addition plastic objects further increase the complexity of the problem due to their translucent nature. To address these limitations we introduce an efficacious segmentation network named COSNet that uses boundary cues along with multi-contextual information to accurately segment the objects in cluttered scenes. COSNet introduces novel components including feature sharpening block (FSB) and boundary enhancement module (BEM) for enhancing the features and highlighting the boundary information of irregular waste objects in cluttered environment. Extensive experiments on three challenging datasets including ZeroWaste-f SpectralWaste and ADE20K demonstrate the effectiveness of the proposed method. Our COSNet achieves a significant gain of 1.8% on ZeroWaste-f and 2.1% on SpectralWaste datasets respectively in terms of mIoU metric.	https://openaccess.thecvf.com//content/WACV2025/html/Ali_COSNet_A_Novel_Semantic_Segmentation_Network_using_Enhanced_Boundaries_in_WACV_2025_paper.html	Muhammad Ali, Mamoona Javaid, Mubashir Noman, Mustansar Fiaz, Salman Khan
CRAAC: Consistency Regularised Active Learning with Automatic Corrections for Real-Life Road Image Annotations	In annotating real-life large noisy and domain-specific images for digitising infrastructure substantial human effort persists despite past advancements. This research provides practical and interpretable scores for human annotators enabling flexible annotation strategies improving automation and reducing the effort required to create and correct image labels. The authors present the CRAAC solution: Consistency Regularised Active learning and Automatic Corrections which builds on Mask R-CNN with three additional modules: consistency regularisation scoring modules for active learning and automatic corrections. Experiments on our pavement image dataset recorded with a low silhouette score of 0.146 and qualitative annotation inconsistencies reduce the human effort of mouse clicks by 5-11% and improve the quality metrics of mAP and AR by approx. 40% from the original Mask R-CNN. The automatic correction further reduces the performance variation.	https://openaccess.thecvf.com//content/WACV2025/html/Lam_CRAAC_Consistency_Regularised_Active_Learning_with_Automatic_Corrections_for_Real-Life_WACV_2025_paper.html	Percy Lam, Sooyong Park, Weiwei Chen, Lavindra de Silva, Ioannis Brilakis
CRAFT: Class Ranking Aware Fine-Tuning for Enhanced Out-of-Distribution Detection	Out-of-distribution (OOD) detection remains a key challenge preventing the rollout of key AI technologies like autonomous vehicles into the mainstream as classifiers trained on in-distribution (ID) data are unable to gracefully handle OOD data. While OOD detection remains an active area of research, current post-hoc methods often suffer from limited separability between ID and OOD, and outlier exposure-based methods lack generalisation to unseen outlier types. We present CRAFT, a fine-tuning approach for arming pre-trained classifiers against OOD inputs without requiring access to outliers. The key insight that underpins our approach is that during pre-training, classifiers implicitly learn a ranking across the ID classes that is not respected by OOD data. Therefore, a form of fine-tuning without outliers of a pre-trained classifier can sharpen the rank order of the classes, making them sensitive to the presence of OOD data. Furthermore, the fine-tuned model does not impact the ability of the classifier to correctly classify ID inputs to their respective classes. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 demonstrate that CRAFT outperforms 33 existing methods, particularly in the more challenging near-OOD detection, as well as in overall OOD detection consistency and ID classification accuracy.	https://openaccess.thecvf.com//content/WACV2025/html/Karunanayake_CRAFT_Class_Ranking_Aware_Fine-Tuning_for_Enhanced_Out-of-Distribution_Detection_WACV_2025_paper.html	Naveen Karunanayake, Suranga Seneviratne, Sanjay Chawla
CRAFT: Designing Creative and Functional 3D Objects	For designing a wide range of everyday objects the design process should be aware of both the human body and the underlying semantics of the design specification. However these two objectives present significant challenges to the current AI-based designing tools. In this work we present a method to synthesize body-aware 3D objects from a base mesh given an input body geometry and either text or image as guidance. The generated objects can be simulated on virtual characters or fabricated for real-world use. We propose to use a mesh deformation procedure that optimizes for both semantic alignment as well as contact and penetration losses. Using our method users can generate both virtual or real-world objects from text image or sketch without the need for manual artist intervention. We present both qualitative and quantitative results on various object categories demonstrating the effectiveness of our approach.	https://openaccess.thecvf.com//content/WACV2025/html/Guo_CRAFT_Designing_Creative_and_Functional_3D_Objects_WACV_2025_paper.html	Michelle Guo, Mia Tang, Hannah Cha, Ruohan Zhang, C. Karen Liu, Jiajun Wu
CT to PET Translation: A Large-Scale Dataset and Domain-Knowledge-Guided Diffusion Approach	Positron Emission Tomography (PET) and Computed Tomography (CT) are essential for diagnosing staging and monitoring various diseases particularly cancer. Despite their importance the use of PET/CT systems is limited by the necessity for radioactive materials the scarcity of PET scanners and the high cost associated with PET imaging. In contrast CT scanners are more widely available and significantly less expensive. In response to these challenges our study addresses the issue of generating PET images from CT images aiming to reduce both the medical examination cost and the associated health risks for patients. Our contributions are twofold: First we introduce a conditional diffusion model named CPDM which to our knowledge is one of the initial attempts to employ a diffusion model for translating from CT to PET images. Second we provide the largest CT-PET dataset to date comprising 2028628 paired CT-PET images which facilitates the training and evaluation of CT-to-PET translation models. For the CPDM model we incorporate domain knowledge to develop two conditional maps: the Attention map and the Attenuation map. The former helps the diffusion process focus on areas of interest while the latter improves PET data correction and ensures accurate diagnostic information. Experimental evaluations across various benchmarks demonstrate that CPDM surpasses existing methods in generating high-quality PET images in terms of multiple metrics. The source code and data samples are available at https://github.com/thanhhff/CPDM.	https://openaccess.thecvf.com//content/WACV2025/html/Nguyen_CT_to_PET_Translation_A_Large-Scale_Dataset_and_Domain-Knowledge-Guided_Diffusion_WACV_2025_paper.html	Dac Thai Nguyen, Trung Thanh Nguyen, Huu Tien Nguyen, Thanh Trung Nguyen, Huy Hieu Pham, Thanh Hung Nguyen, Thao Nguyen Truong, Phi Le Nguyen
CTIP: Towards Accurate Tabular-to-Image Generation for Tire Footprint Generation	Generating images directly from tabular data while ensuring an accurate representation of ground truth is a useful application in manufacturing. Simply embedding tabular data to use it as a condition in image generation models often fails to learn the correspondence between tabular features and their impact on the generated image. To overcome this limitation we propose Contrastive TabularImage Pre-training (CTIP) inspired by the CLIP framework. These pre-train methods help improve the quality of the embedding of the tabular encoder on the tabular data which then helps improve the performance of the image generation model. CTIP uses contrastive learning on multiple tabular and image data pairs allowing the model to learn how changes in certain tabular features affect images. This approach is particularly crucial in manufacturing where accurate capture of product outcomes under varying conditions is essential. We demonstrate that applying CTIP enhances image generation performance yielding images that closely match ground truth images even in Feature Fewshot or Feature Zero-shot scenarios where specific features are sparse or novel. We further show the application of CTIP in tire development where tire footprint images are generated based on tire specifications and test conditions. CTIP produces high-quality embeddings that align well with ground truth images and effectively handle the scarcity or sparseness of specific features addressing common challenges in new product development. Our code is available in https://github.com/Noverse0/CTIP.git.	https://openaccess.thecvf.com//content/WACV2025/html/Roh_CTIP_Towards_Accurate_Tabular-to-Image_Generation_for_Tire_Footprint_Generation_WACV_2025_paper.html	Daeyoung Roh, Donghee Han, Jihyun Nam, Jungsoo Oh, Youngbin You, Jeongheon Park, Mun Yi
CUNSB-RFIE: Context-Aware Unpaired Neural Schrodinger Bridge in Retinal Fundus Image Enhancement	Retinal fundus photography is significant in diagnosing and monitoring retinal diseases. However systemic imperfections and operator/patient-related factors can hinder the acquisition of high-quality retinal images. Previous efforts in retinal image enhancement primarily relied on GANs which are limited by the trade-off between training stability and output diversity. In contrast the Schrodinger Bridge (SB) offers a more stable solution by utilizing Optimal Transport (OT) theory to model a stochastic differential equation (SDE) between two arbitrary distributions. This allows SB to effectively transform low-quality retinal images into their high-quality counterparts. In this work we leverage the SB framework to propose an image-to-image translation pipeline for retinal image enhancement. Additionally previous methods often fail to capture fine structural details such as blood vessels. To address this we enhance our pipeline by introducing Dynamic Snake Convolution whose tortuous receptive field can better preserve tubular structures. We name the resulting retinal fundus image enhancement framework the Context-aware Unpaired Neural Schrodinger Bridge (CUNSB-RFIE). To the best of our knowledge this is the first endeavor to use the SB approach for retinal image enhancement. Experimental results on a large-scale dataset demonstrate the advantage of the proposed method compared to several state-of-the-art supervised and unsupervised methods in terms of image quality and performance on downstream tasks. The code is available at https://github.com/Retinal-Research/CUNSB-RFIE.	https://openaccess.thecvf.com//content/WACV2025/html/Dong_CUNSB-RFIE_Context-Aware_Unpaired_Neural_Schrodinger_Bridge_in_Retinal_Fundus_Image_WACV_2025_paper.html	Xuanzhao Dong, Vamsi Krishna Vasa, Wenhui Zhu, Peijie Qiu, Xiwen Chen, Yi Su, Yujian Xiong, Zhangsihao Yang, Yanxi Chen, Yalin Wang
CabNIR: A Benchmark for In-Vehicle Infrared Monocular Depth Estimation	Accurate in-cabin depth estimation is critical for advancing automotive safety and occupant comfort. However existing datasets for in-vehicle scene understanding tasks often fall short in providing sufficient information and scale needed to evaluate existing depth estimation methods. In this paper we present a novel benchmark tailored for monocular depth estimation in vehicle interiors containing both near-infrared (NIR) images and corresponding ground truth depth data. Featuring over 41000 frames captured across 36 distinct vehicles and 45 different passengers it offers an unprecedented level of variability for this application domain. Evaluation on our testbench of cutting-edge single-view depth models in different flavors including zero-shot affine-invariant depth estimation or indomain specialization reveals that current depth estimation approaches while promising still have a significant performance gap to overcome before achieving the reliability required for downstream safety-critical applications. In light of its diverse range and complex scenarios we believe this benchmark could serve as a common reference for further research concerning in-cabin monocular depth estimation.	https://openaccess.thecvf.com//content/WACV2025/html/Cavalcanti_CabNIR_A_Benchmark_for_In-Vehicle_Infrared_Monocular_Depth_Estimation_WACV_2025_paper.html	Ugo Leone Cavalcanti, Matteo Poggi, Fabio Tosi, Valerio Cambareri, Vladimir Zlokolica, Stefano Mattoccia
Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding	Safety-critical 3D scene understanding tasks necessitate not only accurate but also confident predictions from 3D perception models. This study introduces Calib3D a pioneering effort to benchmark and scrutinize the reliability of 3D scene understanding models from an uncertainty estimation viewpoint. We comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D datasets uncovering insightful phenomena that cope with both the aleatoric and epistemic uncertainties in 3D scene understanding. We discover that despite achieving impressive levels of accuracy existing models frequently fail to provide reliable uncertainty estimates - a pitfall that critically undermines their applicability in safety-sensitive contexts. Through extensive analysis of key factors such as network capacity LiDAR representations rasterization resolutions and 3D data augmentation techniques we correlate these aspects directly with the model calibration efficacy. Furthermore we introduce DeptS a novel depth-aware scaling approach aimed at enhancing 3D model calibration. Extensive experiments across a wide range of configurations validate the superiority of our method. We hope this work could serve as a cornerstone for fostering reliable 3D scene understanding. Code and benchmark toolkit are publicly available.	https://openaccess.thecvf.com//content/WACV2025/html/Kong_Calib3D_Calibrating_Model_Preferences_for_Reliable_3D_Scene_Understanding_WACV_2025_paper.html	Lingdong Kong, Xiang Xu, Jun Cen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu
CamoFA: A Learnable Fourier-Based Augmentation for Camouflage Segmentation	Camouflaged object detection (COD) and camouflaged instance segmentation (CIS) aim to recognize and segment objects that are blended into their surroundings respectively. While several deep neural network models have been proposed to tackle those tasks augmentation methods for COD and CIS have not been thoroughly explored. Augmentation strategies can help improve models' performance by increasing the size and diversity of the training data and exposing the model to a wider range of variations in the data. Besides we aim to automatically learn transformations that help to reveal the underlying structure of camouflaged objects and allow the model to learn to identify better and segment camouflaged objects. To achieve this we propose a learnable augmentation method in the frequency domain for COD and CIS via the Fourier transform approach dubbed CamoFA. Our method leverages a conditional generative adversarial network and cross-attention mechanism to generate a reference image and an adaptive hybrid swapping with parameters to mix the low-frequency component of the reference image and the high-frequency component of the input image. This approach aims to make camouflaged objects more visible for detection and segmentation models. Without bells and whistles our proposed augmentation method boosts the performance of camouflaged object detectors and instance segmenters by large margins.	https://openaccess.thecvf.com//content/WACV2025/html/Le_CamoFA_A_Learnable_Fourier-Based_Augmentation_for_Camouflage_Segmentation_WACV_2025_paper.html	Minh-Quan Le, Minh-Triet Tran, Trung-Nghia Le, Tam V. Nguyen, Thanh-Toan Do
Can Adversarial Examples Be Parsed to Reveal Victim Model Information?	Numerous adversarial attack methods have been developed to generate imperceptible image perturbations that cause erroneous predictions in state-of-the-art machine learning (ML) models particularly deep neural networks (DNNs). Despite extensive research on adversarial examples limited efforts have been made to explore the hidden characteristics carried by these perturbations. In this study we investigate the feasibility of deducing information about the victim model (VM)--specifically characteristics such as architecture type kernel size activation function and weight sparsity--from adversarial examples. We approach this problem as a supervised learning task where we aim to attribute categories of VM characteristics to individual adversarial examples. To facilitate this we have assembled a dataset of adversarial attacks spanning seven types generated from 135 victim models systematically varied across five architecture types three kernel size configurations three activation functions and three levels of weight sparsity. We demonstrate that a supervised model parsing network (MPN) can effectively extract concealed details of the VM from adversarial examples. We also validate the practicality of this approach by evaluating the effects of various factors on parsing performance such as different input formats and generalization to out-of-distribution cases. Furthermore we highlight the connection between model parsing and attack transferability by showing how the MPN can uncover VM attributes in transfer attacks.	https://openaccess.thecvf.com//content/WACV2025/html/Yao_Can_Adversarial_Examples_Be_Parsed_to_Reveal_Victim_Model_Information_WACV_2025_paper.html	Yuguang Yao, Jiancheng Liu, Yifan Gong, Xiaoming Liu, Yanzhi Wang, Xue Lin, Sijia Liu
Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?	Publicly available satellite imagery such as Sentinel-2 often lacks the spatial resolution required for accurate analysis of remote sensing tasks including urban planning and disaster response. Current super-resolution techniques are typically trained on limited datasets leading to poor generalization across diverse geographic regions. In this work we propose a novel super-resolution framework that enhances generalization by incorporating geographic context through location embeddings. Our framework employs GANs and incorporates techniques from diffusion models to enhance image quality. Furthermore we address tiling artifacts by integrating information from neighboring images enabling the generation of seamless high-resolution outputs. We demonstrate the effectiveness of our method on the building segmentation task showing significant improvements over state-of-the-art methods and highlighting its potential for real-world applications.	https://openaccess.thecvf.com//content/WACV2025/html/Panangian_Can_Location_Embeddings_Enhance_Super-Resolution_of_Satellite_Imagery_WACV_2025_paper.html	Daniel Panangian, Ksenia Bittner
Can Multimodal Large Language Models Truly Perform Multimodal In-Context Learning?	Large Language Models (LLMs) with in-context learning (ICL) ability can quickly adapt to a specific context given a few demonstrations (demos). Recently Multimodal Large Language Models (MLLMs) built upon LLMs have also shown multimodal ICL ability i.e. responding to queries given a few multimodal demos including images queries and answers. While ICL has been extensively studied on LLMs its research on MLLMs remains limited. One essential question is whether these MLLMs can truly conduct multimodal ICL or if only the textual modality is necessary. We investigate this question by examining two primary factors that influence ICL: 1) Demo content i.e. understanding the influences of demo content in different modalities. 2) Demo selection strategy i.e. how to select better multimodal demos for improved performance. Experiments revealed that multimodal ICL is predominantly driven by the textual content whereas the visual information in the demos has little influence. Interestingly visual content is still necessary and useful for selecting demos to increase performance. Motivated by our analysis we propose a simple yet effective approach termed Mixed Modality In-Context Example Selection (MMICES) which considers both visual and language modalities when selecting demos. Extensive experiments are conducted to support our findings and verify the improvement brought by our method.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_Can_Multimodal_Large_Language_Models_Truly_Perform_Multimodal_In-Context_Learning_WACV_2025_paper.html	Shuo Chen, Zhen Han, Bailan He, Jianzhe Liu, Mark Buckley, Yao Qin, Philip Torr, Volker Tresp, Jindong Gu
Can Out-of-Domain Data Help to Learn Domain-Specific Prompts for Multimodal Misinformation Detection?	Spread of fake news using out-of-context images and captions has become widespread in this era of information overload. Since fake news can belong to different domains like politics sports etc. with their unique characteristics inference on a test image-caption pair is contingent on how well the model has been trained on similar data. Since training individual models for each domain is not practical we propose a novel framework termed DPOD (Domain-specific Prompt tuning using Out-of-Domain data) which can exploit out-of-domain data during training to improve fake news detection of all desired domains simultaneously. First to compute generalizable features we modify the Vision-Language Model CLIP to extract features that help to align the representations of the images and corresponding captions of both the in-domain and out-of-domain data in a label-aware manner. Further we propose a domain-specific prompt learning technique which leverages training samples of all the available domains based on the extent they can be useful to the desired domain. Extensive experiments on the large-scale NewsCLIPpings and VERITE benchmarks demonstrate that DPOD achieves state of-the-art performance for this challenging task. Code: https://github.com/scviab/DPOD.	https://openaccess.thecvf.com//content/WACV2025/html/Bhattacharya_Can_Out-of-Domain_Data_Help_to_Learn_Domain-Specific_Prompts_for_Multimodal_WACV_2025_paper.html	Amartya Bhattacharya, Debarshi Brahma, Suraj Nagaje, Anmol Asati, Vikas Verma, Soma Biswas
Cap2Aug: Caption Guided Image Data Augmentation	Visual recognition in a low-data regime is challenging and often prone to overfitting. To mitigate this issue several data augmentation strategies have been proposed. However standard transformations e.g. rotation cropping and flipping provide limited semantic variations. To this end we propose Cap2Aug an image-to-image diffusion model-based data augmentation strategy using image captions to condition the image synthesis step. We generate a caption for an image and use this caption as an additional input for an image-to-image diffusion model. This increases the semantic diversity of the augmented images due to caption conditioning compared to the usual data augmentation techniques. We show that Cap2Aug is particularly effective where only a few samples are available for an object class. However naively generating the synthetic images is not adequate due to the domain gap between real and synthetic images. Thus we employ a maximum mean discrepancy loss to align the synthetic images to the real images to minimize the domain gap. We evaluate our method on few-shot classification and image classification with long-tail class distribution tasks. Cap2Aug achieves state-of-the-art performance on both tasks while evaluated on eleven benchmarks. Code: https://github.com/aniket004/Cap_2_Aug.git	https://openaccess.thecvf.com//content/WACV2025/html/Roy_Cap2Aug_Caption_Guided_Image_Data_Augmentation_WACV_2025_paper.html	Aniket Roy, Anshul Shah, Ketul Shah, Anirban Roy, Rama Chellappa
CardioSyntax: End-to-End SYNTAX Score Prediction - Dataset Benchmark and Method	The SYNTAX score has become a widely used measure of coronary disease severity crucial in selecting the optimal mode of the revascularization procedure. This paper introduces a new medical regression and classification problem -- automatically estimating SYNTAX score from coronary angiography. Our study presents a comprehensive CardioSYNTAX dataset of 3018 patients for the SYNTAX score estimation and coronary dominance classification. The dataset features a balanced distribution of individuals with zero and non-zero scores. This dataset includes a first-of-its-kind complete coronary angiography samples captured through a multi-view X-ray video allowing one to observe coronary arteries from multiple perspectives. Furthermore we present a novel fully automatic end-to-end method for estimating the SYNTAX. For such a difficult task we have achieved a solid coefficient of determination R2 of 0.51 in score value prediction and 77.3% accuracy for zero score classification.	https://openaccess.thecvf.com//content/WACV2025/html/Ponomarchuk_CardioSyntax_End-to-End_SYNTAX_Score_Prediction_-_Dataset_Benchmark_and_Method_WACV_2025_paper.html	Alexander Ponomarchuk, Ivan Kruzhilov, Gleb Mazanov, Ruslan Utegenov, Artem Shadrin, Galina Zubkova, Ivan Bessonov, Pavel Blinov
Cascaded Dual Vision Transformer for Accurate Facial Landmark Detection	Facial landmark detection is a fundamental problem in computer vision for many downstream applications. This paper introduces a new facial landmark detector based on vision transformers which consists of two unique designs: Dual Vision Transformer (D-ViT) and Long Skip Connections (LSC). Based on the observation that the channel dimension of feature maps essentially represents the linear bases of the heatmap space we propose learning the interconnections between these linear bases to model the inherent geometric relations among landmarks via Channel-split ViT. We integrate such channel-split ViT into the standard vision transformer (i.e. spatial-split ViT) forming our Dual Vision Transformer to constitute the prediction blocks. We also suggest using long skip connections to deliver low-level image features to all prediction blocks thereby preventing useful information from being discarded by intermediate supervision. Extensive experiments are conducted to evaluate the performance of our proposal on the widely used benchmarks i.e. WFLW COFW and 300W demonstrating that our model outperforms the previous SOTAs across all three benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Dang_Cascaded_Dual_Vision_Transformer_for_Accurate_Facial_Landmark_Detection_WACV_2025_paper.html	Ziqiang Dang, Jianfang Li, Lin Liu
Channel Propagation Networks for Refreshable Vision Transformer	In this paper we introduce the Channel Propagation method which aims to increase the channels of the Vision Transformer systematically. Skip connections are commonly acknowledged as a propagation approach that improves the stability of the performance in Vision Transformers. Nevertheless it is important to note that these skip connections may give rise to the problem of over-smoothing wherein similar features are represented in multiple layers. To tackle this matter our proposed approach for Channel Propagation in Vision Transformers retains the present signal information while concurrently propagating location-specific signals in a newly introduced channel dimension. On the other hand the proposed Channel Propagation method effectively maintains the integrity of identity representation while simultaneously including patch-wise location-specific supervision by introducing a new channel dimension. The inclusion of this approach in Vision Transformers mitigates the issue of over-smoothing while also improving the performance of visual recognition tasks. In our experiments we confirm that the proposed method is effective for various visual recognition tasks. Specifically our method demonstrates enhanced performance when implemented on Vision Transformer models; the classification accuracy is increased considerably for plain and hierarchical architectures on the ImageNet dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Go_Channel_Propagation_Networks_for_Refreshable_Vision_Transformer_WACV_2025_paper.html	Junhyeong Go, Jongbin Ryu
CharDiff: Improving Sampling Convergence via Characteristic Function Consistency in Diffusion Models	Diffusion models have demonstrated extensive capabilities for generative modelling in both conditional and conditional image synthesis tasks. The reverse sampling has been the center of interest to improve the overall image quality without retraining the model from scratch. In this work we propose a plug-and-play module by utilizing the characteristic function of the distributions to minimize sampling drift. We experiment with existing diffusion solvers with our module in-place during denoising step to provide additional performance gain in image synthesis linear inverse problem tasks and text-conditioned image synthesis. Moreover We theoretically establish the effectiveness of the method in terms of improved Frechet Inception Distance (FID) and second order Tweedie moment for reduced trajectory deviation.	https://openaccess.thecvf.com//content/WACV2025/html/Sinha_CharDiff_Improving_Sampling_Convergence_via_Characteristic_Function_Consistency_in_Diffusion_WACV_2025_paper.html	Abhishek Kumar Sinha, S. Manthira Moorthi
ChromaDistill : Colorizing Monochrome Radiance Fields with Knowledge Distillation	Colorization is a well-explored problem in the domains of image and video processing. However extending colorization to 3D scenes presents significant challenges. Recent Neural Radiance Field (NeRF) and Gaussian-Splatting(3DGS) methods enable high-quality novel-view synthesis for multi-view images. However the question arises: How can we colorize these 3D representations? This work presents a method for synthesizing colorized novel views from input grayscale multi-view images. Using image or video colorization methods to colorize novel views from these 3D representations naively will yield output with severe inconsistencies. We introduce a novel method to use powerful image colorization models for colorizing 3D representations. We propose a distillation-based method that transfers color from these networks trained on natural images to the target 3D representation. Notably this strategy does not add any additional weights or computational overhead to the original representation during inference. Extensive experiments demonstrate that our method produces high-quality colorized views for indoor and outdoor scenes showcasing significant cross-view consistency advantages over baseline approaches. Our method is agnostic to the underlying 3D representation and easily generalizable to NeRF and 3DGS methods. Further we validate the efficacy of our approach in several diverse applications: 1.) Infra-Red (IR) multi-view images and 2.) Legacy grayscale multi-view image sequences. Project Webpage: https://val.cds.iisc.ac.in/chroma-distill.github.io/	https://openaccess.thecvf.com//content/WACV2025/html/Dhiman_ChromaDistill__Colorizing_Monochrome_Radiance_Fields_with_Knowledge_Distillation_WACV_2025_paper.html	Ankit Dhiman, Srinath R, Srinjay Sarkar, Lokesh Boregowda, Venkatesh Babu Radhakrishnan
Clarity Amidst Blur: A Deterministic Method for Synthetic Generation of Water Droplets on Camera Lenses	In computer vision image clarity is crucial particularly when challenges like water droplets on camera lenses can significantly impair the accurate analysis of visual data. While existing methods mainly focus on small droplets the impact of larger droplets has been largely overlooked. This paper introduces a novel approach that models water droplets using randomly generated points and Bezier curves to simulate their shape on the lens. Based on this geometric framework we developed a classifier to distinguish between two visual scenarios within the droplet. For larger droplets we use a heuristic method to simulate various lens blockages. We evaluate this simulation framework using a real stereo dataset from [19] with clear and soiled images. Our method evaluated using Mean Squared Error (MSE) and Structural Similarity Index Measure (SSIM) demonstrates superior performance in MSE while also achieving competitive results in SSIM compared to existing techniques for generating realistic water droplets. Additionally we applied this technique for data augmentation in object detection tasks using the YOLOv7 [25] model. The results show improved robustness especially in challenging conditions where large droplets obstruct the lens.	https://openaccess.thecvf.com//content/WACV2025/html/Eberhardt_Clarity_Amidst_Blur_A_Deterministic_Method_for_Synthetic_Generation_of_WACV_2025_paper.html	Tim Dieter Eberhardt, Tim BrÃ¼hl, Robin Schwager, Tin Stribor Sohn, Wilhelm Stork
Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation	Scene sketch semantic segmentation is a crucial task for various applications including sketch-to-image retrieval and scene understanding. Existing sketch segmentation methods treat sketches as bitmap images leading to the loss of temporal order among strokes due to the shift from vector to image format. Moreover these methods struggle to segment objects from categories absent in the training data. In this paper we propose a Class-Agnostic Visio-Temporal Network (CAVT) for scene sketch semantic segmentation. CAVT employs a class-agnostic object detector to detect individual objects in a scene and groups the strokes of instances through its post-processing module. This is the first approach that performs segmentation at both the instance and stroke levels within scene sketches. Furthermore there is a lack of free-hand scene sketch datasets with both instance and stroke-level class annotations. To fill this gap we collected the largest Free-hand Instance- and Stroke-level Scene Sketch Dataset (FrISS) that contains 1K scene sketches and covers 403 object classes with dense annotations. Extensive experiments on FrISS and other datasets demonstrate the superior performance of our method over state-of-the-art scene sketch segmentation models. Our code and dataset can be accessed from https://github.com/aleynakutuk6/CAVT.	https://openaccess.thecvf.com//content/WACV2025/html/Kutuk_Class-Agnostic_Visio-Temporal_Scene_Sketch_Semantic_Segmentation_WACV_2025_paper.html	Aleyna KÃ¼tÃ¼k, Tevfik Metin Sezgin
Class-Conditioned Transformation for Enhanced Robust Image Classification	Robust classification methods predominantly concentrate on algorithms that address a specific threat model resulting in ineffective defenses against other threat models. Real-world applications are exposed to this vulnerability as malicious attackers might exploit alternative threat models. In this work we propose a novel test-time threat model agnostic algorithm that enhances Adversarial-Trained (AT) models. Our method operates through COnditional image transformation and DIstance-based Prediction (CODIP) and includes two main steps: First we transform the input image into each dataset class where the input image might be either clean or attacked. Next we make a prediction based on the shortest transformed distance. The conditional transformation utilizes the perceptually aligned gradients property possessed by AT models and as a result eliminates the need for additional models or additional training. Moreover it allows users to choose the desired balance between clean and robust accuracy without training. The proposed method achieves state-of-the-art results demonstrated through extensive experiments on various models AT methods datasets and attack types. Notably applying CODIP leads to substantial robust accuracy improvement of up to +23% +20% +26% and +22% on CIFAR10 CIFAR100 ImageNet and Flowers datasets respectively.	https://openaccess.thecvf.com//content/WACV2025/html/Blau_Class-Conditioned_Transformation_for_Enhanced_Robust_Image_Classification_WACV_2025_paper.html	Tsachi Blau, Roy Ganz, Chaim Baskin, Michael Elad, Alex Bronstein
Click&Describe: Multimodal Grounding and Tracking for Aerial Objects	The fusion of multiple modalities such as vision and language has led to significant progress in grounding and tracking tasks. However this success has not yet translated to aerial single-object tracking (SOT) due to the lack of text annotations in existing aerial SOT datasets. To overcome this limitation we provide text annotations for five existing aerial datasets designed to support and promote multi-modal research in the aerial tracking domain. Furthermore to address challenges such as small object dimensions similar-looking objects and target size fluctuations we introduce a third input modality: click (or point prompt). To offer a user-friendly and interactive alternative to precise bounding box annotations we seamlessly integrate click and language information in the model's input. This enables approximate target specification with reduced effort and time. We introduce CLaVi a novel multimodal framework that redefines input interaction by incorporating multiple modalities. This integration improves target localization and tracking efficiency providing a significant advancement in the way input is provided to the model. Furthermore we conduct experiments on the five datasets to provide AerTrack-460 benchmark to validate the effectiveness of our approach. AerTrack-460 benchmark shows competitive performance and in some cases outperforms previous language-based grounding and tracking techniques setting a strong baseline for future research. Code and data will be made available soon.	https://openaccess.thecvf.com//content/WACV2025/html/Kukal_ClickDescribe_Multimodal_Grounding_and_Tracking_for_Aerial_Objects_WACV_2025_paper.html	Rupanjali Kukal, Jay Patravali, Fuxun Yu, Simranjit Singh, Nikolaos Karianakis, Rishi Madhok
Closing the Domain Gap in Manga Colorization via Aligned Paired Dataset	This paper addresses the challenge of artwork colorization by proposing a benchmark for manga colorization using real black-and-white and colorized image pairs. Color images are widely recognized for their ability to capture attention and improve memory retention yet the manual process of colorization is labor-intensive. Deep learning methods for supervised image-to-image translation offer a promising solution relying on aligned pairs of black-and-white and color images for training. However these pairs are often generated synthetically introducing a domain gap that limits model performance. To address this we explore the use of real data proposing a method for creating such datasets. Our benchmarks reveal that models trained on real data significantly outperform those trained on synthetic pairs. Furthermore we present a pipeline for text removal and panel segmentation streamlining the comic colorization process. These contributions aim to enhance the generalization and applicability of deep learning models for artwork colorization.	https://openaccess.thecvf.com//content/WACV2025/html/Golyadkin_Closing_the_Domain_Gap_in_Manga_Colorization_via_Aligned_Paired_WACV_2025_paper.html	Maksim Golyadkin, Ianis Plevokas, Ilya Makarov
CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving	Autonomous driving particularly navigating complex and unanticipated scenarios demands sophisticated reasoning and planning capabilities. While Multi-modal Large Language Models (MLLMs) offer a promising avenue for this their use has been largely confined to understanding complex environmental contexts or generating high-level driving commands with few studies extending their application to end-to-end path planning. A major research bottleneck is the lack of large-scale annotated datasets encompassing vision language and action. To address this issue we propose CoVLA (Comprehensive Vision-Language-Action) Dataset an extensive dataset comprising real-world driving videos spanning more than 80 hours. This dataset leverages a novel scalable approach based on automated data processing and a caption generation pipeline to generate accurate driving trajectories paired with detailed natural language descriptions of driving environments and maneuvers. This approach utilizes raw in-vehicle sensor data allowing it to surpass existing datasets in scale and annotation richness. Using CoVLA we investigate the driving capabilities of MLLMs that can handle vision language and action in a variety of driving scenarios. Our results illustrate the strong proficiency of our model in generating coherent language and action outputs emphasizing the potential of Vision-Language-Action (VLA) models in the field of autonomous driving. This dataset establishes a framework for robust interpretable and data-driven autonomous driving systems by providing a comprehensive platform for training and evaluating VLA models contributing to safer and more reliable self-driving vehicles. The dataset is released for academic purpose.	https://openaccess.thecvf.com//content/WACV2025/html/Arai_CoVLA_Comprehensive_Vision-Language-Action_Dataset_for_Autonomous_Driving_WACV_2025_paper.html	Hidehisa Arai, Keita Miwa, Kento Sasaki, Kohei Watanabe, Yu Yamaguchi, Shunsuke Aoki, Issei Yamamoto
ColFigPhotoAttnNet: Reliable Finger Photo Presentation Attack Detection Leveraging Window-Attention on Color Spaces	Finger photo Presentation Attack Detection (PAD) can significantly strengthen smartphone device security. Still these algorithms are trained to detect certain attack types. Furthermore they are designed to operate on images acquired by specific capture devices which leads to poor generalization and a lack of robustness in handling mobile hardware's evolving nature. The proposed investigation is the first to systematically analyze the performance degradation of existing deep learning PAD systems convolutional and transformers in cross-capture device settings. In this paper we introduce the ColFigPhotoAttnNet architecture designed based on window attention on color channels followed by the nested residual network as the predictor to achieve a reliable PAD. Extensive experiments using various capture devices including iPhone13 Pro GooglePixel 3 Nokia C5 and OnePlusOne were carried out to evaluate the performance of proposed and existing methods on three publicly available databases. The findings underscore the effectiveness of our approach.	https://openaccess.thecvf.com//content/WACV2025/html/Vurity_ColFigPhotoAttnNet_Reliable_Finger_Photo_Presentation_Attack_Detection_Leveraging_Window-Attention_on_WACV_2025_paper.html	Anudeep Vurity, Emanuela Marasco, Raghavendra Ramachandra, Jongwoo Park
ColorizeDiffusion: Improving Reference-Based Sketch Colorization with Latent Diffusion Model	"Diffusion models have achieved great success in dual-conditioned image generation. However they still face significant challenges in image-guided sketch colorization where reference and sketch images usually exhibit different spatial structures and semantics. This mismatch termed ""distribution shift"" in this paper results in various artifacts and degrades the colorization quality. To address this issue we conducted thorough investigations into the image-prompted latent diffusion model and developed a two-stage training framework to mitigate the effects of distribution shift based on our analysis. Comprehensive quantitative comparisons qualitative evaluations and user studies were performed to demonstrate the superiority of our proposed methods. Additionally an ablation study was conducted to assess the impact of the distribution shift and the selection of reference embeddings. Codes are made publicly available at https://github.com/tellurionkanata/colorizeDiffusion."	https://openaccess.thecvf.com//content/WACV2025/html/Yan_ColorizeDiffusion_Improving_Reference-Based_Sketch_Colorization_with_Latent_Diffusion_Model_WACV_2025_paper.html	Dingkun Yan, Liang Yuan, Erwin Wu, Yuma Nishioka, Issei Fujishiro, Suguru Saito
ComFace: Facial Representation Learning with Synthetic Data for Comparing Faces	Daily monitoring of intra-personal facial changes associated with health and emotional conditions has great potential to be useful for medical healthcare and emotion recognition fields. However the approach for capturing intra-personal facial changes is relatively unexplored due to the difficulty of collecting temporally changing face images. In this paper we propose a facial representation learning method using synthetic images for comparing faces called ComFace which is designed to capture intra-personal facial changes. For effective representation learning ComFace aims to acquire two feature representations i.e. inter-personal facial differences and intra-personal facial changes. The key point of our method is the use of synthetic face images to overcome the limitations of collecting real intra-personal face images. Facial representations learned by ComFace are transferred to three extensive downstream tasks for comparing faces: estimating facial expression changes weight changes and age changes from two face images of the same individual. Our ComFace trained using only synthetic data achieves comparable to or better transfer performance than general pre-training and state-of-the-art representation learning methods trained using real images.	https://openaccess.thecvf.com//content/WACV2025/html/Akamatsu_ComFace_Facial_Representation_Learning_with_Synthetic_Data_for_Comparing_Faces_WACV_2025_paper.html	Yusuke Akamatsu, Terumi Umematsu, Hitoshi Imaoka, Shizuko Gomi, Hideo Tsurushima
Combining Inherent Knowledge of Vision-Language Models with Unsupervised Domain Adaptation through Strong-Weak Guidance	Unsupervised domain adaptation (UDA) tries to overcome the tedious work of labeling data by leveraging a labeled source dataset and transferring its knowledge to a similar but different target dataset. Meanwhile current vision-language models exhibit remarkable zero-shot prediction capabilities. In this work we combine knowledge gained through UDA with the inherent knowledge of vision-language models. We introduce a strong-weak guidance learning scheme that employs zero-shot predictions to help align the source and target dataset. For the strong guidance we expand the source dataset with the most confident samples of the target dataset. Additionally we employ a knowledge distillation loss as weak guidance. The strong guidance uses hard labels but is only applied to the most confident predictions from the target dataset. Conversely the weak guidance is employed to the whole dataset but uses soft labels. The weak guidance is implemented as a knowledge distillation loss with (adjusted) zero-shot predictions. We show that our method complements and benefits from prompt adaptation techniques for vision-language models. We conduct experiments and ablation studies on three benchmarks (OfficeHome VisDA and DomainNet) outperforming state-of-the-art methods. Our ablation studies further demonstrate the contributions of different components of our algorithm.	https://openaccess.thecvf.com//content/WACV2025/html/Westfechtel_Combining_Inherent_Knowledge_of_Vision-Language_Models_with_Unsupervised_Domain_Adaptation_WACV_2025_paper.html	Thomas Westfechtel, Dexuan Zhang, Tatsuya Harada
Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation	Current generalizable object pose estimators i.e. approaches that do not need to be trained per object rely on accurate 3D models. Predominantly CAD models are used which can be hard to obtain in practice. At the same time it is often possible to acquire images of an object. Naturally this leads to the question of whether 3D models reconstructed from images are sufficient to facilitate accurate object pose estimation. We aim to answer this question by proposing a novel benchmark for measuring the impact of 3D reconstruction quality on pose estimation accuracy. Our benchmark provides calibrated images suitable for reconstruction and registered with the test images of the YCB-V dataset for pose evaluation under the BOP benchmark format. Detailed experiments with multiple state-of-the-art 3D reconstruction and object pose estimation approaches show that the geometry produced by modern reconstruction methods is often sufficient for accurate pose estimation. Our experiments lead to interesting observations: (1) Standard metrics for measuring 3D reconstruction quality are not necessarily indicative of pose estimation accuracy which shows the need for dedicated benchmarks such as ours. (2) Classical non-learning-based approaches can perform on par with modern learning-based reconstruction techniques and can even offer a better reconstruction time-pose accuracy tradeoff. (3) There is still a sizable gap between performance with reconstructed and with CAD models. To foster research on closing this gap the benchmark is made available at https://github.com/VarunBurde/reconstruction_pose_benchmark.	https://openaccess.thecvf.com//content/WACV2025/html/Burde_Comparative_Evaluation_of_3D_Reconstruction_Methods_for_Object_Pose_Estimation_WACV_2025_paper.html	Varun Burde, Assia Benbihi, Pavel Burget, Torsten Sattler
Comparative Knowledge Distillation	"In the era of large-scale pretrained models Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally-heavy teacher models to lightweight efficient student models while preserving performance. Yet KD settings often assume readily available access to teacher models capable of performing many inferences -- a notion increasingly at odds with the realities of costly large-scale models. Addressing this gap we study an important question: how KD algorithms fare as the number of teacher inferences decreases a setting we term Reduced-Teacher-Inference Knowledge Distillation (RTI-KD). We observe that the performance of prevalent KD techniques and state-of-the-art data augmentation strategies suffers considerably as the number of teacher inferences is reduced. One class of approaches termed ""relational"" knowledge distillation underperforms the rest yet we hypothesize that they hold promise for reduced dependency on teacher models because they can augment the effective dataset size without additional teacher calls. We find that a simple change -- performing high-dimensional comparisons instead of low-dimensional relations which we term ""Comparative Knowledge Distillation"" -- vaults performance well over existing KD approaches. We perform empirical evaluation across varied experimental settings and rigorous analysis to understand the learning outcomes of our method. All code is made publicly available."	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Comparative_Knowledge_Distillation_WACV_2025_paper.html	Alex Tianyi Xu, Alex Wilf, Paul Pu Liang, Alexander Obolenskiy, Daniel Fried, Louis-Philippe Morency
Composed Image Retrieval for Training-Free Domain Conversion	This work addresses composed image retrieval in the context of domain conversion where the content of a query image is retrieved in the domain specified by the query text. We show that a strong vision-language model provides sufficient descriptive power without additional training. The query image is mapped to the text input space using textual inversion. Unlike common practice that invert in the continuous space of text tokens we use the discrete word space via a nearest-neighbor search in a text vocabulary. With this inversion the image is softly mapped across the vocabulary and is made more robust using retrieval-based augmentation. Database images are retrieved by a weighted ensemble of text queries combining mapped words with the domain text. Our method outperforms prior art by a large margin on standard and newly introduced benchmarks. Code: https://github.com/NikosEfth/freedom	https://openaccess.thecvf.com//content/WACV2025/html/Efthymiadis_Composed_Image_Retrieval_for_Training-Free_Domain_Conversion_WACV_2025_paper.html	Nikos Efthymiadis, Bill Psomas, Zakaria Laskar, Konstantinos Karantzalos, Yannis Avrithis, Ondrej Chum, Giorgos Tolias
Compositional Segmentation of Cardiac Images Leveraging Metadata	Cardiac image segmentation is essential for automated cardiac function assessment and monitoring of changes in cardiac structures over time. Inspired by coarse-to-fine approaches in image analysis we propose a novel multitask compositional segmentation approach that can simultaneously localize the heart in a cardiac image and perform part-based segmentation of different regions of interest. We demonstrate that this compositional approach achieves better results than direct segmentation of the anatomies. Further we propose a novel Cross-Modal Feature Integration (CMFI) module to leverage the metadata related to cardiac imaging collected during image acquisition. We perform experiments on two different modalities MRI and ultrasound using public datasets Multi-Disease Multi-View and Multi-Centre (M&Ms-2) and Multi-structure Ultrasound Segmentation (CAMUS) data to showcase the efficiency of the proposed compositional segmentation method and Cross-Modal Feature Integration module incorporating metadata within the proposed compositional segmentation network. The source code is available: https://github.com/kabbas570/CompSeg-MetaData.	https://openaccess.thecvf.com//content/WACV2025/html/Khan_Compositional_Segmentation_of_Cardiac_Images_Leveraging_Metadata_WACV_2025_paper.html	Abbas Khan, Muhammad Asad, Martin Benning, Caroline Roney, Gregory Slabaugh
ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalizatio	Medical data often exhibits distribution shifts leading to performance degradation of deep learning models trained using standard supervised learning pipelines. Domain Generalization (DG) addresses this challenge with Single-Domain Generalization (SDG) being notably relevant due to the privacy and logistical constraints often inherent in medical data. Existing disentanglement-based SDG methods heavily rely on structural information from segmentation masks but classification labels do not offer similarly dense information. This work introduces a novel SDG method for medical image classification utilizing channel-wise contrastive disentanglement. The method is further refined with reconstruction-based style regularization to ensure distinct style and structural feature representations are extracted. We evaluate our method on the complex tasks of multicenter histopathology image classification and Diabetic Retinopathy (DR) grading in fundus images benchmarking it against state-of-the-art (SOTA) SDG baselines. Our results demonstrate that our method consistently outperforms the SOTA independently on the choice of the source domain while exhibiting greater performance stability. This study underscores the importance and challenges of exploring SDG frameworks for classification tasks. The code is publicly available at https://github.com/BioMedIA-MBZUAI/ConDiSR	https://openaccess.thecvf.com//content/WACV2025/html/Matsun_ConDiSR_Contrastive_Disentanglement_and_Style_Regularization_for_Single_Domain_Generalizatio_WACV_2025_paper.html	Aleksandr Matsun, Numan Saeed, Fadillah Adamsyah Maani, Mohammad Yaqub
Conceptual Learning via Embedding Approximations for Reinforcing Interpretability and Transparency	Concept bottleneck models (CBMs) have emerged as critical tools in domains where interpretability is paramount. These models rely on predefined textual descriptions referred to as concepts to inform their decision-making process and offer more accurate reasoning. As a result the selection of concepts used in the model is of utmost significance. This study proposes Conceptual Learning via Embedding Approximations for Reinforcing Interpretability and Transparency abbreviated as CLEAR a framework for constructing a CBM for image classification. Using score matching and Langevin sampling we approximate the embedding of concepts within the latent space of a vision-language model (VLM) by learning the scores associated with the joint distribution of images and concepts. A concept selection process is then employed to optimize the similarity between the learned embeddings and the predefined ones. The derived bottleneck offers insights into the CBM's decision-making process enabling more comprehensive interpretations. Our approach was evaluated through extensive experiments and achieved state-of-the-art performance on various benchmarks. The code for our experiments is available at https://github.com/clearProject/CLEAR/tree/main.	https://openaccess.thecvf.com//content/WACV2025/html/Dikter_Conceptual_Learning_via_Embedding_Approximations_for_Reinforcing_Interpretability_and_Transparency_WACV_2025_paper.html	Maor Dikter, Tsachi Blau, Chaim Baskin
Conditional GAN for Enhancing Diffusion Models in Efficient and Authentic Global Gesture Generation from Audios	Audio-driven simultaneous gesture generation is vital for human-computer communication AI games and film production. While previous research has shown promise there are still limitations. Methods based on VAEs are accompanied by issues of local jitter and global instability whereas methods based on diffusion models are hampered by low generation efficiency. This is because the denoising process of DDPM in the latter relies on the assumption that the noise added at each step is sampled from a unimodal distribution and the noise values are small. DDIM borrows the idea from the Euler method for solving differential equations disrupts the Markov chain process and increases the noise step size to reduce the number of denoising steps thereby accelerating generation. However simply increasing the step size during the step-by-step denoising process causes the results to gradually deviate from the original data distribution leading to a significant drop in the quality of the generated actions and the emergence of unnatural artifacts. In this paper we break the assumptions of DDPM and achieves breakthrough progress in denoising speed and fidelity. Specifically we introduce a conditional GAN to capture audio control signals and implicitly match the multimodal denoising distribution between the diffusion and denoising steps within the same sampling step aiming to sample larger noise values and apply fewer denoising steps for high-speed generation. In addition to enable the model to generate high-fidelity global gestures and avoid artifacts we introduce an explicit motion geometric loss to enhance the quality and global stability of the generated gestures. Numerous qualitative and quantitative experiments show that compared to contemporary diffusion-based methods our method offers faster generation speed and higher fidelity and compared to non-diffusion methods it provides a more stable global effect and a more natural user experience.	https://openaccess.thecvf.com//content/WACV2025/html/Cheng_Conditional_GAN_for_Enhancing_Diffusion_Models_in_Efficient_and_Authentic_WACV_2025_paper.html	Yongkang Cheng, Mingjiang Liang, Shaoli Huang, Gaoge Han, Jifeng Ning, Wei Liu
Context-Aware Optimal Transport Learning for Retinal Fundus Image Enhancement	Retinal fundus photography offers a non-invasive way to diagnose and monitor a variety of retinal diseases but is prone to inherent quality glitches arising from systemic imperfections or operator/patient-related factors. However high-quality retinal images are crucial for carrying out accurate diagnoses and automated analyses. The fundus image enhancement is typically formulated as a distribution alignment problem by finding a one-to-one mapping between a low-quality image and its high-quality counterpart. This paper proposes a context-informed optimal transport (OT) learning framework for tackling unpaired fundus image enhancement. In contrast to standard generative image enhancement methods which struggle with handling contextual information (e.g. over-tampered local structures and unwanted artifacts) the proposed context-aware OT learning paradigm better preserves local structures and minimizes unwanted artifacts. Leveraging deep contextual features we derive the proposed context-aware OT using the earth mover's distance and show that the proposed context-OT has a solid theoretical guarantee. Experimental results on a large-scale dataset demonstrate the superiority of the proposed method over several state-of-the-art supervised and unsupervised methods in terms of signal-to-noise ratio structural similarity index as well as two downstream tasks. The code is available at https://github.com/Retinal-Research/Contextual-OT.	https://openaccess.thecvf.com//content/WACV2025/html/Vasa_Context-Aware_Optimal_Transport_Learning_for_Retinal_Fundus_Image_Enhancement_WACV_2025_paper.html	Vamsi Krishna S Vasa, Peijie Qiu, Wenhui Zhu, Yujian Xiong, Oana Dumitrascu, Yalin Wang
Context-Aware Outlier Rejection for Robust Multi-View 3D Tracking of Similar Small Birds in An Outdoor Aviary	This paper presents a novel approach for robust 3D tracking of multiple birds in an outdoor aviary using a multi-camera system. Our method addresses the significant challenges posed by the visual similarity of birds and the complexities of their rapid movements in three-dimensional space. We leverage environmental context called landmarks to enhance feature matching between cameras leading to more accurate 3D reconstructions and tracking of their positions. Removing outliers in multi-view settings is especially challenging because the objects of interest are visually similar. In our approach outliers are rejected based on their nearest landmark. This leads to constructing precise 3D models of individual birds; additionally it applies these models to track multiple birds simultaneously. By utilizing environmental context our approach significantly improves the differentiation between visually similar birds a key obstacle in existing tracking systems. Experimental results demonstrate the effectiveness of our method showing a 20% elimination of outliers in the 3D reconstruction process with a 97% accuracy in matching. This remarkable accuracy in 3D modeling translates to robust and reliable tracking of multiple birds even in challenging outdoor conditions. Our work not only advances the field of computer vision but also provides a valuable tool for studying bird behavior and movement patterns in natural settings. We also provide a large annotated dataset of 80 birds residing in four enclosures for 20 hours of footage which provides a rich testbed for researchers in computer vision ornithologists and ecologists. Code and the link to the dataset is available at https://github.com/airou-lab/3D_Multi_Bird_Tracking.	https://openaccess.thecvf.com//content/WACV2025/html/Moradi_Context-Aware_Outlier_Rejection_for_Robust_Multi-View_3D_Tracking_of_Similar_WACV_2025_paper.html	Keon Moradi, Ethan Haque, Jasmeen Kaur, Alexandra B. Bentz, Eli S. Bridge, Golnaz Habibi
ContextIQ: A Multimodal Expert-Based Video Retrieval System for Contextual Advertising	Contextual advertising serves ads that are aligned to the content that the user is viewing. The rapid growth of video content on social platforms and streaming services along with privacy concerns has increased the need for contextual advertising. Placing the right ad in the right context creates a seamless and pleasant ad viewing experience resulting in higher audience engagement and ultimately better ad monetization. From a technology standpoint effective contextual advertising requires a video retrieval system capable of understanding complex video content at a very granular level. Current text-to-video retrieval models based on joint multimodal training demand large datasets and computational resources limiting their practicality and lacking the key functionalities required for ad ecosystem integration. We introduce ContextIQ a multimodal expert-based video retrieval system designed specifically for contextual advertising. ContextIQ utilizes modality-specific experts--video audio transcript (captions) and metadata such as objects actions emotion etc.--to create semantically rich video representations. We show that our system without joint training achieves better or comparable results to state-of-the-art models and commercial solutions on multiple text-to-video retrieval benchmarks. Our ablation studies highlight the benefits of leveraging multiple modalities for enhanced video retrieval accuracy instead of using a vision-language model alone. Furthermore we show how video retrieval systems such as ContextIQ can be used for contextual advertising in an ad ecosystem while also addressing concerns related to brand safety and filtering inappropriate content.	https://openaccess.thecvf.com//content/WACV2025/html/Chaubey_ContextIQ_A_Multimodal_Expert-Based_Video_Retrieval_System_for_Contextual_Advertising_WACV_2025_paper.html	Ashutosh Chaubey, Anoubhav Agarwaal, Sartaki Sinha Roy, Aayush Agrawal, Susmita Ghose
Continual Learning in 3D Point Clouds: Employing Spectral Techniques for Exemplar Selection	We introduce a novel framework for Continual learning in 3D object classification. Our approach is based on the selection of prototypes from each class using spectral clustering. For non-Euclidean data such as point clouds spectral clustering can be employed as long as one can define a distance measure between pairs of samples. Choosing the appropriate distance measure enables us to leverage 3D geometric characteristics to identify representative prototypes for each class. We explore the effectiveness of clustering in the input space (3D points) local feature space (1024-dimensional points) and global feature space. We conduct experiments on the ModelNet40 ShapeNet and ScanNet datasets achieving state-of-the-art accuracy exclusively through the use of input space features. By leveraging the combined input local and global features we have improved the state-of-the-art on ModelNet40 and ShapeNet utilizing nearly half the memory used by competing approaches. For the challenging ScanNet dataset our method enhances accuracy by 4.1% while consuming just 28% of the memory used by our competitors demonstrating the scalability of our approach.	https://openaccess.thecvf.com//content/WACV2025/html/Resani_Continual_Learning_in_3D_Point_Clouds_Employing_Spectral_Techniques_for_WACV_2025_paper.html	Hossein Resani, Behrooz Nasihatkon, Mohammadreza Alimoradi Jazi
Continual Learning of Personalized Generative Face Models with Experience Replay	We introduce a novel continual learning problem: how to sequentially update the weights of a personalized 2D and 3D generative face model as new batches of photos in different appearances styles poses and lighting are captured regularly. We observe that naive sequential fine-tuning of the model leads to catastrophic forgetting of past representations of the individual's face. We then demonstrate that a simple random sampling-based experience replay method is effective at mitigating catastrophic forgetting when a relatively large number of images can be stored and replayed. However for long-term deployment of these models with relatively smaller storage this simple random sampling-based replay technique also forgets past representations. Thus we introduce a novel experience replay algorithm that combines random sampling with StyleGAN's latent space to represent the buffer as an optimal convex hull. We observe that our proposed convex hull-based experience replay is more effective in preventing forgetting than a random sampling baseline and the lower bound. We introduce continual learning datasets for five celebrities along with the evaluation framework metrics and visualizations to examine this problem. See our project page for more details.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_Continual_Learning_of_Personalized_Generative_Face_Models_with_Experience_Replay_WACV_2025_paper.html	Annie N. Wang, Luchao Qi, Roni Sengupta
Continuous Spatio-Temporal Memory Networks for 4D Cardiac Cine MRI Segmentation	Current cardiac cine magnetic resonance image (cMR) studies focus on the end diastole (ED) and end systole (ES) phases while ignoring the abundant temporal information in the whole image sequence. This is because whole sequence segmentation is currently a tedious process and inaccurate. Conventional whole sequence segmentation approaches first estimate the motion field between frames which is then used to propagate the mask along the temporal axis. However the mask propagation results could be prone to error especially for the basal and apex slices where through-plane motion leads to significant morphology and structural change during the cardiac cycle. Inspired by recent advances in video object segmentation (VOS) based on spatio-temporal memory (STM) networks we propose a continuous STM (CSTM) network for semi-supervised whole heart and whole sequence cMR segmentation. Our CSTM network takes full advantage of the spatial scale temporal and through-plane continuity prior of the underlying heart anatomy structures to achieve accurate and fast 4D segmentation. Results of extensive experiments across multiple cMR datasets show that our method can improve the 4D cMR segmentation performance especially for the hard-to-segment regions. Project page is at https://github.com/DeepTag/CSTM.	https://openaccess.thecvf.com//content/WACV2025/html/Ye_Continuous_Spatio-Temporal_Memory_Networks_for_4D_Cardiac_Cine_MRI_Segmentation_WACV_2025_paper.html	Meng Ye, Bingyu Xin, Leon Axel, Dimitris Metaxas
Contrastive Learning of Image Representations Guided by Spatial Relations	The spatial information contained in images is of critical importance for many computer vision tasks. Current state-of-the-art approaches dealing with spatially-related tasks such as spatial relationship recognition are typically trained in a supervised manner with semantic information carried by annotations. However datasets containing spatial relations (such as VisualGenome and SpatialSense) contain many errors or ambiguities at the label level (e.g. polysemy of the relations use of different reference frames across relations) which might deteriorate the representation learning step. The representations and image embeddings obtained from this training setup carry poor spatial information as they are entangled with other modalities such as semantic information. To deal with this issue we introduce C-SIP (Contrastive Spatial-Image Pre-training) an approach aiming to learn better spatially-aware image representations more in agreement with human perception of a scene where spatial information is structuring. This training strategy focuses on the alignment of the embeddings of an image encoder and a spatial encoder optimized from the image content in a self-supervised manner. We showcase that training a model with spatial information at its core thanks to C-SIP allows for better spatially-aware image representations on three downstream tasks. These representations can be used in a zero-shot setting such as image retrieval or fine-tuned on semantic tasks such as visual question answering and provide better results compared with supervised counterparts. Our source code can be found at https://github.com/Logan-wilson/CSIP.	https://openaccess.thecvf.com//content/WACV2025/html/Servant_Contrastive_Learning_of_Image_Representations_Guided_by_Spatial_Relations_WACV_2025_paper.html	Logan Servant, MichaÃ«l ClÃ©ment, Laurent Wendling, Camille Kurtz
Contrastive Sequential-Diffusion Learning: Non-Linear and Multi-Scene Instructional Video Synthesis	Generated video scenes for action-centric sequence descriptions such as recipe instructions and do-it-yourself projects often include non-linear patterns where the next video may need to be visually consistent not with the immediately preceding video but with earlier ones. Current multi-scene video synthesis approaches fail to meet these consistency requirements. To address this we propose a contrastive sequential video diffusion method that selects the most suitable previously generated scene to guide and condition the denoising process of the next scene. The result is a multi-scene video that is grounded in the scene descriptions and coherent w.r.t. the scenes that require visual consistency. Experiments with action-centered data from the real world demonstrate the practicality and improved consistency of our model compared to previous work. Code and examples available at https://github.com/novasearch/CoSeD	https://openaccess.thecvf.com//content/WACV2025/html/Ramos_Contrastive_Sequential-Diffusion_Learning_Non-Linear_and_Multi-Scene_Instructional_Video_Synthesis_WACV_2025_paper.html	Vasco Ramos, Yonatan Bitton, Michal Yarom, Idan Szpektor, Joao Magalhaes
Controlling Human Shape and Pose in Text-to-Image Diffusion Models via Domain Adaptation	We present a methodology for conditional control of human shape and pose in pretrained text-to-image diffusion models using a 3D human parametric model (SMPL). Fine-tuning these diffusion models to adhere to new conditions requires large datasets and high-quality annotations which can be more cost-effectively acquired through synthetic data generation rather than real-world data. However the domain gap and low scene diversity of synthetic data can compromise the pretrained model's visual fidelity. We propose a domain-adaptation technique that maintains image quality by isolating synthetically trained conditional information in the classifier-free guidance vector and composing it with another control network to adapt the generated images to the input domain. To achieve SMPL control we fine-tune a ControlNet-based architecture on the synthetic SURREAL dataset of rendered humans and apply our domain adaptation at generation time. Experiments demonstrate that our model achieves greater shape and pose diversity than the 2d pose-based ControlNet while maintaining the visual fidelity and improving stability proving its usefulness for downstream tasks such as human animation. Our code is available at https://ivpg.github.io/humanLDM.	https://openaccess.thecvf.com//content/WACV2025/html/Buchheim_Controlling_Human_Shape_and_Pose_in_Text-to-Image_Diffusion_Models_via_WACV_2025_paper.html	Benito Buchheim, Max Reimann, JÃ¼rgen DÃ¶llner
ConvMixFormer- A Resource-Efficient Convolution Mixer for Transformer-Based Dynamic Hand Gesture Recognition	Transformer models have demonstrated remarkable success in many domains such as natural language processing (NLP) and computer vision. With the growing interest in transformer-based architectures they are now utilized for gesture recognition. So we also explore and devise a novel ConvMixFormer architecture for dynamic hand gestures. The transformers use quadratic scaling of the attention features with the sequential data due to which these models are computationally complex and heavy. We have considered this drawback of the transformer and designed a resource-efficient model that replaces the self-attention in the transformer with the simple convolutional layer-based token mixer. The computational cost and the parameters used for the convolution-based mixer are comparatively less than the quadratic self-attention. Convolution-mixer helps the model capture the local spatial features that self-attention struggles to capture due to their sequential processing nature. Further an efficient gate mechanism is employed instead of a conventional feed-forward network in the transformer to help the model control the flow of features within different stages of the proposed model. This design uses fewer learnable parameters which is nearly half the vanilla transformer that helps in fast and efficient training. The proposed method is evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has achieved state-of-the-art results on single and multimodal inputs. We have also shown the parameter efficiency of the proposed ConvMixFormer model compared to other methods. The source code is available at https://github.com/mallikagarg/ConvMixFormer.	https://openaccess.thecvf.com//content/WACV2025/html/Garg_ConvMixFormer-_A_Resource-Efficient_Convolution_Mixer_for_Transformer-Based_Dynamic_Hand_Gesture_WACV_2025_paper.html	Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan
Copy or Not? Reference-Based Face Image Restoration with Fine Details	Reference-guided face restoration can have better identity preservation than non-reference-based methods. However existing methods can (a) easily produce artifacts possibly attributable to inefficient facial priors and (b) do not well preserve fine-grained facial details crucial for identity such as freckles tattoos and scars. In this work we propose solutions for these problems. (1) We incorporate a stronger facial prior generative facial prior (GFP) for reference-based face image restoration. (2) We identify an ambiguity and point out that traditional loss prevents the network from heavily copying facial features from the reference. To address this we set a new goal and come up with a new loss to realize the new goal. More specifically when the ground truth and reference are different (e.g. differences in wrinkles makeup facial hair etc.) which one should the output look like? As a simple example ground truth does not have a mole while reference has one. Traditional loss chose the ground truth which seems natural but then the network also learns to ignore reference's facial features; during testing the network often hesitates. Our new goal is to copy features from the reference as much as possible while maintaining semantic consistency with the degraded input. We propose to use spatial minimum loss and cycle consistency loss to realize the new goal and make the network copy features without hesitation. Using only a single reference image our proposed method is able to restore highly degraded images while accurately capturing fine-grained facial details. To our knowledge we are the first face restoration framework that is able to restore faces at this granularity. Code and data are available at https://github.com/RefineFIR/RefineFIR.	https://openaccess.thecvf.com//content/WACV2025/html/Chong_Copy_or_Not_Reference-Based_Face_Image_Restoration_with_Fine_Details_WACV_2025_paper.html	Min Jin Chong, Dejia Xu, Yi Zhang, Zhangyang Wang, David Forsyth, Gurunandan Krishnan, Yicheng Wu, Jian Wang
Corgi: Cached Memory Guided Video Generation	Text-to-Video generation has achieved remarkable progress with the rise of diffusion models. In this work we introduce Cached Memory-Guided Video Generation (Corgi) aiming to generate multi-scene videos with arbitrary number of video clips conditioned on input images and instruction prompts. This is a challenging task as traditional T2V methods often struggle to maintain the quality of longer videos due to the difficulties in preserving visual context from earlier scenes. We address this by introducing a cached memory mechanism that stores the key frames. Our multi-scene video generation process is explicitly conditioned on the cached memories to avoid forgetting the visual appearance of target subjects. Corgi shows significant improvement in multi-scene video generation compared to the prior art with up to 59.2% in long-term consistency and 7.6% in diversity.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_Corgi_Cached_Memory_Guided_Video_Generation_WACV_2025_paper.html	Xindi Wu, Uriel Singer, Zhaojiang Lin, Andrea Madotto, Xide Xia, Yifan Xu, Paul Crook, Xin Luna Dong, Seungwhan Moon
CorrFill: Enhancing Faithfulness in Reference-Based Inpainting with Correspondence Guidance in Diffusion Models	In the task of reference-based image inpainting an additional reference image is provided to restore a damaged target image to its original state. The advancement of diffusion models particularly Stable Diffusion allows for simple formulations in this task. However existing diffusion-based methods often lack explicit constraints on the correlation between the reference and damaged images resulting in lower faithfulness to the reference images in the inpainting results. In this work we propose CorrFill a training-free module designed to enhance the awareness of geometric correlations between the reference and target images. This enhancement is achieved by guiding the inpainting process with correspondence constraints estimated during inpainting utilizing attention masking in self-attention layers and an objective function to update the input tensor according to the constraints. Experimental results demonstrate that CorrFill significantly enhances the performance of multiple baseline diffusion-based methods including state-of-the-art approaches by emphasizing faithfulness to the reference images.	https://openaccess.thecvf.com//content/WACV2025/html/Liu_CorrFill_Enhancing_Faithfulness_in_Reference-Based_Inpainting_with_Correspondence_Guidance_in_WACV_2025_paper.html	Kuan-Hung Liu, Cheng-Kun Yang, Min-Hung Chen, Yu-Lun Liu, Yen-Yu Lin
Counting Guidance for High Fidelity Text-to-Image Synthesis	"Recently there have been significant improvements in the quality and performance of text-to-image generation largely due to the impressive results attained by diffusion models. However text-to-image diffusion models sometimes struggle to create high-fidelity content for the given input prompt. One specific issue is their difficulty in generating the precise number of objects specified in the text prompt. For example when provided with the prompt ""five apples and ten lemons on a table"" images generated by diffusion models often contain an incorrect number of objects. In this paper we present a method to improve diffusion models so that they accurately produce the correct object count based on the input prompt. We adopt a counting network that performs reference-less class-agnostic counting for any given image. We calculate the gradients of the counting network and refine the predicted noise for each step. To address the presence of multiple types of objects in the prompt we utilize novel attention map guidance to obtain high-quality masks for each object. Finally we guide the denoising process using the calculated gradients for each object. Through extensive experiments and evaluation we demonstrate that the proposed method significantly enhances the fidelity of diffusion models with respect to object count."	https://openaccess.thecvf.com//content/WACV2025/html/Kang_Counting_Guidance_for_High_Fidelity_Text-to-Image_Synthesis_WACV_2025_paper.html	Wonjun Kang, Kevin Galim, Hyung Il Koo, Nam Ik Cho
Covariance-Based Space Regularization for Few-Shot Class Incremental Learning	Few-shot Class Incremental Learning (FSCIL) presents a challenging yet realistic scenario which requires the model to continually learn new classes with limited labeled data (i.e. incremental sessions) while retaining knowledge of previously learned base classes (i.e. base sessions). Due to the limited data in incremental sessions models are prone to overfitting new classes and suffering catastrophic forgetting of base classes. To tackle these issues recent advancements resort to prototype-based approaches to constrain the base class distribution and learn discriminative representations of new classes. Despite the progress the limited data issue still induces ill-divided feature space leading the model to confuse the new class with old classes or fail to facilitate good separation among new classes. In this paper we aim to mitigate these issues by directly constraining the span of each class distribution from a covariance perspective. In detail we propose a simple yet effective covariance constraint loss to force the model to learn each class distribution with the same covariance matrix. In addition we propose a perturbation approach to perturb the few-shot training samples in the feature space which encourages the samples to be away from the weighted distribution of other classes. Regarding perturbed samples as new class data the classifier is forced to establish explicit boundaries between each new class and the existing ones. Our approach is easy to integrate into existing FSCIL approaches to boost performance. Experiments on three benchmarks validate the effectiveness of our approach achieving state-of-the-art performance.	https://openaccess.thecvf.com//content/WACV2025/html/Hu_Covariance-Based_Space_Regularization_for_Few-Shot_Class_Incremental_Learning_WACV_2025_paper.html	Yijie Hu, Guanyu Yang, Zhaorui Tan, Xiaowei Wang, Kaizhu Huang, Qiu-Feng Wang
CrackStructures and CrackEnsembles: The Power of Multi-View for 2.5D Crack Detection	While research on structural crack segmentation at the image level remains highly active progress beyond two dimensions has been limited. This stagnation is largely due to the lack of available data for crack detection in higher dimensions. To address this limitation we introduce CrackStructures a dataset tailored for real-world 2.5D crack segmentation encompassing 15 segments from five distinct structures. Additionally we present CrackEnsembles a complementary semi-synthetic dataset that combines real textures with synthetic geometry to enhance the development of learning-based algorithms. Coupled with a baseline for multi-view crack instance segmentation this work establishes a solid foundation for advancing algorithms that support real-world structural inspection.	https://openaccess.thecvf.com//content/WACV2025/html/Benz_CrackStructures_and_CrackEnsembles_The_Power_of_Multi-View_for_2.5D_Crack_WACV_2025_paper.html	Christian Benz, Volker Rodehorst
Crafting Distribution Shifts for Validation and Training in Single Source Domain Generalization	Single-source domain generalization attempts to learn a model on a source domain and deploy it to unseen target domains. Limiting access only to source domain data imposes two key challenges - how to train a model that can generalize and how to verify that it does. The standard practice of validation on the training distribution does not accurately reflect the model's generalization ability while validation on the test distribution is a malpractice to avoid. In this work we construct an independent validation set by transforming source domain images with a comprehensive list of augmentations covering a broad spectrum of potential distribution shifts in target domains. We demonstrate a high correlation between validation and test performance for multiple methods and across various datasets. The proposed validation achieves a relative accuracy improvement over the standard validation equal to 15.4% or 1.6% when used for method selection or learning rate tuning respectively. Furthermore we introduce a novel family of methods that increase the shape bias through enhanced edge maps. To benefit from the augmentations during training and preserve the independence of the validation set a k-fold validation process is designed to separate the augmentation types used in training and validation. The method that achieves the best performance on the augmented validation is selected from the proposed family. It achieves state-of-the-art performance on various standard benchmarks. Code at: https://github.com/NikosEfth/crafting-shifts	https://openaccess.thecvf.com//content/WACV2025/html/Efthymiadis_Crafting_Distribution_Shifts_for_Validation_and_Training_in_Single_Source_WACV_2025_paper.html	Nikos Efthymiadis, Giorgos Tolias, OndÅej Chum
Cross Image Feature Perturbation with Pseudo Label Fusion for Semi-Supervised Medical Image Segmentation	Semi-supervised medical image segmentation aims to leverage a limited set of labeled images alongside a substantial volume of unlabeled images to train semantic segmentation models. Existing studies often employ consistency regularization to maximize the utilization of unlabeled data thereby enhancing the model's robustness and accuracy. However the methods for constructing perturbations at image-level on unlabeled data are typically simplistic involving techniques such as color transformations additive noise which do not adequately leverage the precise and reliable supervisory information available from labeled images. To address this limitation in addition to image perturbation we propose a cross-image feature perturbation approach for semi-supervised medical image segmentation. This method utilizes feature information from labeled images to guide the refinement of ambiguous semantic representations in unlabeled images thereby expanding the perturbation space more effectively. Moreover recognizing the limitations of existing consistency regularization frameworks that rely on confidence thresholds to filter pseudo-labels we introduce an uncertainty-based pseudo-label fusion strategy. This strategy mitigates the effects of unreliable predictions caused by perturbations by calculating the uncertainty and using it as a weight during pseudo-label fusion. We have conducted extensive experiments on the 2D ACDC and 3D LA datasets. The results demonstrate that our approach achieves performance comparable to the current state-of-the-art (SOTA) methods.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Cross_Image_Feature_Perturbation_with_Pseudo_Label_Fusion_for_Semi-Supervised_WACV_2025_paper.html	Minxia Xu, Han Yang, Bo Song, Weida Hu, Jinshui Miao, Erkang Cheng
Cross-Aligned Fusion for Multimodal Understanding	Recent multimodal frameworks often grapple with semantic misalignment and noise impeding effective integration of diverse modalities. In order to solve this problem this study presents CaMN (Cross-aligned Multimodal Network) a framework designed to enhance multimodal understanding through a robust cross-alignment mechanism. Unlike conventional fusion methods our framework aligns features extracted from images text and graphs via a tailored loss function enabling seamless integration and exploitation of complementary information. Leveraging Abstract Meaning Representation (AMR) we extract intricate semantic structures from textual data enriching the multimodal representation with contextual depth. Furthermore to enhance robustness we employ a masked autoencoder to simulate noise-independent feature space. Through comprehensive evaluation on the crisisMMD dataset CaMN demonstrates superior performance in crisis event classification tasks highlighting its potential in advancing multimodal understanding across diverse domains. Our code is available at https://github.com/brillard1/CaMN.	https://openaccess.thecvf.com//content/WACV2025/html/Rajora_Cross-Aligned_Fusion_for_Multimodal_Understanding_WACV_2025_paper.html	Abhishek Rajora, Shubham Gupta, Suman Kundu
Cross-Domain Multi-Modal Few-Shot Object Detection via Rich Text	Cross-modal feature extraction and integration have led to steady performance improvements in few-shot learning tasks. However existing multi-modal object detection (MM-OD) methods degrade when facing significant domain shift and are sample insufficient. We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate domain shift. Specifically we study the Cross-Domain few-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based multi-modal few-shot object detection method that utilizes rich text semantic information as an auxiliary modality to achieve domain adaptation. Our proposed novel neural network contains a multi-modal feature aggregation module that aligns the vision and language support feature embeddings and a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce multi-modal feature alignment and thus to enhance the model's language understanding capability. We evaluate our model on common standard cross-domain object detection datasets and demonstrate that our approach considerably outperforms existing FSOD methods. Our implementation is publicly available: https://github.com/zshanggu/CDMM	https://openaccess.thecvf.com//content/WACV2025/html/Shangguan_Cross-Domain_Multi-Modal_Few-Shot_Object_Detection_via_Rich_Text_WACV_2025_paper.html	Zeyu Shangguan, Daniel Seita, Mohammad Rostami
Cross-Domain and Cross-Dimension Learning for Image-to-Graph Transformers	Direct image-to-graph transformation is a challenging task solving object detection and relationship prediction in a single model. Due to this task's complexity large training datasets are rare in many domains making the training of deep-learning methods challenging. This data sparsity necessitates transfer learning strategies akin to the state-of-the-art in general computer vision. In this work we introduce a set of methods enabling cross-domain and cross-dimension learning for image-to-graph transformers. We propose (1) a regularized edge sampling loss to effectively learn object relations in multiple domains with different numbers of edges (2) a domain adaptation framework for image-to-graph transformers aligning image- and graph-level features from different domains and (3) a projection function that allows using 2D data for training 3D transformers. We demonstrate our method's utility in cross-domain and cross-dimension experiments where we utilize labeled data from 2D road networks for simultaneous learning in vastly different target domains. Our method consistently outperforms standard transfer learning and self-supervised pretraining on challenging benchmarks such as retinal or whole-brain vessel graph extraction.	https://openaccess.thecvf.com//content/WACV2025/html/Berger_Cross-Domain_and_Cross-Dimension_Learning_for_Image-to-Graph_Transformers_WACV_2025_paper.html	Alexander H. Berger, Laurin Lux, Suprosanna Shit, Ivan Ezhof, Georgios Kaissis, Martin J. Menten, Daniel Rueckert, Johannes C. Paetzold
Cross-Modal Feature Alignment and MMD Improve Robustness of Prompt Tuning	Prompt Tuning has emerged as a prominent research paradigm for adapting vision-language models to various downstream tasks. However recent research indicates that prompt tuning methods often lead to overfitting due to limited training samples. In this paper we propose a Cross-modal Aligned Feature Tuning (CRAFT) method to address this issue. Cross-modal alignment is conducted by first selecting anchors from the alternative domain and deriving relative representations of the embeddings for the selected anchors. Optimizing for a feature alignment loss over anchor-aligned text and image modalities creates a more unified text-image common space. Overfitting in prompt tuning also deteriorates model performance on out-of-distribution samples. To further improve the prompt model's robustness we propose minimizing Maximum Mean Discrepancy (MMD) over the anchor-aligned feature spaces to mitigate domain shift. The experiment on four different prompt tuning structures consistently shows the improvement of our method with increases of up to 6.1% in the Base-to-Novel generalization task 5.8% in the group robustness task and 2.7% in the out-of-distribution tasks. The code is available at https://github.com/Jingchensun/Craft.	https://openaccess.thecvf.com//content/WACV2025/html/Sun_Cross-Modal_Feature_Alignment_and_MMD_Improve_Robustness_of_Prompt_Tuning_WACV_2025_paper.html	Jingchen Sun, Rohan Sharma, Vishnu Lokhande, Changyou Chen
Cross-Task Affinity Learning for Multitask Dense Scene Predictions	Multitask learning (MTL) has become prominent for its ability to predict multiple tasks jointly achieving better per-task performance with fewer parameters than single-task learning. Recently decoder-focused architectures have significantly improved multitask performance by refining task predictions using features from related tasks. However most refinement methods struggle to efficiently capture both local and long-range dependencies between task-specific representations and cross-task patterns. In this paper we introduce the Cross-Task Affinity Learning (CTAL) module a lightweight framework that enhances task refinement in multitask networks. CTAL effectively captures local and long-range cross-task interactions by optimizing task affinity matrices for parameter-efficient grouped convolutions without concern for information loss. Our results demonstrate state-of-the-art MTL performance for both CNN and transformer backbones using significantly fewer parameters than single-task learning.	https://openaccess.thecvf.com//content/WACV2025/html/Sinodinos_Cross-Task_Affinity_Learning_for_Multitask_Dense_Scene_Predictions_WACV_2025_paper.html	Dimitrios Sinodinos, Narges Armanfard
Cross-View Meets Diffusion: Aerial Image Synthesis with Geometry and Text Guidance	Aerial imagery analysis is critical for many research fields. However obtaining frequent high-quality aerial images is not always accessible due to its high effort and cost requirements. One solution is to use the Ground-to-Aerial (G2A) technique to synthesize aerial images from easily collectible ground images. However G2A is rarely studied because of its challenges including but not limited to the drastic view changes occlusion and range of visibility. In this paper we present a novel Geometric Preserving Ground-to-Aerial (G2A) image synthesis (GPG2A) model that can generate realistic aerial images from ground images. GPG2A consists of two stages. The first stage predicts the Bird's Eye View (BEV) segmentation (referred to as the BEV layout map) from the ground image. The second stage synthesizes the aerial image from the predicted BEV layout map and text descriptions of the ground image. To train our model we present a new multi-modal cross-view dataset namely VIGORv2 built upon VIGOR with newly collected aerial images maps and text descriptions. Our extensive experiments illustrate that GPG2A synthesizes better geometry-preserved aerial images than existing models. We also present two applications data augmentation for cross-view geo-localization and sketch-based region search to further verify the effectiveness of our GPG2A. The code and dataset are available at https://github.com/AhmadArrabi/GPG2A	https://openaccess.thecvf.com//content/WACV2025/html/Arrabi_Cross-View_Meets_Diffusion_Aerial_Image_Synthesis_with_Geometry_and_Text_WACV_2025_paper.html	Ahmad Arrabi, Xiaohan Zhang, Waqas Sultani, Chen Chen, Safwan Wshah
Crossroads of Continents: Automated Artifact Extraction for Cultural Adaptation with Large Multimodal Models	We present a comprehensive three-phase study to examine (1) the cultural understanding of Large Multimodal Models (LMMs) by introducing DALLE STREET a large-scale dataset generated by DALL-E 3 and validated by humans containing 9935 images of 67 countries and 10 concept classes; (2) the underlying implicit and potentially stereotypical cultural associations with a cultural artifact extraction task; and (3) an approach to adapt cultural representation in an image based on extracted associations using a modular pipeline CULTUREADAPT. We find disparities in cultural understanding at geographic sub-region levels with both open-source (LLaVA) and closed-source (GPT-4V) models on DALLE STREET and other existing benchmarks which we try to understand using over 18000 artifacts that we identify in association to different countries. Our findings reveal a nuanced picture of the cultural competence of LMMs highlighting the need to develop culture-aware systems.	https://openaccess.thecvf.com//content/WACV2025/html/Mukherjee_Crossroads_of_Continents_Automated_Artifact_Extraction_for_Cultural_Adaptation_with_WACV_2025_paper.html	Anjishnu Mukherjee, Ziwei Zhu, Antonios Anastasopoulos
CrowdMAC: Masked Crowd Density Completion for Robust Crowd Density Forecasting	A crowd density forecasting task aims to predict how the crowd density map will change in the future from observed past crowd density maps. However the past crowd density maps are often incomplete due to the miss-detection of pedestrians and it is crucial to develop a robust crowd density forecasting model against the miss-detection. This paper presents a MAsked crowd density Completion framework for crowd density forecasting (CrowdMAC) which is simultaneously trained to forecast future crowd density maps from partially masked past crowd density maps (i.e. forecasting maps from past maps with miss-detection) while reconstructing the masked observation maps (i.e. imputing past maps with miss-detection). Additionally we propose Temporal-Density-aware Masking (TDM) which non-uniformly masks tokens in the observed crowd density map considering the sparsity of the crowd density maps and the informativeness of the subsequent frames for the forecasting task. Moreover we introduce multi-task masking to enhance training efficiency. In the experiments CrowdMAC achieves state-of-the-art performance on seven large-scale datasets including SDD ETH-UCY inD JRDB VSCrowd FDST and croHD. We also demonstrate the robustness of the proposed method against both synthetic and realistic miss-detections. The code is released at https://fujiry0.github.io/CrowdMAC-project-page.	https://openaccess.thecvf.com//content/WACV2025/html/Fujii_CrowdMAC_Masked_Crowd_Density_Completion_for_Robust_Crowd_Density_Forecasting_WACV_2025_paper.html	Ryo Fujii, Ryo Hachiuma, Hideo Saito
CryoMAE: Few-Shot Cryo-EM Particle Picking with Masked Autoencoders	Cryo-electron microscopy (cryo-EM) emerges as a pivotal technology for determining the architecture of cells viruses and protein assemblies at near-atomic resolution. Traditional particle picking a key step in cryo-EM struggles with manual effort and automated methods' sensitivity to low signal-to-noise ratio (SNR) and varied particle orientations. Furthermore existing neural network (NN)-based approaches often require extensive labeled datasets limiting their practicality. To overcome these obstacles we introduce cryoMAE a novel approach based on few-shot learning that harnesses the capabilities of Masked Autoencoders (MAE) to enable efficient selection of single particles in cryo-EM images. Contrary to conventional NN-based techniques cryoMAE requires only a minimal set of positive particle images for training yet demonstrates high performance in particle detection. Furthermore the implementation of a self-cross similarity loss ensures distinct features for particle and background regions thereby enhancing the discrimination capability of cryoMAE. Experiments on large-scale cryo-EM datasets show that cryoMAE outperforms existing state-of-the-art (SOTA) methods improving 3D reconstruction resolution by up to 22.4%. Our code is available at: https://github.com/xulabs/aitom.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_CryoMAE_Few-Shot_Cryo-EM_Particle_Picking_with_Masked_Autoencoders_WACV_2025_paper.html	Chentianye Xu, Xueying Zhan, Min Xu
CusConcept: Customized Visual Concept Decomposition with Diffusion Models	Enabling generative models to decompose visual concepts from a single image is a complex and challenging problem. In this paper we study a new and challenging task customized concept decomposition wherein the objective is to leverage diffusion models to decompose a single image and generate visual concepts from various perspectives. To address this challenge we propose a two-stage framework CusConcept (short for Customized Visual Concept Decomposition) to extract customized visual concept embedding vectors that can be embedded into prompts for text-to-image generation. In the first stage CusConcept employs a vocabulary-guided concept decomposition mechanism to build vocabularies along human-specified conceptual axes. The decomposed concepts are obtained by retrieving corresponding vocabularies and learning anchor weights. In the second stage joint concept refinement is performed to enhance the fidelity and quality of generated images. We further curate an evaluation benchmark for assessing the performance of the open-world concept decomposition task. Our approach can effectively generate high-quality images of the decomposed concepts and produce related lexical predictions as secondary outcomes. Extensive qualitative and quantitative experiments demonstrate the effectiveness of CusConcept. Our code and data are available at https://github.com/xzLcan/CusConcept.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_CusConcept_Customized_Visual_Concept_Decomposition_with_Diffusion_Models_WACV_2025_paper.html	Zhi Xu, Shaozhe Hao, Kai Han
CycleCrash: A Dataset of Bicycle Collision Videos for Collision Prediction and Analysis	Self-driving research often underrepresents cyclist collisions and safety. To address this we present CycleCrash a novel dataset consisting of 3000 dashcam videos with 436347 frames that capture cyclists in a range of critical situations from collisions to safe interactions. This dataset enables 9 different cyclist collision prediction and classification tasks focusing on potentially hazardous conditions for cyclists and is annotated with collision-related cyclistrelated and scene-related labels. Next we propose Vid- NeXt a novel method that leverages a ConvNeXt spatial encoder and a non-stationary transformer to capture the temporal dynamics of videos for the tasks defined in our dataset. To demonstrate the effectiveness of our method and create additional baselines on CycleCrash we apply and compare 7 models along with a detailed ablation. We release the dataset and code at https://github.com/ DeSinister/CycleCrash/.	https://openaccess.thecvf.com//content/WACV2025/html/Desai_CycleCrash_A_Dataset_of_Bicycle_Collision_Videos_for_Collision_Prediction_WACV_2025_paper.html	Nishq Poorav Desai, Ali Etemad, Michael Greenspan
D-LUT: Photorealistic Style Transfer via Diffusion Process	Post-editing color in photographs is a crucial process for enhancing a photograph's aesthetic value. Traditionally this process has required a significant investment of time and manual effort. Previous color transfer algorithms achieved through encoder-decoder deep learning architectures have simplified this process. However these techniques may introduce artifacts and decrease image quality. Moreover previous approaches are not explainable making the method less user-friendly. In addition the computational requirements of these models limit their deployment across various devices. To address these challenges we introduce the Diffusion-based Look-Up Table (D-LUT). This approach is artifact-free explainable computationally efficient and does not require pretraining stage. It derives a 3D Look-Up Table (3D LUT) for transitioning between the color styles of different images. Specifically this 3D LUT is obtained using a score-matching algorithm followed by color distribution alignment through Langevin dynamics. Our proposed D-LUT approach has achieved state-of-the-art performance while requiring significantly less GPU memory than previous baselines. Importantly the 3D LUTs explicitly derived from the D-LUT algorithm enable color style transfer across broader visual modalities such as real-time color transfer for videos.	https://openaccess.thecvf.com//content/WACV2025/html/Li_D-LUT_Photorealistic_Style_Transfer_via_Diffusion_Process_WACV_2025_paper.html	Mujing Li, Guanjie Wang, Xingguang Zhang, Qifeng Liao, Chenxi Xiao
D2FP: Learning Implicit Prior for Human Parsing	Human parsing aims to segment human images into fine-grained semantic parts. Considering the underlying structure of the human body state-of-the-art methods typically depend on prior assumptions to represent intrinsic body relationships. However leveraging the same structural prior knowledge across various scenarios poses challenges in achieving stable prediction and requires additional network design efforts. To address these issues we introduce a novel method the Dynamic Dual Transformer for Parsing (D2FP) which dynamically learns the implicit prior structures of the human body. Specifically we derive input-dependent prior features from the learnable semantics of human images generating prior-embedded object queries accordingly before feeding them into the Transformer decoders. Our model includes three major components to effectively learn prior object queries: a prior extraction module a prior embedding module and a multi-scale dual Transformer decoder. Furthermore a novel prior enhancement strategy is introduced where the final decoded object queries provide structural clues to enhance initial prior features. Experimental results demonstrate the superiority and effectiveness of the proposed method across two well-known human parsing benchmarks: LIP and CIHP. Code and models are available at https://github.com/cvlab-yongin/D2FP.	https://openaccess.thecvf.com//content/WACV2025/html/Hong_D2FP_Learning_Implicit_Prior_for_Human_Parsing_WACV_2025_paper.html	Junyoung Hong, Hyeri Yang, Ye Ju Kim, Haerim Kim, Shinwoong Kim, Euna Shim, Kyungjae Lee
DAM: Dynamic Adapter Merging for Continual Video QA Learning	We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method named DAM uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting (ii) enable efficient adaptation to continually arriving datasets (iii) handle inputs from unknown datasets during inference and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference given a video-question sample from an unknown domain our method first uses the proposed non-parametric router function to compute a probability for each adapter reflecting how relevant that adapter is to the current video-question input instance. Subsequently the proposed dynamic adapter merging scheme aggregates all the adapter weights into a new adapter instance tailored for that particular test sample to compute the final VidQA prediction mitigating the impact of inaccurate router predictions and facilitating knowledge sharing across domains. Our DAM model outperforms prior state-of-the-art continual learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets spanning various domains. We further extend DAM to continual image classification and image QA and outperform prior methods by a large margin. The code will be publicly released.	https://openaccess.thecvf.com//content/WACV2025/html/Cheng_DAM_Dynamic_Adapter_Merging_for_Continual_Video_QA_Learning_WACV_2025_paper.html	Feng Cheng, Ziyang Wang, Yi-Lin Sung, Yan-Bo Lin, Mohit Bansal, Gedas Bertasius
DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation	Test Time Adaptation (TTA) has emerged as a practical solution to mitigate the performance degradation of Deep Neural Networks (DNNs) in the presence of corrupti noise affecting inputs. Existing approaches in TTA continuously adapt the DNN leading to excessive resource consumption and performance degradation due to accumulation of error stemming from lack of supervision. In this work we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to address such issues. Our key approach is to proactively learn latent representations of some corruption types each one associated with a sub-network state tailored to correctly classify inputs affected by that corruption. fter deployment DARDA adapts the DNN to previously unseen corruptions in an unsupervised fashion by (i) estimating the latent representation of the ongoing corruption; (ii) selecting the sub-network whose associated corruption is the closest in the latent space to the ongoing corruption; and (iii) adapting DNN state so that its representation matches the ongoing corruption. This way DARDA is more resource-efficient and can swiftly adapt to new distributions caused by different corruptions without requiring a large variety of input data. Through experiments with two popular mobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA reduces energy consumption and average cache memory footprint respectively by 1.74x and 2.64x with respect to the state of the art while increasing the performance by 10.4% 5.7% and 4.4% on CIFAR-10 CIFAR-100 and TinyImagenet.	https://openaccess.thecvf.com//content/WACV2025/html/Rifat_DARDA_Domain-Aware_Real-Time_Dynamic_Neural_Network_Adaptation_WACV_2025_paper.html	Shahriar Rifat, Jonathan Ashdown, Francesco Restuccia
DASC-SPT: Towards Self-Supervised Panoramic Semantic Segmentation	Self-Supervised Semantic Segmentation aiming to leverage masses of unlabeled data for boosting semantic segmentation has been rapidly emerging as an active task in recent years. However existing self-supervised semantic segmentation approaches mainly focus on planar images leaving multiple distorted objects encountered in panoramic images unexplored due to the formidable challenge of handling heterogeneous degrees of distortions across different locations. In this paper we propose a novel Self-Supervised Panoramic Semantic Segmentation model termed DASC-SPT built upon the mainstream contrastive learning framework. Towards distortions in panoramic images we present two structures to better learn from distorted features by applying planar images. For the input images of self-supervision we design a Spherical Projection Transformation (SPT) strategy that involves randomly projecting planar images onto various locations of the sphere to introduce the distortions. For pixel-wise distorted features we construct a Deformation-aware Sampling Consistency (DASC) framework to further utilize the shared content and discrepancies caused by different distortions of paired views where the deformation-aware consistency can be quantified on pixel-wise features. Both of the two components facilitate the model to adapt to distortions and boost panoramic semantic segmentation. Extensive comprehensive experiments on three panoramic datasets demonstrate the effectiveness and superiority of DASC-SPT approach.	https://openaccess.thecvf.com//content/WACV2025/html/Tan_DASC-SPT_Towards_Self-Supervised_Panoramic_Semantic_Segmentation_WACV_2025_paper.html	Tianlong Tan, Bin Chen, Hongliang Cao, Chenggang Yan, Yike Ma, Feng Dai
DDPM-CD: Denoising Diffusion Probabilistic Models as Feature Extractors for Remote Sensing Change Detection	Remote sensing change detection is crucial for understanding the dynamics of our planet's surface facilitating the monitoring of environmental changes evaluating human impact predicting future trends and supporting decision-making. In this work we introduce a novel approach for change detection that can leverage off-the-shelf unlabeled remote sensing images in the training process by pre-training a Denoising Diffusion Probabilistic Model (DDPM) - a class of generative models used in image synthesis. DDPMs learn the training data distribution by gradually converting training images into a Gaussian distribution using a Markov chain. During inference (i.e. sampling) they can generate a diverse set of samples closer to the training distribution starting from Gaussian noise achieving state-of-the-art image synthesis results. However in this work our focus is not on image synthesis but on utilizing it as a pre-trained feature extractor for the downstream application of change detection. Specifically we fine-tune a lightweight change classifier utilizing the feature representations produced by the pre-trained DDPM alongside change labels. Experiments conducted on the LEVIR-CD WHU-CD DSIFN-CD and CDD datasets demonstrate that the proposed DDPM-CD method significantly outperforms the existing self supervised state-of-the-art change detection methods in terms of F1 score IoU and overall accuracy highlighting the pivotal role of pre-trained DDPM as a feature extractor for downstream applications. Code and pre-trained models available at https://github.com/wgcban/ddpm-cd	https://openaccess.thecvf.com//content/WACV2025/html/Bandara_DDPM-CD_Denoising_Diffusion_Probabilistic_Models_as_Feature_Extractors_for_Remote_WACV_2025_paper.html	Wele Gedara Chaminda Bandara, Nithin Gopalakrishnan Nair, Vishal Patel
DDS: Decoupled Dynamic Scene-Graph Generation Network	Scene-graph generation involves creating a structural representation of the relationships between objects in a scene by predicting subject-object-relation triplets from input data. Existing methods show poor performance in detecting triplets outside of a predefined set primarily due to their reliance on dependent feature learning. To address this issue we propose DDS- a decoupled dynamic scene-graph generation network- that consists of two independent branches that can disentangle extracted features. The key innovation of the current paper is the decoupling of the features representing the relationships from those of the objects which enables the detection of novel object-relationship combinations. The DDS model is evaluated on three datasets and outperforms previous methods by a significant margin especially in detecting previously unseen triplets.	https://openaccess.thecvf.com//content/WACV2025/html/Iftekhar_DDS_Decoupled_Dynamic_Scene-Graph_Generation_Network_WACV_2025_paper.html	A S M Iftekhar, Raphael Ruschel, Satish Kumar, Suya You, B. S. Manjunath
DLCR: A Generative Data Expansion Framework via Diffusion for Clothes-Changing Person Re-ID	With the recent exhibited strength of generative diffusion models an open research question is if images generated by these models can be used to learn better visual representations. While this generative data expansion may suffice for easier visual tasks we explore its efficacy on a more difficult discriminative task: clothes-changing person re-identification (CC-ReID). CC-ReID aims to match people appearing in non-overlapping cameras even when they change their clothes across cameras. Not only are current CC-ReID models constrained by the limited diversity of clothing in current CC-ReID datasets but generating additional data that retains important personal features for accurate identification is a current challenge. To address this issue we propose DLCR a novel data expansion framework that leverages pre-trained diffusion and large language models (LLMs) to accurately generate diverse images of individuals in varied attire. We generate additional data for five benchmark CC-ReID datasets (PRCC CCVID LaST VC-Clothes and LTCC) and increase their clothing diversity by 10X totaling over 2.1M images generated. DLCR employs diffusion-based text-guided inpainting conditioned on clothing prompts constructed using LLMs to generate synthetic data that only modifies a subject's clothes while preserving their personally identifiable features. With this massive increase in data we introduce two novel strategies - progressive learning and test-time prediction refinement - that respectively reduce training time and further boosts CC-ReID performance. On the PRCC dataset we obtain a large top-1 accuracy improvement of 11.3% by training CAL a previous state of the art (SOTA) method with DLCR-generated data. We publicly release our code and generated data for each dataset here: https://github.com/CroitoruAlin/dlcr.	https://openaccess.thecvf.com//content/WACV2025/html/Siddiqui_DLCR_A_Generative_Data_Expansion_Framework_via_Diffusion_for_Clothes-Changing_WACV_2025_paper.html	Nyle Siddiqui, Florinel Alin Croitoru, Gaurav Kumar Nayak, Radu Tudor Ionescu, Mubarak Shah
DMPT: Decoupled Modality-Aware Prompt Tuning for Multi-Modal Object Re-Identification	Current multi-modal object re-identification approaches based on large-scale pre-trained backbones (i.e. ViT) have displayed remarkable progress and achieved excellent performance. However these methods usually adopt the standard full fine-tuning paradigm which requires the optimization of considerable backbone parameters causing extensive computational and storage requirements. In this work we propose an efficient prompt-tuning framework tailored for multi-modal object re-identification dubbed DMPT which freezes the main backbone and only optimizes several newly added decoupled modality-aware parameters. Specifically we explicitly decouple the visual prompts into modality-specific prompts which leverage prior modality knowledge from a powerful text encoder and modality-independent semantic prompts which extract semantic information from multi-modal inputs such as visible near-infrared and thermal-infrared. Built upon the extracted features we further design a Prompt Inverse Bind (PromptIBind) strategy that employs bind prompts as a medium to connect the semantic prompt tokens of different modalities and facilitates the exchange of complementary multi-modal information boosting final re-identification results. Experimental results on multiple common benchmarks demonstrate that our DMPT can achieve competitive results to existing state-of-the-art methods while requiring only 6.5% fine-tuning of the backbone parameters.	https://openaccess.thecvf.com//content/WACV2025/html/Lin_DMPT_Decoupled_Modality-Aware_Prompt_Tuning_for_Multi-Modal_Object_Re-Identification_WACV_2025_paper.html	Minghui Lin, Shu Wang, Xiang Wang, Jianhua Tang, Longbin Fu, Zhengrong Zuo, Nong Sang
DMRN: A Dynamical Multi-Order Response Network for the Robust Lung Airway Segmentation	Automated airway segmentation in CT images is crucial for lung diseases' diagnosis. However manual annotation scarcity hinders supervised learning efficacy while unlimited intensities and sample imbalance lead to discontinuity and false-negative issues. To address these challenges we propose a novel airway segmentation model named Dynamical Multi-order Response Network (DMRN) integrating the unsupervised and supervised learning in parallel to alleviate the label scarcity of airway. In the unsupervised branch (1) we propose several novel strategies of Dynamic Mask-Ratio (DMR) to enable the model to perceive context information of varying sizes mimicking the laws of human learning vividly; (2) we present a novel target of Multi-Order Normalized Responses (MONR) exploiting the distinct order exponential operation of raw images and oriented gradients to enhance the textural representations of bronchioles. For the supervised branch we directly predict the final full segmentation map by the large-ratio cube-masked input instead of full input. Ultimately we verify the method performance and robustness by training on normal lung disease datasets while testing on lung cancer COVID-19 and Lung fibrosis datasets. All experimental results have proved that our method exceeds state-of-the-art methods significantly. Code will be released in the future.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_DMRN_A_Dynamical_Multi-Order_Response_Network_for_the_Robust_Lung_WACV_2025_paper.html	Sheng Zhang, Jinge Wu, Junzhi Ning, Guang Yang
DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing	High-fidelity 3D reconstruction of common indoor scenes is crucial for VR and AR applications. 3D Gaussian splatting a novel differentiable rendering technique has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. In this work we explore the use of readily accessible geometric cues to enhance Gaussian splatting optimization in challenging ill-posed and textureless scenes. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction. Specifically we regularize the optimization procedure with depth information enforce local smoothness of nearby Gaussians and use off-the-shelf monocular networks to achieve better alignment with the true scene geometry. We propose an adaptive depth loss based on the gradient of color images improving depth estimation and novel view synthesis results over various baselines. Our simple yet effective regularization technique enables direct mesh extraction from the Gaussian representation yielding more physically accurate reconstructions of indoor scenes. Our code will be released in https://github.com/maturk/dn-splatter.	https://openaccess.thecvf.com//content/WACV2025/html/Turkulainen_DN-Splatter_Depth_and_Normal_Priors_for_Gaussian_Splatting_and_Meshing_WACV_2025_paper.html	Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala
DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models	Vision-language models (VLMs) e.g. CLIP have shown remarkable potential in zero-shot image classification. However adapting these models to new domains remains challenging especially in unsupervised settings where labeled data is unavailable. Recent research has proposed pseudo-labeling approaches to adapt CLIP in an unsupervised manner using unlabeled target data. Nonetheless these methods struggle due to noisy pseudo-labels resulting from the misalignment between CLIP's visual and textual representations. This study introduces DPA an unsupervised domain adaptation method for VLMs. DPA introduces the concept of dual prototypes acting as distinct classifiers along with the convex combination of their outputs thereby leading to accurate pseudo-label construction. Next it ranks pseudo-labels to facilitate robust self-training particularly during early training. Finally it addresses visual-textual misalignment by aligning textual prototypes with image prototypes to further improve the adaptation performance. Experiments on 13 downstream vision tasks demonstrate that DPA significantly outperforms zero-shot CLIP and the state-of-the-art unsupervised adaptation baselines.	https://openaccess.thecvf.com//content/WACV2025/html/Ali_DPA_Dual_Prototypes_Alignment_for_Unsupervised_Adaptation_of_Vision-Language_Models_WACV_2025_paper.html	Eman Ali, Sathira Silva, Muhammad Haris Khan
DSTR: Dual Scenes Transformer for Cross-Modal Fusion in 3D Object Detection	Increasing attention has been garnered by LiDAR points and multi-view images fusion based on Transformer to supplement another modality in 3D object detection. However most current methods perform data fusion based on the entire scene which entails substantial redundant background information and lacks fine-grained local details of the foreground objects to be detected. Furthermore global scene fusion results in coarse fusion granularity and the excessive redundancy leads to slow convergence and reduced accuracy. In this work a novel Dual Scenes Transformer pipeline (DSTR) which comprises a Global-Scene Integration (GSI) module Local-Scene Integration (LSI) module and Dual Scenes Fusion (DSF) module is presented to tackle the above challenge. Concretely features from point clouds and images are utilized for gathering the global scene information in GSI. The insufficiency issues of global scene fusion are addressed by extracting local instance features for both modalities in LSI supplementing GSI in a more fine-grained way. Furthermore DSF is proposed to aggregate the local scene to the global scene which fully explores dual-modal information. Experiments on the nuScenes dataset show that our DSTR has state-of-the-art (SOTA) performance in certain 3D object detection benchmark categories on validation and test sets.	https://openaccess.thecvf.com//content/WACV2025/html/Cai_DSTR_Dual_Scenes_Transformer_for_Cross-Modal_Fusion_in_3D_Object_WACV_2025_paper.html	Haojie Cai, Dongfu Yin, Fei Richard Yu, Siting Xiong
DT-LSD: Deformable Transformer-Based Line Segment Detection	Line segment detection is a fundamental low-level task in computer vision and improvements in this task can impact more advanced methods that depend on it. Most new methods developed for line segment detection are based on Convolutional Neural Networks (CNNs). Our paper seeks to address challenges that prevent the wider adoption of transformer-based methods for line segment detection. More specifically we introduce a new model called Deformable Transformer-based Line Segment Detection (DT-LSD) that supports cross-scale interactions and can be trained quickly. This work proposes a novel Deformable Transformer-based Line Segment Detector (DT-LSD) that addresses LETR's drawbacks. For faster training we introduce Line Contrastive DeNoising (LCDN) a technique that stabilizes the one-to-one matching process and speeds up training by 34X. We show that DT-LSD is faster and more accurate than its predecessor transformer-based model (LETR) and outperforms all CNN-based models in terms of accuracy. In the Wireframe dataset DT-LSD achieves 71.7 for sAP^10 and 73.9 for sAP^15; while 33.2 for sAP^10 and 35.1 for sAP^15 in the YorkUrban dataset. Code available at https://github.com/SebastianJanampa/DT-LSD.	https://openaccess.thecvf.com//content/WACV2025/html/Janampa_DT-LSD_Deformable_Transformer-Based_Line_Segment_Detection_WACV_2025_paper.html	Sebastian Janampa, Marios Pattichis
DTA: Dual Temporal-Channel-Wise Attention for Spiking Neural Networks	Spiking Neural Networks (SNNs) present a more energy-efficient alternative to Artificial Neural Networks (ANNs) by harnessing spatio-temporal dynamics and event-driven spikes. Effective utilization of temporal information is crucial for SNNs leading to the exploration of attention mechanisms to enhance this capability. Conventional attention operations either apply identical operation or employ non-identical operations across target dimensions. We identify that these approaches provide distinct perspectives on temporal information. To leverage the strengths of both operations we propose a novel Dual Temporal-channel-wise Attention (DTA) mechanism that integrates both identical/non-identical attention strategies. To the best of our knowledge this is the first attempt to concentrate on both the correlation and dependency of temporal-channel using both identical and non-identical attention operations. Experimental results demonstrate that the DTA mechanism achieves state-of-the-art performance on both static datasets (CIFAR10 CIFAR100 ImageNet-1k) and dynamic dataset (CIFAR10-DVS) elevating spike representation and capturing complex temporal-channel relationship. We open-source our code: https://github.com/MnJnKIM/DTA-SNN.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_DTA_Dual_Temporal-Channel-Wise_Attention_for_Spiking_Neural_Networks_WACV_2025_paper.html	Minje Kim, Minjun Kim, Xu Yang
Dance Any Beat: Blending Beats with Visuals in Dance Video Generation	Generating dance from music is crucial for advancing automated choreography. Current methods typically produce skeleton keypoint sequences instead of dance videos and lack the capability to make specific individuals dance which reduces their real-world applicability. These methods also require precise keypoint annotations complicating data collection and limiting the use of self-collected video datasets. To overcome these challenges we introduce a novel task: generating dance videos directly from images of individuals guided by music. This task enables the dance generation of specific individuals without requiring keypoint annotations making it more versatile and applicable to various situations. Our solution the Dance Any Beat Diffusion model (DabFusion) utilizes a reference image and a music piece to generate dance videos featuring various dance types and choreographies. The music is analyzed by our specially designed music encoder which identifies essential features including dance style movement and rhythm. DabFusion excels in generating dance videos not only for individuals in the training dataset but also for any previously unseen person. This versatility stems from its approach of generating latent optical flow which contains all necessary motion information to animate any person in the image. We evaluate DabFusion's performance using the AIST++ dataset focusing on video quality audio-video synchronization and motion-music alignment. We propose a 2D Motion-Music Alignment Score (2D-MM Align) which builds on the Beat Alignment Score to more effectively evaluate motion-music alignment for this new task. Experiments show that our DabFusion establishes a solid baseline for this innovative task. Video results can be found on our project page: https://DabFusion.github.io.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_Dance_Any_Beat_Blending_Beats_with_Visuals_in_Dance_Video_WACV_2025_paper.html	Xuanchen Wang, Heng Wang, Dongnan Liu, Weidong Cai
DarSwin-Unet: Distortion Aware Architecture	Wide angle fisheye images are becoming increasingly common for perception tasks in applications such as robotics security and mobility (e.g. drones avionics). However current models often either ignore the distortions in wide angle images or are not suitable to perform pixel-level tasks. In this paper we present an encoder-decoder model based on a radial transformer architecture that adapts to distortions in wide angle lenses by leveraging the physical characteristics defined by the radial distortion profile. In contrast to the original model which only performs classification tasks we introduce a U-Net architecture DarSwin-Unet designed for pixel level tasks. Furthermore we propose a novel strategy that minimizes sparsity when sampling the image for creating its input tokens. Our approach enhances the model capability to handle pixel-level tasks in wide angle fisheye images making it more effective for real-world applications. Compared to other baselines DarSwin-Unet achieves the best results across different datasets with significant gains when trained on bounded levels of distortions (very low low medium and high) and tested on all including out-of-distribution distortions. We demonstrate its performance on depth estimation and show through extensive experiments that DarSwin-Unet can perform zero-shot adaptation to unseen distortions of different wide angle lenses. The code and models are publicly available at https://lvsn.github.io/darswin-unet/.	https://openaccess.thecvf.com//content/WACV2025/html/Athwale_DarSwin-Unet_Distortion_Aware_Architecture_WACV_2025_paper.html	Akshaya Athwale, Ichrak Shili, Ãmile Bergeron, Ola Ahmad, Jean-Francois Lalonde
DashCop: Automated E-Ticket Generation for Two-Wheeler Traffic Violations using Dashcam Videos	Motorized two-wheelers are a prevalent and economical means of transportation particularly in the Asia-Pacific region. However hazardous driving practices such as triple riding and non-compliance with helmet regulations contribute significantly to accident rates. Addressing these violations through automated enforcement mechanisms can enhance traffic safety. In this paper we propose DashCop an end-to-end system for automated E-ticket generation. The system processes vehicle-mounted dashcam videos to detect two-wheeler traffic violations. Our contributions include: (1) a novel Segmentation and Cross-Association (SAC) module to accurately associate riders with their motorcycles (2) a robust cross-association-based tracking algorithm optimized for the simultaneous presence of riders and motorcycles and (3) the RideSafe-400 dataset a comprehensive annotated dashcam video dataset for triple riding and helmet rule violations. Our system demonstrates significant improvements in violation detection validated through extensive evaluations on the RideSafe-400 dataset. Project page: https://dash-cop.github.io/	https://openaccess.thecvf.com//content/WACV2025/html/Rawat_DashCop_Automated_E-Ticket_Generation_for_Two-Wheeler_Traffic_Violations_using_Dashcam_WACV_2025_paper.html	Deepti Rawat, Keshav Gupta, Aryamaan Basu Roy, Ravi Kiran Sarvadevabhatla
Data Augmentation for Image Classification using Generative AI	Scaling laws dictate that the performance of AI models is proportional to the amount of available data. Data augmentation is a promising solution to expanding the dataset size. Traditional approaches focused on augmentation using rotation translation and resizing. Recent approaches use generative AI models to improve dataset diversity. However the generative methods struggle with issues such as subject corruption and the introduction of irrelevant artifacts. In this paper we propose the Automated Generative Data Augmentation (AGA). The framework combines the utility of large language models (LLMs) diffusion models and segmentation models to augment data. AGA preserves foreground authenticity while ensuring background diversity. Specific contributions include: i) segment and superclass based object extraction ii) prompt diversity with combinatorial complexity using prompt decomposition and iii) affine subject manipulation. We evaluate AGA against state-of-the-art (SOTA) techniques on three representative datasets ImageNet CUB and iWildCam. The experimental evaluation demonstrates an accuracy improvement of 15.6% and 23.5% for in and out-of-distribution data compared to baseline models respectively. There is also 64.3% improvement in SIC score compared to the baselines.	https://openaccess.thecvf.com//content/WACV2025/html/Rahat_Data_Augmentation_for_Image_Classification_using_Generative_AI_WACV_2025_paper.html	Fazle Rahat, M Shifat Hossain, Md Rubel Ahmed, Sumit Kumar Jha, Rickard Ewetz
Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware Diffusion Models	In computer-assisted surgery automatically recognizing anatomical organs is crucial for understanding the surgical scene and providing intraoperative assistance. While machine learning models can identify such structures their deployment is hindered by the need for labeled diverse surgical datasets with anatomical annotations. Labeling multiple classes (i.e. organs) in a surgical scene is time-intensive requiring medical experts. Although synthetically generated images can enhance segmentation performance maintaining both organ structure and texture during generation is challenging. We introduce a multi-stage approach using diffusion models to generate multi-class surgical datasets with annotations. Our framework improves anatomy awareness by training organ specific models with an inpainting objective guided by binary segmentation masks. The organs are generated with an inference pipeline using pre-trained ControlNet to maintain the organ structure. The synthetic multi-class datasets are constructed through an image composition step ensuring structural and textural consistency. This versatile approach allows the generation of multi-class datasets from real binary datasets and simulated surgical masks. We thoroughly evaluate the generated datasets on image quality and downstream segmentation achieving a 15% improvement in segmentation scores when combined with real images.	https://openaccess.thecvf.com//content/WACV2025/html/Venkatesh_Data_Augmentation_for_Surgical_Scene_Segmentation_with_Anatomy-Aware_Diffusion_Models_WACV_2025_paper.html	Danush Kumar Venkatesh, Dominik Rivoir, Micha Pfeiffer, Fiona Kolbinger, Stefanie Speidel
Data Generation for Hardware-Friendly Post-Training Quantization	Zero-shot quantization (ZSQ) using synthetic data is a key approach for post-training quantization (PTQ) under privacy and security constraints. However existing data generation methods often struggle to effectively generate data suitable for hardware-friendly quantization where all model layers are quantized. We analyze existing data generation methods based on batch normalization (BN) matching and identify several gaps between synthetic and real data: 1) Current generation algorithms do not optimize the entire synthetic dataset simultaneously; 2) Data augmentations applied during training are often overlooked; and 3) A distribution shift occurs in the final model layers due to the absence of BN in those layers. These gaps negatively impact ZSQ performance particularly in hardware-friendly quantization scenarios. In this work we propose Data Generation for Hardware-Friendly Quantization (DGH) a novel method that addresses these gaps. DGH jointly optimizes all generated images regardless of the image set size or GPU memory constraints. To address data augmentation mismatches DGH includes a preprocessing stage that mimics the augmentation process and enhances image quality by incorporating natural image priors. Finally we propose a new distribution-stretching loss that aligns the support of the feature map distribution between real and synthetic data. This loss is applied to the model's output and can be adapted to various tasks. DGH demonstrates significant improvements in quantization performance across multiple tasks achieving up to a 30% increase in accuracy for hardware-friendly ZSQ in both classification and object detection often performing on par with real data.	https://openaccess.thecvf.com//content/WACV2025/html/Dikstein_Data_Generation_for_Hardware-Friendly_Post-Training_Quantization_WACV_2025_paper.html	Lior Dikstein, Ariel Lapid, Arnon Netzer, Hai Victor Habi
Data-Efficient 3D Visual Grounding via Order-Aware Referring	3D visual grounding aims to identify the target object within a 3D point cloud scene referred to by a natural language description. Previous works usually require significant data relating to point color and their descriptions to exploit the corresponding complicated verbo-visual relations. In our work we introduce Vigor a novel Data-Efficient 3D Visual Grounding framework via Order-aware Referring. Vigor leverages LLM to produce a desirable referential order from the input description for 3D visual grounding. With the proposed stacked object-referring blocks the predicted anchor objects in the above order allow one to locate the target object progressively without supervision on the identities of anchor objects or exact relations between anchor/target objects. We also present an order-aware warm-up training strategy which augments referential orders for pre-training the visual grounding framework allowing us to better capture the complex verbo-visual relations and benefit the desirable data-efficient learning scheme. Experimental results on the NR3D and ScanRefer datasets demonstrate our superiority in low-resource scenarios. In particular Vigor surpasses current state-of-the-art frameworks by 9.3% and 7.6% grounding accuracy under 1% data and 10% data settings on the NR3D dataset respectively.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_Data-Efficient_3D_Visual_Grounding_via_Order-Aware_Referring_WACV_2025_paper.html	Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank Wang
Data-Efficient Alignment in Medical Imaging via Reconfigurable Generative Networks	Recent advances in deep learning have witnessed many successful medical image translation models that learn correspondences between two visual domains. However building robust mappings between domains is a significant challenge when handling misalignments caused by factors such as respiratory motion and anatomical changes. This issue is further exacerbated in scenarios with limited data availability leading to a significant degradation in translation quality. In this paper we introduce a novel data-efficient framework for aligning medical images via Reconfigurable Generative Network (Reconfig-MIT) for high-quality image translation. The key idea of Reconfig-MIT is to adaptively expand the generative network width within a Generative Adversarial Networks (GAN) architecture initially expanding rapidly to capture low-level features and then slowing to refine high-level complexities. This dynamic network adaptation mechanism allows to adaptively learn at different rates thus the model can better respond to deviations in the data caused by misalignments while maintaining an effective equilibrium with the discriminator (D). We also introduce the Recursive Cycle-Consistency Loss (R-CCL) which extends the cycle consistency loss to effectively preserve key anatomical structures and their spatial relationships improving translation quality. Extensive experiments show that Reconfig-MIT is a generic framework that enables easy integration with existing image translation methods including those incorporating registration networks used for correcting misalignments and provides robust and high-quality translation on paired and unpaired misaligned data in both data-rich and data-limited scenarios. https://github.com/IntellicentAI-Lab/Reconfig-MIT.	https://openaccess.thecvf.com//content/WACV2025/html/Saxena_Data-Efficient_Alignment_in_Medical_Imaging_via_Reconfigurable_Generative_Networks_WACV_2025_paper.html	Divya Saxena, Jiannong Cao, Jiahao Xu, Tarun Kulshrestha
Dataset Augmentation by Mixing Visual Concepts	This paper proposes a dataset augmentation method by fine-tuning pre-trained diffusion models. Generating images using a pre-trained diffusion model with textual conditioning often results in domain discrepancy between real data and generated images. We propose a fine-tuning approach where we adapt the diffusion model by conditioning it with real images and novel text embeddings. We introduce a unique procedure called Mixing Visual Concepts (MVC) where we create novel text embeddings from image captions. The MVC enables us to generate multiple images which are diverse and yet similar to the real data enabling us to perform effective dataset augmentation. We perform comprehensive qualitative and quantitative evaluations with the proposed dataset augmentation approach showcasing both coarse-grained and fine-grained changes in generated images. Our approach outperforms state-of-the-art augmentation techniques on benchmark classification tasks. The code is available at https://github.com/rahatkutubi/MVC	https://openaccess.thecvf.com//content/WACV2025/html/Al_Rahat_Kutubi_Dataset_Augmentation_by_Mixing_Visual_Concepts_WACV_2025_paper.html	Md Abdullah Al Rahat Kutubi, Hemanth Venkateswara
DeCLIP: Decoding CLIP Representations for Deepfake Localization	Generative models can create entirely new images but they can also partially modify real images in ways that are undetectable to the human eye. In this paper we address the challenge of automatically detecting such local manipulations. One of the most pressing problems in deepfake detection remains the ability of models to generalize to different classes of generators. In the case of fully manipulated images representations extracted from large self-supervised models (such as CLIP) provide a promising direction towards more robust detectors. Here we introduce DeCLIP-- a first attempt to leverage such large pretrained features for detecting local manipulations. We show that when combined with a reasonably large convolutional decoder pretrained self-supervised representations are able to perform localization and improve generalization capabilities over existing methods. Unlike previous work our approach is able to perform localization on the challenging case of latent diffusion models where the entire image is affected by the fingerprint of the generator. Moreover we observe that this type of data which combines local semantic information with a global fingerprint provides more stable generalization than other categories of generative methods	https://openaccess.thecvf.com//content/WACV2025/html/Smeu_DeCLIP_Decoding_CLIP_Representations_for_Deepfake_Localization_WACV_2025_paper.html	Stefan Smeu, Elisabeta Oneata, Dan Oneata
Debiasify: Self-Distillation for Unsupervised Bias Mitigation	Simplicity bias is a critical challenge in neural networks since it often leads to favoring simpler solutions and learning unintended decision rules captured by spurious correlations causing models to be biased and diminishing their generalizability. While existing solutions rely on human supervision obtaining annotations of the different bias attributes is often impractical. To tackle this we present Debiasify a novel self-distillation approach that works without any prior information about the nature of biases. Our method leverages a new distillation loss to distill knowledge within a network; from a deep layer where complex highly-predictive features reside to a shallow layer where simpler yet attribute-conditioned features are found in an unsupervised manner. In this way Debiasify learns robust debiased representations that generalize well across various biases and datasets enhancing worst-group performance and overall accuracy. Extensive experiments on computer vision and medical imaging benchmarks show the efficacy of our method significantly outperforming the previous unsupervised debiasing methods (e.g. a 10.13% improvement in worst-group accuracy on Wavy Hair classification in CelebA) while achieving comparable or superior performance to supervised methods. Our code is publicly available at the following link:Debiasify.	https://openaccess.thecvf.com//content/WACV2025/html/Bayasi_Debiasify_Self-Distillation_for_Unsupervised_Bias_Mitigation_WACV_2025_paper.html	Nourhan Bayasi, Jamil Fayyad, Ghassan Hamarneh, Rafeef Garbi, Homayoun Najjaran
Deciphering the Complaint Aspects: Towards an Aspect-Based Complaint Identification Model with Video Complaint Dataset in Finance	In today's competitive marketing landscape effective complaint management is crucial for customer service and business success. Video complaints integrating text and image content offer invaluable insights by addressing customer grievances and delineating product benefits and drawbacks. However comprehending nuanced complaint aspects within vast daily multimodal financial data remains a formidable challenge. Addressing this gap we have curated a proprietary multimodal video complaint dataset comprising 433 publicly accessible instances. Each instance is meticulously annotated at the utterance level encompassing five distinct categories of financial aspects and their associated complaint labels. To support this endeavour we introduce Solution 3.0 a model designed for multimodal aspect-based complaint identification task. Solution 3.0 is tailored to perform three key tasks: 1) handling multimodal features (audio and video) 2) facilitating multilabel aspect classification and 3) conducting multitasking for aspect classifications and complaint identification parallelly. Solution 3.0 utilizes a CLIP-based dual frozen encoder with an integrated image segment encoder for global feature fusion enhanced by contextual attention (ISEC) to improve accuracy and efficiency. Our proposed framework surpasses current multimodal baselines exhibiting superior performance across nearly all metrics by opening new ways to strengthen appropriate customer care initiatives and effectively assisting individuals in resolving their problems.	https://openaccess.thecvf.com//content/WACV2025/html/Das_Deciphering_the_Complaint_Aspects_Towards_an_Aspect-Based_Complaint_Identification_Model_WACV_2025_paper.html	Sarmistha Das, Basha Mujavarsheik, R E Zera Lyngkhoi, Sriparna Saha, Alka Maurya
Decomposed Distribution Matching in Dataset Condensation	Dataset Condensation (DC) aims to reduce deep neural networks training efforts by synthesizing a small dataset such that it will be as effective as the original large dataset. Conventionally DC relies on a costly bi-level optimization which prohibits its practicality. Recent research formulates DC as a distribution matching problem which circumvents the costly bi-level optimization. However this efficiency sacrifices the DC performance. To investigate this performance degradation we decomposed the dataset distribution into content and style. Our observations indicate two major shortcomings of: 1) style discrepancy between original and condensed data and 2) limited intra-class diversity of condensed dataset. We present a simple yet effective method to match the style information between original and condensed data employing statistical moments of feature maps as well-established style indicators. Moreover we enhance the intra-class diversity by maximizing the Kullback-Leibler divergence within each synthetic class i.e. content. We demonstrate the efficacy of our method through experiments on diverse datasets of varying size and resolution achieving improvements of up to 4.1% on CIFAR10 4.2% on CIFAR100 4.3% on TinyImageNet 2.0% on ImageNet-1K 3.3% on ImageWoof 2.5% on ImageNette and 5.5% in continual learning accuracy.	https://openaccess.thecvf.com//content/WACV2025/html/Malakshan_Decomposed_Distribution_Matching_in_Dataset_Condensation_WACV_2025_paper.html	Sahar Rahimi Malakshan, Mohammad Saeed Ebrahimi Saadabadi, Ali Dabouei, Nasser Nasrabadi
Decoupled PROB: Decoupled Query Initialization Tasks and Objectness-Class Learning for Open World Object Detection	Open World Object Detection (OWOD) is a challenging computer vision task that extends standard object detection by (1) detecting and classifying unknown objects without supervision and (2) incrementally learning new object classes without forgetting previously learned ones. The absence of ground truths for unknown objects makes OWOD tasks particularly challenging. Many methods have addressed this by using pseudo-labels for unknown objects. The recently proposed Probabilistic Objectness transformer-based open-world detector (PROB) is a state-of-the-art model that does not require pseudo-labels for unknown objects as it predicts probabilistic objectness. However this method faces issues with learning conflicts between objectness and class predictions. To address this issue and further enhance performance we propose a novel model Decoupled PROB. Decoupled PROB introduces Early Termination of Objectness Prediction (ETOP) to stop objectness predictions at appropriate layers in the decoder resolving the learning conflicts between class and objectness predictions in PROB. Additionally we introduce Task-Decoupled Query Initialization (TDQI) which efficiently extracts features of known and unknown objects thereby improving performance. TDQI is a query initialization method that combines query selection and learnable queries and it is a module that can be easily integrated into existing DETR-based OWOD models. Extensive experiments on OWOD benchmarks demonstrate that Decoupled PROB surpasses all existing methods across several metrics significantly improving performance.	https://openaccess.thecvf.com//content/WACV2025/html/Inoue_Decoupled_PROB_Decoupled_Query_Initialization_Tasks_and_Objectness-Class_Learning_for_WACV_2025_paper.html	Riku Inoue, Masamitsu Tsuchiya, Yuji Yasui
Deduce and Select Evidences with Language Models for Training-Free Video Goal Inference	We introduce ViDSE a Video framework that Deduce and Selects visual Evidence for training-free video goal inference using language models. Unlike approaches that directly apply vision-language models (VLM) or combine VLM+LLM to process dense video visuals ViDSE explicitly selects relevant visual evidence (e.g. frames) based on the hypothesis deduced by the LLM. This approach not only improves accuracy but also reveals the logical process behind the model's decisions enhancing explainability. Our experiments demonstrate that this selection process significantly reduces ambiguity in the subsequent inference reasoning stage and outperforms VLM-only and VLM+LLM models on goal inference tasks such as CrossTask and COIN. We further validate ViDSE's generalizability and robustness on action recognition tasks such as ActivityNet and UCF-101 under training-free and open-vocabulary conditions. We observe that ViDSE easily generalizes to other video tasks (e.g. action recognition) requiring filtering of redundant and irrelevant information.	https://openaccess.thecvf.com//content/WACV2025/html/Ee_Deduce_and_Select_Evidences_with_Language_Models_for_Training-Free_Video_WACV_2025_paper.html	Yeo Keat Ee, Hao Zhang, Alexander Matyasko, Basura Fernando
Deep Geometric Moments Promote Shape Consistency in Text-to-3D Generation	To address the data scarcity associated with 3D assets 2D-lifting techniques such as Score Distillation Sampling (SDS) have become a widely adopted practice in text-to-3D generation pipelines. However the diffusion models used in these techniques are prone to viewpoint bias and thus lead to geometric inconsistencies such as the Janus problem. To counter this we introduce MT3D a text-to-3D generative model that leverages a high-fidelity 3D object to overcome viewpoint bias and explicitly infuse geometric understanding into the generation pipeline. Firstly we employ depth maps derived from a high-quality 3D model as control signals to guarantee that the generated 2D images preserve the fundamental shape and structure thereby reducing the inherent viewpoint bias. Next we utilize deep geometric moments to ensure geometric consistency in the 3D representation explicitly. By incorporating geometric details from a 3D asset MT3D enables the creation of diverse and geometrically consistent objects thereby improving the quality and usability of our 3D representations. Project page and code: https://moment-3d.github.io/	https://openaccess.thecvf.com//content/WACV2025/html/Nath_Deep_Geometric_Moments_Promote_Shape_Consistency_in_Text-to-3D_Generation_WACV_2025_paper.html	Utkarsh Nath, Rajeev Goel, Eun Som Jeon, Changhoon Kim, Kyle Min, Yezhou Yang, Yingzhen Yang, Pavan Turaga
Deep Joint Unrolling for Deblurring and Low-Light Image Enhancement (JUDE)	Low-light and blurring issues are prevalent when capturing photos at night often due to the use of long exposure to address dim environments. Addressing these joint problems can be challenging and error-prone if an end-to-end model is trained without incorporating an appropriate physical model. In this paper we introduce JUDE a Deep Joint Unrolling for Deblurring and Low-Light Image Enhancement inspired by the image physical model. Based on Retinex theory and the blurring model the low-light blurry input is iteratively deblurred and decomposed producing sharp low-light reflectance and illuminance through an unrolling mechanism. Additionally we incorporate various modules to estimate the initial blur kernel enhance brightness and eliminate noise in the final image. Comprehensive experiments on LOL-Blur and Real-LOL-Blur demonstrate that our method outperforms existing techniques both quantitatively and qualitatively.	https://openaccess.thecvf.com//content/WACV2025/html/Vo_Deep_Joint_Unrolling_for_Deblurring_and_Low-Light_Image_Enhancement_JUDE_WACV_2025_paper.html	Tu Vo, Chan Y. Park
Deep Metric Learning for Unsupervised Remote Sensing Change Detection	Remote Sensing Change Detection (RS-CD) aims to detect relevant changes from Multi-Temporal Remote Sensing Images (MT-RSIs) which aids in various RS applications such as land cover land use human development analysis and disaster response. The performance of existing RS-CD methods is attributed to training on large annotated datasets. Furthermore most of these models are less transferable in the sense that the trained model often performs very poorly when there is a domain gap between training and test datasets. This paper proposes an unsupervised CD method based on deep metric learning that can deal with both of these issues. Given an MT-RSI the proposed method generates corresponding change probability map by iteratively optimizing an unsupervised CD loss without training it on a large dataset. Our unsupervised CD method consists of two interconnected deep networks namely Deep-Change Probability Generator (D-CPG) and Deep-Feature Extractor (D-FE). The D-CPG is designed to predict change and no change probability maps for a given MT-RSI while D-FE is used to extract deep features of MT-RSI that will be further used in the proposed unsupervised CD loss. We use transfer learning capability to initialize the parameters of D-FE. We iteratively optimize the parameters of D-CPG and D-FE for a given MT-RSI by minimizing the proposed unsupervised similarity-dissimilarity loss. This loss is motivated by the principle of metric learning where we simultaneously maximize the distance between change pair-wise pixels while minimizing the distance between no-change pair-wise pixels in bi-temporal image domain and their deep feature domain. The experiments conducted on three CD datasets show that our unsupervised CD method achieves significant improvements over the state-of-the-art supervised and unsupervised CD methods. Code and pre-trained models available at https://github.com/wgcban/Metric-CD	https://openaccess.thecvf.com//content/WACV2025/html/Bandara_Deep_Metric_Learning_for_Unsupervised_Remote_Sensing_Change_Detection_WACV_2025_paper.html	Wele Gedara Chaminda Bandara, Vishal M. Patel
DeepCA: Deep Learning-Based 3D Coronary Artery Tree Reconstruction from Two 2D Non-Simultaneous X-ray Angiography Projections	Cardiovascular diseases (CVDs) are the most common cause of death worldwide. Invasive x-ray coronary angiography (ICA) is one of the most important imaging modalities for the diagnosis of CVDs. ICA typically acquires only two 2D projections which makes the 3D geometry of coronary vessels difficult to interpret thus requiring 3D coronary artery tree reconstruction from two projections. State-of-the-art approaches require significant manual interactions and cannot correct the non-rigid cardiac and respiratory motions between non-simultaneous projections. In this study we propose a novel deep learning pipeline named DeepCA. We leverage the Wasserstein conditional generative adversarial network with gradient penalty latent convolutional transformer layers and a dynamic snake convolutional critic to implicitly compensate for the non-rigid motion and provide 3D coronary artery tree reconstruction. Through simulating projections from coronary computed tomography angiography (CCTA) we achieve the generalisation of 3D coronary tree reconstruction on real non-simultaneous ICA projections. We incorporate an application-specific evaluation metric to validate our proposed model on both a CCTA dataset and a real ICA dataset together with Chamfer l_2 distance. The results demonstrate promising performance of our DeepCA model in vessel topology preservation recovery of missing features and generalisation ability to real ICA data. To the best of our knowledge this is the first study that leverages deep learning to achieve 3D coronary tree reconstruction from two real non-simultaneous x-ray angiographic projections. The implementation of this work is available at: https://github.com/WangStephen/DeepCA.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_DeepCA_Deep_Learning-Based_3D_Coronary_Artery_Tree_Reconstruction_from_Two_WACV_2025_paper.html	Yiying Wang, Abhirup Banerjee, Robin P. Choudhury, Vicente Grau
DeepMIM: Deep Supervision for Masked Image Modeling	Deep supervision which involves extra supervisions to the intermediate features of a neural network was widely used in image classification in the early deep learning era since it significantly reduces the training difficulty and eases the optimization like avoiding gradient vanish over the vanilla training. Nevertheless with the emergence of normalization techniques and residual connection deep supervision in image classification was gradually phased out. In this paper we revisit deep supervision for masked image modeling (MIM) that pre-trains a Vision Transformer (ViT) via a mask-and-predict scheme. Experimentally we find that deep supervision drives the shallower layers to learn more meaningful representations accelerates model convergence and expands attention diversities. Our approach called DeepMIM significantly boosts the representation capability of each layer. In addition DeepMIM is compatible with many MIM models across a range of reconstruction targets. For instance using ViT-B DeepMIM on MAE achieves 84.2 top-1 accuracy on ImageNet outperforming MAE by +0.6. By combining DeepMIM with a stronger tokenizer CLIP our model achieves state-of-the-art performance on various downstream tasks including image classification (85.6 top-1 accuracy on ImageNet-1K outperforming MAE-CLIP by +0.8) object detection (52.8 AP^box on COCO) and semantic segmentation (53.1 mIoU on ADE20K). Code and models are available at https://github.com/OliverRensu/DeepMIM.	https://openaccess.thecvf.com//content/WACV2025/html/Ren_DeepMIM_Deep_Supervision_for_Masked_Image_Modeling_WACV_2025_paper.html	Sucheng Ren, Fangyun Wei, Samuel Albanie, Zheng Zhang, Han Hu
Defending Against Repetitive Backdoor Attacks on Semi-Supervised Learning through Lens of Rate-Distortion-Perception Trade-Off	Semi-supervised learning (SSL) has achieved remarkable performance with a small fraction of labeled data by leveraging vast amounts of unlabeled data from the Internet. However this large pool of untrusted data is extremely vulnerable to data poisoning leading to potential backdoor attacks. Current backdoor defenses are not yet effective against such a vulnerability in SSL. In this study we propose a novel method Unlabeled Data Purification (UPure) to disrupt the association between trigger patterns and target classes by introducing perturbations in the frequency domain. By leveraging the Rate-Distortion-Perception (RDP) trade-off we further identify the frequency band where the perturbations are added and justify this selection. Notably UPure purifies poisoned unlabeled data without the need of extra clean labeled data. Extensive experiments on four benchmark datasets and five SSL algorithms demonstrate that UPure effectively reduces the attack success rate from 99.78% to 0% while maintaining model accuracy. Code is available here: https://github.com/chengyi-chris/UPure.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Defending_Against_Repetitive_Backdoor_Attacks_on_Semi-Supervised_Learning_through_Lens_WACV_2025_paper.html	Cheng-Yi Lee, Ching-Chia Kao, Cheng-Han Yeh, Chun-Shien Lu, Chia-Mu Yu, Chu-Song Chen
Delta-NAS: Difference of Architecture Encoding for Predictor-Based Evolutionary Neural Architecture Search	Neural Architecture Search (NAS) continues to serve a key roll in the design and development of neural networks for task specific deployment. Modern NAS techniques struggle to deal with ever increasing search space complexity and compute cost constraints. Existing approaches can be categorized into two buckets: fine-grained computational expensive NAS and coarse-grained low cost NAS. Our objective is to craft an algorithm with the capability to perform fine-grain NAS at a low cost. We propose projecting the problem to a lower dimensional space through predicting the difference in accuracy of a pair of similar networks. This paradigm shift allows for reducing computational complexity from exponential down to linear with respect to the size of the search space. We present a strong mathematical foundation for our algorithm in addition to extensive experimental results across a host of common NAS Benchmarks. Our methods significantly out performs existing works achieving better performance coupled with a significantly higher sample efficiency.	https://openaccess.thecvf.com//content/WACV2025/html/Sridhar_Delta-NAS_Difference_of_Architecture_Encoding_for_Predictor-Based_Evolutionary_Neural_Architecture_WACV_2025_paper.html	Arjun Sridhar, Yiran Chen
Denoising Diffusion Models for High-Resolution Microscopy Image Restoration	Advances in microscopy imaging enable researchers to visualize structures at the nanoscale level thereby unraveling intricate details of biological organization. However challenges such as image noise photobleaching of fluorophores and low tolerability of biological samples to high light doses remain restricting temporal resolutions and experiment durations. Reduced laser doses enable longer measurements at the cost of lower resolution and increased noise which hinders accurate downstream analyses. Here we train a denoising diffusion probabilistic model (DDPM) to predict high-resolution images by conditioning the model on low-resolution information. Additionally the probabilistic aspect of the DDPM allows for repeated generation of images that tend to further increase the signal-to-noise ratio. We show that our model achieves a performance that is better or similar to the previously best-performing methods across four highly diverse datasets. Importantly while any of the previous methods show competitive performance for some but not all datasets our method consistently achieves high performance across all four data sets suggesting high generalizability. Our code and datasets are available at https://github.com/kaschube-lab/ddpm_highres_microscopy https://doi.org/10.5281/zenodo.14178277 https://doi.org/10.5281/zenodo.14215837.	https://openaccess.thecvf.com//content/WACV2025/html/Osuna-Vargas_Denoising_Diffusion_Models_for_High-Resolution_Microscopy_Image_Restoration_WACV_2025_paper.html	Pamela Osuna-Vargas, Maren H. Wehrheim, Lucas Zinz, Johanna Rahm, Ashwin Balakrishnan, Alexandra Kaminer, Mike Heilemann, Matthias Kaschube
Dense Depth from Event Focal Stack	"We propose a method for dense depth estimation from an event stream generated when sweeping the focal plane of the driving lens attached to an event camera. In this method a depth map is inferred from an ""event focal stack"" composed of the event stream using a convolutional neural network trained with synthesized event focal stacks. The synthesized event stream is created from a focal stack generated by Blender for any arbitrary 3D scene. This allows for training on scenes with diverse structures. Additionally we explored methods to eliminate the domain gap between real event streams and synthetic event streams. Our method demonstrates superior performance over a depth-from-defocus method in the image domain on synthetic and real datasets."	https://openaccess.thecvf.com//content/WACV2025/html/Horikawa_Dense_Depth_from_Event_Focal_Stack_WACV_2025_paper.html	Kenta Horikawa, Mariko Isogawa, Hideo Saito, Shohei Mori
Dense Scene Reconstruction from Light-Field Images Affected by Rolling Shutter	"This paper presents a dense depth estimation approach from light-field (LF) images that is able to compensate for strong rolling shutter (RS) effects. Our method estimates RS compensated views and dense RS compensated disparity maps. We present a two-stage method based on a 2D Gaussians Splatting that allows for a ""render and compare"" strategy with a point cloud formulation. In the first stage a subset of sub-aperture images is used to estimate an RS agnostic 3D shape that is related to the scene target shape ""up to a motion"". In the second stage the deformation of the 3D shape is computed by estimating an admissible camera motion. We demonstrate the effectiveness and advantages of this approach through several experiments conducted for different scenes and types of motions. Due to lack of suitable datasets for evaluation we also present a new carefully designed synthetic dataset of RS LF images. The source code trained models and dataset will be made publicly available at: https://github.com/ICB-Vision-AI/DenseRSLF."	https://openaccess.thecvf.com//content/WACV2025/html/McGriff_Dense_Scene_Reconstruction_from_Light-Field_Images_Affected_by_Rolling_Shutter_WACV_2025_paper.html	Hermes McGriff, Renato Martins, Nicolas Andreff, Cedric Demonceaux
DepthSSC: Monocular 3D Semantic Scene Completion via Depth-Spatial Alignment and Voxel Adaptation	The task of 3D semantic scene completion using monocular cameras is gaining significant attention in the field of autonomous driving. This task aims to predict the occupancy status and semantic labels of each voxel in a 3D scene from partial image inputs. Despite numerous existing methods many face challenges such as inaccurately predicting object shapes and misclassifying object boundaries. To address these issues we propose DepthSSC an advanced method for semantic scene completion using only monocular cameras. DepthSSC integrates the Spatial Transformation Graph Fusion (ST-GF) module with Geometric-Aware Voxelization (GAV) enabling dynamic adjustment of voxel resolution to accommodate the geometric complexity of 3D space. This ensures precise alignment between spatial and depth information effectively mitigating issues such as object boundary distortion and incorrect depth perception found in previous methods. Evaluations on the SemanticKITTI and SSCBench-KITTI-360 dataset demonstrate that DepthSSC not only captures intricate 3D structural details effectively but also achieves state-of-the-art performance.	https://openaccess.thecvf.com//content/WACV2025/html/Yao_DepthSSC_Monocular_3D_Semantic_Scene_Completion_via_Depth-Spatial_Alignment_and_WACV_2025_paper.html	Jiawei Yao, Jusheng Zhang, Xiaochao Pan, Tong Wu, Canran Xiao
Dequantization and Color Transfer with Diffusion Models	We demonstrate an image dequantizing diffusion model that enables novel edits on natural images. We propose operating on quantized images because they offer easy abstraction for patch-based edits and palette transfer. In particular we show that color palettes can make the output of the diffusion model easier to control and interpret. We first establish that existing image restoration methods are not sufficient such as JPEG noise reduction models. We then demonstrate that our model can generate natural images that respect the color palette the user asked for. For palette transfer we propose a method based on weighted bipartite matching. We then show that our model generates plausible images even after extreme palette transfers respecting user query. Our method can optionally condition on the source texture in part or all of the image. In doing so we overcome a common problem in existing image colorization methods that are unable to produce colors with a different luminance than the input. We evaluate several possibilities for texture conditioning and their trade-offs including luminance image gradients and thresholded gradients the latter of which performed best in maintaining texture and color control simultaneously. Our method can be usefully extended to another practical edit: recoloring patches of an image while respecting the source texture. Our procedure is supported by several qualitative and quantitative evaluations.	https://openaccess.thecvf.com//content/WACV2025/html/Vavilala_Dequantization_and_Color_Transfer_with_Diffusion_Models_WACV_2025_paper.html	Vaibhav Vavilala, Faaris Shaik, David Forsyth
Design Principles of Multi-Scale J-Invariant Networks for Self-Supervised Image Denoising	Recent advancements in image denoising have leveraged neural networks to enhance performance particularly in scenarios where clean-noisy image pairs are unavailable. In this context self-supervised image denoising methods have gained prominence centered around the principle of J-invariance -- ensuring that the output pixel is not influenced by its corresponding input pixel. Traditionally enforcing J-invariance has constrained blind spot network (BSN) designs requiring even core operations such as upsampling or downsampling to follow complex rules. This limitation has led to the exclusion of efficient multi-resolution architectures such as U-net increasing computational complexity. To address these constraints we introduce generalized design principles for multi-scale J-invariant networks that allow for the flexible incorporation of nearly any architectural elements. This approach challenges the prevailing notion that J-invariance must be maintained throughout the entire process. Based on our design principles we present U-BSN a novel J-invariant network design that utilizes the versatile U-Net architecture adapting it to accommodate self-supervised learning effectively. We also propose randomized PD an advanced technique that enhances denoising of real-world images with structured noise. Experimental results validate that U-BSN surpasses existing BSNs in handling real-world noise scenarios and achieves the lowest computational complexity among comparable networks thus confirming the effectiveness of our design principles and proposed methodologies.	https://openaccess.thecvf.com//content/WACV2025/html/Yu_Design_Principles_of_Multi-Scale_J-Invariant_Networks_for_Self-Supervised_Image_Denoising_WACV_2025_paper.html	Hayeong Yu, Seungjae Han, Young-Gyu Yoon
Design-o-Meter: Towards Evaluating and Refining Graphic Designs	Graphic designs are an effective medium for visual communication. They range from greeting cards to corporate flyers and beyond. Off-late machine learning techniques are able to generate such designs which accelerates the rate of content production. An automated way of evaluating their quality becomes critical. Towards this end we introduce Design-o-meter a data-driven methodology to quantify the goodness of graphic designs. Further our approach can suggest modifications to these designs to improve its visual appeal. To the best of our knowledge Design-o-meter is the first approach that scores and refines designs in a unified framework despite the inherent subjectivity and ambiguity of the setting. Our exhaustive quantitative and qualitative analysis of our approach against baselines adapted for the task (including recent Multimodal LLM based approaches) brings out the efficacy of our methodology. We hope our work will usher more interest in this important and pragmatic problem setting. Project Page: https://sahilg06.github.io/Design-o-meter/	https://openaccess.thecvf.com//content/WACV2025/html/Goyal_Design-o-Meter_Towards_Evaluating_and_Refining_Graphic_Designs_WACV_2025_paper.html	Sahil Goyal, Abhinav Mahajan, Swasti Mishra, Prateksha Udhayanan, Tripti Shukla, KJ Joseph, Balaji Vasan Srinivasan
Detecting Origin Attribution for Text-to-Image Diffusion Models	Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution yet prior studies have not fully explored the practical and scientific dimensions of this task. In addition to attributing images to 12 state-of-the-art T2I generators we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible. Our experiments reveal that initialization seeds are highly detectable along with other subtle variations in the image generation process to some extent. We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably altering high-frequency information causes only slight reductions in accuracy and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Detecting_Origin_Attribution_for_Text-to-Image_Diffusion_Models_WACV_2025_paper.html	Katherine Xu, Lingzhi Zhang, Jianbo Shi
Detecting Wildfires on UAVs with Real-Time Segmentation Trained by Larger Teacher Models	Early detection of wildfires is essential to prevent large-scale fires resulting in extensive environmental structural and societal damage. Uncrewed aerial vehicles (UAVs) can cover large remote areas effectively with quick deployment requiring minimal infrastructure and equipping them with small cameras and computers enables autonomous real-time detection. In remote areas however detection methods are limited to onboard computation due to the lack of high-bandwidth mobile networks. For accurate camera-based localisation segmentation of the detected smoke is essential but training data for deep learning-based wildfire smoke segmentation is limited. This study shows how small specialised segmentation models can be trained using only bounding box labels leveraging zero-shot foundation model supervision. The method offers the advantages of needing only fairly easily obtainable bounding box labels and requiring training solely for the smaller student network. The proposed method achieved 63.3 percent mIoU on a manually annotated and diverse wildfire dataset. The used model can perform in real-time at 25 fps with a UAV-carried NVIDIA Jetson Orin NX computer while reliably recognising smoke as demonstrated at real-world forest burning events. Code is available at: https://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation	https://openaccess.thecvf.com//content/WACV2025/html/Pesonen_Detecting_Wildfires_on_UAVs_with_Real-Time_Segmentation_Trained_by_Larger_WACV_2025_paper.html	Julius Pesonen, Teemu Hakala, VÃ¤inÃ¶ Karjalainen, Niko KoivumÃ¤ki, Lauri Markelin, Anna-Maria Raita-Hakola, Juha Suomalainen, Ilkka PÃ¶lÃ¶nen, Eija Honkavaara
Detective Networks: Enhancing Disaster Recognition in Images Through Attention Shifting using Optimal Masking	Aerial investigation is used for surveying damage and identifying post-disaster events through imagery data. However the challenge lies in detecting disaster-related areas within aerial or shipborne images as these can appear as minor regions making recognition difficult. To address this challenge we introduce the Detective Network (DeNet) designed to optimally mask images thereby shifting the attention of machine learning models towards these small yet crucial regions. Utilizing the concepts of patch and anchor box DeNet incorporates a masking candidate layer and a masking layer to facilitate optimal masking. Our experimental findings are compelling; by preprocessing images with DeNet before analysis using an image captioning model we achieved a remarkable accuracy of 92.91% in landslide detection from side-view image captions and 87.50% for shipborne view detection. The result demonstrates the efficacy of DeNet in enhancing the recognition of disaster-related areas in challenging imaging conditions.	https://openaccess.thecvf.com//content/WACV2025/html/Thanyawet_Detective_Networks_Enhancing_Disaster_Recognition_in_Images_Through_Attention_Shifting_WACV_2025_paper.html	Narongthat Thanyawet, Photchara Ratsamee, Yuki Uranishi, Haruo Takemura
DiHuR: Diffusion-Guided Generalizable Human Reconstruction	We introduce DiHuR a novel Diffusion-guided model for generalizable Human 3D Reconstruction and view synthesis from sparse minimally overlapping images. While existing generalizable human radiance fields excel at novel view synthesis they often struggle with comprehensive 3D reconstruction. Similarly directly optimizing implicit Signed Distance Function (SDF) fields from sparse-view images typically yields poor results due to limited overlap. To enhance 3D reconstruction quality we propose using learnable tokens associated with SMPL vertices to aggregate sparse view features and then to guide SDF prediction. These tokens learn a generalizable prior across different identities in training datasets leveraging the consistent projection of SMPL vertices onto similar semantic areas across various human identities. This consistency enables effective knowledge transfer to unseen identities during inference. Recognizing SMPL's limitations in capturing clothing details we incorporate a diffusion model as an additional prior to fill in missing information particularly for complex clothing geometries. Our method integrates two key priors in a coherent manner: the prior from generalizable feed-forward models and the 2D diffusion prior and it requires only multi-view image training without 3D supervision. DiHuR demonstrates superior performance in both within-dataset and cross-dataset generalization settings as validated on THuman ZJU-MoCap and HuMMan datasets compared to existing methods.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_DiHuR_Diffusion-Guided_Generalizable_Human_Reconstruction_WACV_2025_paper.html	Jinnan Chen, Chen Li, Gim Hee Lee
DiL: An Explainable and Practical Metric for Abnormal Uncertainty in Object Detection	Although object detection models are widely used their predictive performance has been shown to deteriorate when faced with abnormal scenes. Such abnormalities can occur naturally (by partially occluded or out-of-distribution objects) or deliberately (in the case of an adversarial attack). Existing uncertainty quantification methods such as object detection evaluation metrics and label-uncertainty quantification techniques do not consider the abnormalities' effect on the model's internal decision-making process. Furthermore practical methods that consider the effects of abnormalities (such as abnormality detection and mitigation) are designed to deal with one type of abnormality. We present distinctive localization (DiL) an unsupervised practical and explainable metric that quantitatively interprets any type of abnormality and can be leveraged for preventive purposes. By utilizing XAI techniques (saliency maps) DiL maps the objectness of a given scene and captures the model's inner uncertainty regarding the identified (and missed) objects. DiL was evaluated across nine use cases including partially occluded and out-of-distribution objects as well as adversarial patches in both physical and digital spaces on benchmark datasets and our newly E-PO dataset (generated with DALL-E 2). Our results show that DiL: i) successfully interprets and quantifies an abnormality's effect on the model's decision-making process regardless of the abnormality type; and ii) can be leveraged to detect and mitigate this effect.	https://openaccess.thecvf.com//content/WACV2025/html/Giloni_DiL_An_Explainable_and_Practical_Metric_for_Abnormal_Uncertainty_in_WACV_2025_paper.html	Amit Giloni, Omer Hofman, Ikuya Morikawa, Toshiya Shimizu, Yuval Elovici, Asaf Shabtai
DiTAS: Quantizing Diffusion Transformers via Enhanced Activation Smoothing	Diffusion Transformers (DiTs) have recently attracted significant interest from both industry and academia due to their enhanced capabilities in visual generation surpassing the performance of traditional diffusion models that employ U-Net. However the improved performance of DiTs comes at the expense of higher parameter counts and implementation costs which significantly limits their deployment on resource-constrained devices like mobile phones. We propose DiTAS a data-free post-training quantization (PTQ) method for efficient DiT inference. DiTAS relies on the proposed temporal-aggregated smoothing techniques to mitigate the impact of the channel-wise outliers within the input activations leading to much lower quantization error under extremely low bitwidth. To further enhance the performance of the quantized DiT we adopt the layer-wise grid search strategy to optimize the smoothing factor. Moreover we integrate a training-free LoRA module for weight quantization leveraging alternating optimization to minimize quantization errors without additional fine-tuning. Experimental results demonstrate that our approach enables 4-bit weight 8-bit activation (W4A8) quantization for DiTs while maintaining comparable performance as the full-precision model. Code is available at https://github.com/DZY122/DiTAS.	https://openaccess.thecvf.com//content/WACV2025/html/Dong_DiTAS_Quantizing_Diffusion_Transformers_via_Enhanced_Activation_Smoothing_WACV_2025_paper.html	Zhenyuan Dong, Sai Qian Zhang
DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers using MRI and PET	Diagnosing dementia particularly for Alzheimer's Disease (AD) and frontotemporal dementia (FTD) is complex due to overlapping symptoms. While magnetic resonance imaging (MRI) and positron emission tomography (PET) data are critical for the diagnosis integrating these modalities in deep learning faces challenges often resulting in suboptimal performance compared to using single modalities. Moreover the potential of multi-modal approaches in differential diagnosis which holds significant clinical importance remains largely unexplored. We propose a novel framework DiaMond to address these issues with vision Transformers to effectively integrate MRI and PET. DiaMond is equipped with self-attention and a novel bi-attention mechanism that synergistically combine MRI and PET alongside a multi-modal normalization to reduce redundant dependency thereby boosting the performance. DiaMond significantly outperforms existing multi-modal methods across various datasets achieving a balanced accuracy of 92.4% in AD diagnosis 65.2% for AD-MCI-CN classification and 76.5% in differential diagnosis of AD and FTD. We also validated the robustness of DiaMond in a comprehensive ablation study. The code is available at https://github.com/ai-med/DiaMond.	https://openaccess.thecvf.com//content/WACV2025/html/Li_DiaMond_Dementia_Diagnosis_with_Multi-Modal_Vision_Transformers_using_MRI_and_WACV_2025_paper.html	Yitong Li, Morteza Ghahremani, Youssef Wally, Christian Wachinger
DiffMesh: A Motion-Aware Diffusion Framework for Human Mesh Recovery from Videos	Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming human-computer interaction and virtual reality. While image-based HMR methods have achieved impressive results they often struggle to recover humans in dynamic scenarios leading to temporal inconsistencies and non-smooth 3D motion predictions due to the absence of human motion. In contrast video-based approaches leverage temporal information to mitigate this issue. In this paper we present DiffMesh an innovative motion-aware diffusion framework for video-based HMR. DiffMesh establishes a bridge between diffusion models and human motion efficiently generating accurate and smooth output mesh sequences by incorporating human motion within the forward process and reverse process in the diffusion model. Extensive experiments are conducted on the widely used datasets (Human3.6M and 3DPW) which demonstrate the effectiveness and efficiency of our DiffMesh. Visual comparisons in real-world scenarios further highlight DiffMesh's suitability for practical applications. The project webpage is: https://zczcwh.github.io/ diffmesh_page/	https://openaccess.thecvf.com//content/WACV2025/html/Zheng_DiffMesh_A_Motion-Aware_Diffusion_Framework_for_Human_Mesh_Recovery_from_WACV_2025_paper.html	Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen
DiffPAD: Denoising Diffusion-Based Adversarial Patch Decontamination	In the ever-evolving adversarial machine learning landscape developing effective defenses against patch attacks has become a critical challenge necessitating reliable solutions to safeguard real-world AI systems. Although diffusion models have shown remarkable capacity in image synthesis and have been recently utilized to counter l_p-norm bounded attacks their potential in mitigating localized patch attacks remains largely underexplored. In this work we propose DiffPAD a novel framework that harnesses the power of diffusion models for adversarial patch decontamination. DiffPAD first performs super-resolution restoration on downsampled input images then adopts binarization dynamic thresholding scheme and sliding window for effective localization of adversarial patches. Such a design is inspired by the theoretically derived correlation between patch size and diffusion restoration error that is generalized across diverse patch attack scenarios. Finally DiffPAD applies inpainting techniques to the original input images with the estimated patch region being masked. By integrating closed-form solutions for super-resolution restoration and image inpainting into the conditional reverse sampling process of a pre-trained diffusion model DiffPAD obviates the need for text guidance or fine-tuning. Through comprehensive experiments we demonstrate that DiffPAD not only achieves state-of-the-art adversarial robustness against patch attacks but also excels in recovering naturalistic images without patch remnants. The source code is available at https://github.com/JasonFu1998/DiffPAD.	https://openaccess.thecvf.com//content/WACV2025/html/Fu_DiffPAD_Denoising_Diffusion-Based_Adversarial_Patch_Decontamination_WACV_2025_paper.html	Jia Fu, Xiao Zhang, Sepideh Pashami, Fatemeh Rahimian, Anders Holst
DiffQRCoder: Diffusion-Based Aesthetic QR Code Generation with Scanning Robustness Guided Iterative Refinement	With the success of Diffusion Models for image generation the technologies also have revolutionized the aesthetic Quick Response (QR) code generation. Despite significant improvements in visual attractiveness for the beautified codes their scannabilities are usually sacrificed and thus hinder their practical uses in real-world scenarios. To address this issue we propose a novel training-free Diffusion-based QR Code generator (DiffQRCoder) to effectively craft both scannable and visually pleasing QR codes. The proposed approach introduces Scanning-Robust Perceptual Guidance (SRPG) a new diffusion guidance for Diffusion Models to guarantee the generated aesthetic codes to obey the ground-truth QR codes while maintaining their attractiveness during the denoising process. Additionally we present another post-processing technique Scanning Robust Manifold Projected Gradient Descent (SR-MPGD) to further enhance their scanning robustness through iterative latent space optimization. With extensive experiments the results demonstrate that our approach not only outperforms other compared methods in Scanning Success Rate (SSR) with better or comparable CLIP aesthetic score (CLIP-aes.) but also significantly improves the SSR of the ControlNet-only approach from 60% to 99%. The subjective evaluation indicates that our approach achieves promising visual attractiveness to users as well. Finally even with different scanning angles and the most rigorous error tolerance settings our approach robustly achieves over 95% SSR demonstrating its capability for real-world applications. Our project page is available at https://jwliao1209.github.io/DiffQRCoder.	https://openaccess.thecvf.com//content/WACV2025/html/Liao_DiffQRCoder_Diffusion-Based_Aesthetic_QR_Code_Generation_with_Scanning_Robustness_Guided_WACV_2025_paper.html	Jia-Wei Liao, Winston Wang, Tzu-Sian Wang, Li-Xuan Peng, Ju-Hsuan Weng, Cheng-Fu Chou, Jun-Cheng Chen
Differential Privacy Mechanisms in Neural Tangent Kernel Regression	Training data privacy is a fundamental problem in modern Artificial Intelligence (AI) applications such as face recognition recommendation systems language generation and many others as it may contain sensitive user information related to legal issues. To fundamentally understand how privacy mechanisms work in AI applications we study differential privacy (DP) in the Neural Tangent Kernel (NTK) regression setting where DP is one of the most powerful tools for measuring privacy under statistical learning and NTK is one of the most popular analysis frameworks for studying the learning mechanisms of deep neural networks. In our work we can show provable guarantees for both differential privacy and test accuracy of our NTK regression. Furthermore we conduct experiments on the basic image classification dataset CIFAR10 to demonstrate that NTK regression can preserve good accuracy under a modest privacy budget supporting the validity of our analysis. To our knowledge this is the first work to provide a DP guarantee for NTK regression.	https://openaccess.thecvf.com//content/WACV2025/html/Gu_Differential_Privacy_Mechanisms_in_Neural_Tangent_Kernel_Regression_WACV_2025_paper.html	Jiuxiang Gu, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song
Differentially Private Integrated Decision Gradients (IDG-DP) for Radar-Based Human Activity Recognition	Human motion analysis offers significant potential for healthcare monitoring and early detection of diseases. The advent of radar-based sensing systems has captured the spotlight for they are able to operate without physical contact and they can integrate with pre-existing Wi-Fi networks. They are also seen as less privacy-invasive compared to camera-based systems. However recent research has shown high accuracy in recognizing subjects or gender from radar gait patterns raising privacy concerns. This study addresses these issues by investigating privacy vulnerabilities in radar-based Human Activity Recognition (HAR) systems and proposing a novel method for privacy preservation using Differential Privacy (DP) driven by attributions derived with Integrated Decision Gradient (IDG) algorithm. We investigate Black-box Membership Inference Attack (MIA) Models in HAR settings across various levels of attacker-accessible information. We extensively evaluated the effectiveness of the proposed IDG-DP method by designing a CNN-based HAR model and rigorously assessing its resilience against MIAs. Experimental results demonstrate the potential of IDG-DP in mitigating privacy attacks while maintaining utility across all settings particularly excelling against label-only and shadow model black-box MIA attacks. This work represents a crucial step towards balancing the need for effective radar-based HAR with robust privacy protection in healthcare environments.	https://openaccess.thecvf.com//content/WACV2025/html/Zakariyya_Differentially_Private_Integrated_Decision_Gradients_IDG-DP_for_Radar-Based_Human_Activity_WACV_2025_paper.html	Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
Difficulty Diversity and Plausibility: Dynamic Data-Free Quantization	Without access to the original training data data-free quantization (DFQ) aims to recover the performance loss induced by quantization. Most previous works have focused on using an original network to extract the train data information which is instilled into surrogate synthesized images. However existing DFQ methods do not take into account important aspects of quantization: the extent of a computational-cost-and-accuracy trade-off varies for each image depending on its task difficulty. To handle such varying trade-offs several efforts have been made to dynamically allocate bit-widths for each image. Such dynamic quantization however remains challenging and unexplored in the data-free domain because synthesized images of previous works fail to possess properties in natural test images that are crucial for learning the appropriate dynamic allocation policy: difficulty its diversity and its plausibility. By contrast we propose a data-free quantization framework that is dynamic-friendly by modeling varying extents of task difficulties with plausibility. We generate plausibly difficult images with soft labels whose probabilities are allocated to a group of similar classes. Images with diverse and plausible difficulties enable us to train the framework to dynamically handle the varying trade-offs. Consequently our framework achieves better accuracy-complexity Pareto front than existing data-free quantization approaches.	https://openaccess.thecvf.com//content/WACV2025/html/Hong_Difficulty_Diversity_and_Plausibility_Dynamic_Data-Free_Quantization_WACV_2025_paper.html	Cheeun Hong, Sungyong Baik, Junghun Oh, Kyoung Mu Lee
DiffuCE: Expert-Level CBCT Image Enhancement using a Novel Conditional Denoising Diffusion Model with Latent Alignment	Cone-Beam Computed Tomography (CBCT) has garnered significant attention due to lower radiation dosage and faster scanning time which has been widely used in clinical applications for decades. However its poor image quality is always challenging to clinical experts. To address this problem we propose our work DiffuCE a Diffusion model framework for CBCT Enhancement. The main contributions of our work are three-fold: (1) Increased Generalizability: Our training data exclusively comprises pixel space data eliminating the necessity for additional imaging machine settings. This emphasizes the model's ability to generalize effectively across diverse conditions. (2) Efficient Training: Rather than starting from scratch our approach fine-tunes from a well-established foundation model. This illustrates the viability of efficient training strategies for medical image restoration tasks optimizing resource utilization. (3) Competitive Performance: DiffuCE exhibits outstanding performance excelling in FID and LPIPS with 0.01 and 36.99 ahead of the second place in the private set. In the public dataset DiffuCE has a competitive performance compared to other SOTAs. Moreover in expert assessments DiffuCE achieves the highest score of 7.06 for overall satisfaction which is 1.38 ahead of the second place affirming its performance from a clinical standpoint. Codes are available at https://github.com/lzh107u/DiffuCE	https://openaccess.thecvf.com//content/WACV2025/html/Su_DiffuCE_Expert-Level_CBCT_Image_Enhancement_using_a_Novel_Conditional_Denoising_WACV_2025_paper.html	Fang-Yi Su, Tzu-Hung Chang, Jung-Hsien Chiang
DiffuPT: Class Imbalance Mitigation for Glaucoma Detection via Diffusion Based Generation and Model Pretraining	"Glaucoma is a progressive optic neuropathy characterized by structural damage to the optic nerve head and functional changes in the visual field. Detecting glaucoma early is crucial to preventing loss of eyesight. However medical datasets often suffer from class imbalances making detection more difficult for deep-learning algorithms. We use a generative-based framework to enhance glaucoma diagnosis specifically addressing class imbalance through synthetic data generation. In addition we collected the largest national dataset for glaucoma detection to support our study. The imbalance between normal and glaucomatous cases leads to performance degradation of classifier models. By combining our proposed framework leveraging diffusion models with a pretraining approach we created a more robust classifier training process. This training process results in a better-performing classifier. The proposed approach shows promising results in improving the harmonic mean ""sensitivity and specificity"" and AUC for the roc for the glaucoma classifier. We report an improvement in the harmonic mean metric from 89.09% to 92.59% on the test set of our national dataset. We examine our method against other methods to overcome imbalance through extensive experiments. We report similar improvements on the AIROGS dataset. This study highlights that diffusion-based generation can be of great importance in tackling class imbalances in medical datasets to improve diagnostic performance."	https://openaccess.thecvf.com//content/WACV2025/html/Nawar_DiffuPT_Class_Imbalance_Mitigation_for_Glaucoma_Detection_via_Diffusion_Based_WACV_2025_paper.html	Youssof Nawar, Nouran Soliman, Moustafa Wassel, Mohamed ElHabebe, Noha Adly, Marwan Torki, Ahmed Elmassry, Islam Ahmed
DiffuseKronA: A Parameter Efficient Fine-Tuning Method for Personalized Diffusion Models	In the realm of subject-driven text-to-image (T2I) generative models recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters it introduces a pronounced sensitivity to hyperparameters leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints we introduce DiffuseKronA a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35% and 99.947% compared to LoRA-DreamBooth and the original DreamBooth respectively but also enhances the quality of image synthesis. Crucially DiffuseKronA mitigates the issue of hyperparameter sensitivity delivering consistent high-quality generations across a wide range of hyperparameters thereby diminishing the necessity for extensive fine-tuning. Furthermore a more controllable decomposition makes DiffuseKronA more interpretable and can even achieve up to a 50% reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse and complex input images and text prompts DiffuseKronA consistently outperforms existing low-rank models producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects all the while upholding exceptional parameter efficiency thus presenting a substantial advancement in the field of T2I generative modeling.	https://openaccess.thecvf.com//content/WACV2025/html/Marjit_DiffuseKronA_A_Parameter_Efficient_Fine-Tuning_Method_for_Personalized_Diffusion_Models_WACV_2025_paper.html	Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, Pin-Yu Chen
Diffusion Model Guided Sampling with Pixel-Wise Aleatoric Uncertainty Estimation	Despite the remarkable progress in generative modelling current diffusion models lack a quantitative approach to assess image quality. To address this limitation we propose to estimate the pixel-wise aleatoric uncertainty during the sampling phase of diffusion models and utilise the uncertainty to improve the sample generation quality. The uncertainty is computed as the variance of the denoising scores with a perturbation scheme that is specifically designed for diffusion models. We then show that the aleatoric uncertainty estimates are related to the second-order derivative of the diffusion noise distribution. We evaluate our uncertainty estimation algorithm and the uncertainty-guided sampling on the ImageNet and CIFAR- 10 datasets. In our comparisons with the related work we demonstrate promising results in filtering out low quality samples. Furthermore we show that our guided approach leads to better sample generation in terms of FID scores	https://openaccess.thecvf.com//content/WACV2025/html/De_Vita_Diffusion_Model_Guided_Sampling_with_Pixel-Wise_Aleatoric_Uncertainty_Estimation_WACV_2025_paper.html	Michele De Vita, Vasileios Belagiannis
Diffusion-Based Conditional Image Editing through Optimized Inference with Guidance	We present a simple but effective training-free approach for text-driven image-to-image translation based on a pretrained text-to-image diffusion model. Our goal is to generate an image that aligns with the target task while preserving the structure and background of a source image. To this end we derive the representation guidance with a combination of two objectives: maximizing the similarity to the target prompt based on the CLIP score and minimizing the structural distance to the source latent variable. This guidance improves the fidelity of the generated target image to the given target prompt while maintaining the structure integrity of the source image. To incorporate the representation guidance component we optimize the target latent variable of diffusion model's reverse process with the guidance. Experimental results demonstrate that our method achieves outstanding image-to-image translation performance on various tasks when combined with the pretrained Stable Diffusion model.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Diffusion-Based_Conditional_Image_Editing_through_Optimized_Inference_with_Guidance_WACV_2025_paper.html	Hyunsoo Lee, Minsoo Kang, Bohyung Han
Diffusion-Based Generative Regularization for Supervised Discriminative Learning	Ensuring the quality and quantity of labeled training data has long been a challenge in training deep neural networks for discriminative tasks. One solution to this problem is to use a generative model to augment training data and learn a discriminative model with it. For image classification with the recent development of diffusion models it has become possible to generate a variety synthetic images and there are high expectations for their use as training data. However to obtain high-quality labeled synthetic images the hyperparameters and prompts often need to be manually tuned and the accuracy of the trained image classification model is highly dependent on them. To address this issue this paper proposes diffusion-based generative regularization a supervised discriminative learning framework that utilizes a diffusion-based image generation model as a regularizer to robustly learn discriminative representations without the need to synthesize images. Our experiments using vision transformers and stable diffusion models on ImageNet-1k demonstrate that the proposed framework improves classification accuracy on both in-distribution and distribution-shifted data.	https://openaccess.thecvf.com//content/WACV2025/html/Asakura_Diffusion-Based_Generative_Regularization_for_Supervised_Discriminative_Learning_WACV_2025_paper.html	Takuya Asakura, Nakamasa Inoue, Koichi Shinoda
Diffusion-Based Particle-DETR for BEV Perception	The Bird-Eye-View (BEV) is one of the most widely-used scene representations for visual perception in Autonomous Vehicles (AVs) due to its well suited compatibility to downstream tasks. For the enhanced safety of AVs modeling perception uncertainty in BEV is crucial. Recent diffusion-based methods offer a promising approach to uncertainty modeling for visual perception but fail to effectively detect small objects in the large coverage of the BEV. Such degradation of performance can be attributed primarily to the specific network architectures and the matching strategy used when training. Here we address this problem by combining the diffusion paradigm with current state-of-the-art 3D object detectors in BEV. We analyze the unique challenges of this approach which do not exist with deterministic detectors and present a simple technique based on object query interpolation that allows the model to learn positional dependencies even in the presence of the diffusion noise. Based on this we present a diffusion-based DETR model for object detection that bears similarities to particle methods. Abundant experimentation on the NuScenes dataset shows equal or better performance for our generative approach compared to deterministic state-of-the-art methods. The source code is at https://github.com/insait-institute/ParticleDETR.	https://openaccess.thecvf.com//content/WACV2025/html/Nachkov_Diffusion-Based_Particle-DETR_for_BEV_Perception_WACV_2025_paper.html	Asen Nachkov, Danda Pani Paudel, Martin Danelljan, Luc Van Gool
Diffusion-Based Visual Anagram as Multi-Task Learning	Visual anagrams are images that change appearance upon transformation like flipping or rotation. With the advent of diffusion models generating such optical illusions can be achieved by averaging noise across multiple views during the reverse denoising process. However we observe two critical failure modes in this approach: (i) concept segregation where concepts in different views are independently generated which can not be considered a true anagram and (ii) concept domination where certain concepts overpower others. In this work we cast the visual anagram generation problem in a multi-task learning setting where different viewpoint prompts are analogous to different tasks and derive denoising trajectories that align well across tasks simultaneously. At the core of our designed framework are two newly introduced techniques where (i) an anti-segregation optimization strategy that promotes overlap in cross-attention maps between different concepts and (ii) a noise vector balancing method that adaptively adjusts the influence of different tasks. Additionally we observe that directly averaging noise predictions yields suboptimal performance because statistical properties may not be preserved prompting us to derive a noise variance rectification method. Extensive qualitative and quantitative experiments demonstrate our method's superior ability to generate visual anagrams spanning diverse concepts.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Diffusion-Based_Visual_Anagram_as_Multi-Task_Learning_WACV_2025_paper.html	Zhiyuan Xu, Yinhe Chen, Huan-ang Gao, Weiyan Zhao, Guiyu Zhang, Hao Zhao
DisCo: Discovering Common Affordance from Large Models for Actionable Part Perception	Actionable part perception for robotic object manipulation needs to perceive parts over open-world object categories within 3D space which is challenging as the appearance of the same part on different objects varies greatly. It is frequently observed that despite the huge intra-class difference in appearance the parts share common interactive functions over different objects i.e. common affordance. According to this observation we propose DisCo a novel technique that Discovers Common affordance information from powerful large models for guiding the actionable part perception across open-world objects. Specifically we first use a large language model to identify the object names that each part potentially belongs to and a text-to-image generative model to generate image examples for the queried objects constructing image-text paired data that indicate visual and semantic information of common affordance. Then our model encodes the common affordance information by learning to pair the object-part images with their text descriptions. Subsequently the 2D-pixel features are distilled into 3D space thus the 3D point features are enriched with not only the semantic information of open-set objects but also the common affordance information which is highly generalizable. Finally a segmentation head and a pose regression network are developed to predict more accurate results of part segmentation and pose estimation improving the success rate of robotic object manipulation. Extensive experiments show that our method outperforms existing methods on the part instance and semantic segmentation by significant margins of 4.8% mAP 5.4% AP50 and 3.9% mIoU on the unseen object categories.	https://openaccess.thecvf.com//content/WACV2025/html/Wen_DisCo_Discovering_Common_Affordance_from_Large_Models_for_Actionable_Part_WACV_2025_paper.html	Youpeng Wen, Yi Zhu, Zhihao Zhan, Pengzhen Ren, Jianhua Han, Hang Xu, Shen Zhao, Xiaodan Liang
DisFlowEm : One-Shot Emotional Talking Head Generation using Disentangled Pose and Expression Flow-Guidance	Generating realistic one-shot emotional talking head animation on arbitrary faces is a challenging problem as it requires realistic emotions head movements identity preservation and accurate lip sync. Existing emotional talking face generation methods either fail to retain the identity information of arbitrary subjects owing to the limited variability of existing emotional datasets or they fail to capture emotions accurately even if they preserve identity of arbitrary faces. Moreover most of the methods rely on additional input videos for driving poses and/or or expressions on the generated video. For practical applications it is infeasible to obtain driving videos of the same or different subject with variations in head pose expressions etc. In this paper we propose a novel approach for Audio-driven Emotional Talking Head generation from a single image with emotion-controllable head pose generation. Unlike existing methods our method does not require a driving video either for pose or emotions and can generate different emotions and diverse head pose variations from input speech and a single image of an arbitrary subject in neutral emotion. Our method overcomes the limitations of existing emotional audio-visual datasets by learning a disentangled approach for optical flow computation approach for pose and expression. Using our proposed method of independently computing pose-driven and expression-driven optical flow our image generation network can be pre-trained on a large dataset with greater pose variability but lacking emotion annotations. The expression flow generation branch is fine-tuned on a smaller emotional dataset to accurately capture different emotions not present in the original dataset while retaining the pose variability from the original dataset. We present extensive experiments to demonstrate the superiority of our proposed method in generating talking head animation with accurate emotions diverse head movements and generalization to arbitrary faces.	https://openaccess.thecvf.com//content/WACV2025/html/Sinha_DisFlowEm__One-Shot_Emotional_Talking_Head_Generation_using_Disentangled_Pose_WACV_2025_paper.html	Sanjana Sinha, Brojeshwar Bhowmick, Lokender Tiwari, Sushovan Chanda
Discriminative Score Suppression for Weakly Supervised Video Anomaly Detection	Weakly supervised video anomaly detection (WSVAD) often relies on Multiple Instance Learning (MIL). However selecting only the most discriminative segments for training limits the model's ability to comprehensively detect anomalous events particularly hard anomalies. To overcome this limitation we propose the Discriminative Score Suppression (DSS) module. This module suppresses the discriminative scores of the most prominent anomalies shifting the model's attention to less obvious but important hard anomalies. This approach guides the model to learn the critical features of hard anomalies enabling a more comprehensive detection of anomalous events. Additionally the Anomaly Score Refinement (ASR) module constructs a dissimilarity-based classifier by storing normal patterns as prototypes and integrates this with a neural network classifier. By combining the anomaly scores from both classifiers more accurate detection of true hard anomalies is achieved. A score-sensitive inner-bag loss function not only adjusts penalties based on anomaly scores but also ensures that the model avoids erroneous selections. Our method accurately detects various anomalies including challenging and multi-segment anomalies while minimizing false positives for normal events. Extensive experiments show that the proposed framework outperforms state-of-the-art methods on the UCF-Crime and XD-Violence datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Discriminative_Score_Suppression_for_Weakly_Supervised_Video_Anomaly_Detection_WACV_2025_paper.html	Chen Xu, Chunguo Li, Hongjie Xing
Disentangle Source and Target Knowledge for Continual Test-Time Adaptation	Continual Test-Time Adaptation (CoTTA) task is proposed to tackle the challenges of constant domain shifts during testing. The goals are twofold: 1) to preserve the knowledge from the source domain without source data and 2) to effectively extract target knowledge using unlabeled target domain data. Existing works primarily focus on either source or target knowledge attempting to learn both in a mixed manner. We argue that this may harm the source knowledge preservation and target knowledge extraction. To this end this paper proposes a Source and Target knowledge Disentangle Transformer (SoTa-DiT) with the prompting mechanism. Specifically in a vision transformer (ViT) we incorporate source and target prompts supervised by two groups of deliberately designed loss functions to learn source and target knowledge separately. The source prompt focuses on anti-source-forgetting by extracting and preserving knowledge from the source model while the target prompt focuses on pro-target-extracting using target data contrastive learning. With comprehensive evaluations across various datasets using different ViT backbones we demonstrate that this dual-prompt architecture of SoTa-DiT is effective and that disentangling knowledge with the prompts benefits CoTTA. As a result SoTa-DiT significantly improves image classification accuracy under the CoTTA setting.	https://openaccess.thecvf.com//content/WACV2025/html/Ma_Disentangle_Source_and_Target_Knowledge_for_Continual_Test-Time_Adaptation_WACV_2025_paper.html	Tianyi Ma, Maoying Qiao
Disentangling Disentangled Representations: Towards Improved Latent Units via Diffusion Models	Disentangled representation learning (DRL) aims to break down observed data into core intrinsic factors for a profound understanding of the data. In real-world scenarios manually defining and labeling these factors are non-trivial making unsupervised methods attractive. Recently there have been limited explorations of utilizing diffusion models (DMs) which are already mainstream in generative modeling for unsupervised DRL. They implement their own inductive bias to ensure that each latent unit input to the DM expresses only one distinct factor. In this context we design Dynamic Gaussian Anchoring to enforce attribute-separated latent units for more interpretable DRL. This unconventional inductive bias explicitly delineates the decision boundaries between attributes while also promoting the independence among latent units. Additionally we also propose Skip Dropout technique which easily modifies the denoising U-Net to be more DRL-friendly addressing its uncooperative nature with the disentangling feature extractor. Our methods which carefully consider the latent unit semantics and the distinct DM structure enhance the practicality of DM-based disentangled representations demonstrating state-of-the-art disentanglement performance on both synthetic and real data as well as advantages in downstream tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Jun_Disentangling_Disentangled_Representations_Towards_Improved_Latent_Units_via_Diffusion_Models_WACV_2025_paper.html	Youngjun Jun, Jiwoo Park, Kyobin Choo, Tae Eun Choi, Seong Jae Hwang
Disentangling Spatio-Temporal Knowledge for Weakly Supervised Object Detection and Segmentation in Surgical Video	Weakly supervised video object segmentation (WSVOS) enables the identification of segmentation maps without requiring extensive annotations of object masks relying instead on coarse video labels indicating object presence. Weakly supervised semantic segmentation of objects in surgical videos is however more challenging due to a complex interaction of multiple transient objects such as surgical tools moving in and out of the surgical field. In this scenario state-of-the-art WSVOS methods struggle to learn accurate segmentation maps. We address this problem by introducing ViDeo Spatio-Temporal disentanglement Networks (VDST-Net) a framework to disentangle complex spatiotemporal object interactions using semi-decoupled knowledge distillation to predict high-quality class activation maps (CAMs). A teacher network is designed to help a temporal-reasoning student network resolve activation conflicts as the student leverages temporal dependencies when specifics about object location and timing in the video are not provided. We demonstrate the efficacy of our framework on a challenging surgical video dataset where objects are on average present in less than 60% of annotated frames and compare our method to state-of-the-art methods on surgical data and on a public dataset commonly used to benchmark WSVOS. Our method outperforms state-of-the-art techniques and generates accurate segmentation masks under video-level weak supervision. Our code is available at: https://github.com/PCASOlab/VDST-net.	https://openaccess.thecvf.com//content/WACV2025/html/Liao_Disentangling_Spatio-Temporal_Knowledge_for_Weakly_Supervised_Object_Detection_and_Segmentation_WACV_2025_paper.html	Guiqiu Liao, Matjaz Jogan, Sai Koushik, Eric Eaton, Daniel A. Hashimoto
Disentangling Subject-Irrelevant Elements in Personalized Text-to-Image Diffusion via Filtered Self-Distillation	Recent research has unveiled the development of customizing large-scale text-to-image models. These models bind a unique subject desired by a user to a specific token using the token to generate the subject in various contexts. However models from previous studies also bind elements unrelated to the subject's identity such as common backgrounds or poses in the reference images. This often leads to conflicts between the token and the context of text prompts during inference causing the model to fail to generate both the subject and the prompted context. In this work we approach this issue from a data scarcity perspective and propose to augment the number of reference images through a novel self-distillation framework. Our framework selects high-quality samples from images generated by a teacher model and uses them in student training. Our framework can be applied to any models that suffer from the conflicts and we demonstrate that our framework most effectively resolves the issue through comprehensive evaluations.	https://openaccess.thecvf.com//content/WACV2025/html/Choi_Disentangling_Subject-Irrelevant_Elements_in_Personalized_Text-to-Image_Diffusion_via_Filtered_Self-Distillation_WACV_2025_paper.html	Seunghwan Choi, Jooyeol Yun, Jeonghoon Park, Jaegul Choo
Distillation of Diffusion Features for Semantic Correspondence	Semantic correspondence the task of determining relationships between different parts of images underpins various applications including 3D reconstruction image-to-image translation object tracking and visual place recognition. Recent studies have begun to explore representations learned in large generative image models for semantic correspondence demonstrating promising results. Building on this progress current state-of-the-art methods rely on combining multiple large models resulting in high computational demands and reduced efficiency. In this work we address this challenge by proposing a more computationally efficient approach. We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency. We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost. Furthermore we demonstrate that by incorporating 3D data we are able to further improve performance without the need for human-annotated correspondences. Overall our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications such as semantic video correspondence. Our code and weights are publicly available on our project page.	https://openaccess.thecvf.com//content/WACV2025/html/Fundel_Distillation_of_Diffusion_Features_for_Semantic_Correspondence_WACV_2025_paper.html	Frank Fundel, Johannes Schusterbauer, Vincent Tao Hu, BjÃ¶rn Ommer
Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection	Video anomaly detection aims to develop automated models capable of identifying abnormal events in surveillance videos. The benchmark setup for this task is extremely challenging due to: i) the limited size of the training sets ii) weak supervision provided in terms of video-level labels and iii) intrinsic class imbalance induced by the scarcity of abnormal events. In this work we show that distilling knowledge from aggregated representations of multiple backbones into a single-backbone Student model achieves state-of-the-art performance. In particular we develop a bi-level distillation approach along with a novel disentangled cross-attention-based feature aggregation network. Our proposed approach DAKD (Distilling Aggregated Knowledge with Disentangled Attention) demonstrates superior performance compared to existing methods across multiple benchmark datasets. Notably we achieve significant improvements of 1.36% 0.78% and 7.02% on the UCF-Crime ShanghaiTech and XD-Violence datasets respectively.	https://openaccess.thecvf.com//content/WACV2025/html/Dalvi_Distilling_Aggregated_Knowledge_for_Weakly-Supervised_Video_Anomaly_Detection_WACV_2025_paper.html	Jash Dalvi, Ali Dabouei, Gunjan Dhanuka, Min Xu
Distribution Optimization under Gaussian Hypothesis for Domain Adaptive Semantic Segmentation	Domain adaptive semantic segmentation aims to transfer a model proficient in dense image classification from a source domain to a target domain. While various transfer methods have been explored in previous studies we argue that the modeling of categories within the model significantly affects its transferability. Building on the Gaussian Hypothesis which posits that each category in the feature space adheres to a multidimensional Gaussian distribution we propose a Class-Aware Variational Inference (CAVI) training method. This approach normalizes features of different categories into distinct multidimensional Gaussian distributions. To further learn domain-independent feature distributions we optimize the feature space using a Gaussian-based alignment strategy and incorporate Gaussian-based contrastive learning. Experimental results demonstrate that our method achieves state-of-the-art performance on the GTAV-to-Cityscapes and Synthia-to-Cityscapes benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Liang_Distribution_Optimization_under_Gaussian_Hypothesis_for_Domain_Adaptive_Semantic_Segmentation_WACV_2025_paper.html	Chen Liang, Weihua Chen, Xin Zhao, Junyan Wang, Lijun Cao, Junge Zhang
DivAvatar: Diverse 3D Avatar Generation with a Single Prompt	Text-to-Avatar generation has recently made significant strides due to advancements in diffusion models. However most existing work remains constrained by limited diversity producing avatars with subtle differences in appearance for a given text prompt. We design DivAvatar a novel framework that generates diverse avatars empowering 3D creatives with a multitude of distinct and richly varied 3D avatars from a single text prompt. Different from most existing work that exploits scene-specific 3D representations such as NeRF DivAvatar finetunes a 3D generative model (i.e. EVA3D) allowing diverse avatar generation from simply noise sampling in inference time. DivAvatar has two key designs that help achieve generation diversity and visual quality. The first is a noise sampling technique during training phase which is critical in generating diverse appearances. The second is a semantic-aware zoom mechanism and a novel depth loss the former producing appearances of high textual fidelity by separate fine-tuning of specific body parts and the latter improving geometry quality greatly by smoothing the generated mesh in the features space. Extensive experiments show that DivAvatar is highly versatile in generating avatars of diverse appearances.	https://openaccess.thecvf.com//content/WACV2025/html/Tao_DivAvatar_Diverse_3D_Avatar_Generation_with_a_Single_Prompt_WACV_2025_paper.html	Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie
Divergent Domains Convergent Grading: Enhancing Generalization in Diabetic Retinopathy Grading	Diabetic Retinopathy (DR) constitutes 5% of global blindness cases. While numerous deep learning approaches have sought to enhance traditional DR grading methods they often falter when confronted with new out-of-distribution data thereby impeding their widespread application. In this study we introduce a novel deep learning method for achieving domain generalization (DG) in DR grading and make the following contributions. First we propose a new way of generating image-to-image diagnostically relevant fundus augmentations conditioned on the grade of the original fundus image. These augmentations are tailored to emulate the types of shifts in DR datasets thus increase the model's robustness. Second we address the limitations of the standard classification loss in DG for DR fundus datasets by proposing a new DG-specific loss - domain alignment loss; which ensures that the feature vectors from all domains corresponding to the same class converge onto the same manifold for better domain generalization. Third we tackle the coupled problem of data imbalance across DR domains and classes by proposing to employ Focal loss which seamlessly integrates with our new alignment loss. Fourth due to inevitable observer variability in DR diagnosis that induces label noise we propose leveraging self-supervised pretraining. This approach ensures that our DG model remains robust against early susceptibility to label noise even when only a limited dataset of non-DR fundus images is available for pretraining. Our method demonstrates significant improvements over the strong Empirical Risk Minimization baseline and other recently proposed state-of-the-art DG methods for DR grading. Code is available at https://github.com/sharonchokuwa/dg-adr.	https://openaccess.thecvf.com//content/WACV2025/html/Chokuwa_Divergent_Domains_Convergent_Grading_Enhancing_Generalization_in_Diabetic_Retinopathy_Grading_WACV_2025_paper.html	Sharon Chokuwa, Muhammad Haris Khan
DocMatcher: Document Image Dewarping via Structural and Textual Line Matching	Document image dewarping is a crucial step in the digitization of physical documents as it aims to remove the distortions induced by challenging environment settings and document sheet deformations often encountered when using smartphone cameras for image capture. Recently deep learning-based methods were combined with knowledge about the expected document structure also known as a template at inference time to improve the dewarping results. Our contributions in this work are threefold: (1) we propose a novel document image dewarping approach that leverages the prior knowledge about the document structure effectively by detecting and matching lines from the warped and the template domain and (2) we introduce a novel evaluation metric called matched normalized character error rate (mnCER) to overcome the limitations of existing metrics in evaluating the dewarping process. (3) Finally we evaluate our approach on the Inv3DReal dataset and show that our approach outperforms the state-of-the-art methods in terms of visual and text-based metrics. Our approach improves upon the state-of-the-art methods by 32.6% in Local Distortion and 40.2% in mnCER. Our code and models are available at https://felixhertlein.github.io/doc-matcher.	https://openaccess.thecvf.com//content/WACV2025/html/Hertlein_DocMatcher_Document_Image_Dewarping_via_Structural_and_Textual_Line_Matching_WACV_2025_paper.html	Felix Hertlein, Alexander Naumann, York Sure-Vetter
DocTTT: Test-Time Training for Handwritten Document Recognition using Meta-Auxiliary Learning	Despite recent significant advancements in Handwritten Document Recognition (HDR) the efficient and accurate recognition of text against complex backgrounds diverse handwriting styles and varying document layouts remains a practical challenge. Moreover this issue is seldom addressed in academic research particularly in scenarios with minimal annotated data available. In this paper we introduce the DocTTT framework to address these challenges. The key innovation of our approach is that it uses test-time training to adapt the model to each specific input during testing. We propose a novel Meta-Auxiliary learning approach that combines Meta-learning and self-supervised Masked Autoencoder(MAE). During testing we adapt the visual representation parameters using a self-supervised MAE loss. During training we learn the model parameters using a meta-learning framework so that the model parameters are learned to adapt to a new input effectively. Experimental results show that our proposed method significantly outperforms existing state-of-the-art approaches on benchmark datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Gu_DocTTT_Test-Time_Training_for_Handwritten_Document_Recognition_using_Meta-Auxiliary_Learning_WACV_2025_paper.html	Wenhao Gu, Li Gu, Ziqiang Wang, Ching Y Suen, Yang Wang
Domain Generalization using Large Pretrained Models with Mixture-of-Adapters	Learning robust vision models that perform well in out-of-distribution (OOD) situations is an important task for model deployment in real-world settings. Despite extensive research in this field many proposed methods have only shown minor performance improvements compared to the simplest empirical risk minimization (ERM) approach which was evaluated on a benchmark with a limited hyperparameter search space. Our focus in this study is on leveraging the knowledge of large pretrained models to improve handling of OOD scenarios and tackle domain generalization problems. However prior research has revealed that naively fine-tuning a large pretrained model can impair OOD robustness. Thus we employ parameter-efficient fine-tuning (PEFT) techniques to effectively preserve OOD robustness while working with large models. Our extensive experiments and analysis confirm that the most effective approaches involve ensembling diverse models and increasing the scale of pretraining. As a result we achieve state-of-the-art performance in domain generalization tasks. Our code and project page are available at: https://cvlab-kaist.github.io/MoA	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Domain_Generalization_using_Large_Pretrained_Models_with_Mixture-of-Adapters_WACV_2025_paper.html	Gyuseong Lee, Wooseok Jang, Jinhyeon Kim, Jaewoo Jung, Seungryong Kim
Domain-Generalized Object Anti-Spoofing: Bridging Gaps and Patch Selection for Robust Detection Across Domains	In online applications significant risks exist in peer-to-peer transactions due to malicious behaviors of arbitrary users such as taking advantage of manipulated images or impersonating others using recaptured images. Moreover recent advancements in display screens and imaging devices have made it increasingly challenging to distinguish such spoofing images from the naked eye. However a lack of datasets for object anti-spoofing significantly hinders the practical implementation of object anti-spoofing techniques compared to facial anti-spoofing tasks. To address this data scarcity issue for object anti-spoofing we propose a method that utilizes face anti-spoofing images for training. Our approach leverages low-rank adaptation employing fine-tuning with downstream tasks of large language models to facilitate domain transition between faces and generic objects. We also analyze a power spectrum to select useful patches for spoofing detection and introduce a patch-based learning method to effectively capture spoofing patterns. Lastly we present a novel protocol for assessing domain generalization in the generic object anti-spoofing task. Our model demonstrates state-of-the-art generalization performance compared to existing object anti-spoofing models surpassing even those simply augmented with face datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Domain-Generalized_Object_Anti-Spoofing_Bridging_Gaps_and_Patch_Selection_for_Robust_WACV_2025_paper.html	Geonu Lee, Yonghyun Jeong, Haneol Jang, Youngjoon Yoo
Domain-Guided Weight Modulation for Semi-Supervised Domain Generalization	Unarguably deep learning models capable of generalizing to unseen domain data while leveraging a few labels are of great practical significance due to low developmental costs. In search of this endeavor we study the challenging problem of semi-supervised domain generalization (SSDG) where the goal is to learn a domain-generalizable model while using only a small fraction of labeled data and a relatively large fraction of unlabeled data. Domain generalization (DG) methods show subpar performance under the SSDG setting whereas semi-supervised learning (SSL) methods demonstrate relatively better performance however they are considerably poor compared to the fully-supervised DG methods. Towards handling this new but challenging problem of SSDG we propose a novel method that can facilitate the generation of accurate pseudo-labels under various domain shifts. This is accomplished by retaining the domain-level specialism in the classifier during training corresponding to each source domain. Specifically we first create domain-level information vectors on the fly which are then utilized to learn a domain-aware mask for modulating the classifier's weights. We provide a mathematical interpretation for the effect of this modulation procedure on both pseudo-labeling and model training. Our method is plug-and-play and can be readily applied to different SSL baselines for SSDG. Extensive experiments on six challenging datasets in two different SSDG settings show that our method provides visible gains over the various strong SSL-based SSDG baselines. Our code is available at github.com/DGWM.	https://openaccess.thecvf.com//content/WACV2025/html/Galappaththige_Domain-Guided_Weight_Modulation_for_Semi-Supervised_Domain_Generalization_WACV_2025_paper.html	Chamuditha Jayanga Galappaththige, Zachary Izzo, Xilin He, Honglu Zhou, Muhammad Haris Khan
DrIFT: Autonomous Drone Dataset with Integrated Real and Synthetic Data Flexible Views and Transformed Domains	Dependable visual drone detection is crucial for the secure integration of drones into the airspace. However drone detection accuracy is significantly affected by domain shifts due to environmental changes varied points of view and background shifts. To address these challenges we present the DrIFT dataset specifically developed for visual drone detection under domain shifts. DrIFT includes fourteen distinct domains each characterized by shifts in point of view synthetic-to-real data season and adverse weather. DrIFT uniquely emphasizes background shift by providing background segmentation maps to enable background-wise metrics and evaluation. Our new uncertainty estimation metric MCDO-map features lower postprocessing complexity surpassing traditional methods. We use the MCDO-map in our uncertainty-aware unsupervised domain adaptation method demonstrating superior performance to SOTA unsupervised domain adaptation techniques. The dataset is available at: https://github.com/CARG-uOttawa/DrIFT.git.	https://openaccess.thecvf.com//content/WACV2025/html/Dadboud_DrIFT_Autonomous_Drone_Dataset_with_Integrated_Real_and_Synthetic_Data_WACV_2025_paper.html	Fardad Dadboud, Hamid Azad, Varun Mehta, Miodrag Bolic, Iraj Mantegh
DragText: Rethinking Text Embedding in Point-Based Image Editing	Point-based image editing enables accurate and flexible control through content dragging. However the role of text embedding during the editing process has not been thoroughly investigated. A significant aspect that remains unexplored is the interaction between text and image embeddings. During the progressive editing in a diffusion model the text embedding remains constant. As the image embedding increasingly diverges from its initial state the discrepancy between the image and text embeddings presents a significant challenge. In this study we found that the text prompt significantly influences the dragging process particularly in maintaining content integrity and achieving the desired manipulation. Upon these insights we propose DragText which optimizes text embedding in conjunction with the dragging process to pair with the modified image embedding. Simultaneously we regularize the text optimization process to preserve the integrity of the original text prompt. Our approach can be seamlessly integrated with existing diffusion-based drag methods enhancing performance with only a few lines of code.	https://openaccess.thecvf.com//content/WACV2025/html/Choi_DragText_Rethinking_Text_Embedding_in_Point-Based_Image_Editing_WACV_2025_paper.html	Gayoon Choi, Taejin Jeong, Sujung Hong, Seong Jae Hwang
DragonTrack: Transformer-Enhanced Graphical Multi-Person Tracking in Complex Scenarios	This paper introduces the dynamic robust adaptive graph-based tracker (DragonTrack) as a novel end-to-end framework for multi-person tracking (MPT) by integrating a detection transformer model for object detection and feature extraction with a graph convolutional network for re-identification. DragonTrack leverages encoded features from the transformer for precise subject matching and track maintenance while the graphical component processes these features alongside geometric data to predict subsequent positions of tracked people. This methodology aims to enhance tracking accuracy and reliability as evidenced by improvements in key metrics such as higher order tracking accuracy (HOTA) and multiple object tracking accuracy (MOTA). We quantitatively compare DragonTrack with state-of-the-art methods on MOT17 MOT20 and DanceTrack datasets in which DragonTrack outperforms other methods. In challenging scenarios such as DanceTrack DragonTrack achieves an impressive MOTA score of 93.4 significantly higher than the second-best SOTA method ByteTrack which achieves only 89.6. Similarly on MOT17 DragonTrack scores 82.0 in MOTA surpassing the closest competitor with a score of 80.3. On MOT20 DragonTrack attains a HOTA score of 63.2 outperforming the next best method scoring 62.6	https://openaccess.thecvf.com//content/WACV2025/html/Galoaa_DragonTrack_Transformer-Enhanced_Graphical_Multi-Person_Tracking_in_Complex_Scenarios_WACV_2025_paper.html	Bishoy Galoaa, Somaieh Amraee, Sarah Ostadabbas
DreaMo: Articulated 3D Reconstruction from a Single Casual Video	Articulated 3D reconstruction has valuable applications in various domains yet it remains costly and demands intensive work from domain experts. Recent advancements in template-free learning methods show promising results with monocular videos. Nevertheless these approaches necessitate a comprehensive coverage of all viewpoints of the subject in the input video thus limiting their applicability to casually captured videos from online sources. In this work we study articulated 3D shape reconstruction from a single and casually captured Internet video where the subject's view coverage is incomplete. We propose DreaMo that jointly performs shape reconstruction while solving the challenging low-coverage regions with view-conditioned diffusion prior and several tailored regularizations. In addition we introduce a skeleton generation strategy to create human-interpretable skeletons from the learned neural bones and skinning weights without any predefined skeleton structures. We conduct our study on a self-collected internet video collection characterized by incomplete view coverage. DreaMo shows promising quality in novel-view rendering detailed articulated shape reconstruction and skeleton generation. Extensive qualitative and quantitative studies validate the efficacy of each proposed component and show existing methods are unable to solve correct geometry due to the incomplete view coverage.	https://openaccess.thecvf.com//content/WACV2025/html/Tu_DreaMo_Articulated_3D_Reconstruction_from_a_Single_Casual_Video_WACV_2025_paper.html	Tao Tu, Ming-Feng Li, Chieh Hubert Lin, Yen-Chi Cheng, Min Sun, Ming-Hsuan Yang
DreamBlend: Advancing Personalized Fine-Tuning of Text-to-Image Diffusion Models	Given a small number of images of a subject personalized image generation techniques can fine-tune large pre-trained text-to-image diffusion models to generate images of the subject in novel contexts conditioned on text prompts. In doing so a trade-off is made between prompt fidelity subject fidelity and diversity. As the pre-trained model is fine-tuned earlier checkpoints synthesize images with low subject fidelity but high prompt fidelity and diversity. In contrast later checkpoints generate images with low prompt fidelity and diversity but high subject fidelity. This inherent trade-off limits the prompt fidelity subject fidelity and diversity of generated images. In this work we propose DreamBlend to combine the prompt fidelity from earlier checkpoints and the subject fidelity from later checkpoints during inference. We perform a cross attention guided image synthesis from a later checkpoint guided by an image generated by an earlier checkpoint for the same prompt. This enables generation of images with better subject fidelity prompt fidelity and diversity on challenging prompts outperforming state-of-the-art fine-tuning methods.	https://openaccess.thecvf.com//content/WACV2025/html/Ram_DreamBlend_Advancing_Personalized_Fine-Tuning_of_Text-to-Image_Diffusion_Models_WACV_2025_paper.html	Shwetha Ram, Tal Neiman, Qianli Feng, Andrew M Stuart, Son Tran, Trishul A Chilimbi
Dropout Connects Transformers and CNNs: Transfer General Knowledge for Knowledge Distillation	Thanks to their long-range dependencies transformers obtain state-of-the-art performance in diverse research fields such as computer vision and audio processing. In practical scenarios convolutional neural networks (CNNs) are used more than Transformers due to their low complexity. So Transformer-to-CNN knowledge distillation (KD) research where the Transformer is the teacher and the CNN is the student is in demand and receiving attention. In Transformer-to-CNN KD training the capacity gap problem arising from structural differences between the teacher and student networks is the main factor of performance degradation of the student network unlike homogenous architecture KD. However previous KD studies transfer all of a teacher's knowledge to the student without considering structural differences. They cannot overcome problems caused by structural differences and show poor performance in Transformer-to-CNN KD. In this paper we identify general and specific knowledge in feature maps of the teacher and student. General and specific knowledge are the generalized and non-generalized feature representation. We propose a novel KD framework DropKD which extracts general knowledge from the teacher and student while removing specific knowledge and then allows general knowledge of the student network to learn general knowledge of the teacher. Our DropKD empowers the student network to achieve generalization by effectively managing general and specific knowledge. Through extensive experiments on challenging image classification datasets we demonstrate that the proposed method is superior to existing methods.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Dropout_Connects_Transformers_and_CNNs_Transfer_General_Knowledge_for_Knowledge_WACV_2025_paper.html	Bokyeung Lee, Jonghwan Hong, Hyunuk Shin, Bonwha Ku, Hanseok Ko
Dropout the High-Rate Downsampling: A Novel Design Paradigm for UHD Image Restoration	With the popularization of high-end mobile devices Ultra-high-definition (UHD) images have become ubiquitous in our lives. The restoration of UHD images is a highly challenging problem due to the exaggerated pixel count which often leads to memory overflow during processing. Existing methods either downsample UHD images at a high rate before processing or split them into multiple patches for separate processing. However high-rate downsampling leads to significant information loss while patch-based approaches inevitably introduce boundary artifacts. In this paper we propose a novel design paradigm to solve the UHD image restoration problem called D2Net. D2Net enables direct full-resolution inference on UHD images without the need for high-rate downsampling or dividing the images into several patches. Specifically we ingeniously utilize the characteristics of the frequency domain to establish long-range dependencies of features. Taking into account the richer local patterns in UHD images we also design a multi-scale convolutional group to capture local features. Additionally during the decoding stage we dynamically incorporate features from the encoding stage to reduce the flow of irrelevant information. Extensive experiments on three UHD image restoration tasks including low-light image enhancement image dehazing and image deblurring show that our model achieves better quantitative and qualitative results than state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_Dropout_the_High-Rate_Downsampling_A_Novel_Design_Paradigm_for_UHD_WACV_2025_paper.html	Chen Wu, Ling Wang, Long Peng, Dianjie Lu, Zhuoran Zheng
Dual-Representation Interaction Driven Image Quality Assessment with Restoration Assistance	No-Reference Image Quality Assessment for distorted images has always been a challenging problem due to image content variance and distortion diversity. Previous IQA models mostly encode explicit single-quality features of synthetic images to obtain quality-aware representations for quality score prediction. However performance decreases when facing real-world distortion and restored images from restoration models. The reason is that they do not consider the degradation factors of the low-quality images adequately. To address this issue we first introduce the DRI method to obtain degradation vectors and quality vectors of images which separately model the degradation and quality information of low-quality images. After that we add the restoration network to provide the MOS score predictor with degradation information. Then we design the Representation-based Semantic Loss (RS Loss) to assist in enhancing effective interaction between representations. Extensive experimental results demonstrate that the proposed method performs favorably against existing state-of-the-art models on both synthetic and real-world datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Yue_Dual-Representation_Interaction_Driven_Image_Quality_Assessment_with_Restoration_Assistance_WACV_2025_paper.html	Jingtong Yue, Xin Lin, Zijiu Yang, Chao Ren
Dual-Schedule Inversion: Training- and Tuning-Free Inversion for Real Image Editing	Text-conditional image editing is a practical AIGC task that has recently emerged with great commercial and academic value. For real image editing most diffusion model-based methods use DDIM Inversion as the first stage before editing. However DDIM Inversion often results in reconstruction failure leading to unsatisfactory performance for downstream editing. To address this problem we first analyze why the reconstruction via DDIM Inversion fails. We then propose a new inversion and sampling method named Dual-Schedule Inversion. We also design a classifier to adaptively combine Dual-Schedule Inversion with different editing methods for user-friendly image editing. Our work can achieve superior reconstruction and editing performance with the following advantages: 1) It can reconstruct real images perfectly without fine-tuning and its reversibility is guaranteed mathematically. 2) The edited object/scene conforms to the semantics of the text prompt. 3) The unedited parts of the object/scene retain the original identity.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_Dual-Schedule_Inversion_Training-_and_Tuning-Free_Inversion_for_Real_Image_Editing_WACV_2025_paper.html	Jiancheng Huang, Yi Huang, Jianzhuang Liu, Donghao Zhou, Yifan Liu, Shifeng Chen
DualCIR: Enhancing Training-Free Composed Image Retrieval via Dual-Directional Descriptions	Integrating language and images for Composed Image Retrieval (CIR) allows for a flexible representation of search intent making it a focal point in multi-modal research. Traditional CIR methods train models on complex triplet datasets. In contrast Zero-Shot Composed Image Retrieval (ZS-CIR) eliminates the need for constructing datasets and training models for each specific task attracting significant attention from researchers. CIReVL is a training-free method that translates visual content into textual representations employing large language models (LLMs) to perform robust text inference and visual language models (VLMs) to ensure multi-modal alignment during retrieval. Although this approach achieves high performance without training and offers strong interpretability relying solely on single descriptive texts for retrieval has limitations in handling complex search demands. To address this issue we propose a DualCIR framework which utilizes an LLM to generate refined dual-directional descriptions--both positive and negative--alongside generating holistic target descriptions. We score retrieval data separately via these descriptions merge the scores perform ranking and ultimately achieve retrieval. Extensive experiments across three datasets in the natural image and fashion domains show that our approach maintains low inference costs while noticeably enhancing retrieval effectiveness. Compared to existing training-free methods our DualCIR achieves state-of-the-art performance and is even competitive with the training-based methods.	https://openaccess.thecvf.com//content/WACV2025/html/Zhao_DualCIR_Enhancing_Training-Free_Composed_Image_Retrieval_via_Dual-Directional_Descriptions_WACV_2025_paper.html	Jingjiao Zhao, Jiaju Li, Dongze Lian, Liguo Sun, Pin Lv
DyRoNet: Dynamic Routing and Low-Rank Adapters for Autonomous Driving Streaming Perception	The advancement of autonomous driving systems hinges on the ability to achieve low-latency and high-accuracy perception. To address this critical need this paper introduces Dynamic Routering Network (DyRoNet) a low-rank enhanced dynamic routing framework designed for streaming perception in autonomous driving systems. DyRoNet integrates a suite of pre-trained branch networks each meticulously fine-tuned to function under distinct environmental conditions. At its core the framework offers a speed router module developed to assess and route input data to the most suitable branch for processing. This approach not only addresses the inherent limitations of conventional models in adapting to diverse driving conditions but also ensures the balance between performance and efficiency. Extensive experimental evaluations demonstrating the adaptability of DyRoNet to diverse branch selection strategies resulting in significant performance enhancements across different scenarios. This work not only establishes a new benchmark for streaming perception but also provides valuable engineering insights for future work.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_DyRoNet_Dynamic_Routing_and_Low-Rank_Adapters_for_Autonomous_Driving_Streaming_WACV_2025_paper.html	Xiang Huang, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Wangmeng Xiang, Baigui Sun
Dynamic Adapter Tuning for Long-Tailed Class-Incremental Learning	Long-tailed class-incremental learning (LT-CIL) aims to learn new classes continuously from a long-tailed data stream while simultaneously dealing with challenges such as imbalanced learning of tail classes and catastrophic forgetting. To address these challenges most existing methods employ a two-stage strategy by initializing model training from scratch with further balanced knowledge driven calibration. This strategy faces challenges in deriving discriminative features from cold-started backbones for the long-tailed distribution of data consequently leading to relatively diminished performance. In this paper with the powerful feature extraction capability of pre-trained foundation models we have achieved a one-stage approach that delivers superior performance. Specifically we propose Dynamic Adapter Tuning (DAT) which employs a dynamic adapter cache mechanism to adapt a pre-trained model to learn tasks sequentially. The adapter in the cache is either dynamically selected or created according to task similarity and further compactified with the new task's adapter to mitigate cross-task and cross-class gaps in LT-CIL significantly alleviating catastrophic forgetting and imbalance learning issues respectively. With extensive experimental validation our method consistently achieves state-of-the-art performance under the challenging LT-CIL setting.	https://openaccess.thecvf.com//content/WACV2025/html/Gu_Dynamic_Adapter_Tuning_for_Long-Tailed_Class-Incremental_Learning_WACV_2025_paper.html	Yanan Gu, Muli Yang, Xu Yang, Kun Wei, Hongyuan Zhu, Gabriel James Goenawan, Cheng Deng
Dynamic Attention-Guided Diffusion for Image Super-Resolution	"Diffusion models in image Super-Resolution (SR) treat all image regions uniformly which risks compromising the overall image quality by potentially introducing artifacts during denoising of less-complex regions. To address this we propose ""You Only Diffuse Areas"" (YODA) a dynamic attention-guided diffusion process for image SR. YODA selectively focuses on spatial regions defined by attention maps derived from the low-resolution images and the current denoising time step. This time-dependent targeting enables a more efficient conversion to high-resolution outputs by focusing on areas that benefit the most from the iterative refinement process i.e. detail-rich objects. We empirically validate YODA by extending leading diffusion-based methods SR3 DiffBIR and SRDiff. Our experiments demonstrate new state-of-the-art performances in face and general SR tasks across PSNR SSIM and LPIPS metrics. As a side effect we find that YODA reduces color shift issues and stabilizes training with small batches."	https://openaccess.thecvf.com//content/WACV2025/html/Moser_Dynamic_Attention-Guided_Diffusion_for_Image_Super-Resolution_WACV_2025_paper.html	Brian B. Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio, Andreas Dengel
ECF-YOLOv7-Tiny: Improving Feature Fusion and the Receptive Field for Lightweight Object Detectors	In this work we aim to increase the efficiency and the detection performance of lightweight object detectors with focus on feature fusion and receptive field of the models. For improved feature fusion we introduce the Convolutional Squeeze-and-Excitation (CSE) module which requires only minimal additional computation. For improving the receptive field and feature extraction capabilities in a resource effective manner we introduce the Cross-Stage Partial Context Augmentation Module (CSP-CAM). Furthermore for improving real-time performance we apply two model scaling techniques with minimal impact on the detection performance. We prove the effectiveness of the proposed modules by inserting them into YOLOv7-tiny and YOLOv9-t. We build a new network architecture ECFYOLOv7-tiny which we train on the MS COCO dataset and evaluate the inference speed on NVIDIA Jetson Nano. ECFYOLOv7-tiny achieves 37.8% mAP@[0.5:0.95] on the test set while reaching 9.3 FPS on Jetson Nano outperforming the other state-of-the-art lightweight object detectors on the 416 input image resolution. Code and models are released at: https://github.com/dbacea/ecf-yolov7-tiny.	https://openaccess.thecvf.com//content/WACV2025/html/Bacea_ECF-YOLOv7-Tiny_Improving_Feature_Fusion_and_the_Receptive_Field_for_Lightweight_WACV_2025_paper.html	Dan-Sebastian Bacea, Florin Oniga
EDMB: Edge Detector with Mamba	Transformer-based models have made significant progress in edge detection but their high computational cost is prohibitive. Recently vision Mamba have shown excellent ability in efficiently capturing long-range dependencies. Drawing inspiration from this we propose a novel edge detector with Mamba termed EDMB to efficiently generate high-quality multi-granularity edges. In EDMB Mamba is combined with a global-local architecture therefore it can focus on both global information and fine-grained cues. The fine-grained cues play a crucial role in edge detection but are usually ignored by ordinary Mamba. We design a novel decoder to construct learnable Gaussian distributions by fusing global features and fine-grained features. And the multi-grained edges are generated by sampling from the distributions. In order to make multi-granularity edges applicable to single-label data we introduce Evidence Lower Bound loss to supervise the learning of the distributions. On the multi-label dataset BSDS500 our proposed EDMB achieves competitive single-granularity ODS 0.837 and multi-granularity ODS 0.851 without multi-scale test or extra PASCAL-VOC data. Remarkably EDMB can be extended to single-label datasets such as NYUDv2 and BIPED. The source code is available at https://github.com/Li-yachuan/EDMB.	https://openaccess.thecvf.com//content/WACV2025/html/Li_EDMB_Edge_Detector_with_Mamba_WACV_2025_paper.html	Yachuan Li, Xavier Soria Poma, Yun Bai, Qian Xiao, Chaozhi Yang, Guanlin Li, Zongmin Li
EI-Nexus: Towards Unmediated and Flexible Inter-Modality Local Feature Extraction and Matching for Event-Image Data	Event cameras with high temporal resolution and high dynamic range have limited research on the inter-modality local feature extraction and matching of event-image data. We propose EI-Nexus an unmediated and flexible framework that integrates two modality-specific keypoint extractors and a feature matcher. To achieve keypoint extraction across viewpoint and modality changes we bring Local Feature Distillation (LFD) which transfers the viewpoint consistency from a well-learned image extractor to the event extractor ensuring robust feature correspondence. Furthermore with the help of Context Aggregation (CA) a remarkable enhancement is observed in feature matching. We further establish the first two inter-modality feature matching benchmarks MVSEC-RPE and EC-RPE to assess relative pose estimation on event-image data. Our approach outperforms traditional methods that rely on explicit modal transformation offering more unmediated and adaptable feature extraction and matching achieving better keypoint similarity and state-of-the-art results on the MVSEC-RPE and EC-RPE benchmarks. The source code and benchmarks will be made publicly available at EI-Nexus.	https://openaccess.thecvf.com//content/WACV2025/html/Yi_EI-Nexus_Towards_Unmediated_and_Flexible_Inter-Modality_Local_Feature_Extraction_and_WACV_2025_paper.html	Zhonghua Yi, Hao Shi, Qi Jiang, Kailun Yang, Ze Wang, Diyang Gu, Yufan Zhang, Kaiwei Wang
ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion	The research community has shown increasing interest in designing intelligent embodied agents that can assist humans in accomplishing tasks. Although there have been significant advancements in related vision-language benchmarks most prior work has focused on building agents that follow instructions rather than endowing agents the ability to ask questions to actively resolve ambiguities arising naturally in embodied environments. To address this gap we propose an Embodied Learning-By-Asking (ELBA) model that learns when and what questions to ask to dynamically acquire additional information for completing the task. We evaluate ELBA on the TEACh vision-dialog navigation and task completion dataset. Experimental results show that the proposed method achieves improved task performance compared to baseline models without question-answering capabilities. Code is available at https://github.com/PLAN-Lab/ELBA.	https://openaccess.thecvf.com//content/WACV2025/html/Shen_ELBA_Learning_by_Asking_for_Embodied_Visual_Navigation_and_Task_WACV_2025_paper.html	Ying Shen, Daniel Bis, Cynthia Lu, Ismini Lourentzou
ELMGS: Enhancing Memory and Computation Scalability through Compression for 3D Gaussian Splatting	3D models have recently been popularized by the potentiality of end-to-end training offered first by Neural Radiance Fields and most recently by 3D Gaussian Splatting models. The latter has the big advantage of naturally providing fast training convergence and high editability. However as the research around these is still in its infancy there is still a gap in the literature regarding the model's scalability. In this work we propose an approach enabling both memory and computation scalability of such models. More specifically we propose an iterative pruning strategy that removes redundant information encoded in the model. We also enhance compressibility for the model by including a differentiable quantization and entropy coding estimator in the optimization strategy. Our results on popular benchmarks showcase the effectiveness of the proposed approach and open the road to the broad deployability of such a solution even on resource-constrained devices.	https://openaccess.thecvf.com//content/WACV2025/html/Ali_ELMGS_Enhancing_Memory_and_Computation_Scalability_through_Compression_for_3D_WACV_2025_paper.html	Muhammad Salman Ali, Sung-Ho Bae, Enzo Tartaglione
ENAF: A Multi-Exit Network with an Adaptive Patch Fusion for Large Image Super Resolution	To accelerate single image super-resolution (SISR) networks on large images (2K-8K) many recent approaches decompose an image into small patches and dynamically determine an execution path according to its difficulty (referred to as a dynamic network). To quantify the hardness of a patch they mainly rely on a handcrafted assessment score e.g. edge which weakly associates a patch's texture with the computational complexity of a SISR model. To address the problem we introduce ENAF - a dynamic network for SISR with an adaptive patch fusion. Built on top of a backbone ENAF incorporates multiple early exits (EEs) to tackle the over-parameterized SISR model. More importantly ENAF plugs a tiny network that estimates PSNR to associate data texture with a computation cost at an EE. Based on the scores ENAF effectively assigns image patches to an exit enhancing the quality-complexity trade-off. Extensive experiments on common datasets with popular SISR backbones demonstrate the effectiveness of ENAF in various settings. The source code will be available.	https://openaccess.thecvf.com//content/WACV2025/html/Nguyen_ENAF_A_Multi-Exit_Network_with_an_Adaptive_Patch_Fusion_for_WACV_2025_paper.html	Manh Duong Nguyen, Tuan Nghia Nguyen, Xuan Truong Nguyen
ERM++: An Improved Baseline for Domain Generalization	Domain Generalization (DG) aims to develop classifiers that can generalize to new unseen data distributions a critical capability when collecting new domain-specific data is impractical. A common DG baseline minimizes the empirical risk on the source domains. Recent studies have shown that this approach known as Empirical Risk Minimization (ERM) can outperform most more complex DG methods when properly tuned. However these studies have primarily focused on a narrow set of hyperparameters neglecting other factors that can enhance robustness and prevent overfitting and catastrophic forgetting properties which are critical for strong DG performance. In our investigation of training data utilization (i.e. duration and setting validation splits) initialization and additional regularizers we find that tuning these previously overlooked factors significantly improves model generalization across diverse datasets without adding much complexity. We call this improved yet simple baseline ERM++. Despite its ease of implementation ERM++ improves DG performance by over 5% compared to prior ERM baselines on a standard benchmark of 5 datasets with a ResNet-50 and over 15% with a ViT-B/16. It also outperforms all state-of-the-art methods on DomainBed datasets with both architectures. Importantly ERM++ is easy to integrate into existing frameworks like DomainBed making it a practical and powerful tool for researchers and practitioners. Overall ERM++ challenges the need for more complex DG methods by providing a stronger more reliable baseline that maintains simplicity and ease of use. Code is available at https://github.com/piotr-teterwak/erm_plusplus.	https://openaccess.thecvf.com//content/WACV2025/html/Teterwak_ERM_An_Improved_Baseline_for_Domain_Generalization_WACV_2025_paper.html	Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Kate Saenko, Bryan Plummer
ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather Condition by Unified Image-Adaptive Processing	We propose an image-adaptive object detection method for adverse weather conditions such as fog and low-light. Our framework employs differentiable preprocessing filters to perform image enhancement suitable for later-stage object detections. Our framework introduces two differentiable filters: a Bezier curve-based pixel-wise (BPW) filter and a kernel-based local (KBL) filter. These filters unify the functions of classical image processing filters and improve performance of object detection. We also propose a domain-agnostic data augmentation strategy using the BPW filter. Our method does not require data-specific customization of the filter combinations parameter ranges and data augmentation. We evaluate our proposed approach called Enhanced Robustness by Unified Image Processing (ERUP)-YOLO by applying it to the YOLOv3 detector. Experiments on adverse weather datasets demonstrate that our proposed filters match or exceed the expressiveness of conventional methods and our ERUP-YOLO achieved superior performance in a wide range of adverse weather conditions including fog and low-light conditions.	https://openaccess.thecvf.com//content/WACV2025/html/Ogino_ERUP-YOLO_Enhancing_Object_Detection_Robustness_for_Adverse_Weather_Condition_by_WACV_2025_paper.html	Yuka Ogino, Yuho Shoji, Takahiro Toizumi, Atsushi Ito
EasyRet3D: Uncalibrated Multi-View Multi-Human 3D Reconstruction and Tracking	Current methods performing 3D human pose estimation from multi-view still bear several key limitations. First most methods require manual intrinsic and extrinsic camera calibration which is laborious and difficult in many settings. Second more accurate models rely on further training on the same datasets they evaluate severely limiting their generalizability in real-world settings. We address these limitations with EasyRet3D (Easy REconstruction and Tracking in 3D) which simultaneously reconstructs and tracks 3D humans in a global coordinate frame across all views with uncalibrated cameras and videos in the wild. EasyRet3D is a compositional framework that composes our proposed modules (Automatic Calibration module Adaptive Stitching Module and Optimization Module) and off-the-shelf large pre-trained models at intermediate steps to avoid manual intrinsic and extrinsic calibration and task-specific training. EasyRet3D outperforms all existing multi-view 3D tracking or pose estimation methods in Panoptic EgoHumans Shelf and Human3.6M datasets. Codebase and demos will be released on the project website.	https://openaccess.thecvf.com//content/WACV2025/html/Yin_EasyRet3D_Uncalibrated_Multi-View_Multi-Human_3D_Reconstruction_and_Tracking_WACV_2025_paper.html	Junjie Oscar Yin, Ting Li, Jiahao Wang, Yi Zhang, Alan Yuille
EchoDFKD: Data-Free Knowledge Distillation for Cardiac Ultrasound Segmentation using Synthetic Data	The application of machine learning to medical ultrasound videos of the heart i.e. echocardiography has recently gained traction with the availability of large public datasets. Traditional supervised tasks such as ejection fraction regression are now making way for approaches focusing more on the latent structure of data distributions as well as generative methods. We propose a model trained exclusively by knowledge distillation either on real or synthetical data involving retrieving masks suggested by a teacher model. We achieve state-of-the-art (SOTA) values on the task of identifying end-diastolic and end-systolic frames. By training the model only on synthetic data it reaches segmentation capabilities close to the performance when trained on real data with a significantly reduced number of weights. A comparison with the 5 main existing methods shows that our method outperforms the others in most cases. We also present a new evaluation method that does not require human annotation and instead relies on a large auxiliary model. We show that this method produces scores consistent with those obtained from human annotations. Relying on the integrated knowledge from a vast amount of records this method overcomes certain inherent limitations of human annotator labeling. Code: https://github.com/GregoirePetit/EchoDFKD	https://openaccess.thecvf.com//content/WACV2025/html/Petit_EchoDFKD_Data-Free_Knowledge_Distillation_for_Cardiac_Ultrasound_Segmentation_using_Synthetic_WACV_2025_paper.html	GrÃ©goire Petit, Nathan Palluau, Axel Bauer, Clemens Dlaska
EdgeGaussians - 3D Edge Mapping via Gaussian Splatting	With their meaningful geometry and omnipresence in the 3D world edges are extremely useful primitives in computer vision. Methods for 3D edge reconstruction have 1) either focused on reconstructing 3D edges by triangulating tracks of 2D line segments across images or 2) more recently learning a 3D edge distance field from multi-view images. The triangulation-based methods struggle to repeatedly detect and robustly match line segments resulting in noisy and incomplete reconstructions in many cases. Methods in the latter class rely on sampling edge points from the learnt implicit field which is limited by the spatial resolution of the voxel grid used for sampling resulting in imprecise points that require refinement. Further such methods require a long training that scales poorly with the size of the scene. In this paper we propose a method that explicitly learns 3D edge points with a 3D Gaussian Splatting representation trained from edge images. Backed by efficient training the proposed method produces results better than or at-par with the current state-of-the-art methods while being an order of magnitude faster. The code is released at https://github.com/kunalchelani/EdgeGaussians.	https://openaccess.thecvf.com//content/WACV2025/html/Chelani_EdgeGaussians_-_3D_Edge_Mapping_via_Gaussian_Splatting_WACV_2025_paper.html	Kunal Chelani, Assia Benbihi, Torsten Sattler, Fredrik Kahl
Effective Backdoor Learning on Open-Set Face Recognition Systems	Backdoor attacks pose a serious threat to the security of face recognition systems. These involve the insertion of poisoned inputs into the training data to manipulate the model's behavior at inference time and can cause severe consequences such as unauthorized access to secure systems or impersonation of legitimate users. Previous works on backdoor attacks have primarily focused on closed-set classification systems. However open-set face recognition systems are commonly utilized in practical applications which operate fundamentally differently from closedset systems. In this paper we propose two main contributions. First we demonstrate that closed-set backdoor attacks are effective in basic classification scenarios but fail to perform well in complex open-set face recognition tasks. Second we introduce Feature Stabilized Trigger Loss (FSTL) a novel loss function designed to facilitate the learning of backdoors in open-set recognition models. The experiments were conducted on two large-scale datasets using a variety of high-performing face recognition systems and by training with both physical and digital triggers. Since developing effective attack countermeasures requires knowledge of effective attacks this work will enable future works on developing more secure recognition systems.	https://openaccess.thecvf.com//content/WACV2025/html/Voth_Effective_Backdoor_Learning_on_Open-Set_Face_Recognition_Systems_WACV_2025_paper.html	Diana Voth, Leonidas Dane, Jonas Grebe, Sebastian Peitz, Philipp TerhÃ¶rst
Effective Scene Graph Generation by Statistical Relation Distillation	Annotating scene graphs for images is a time-consuming task resulting in many instances of missing relations within existing datasets. In this paper we introduce the Statistical Relation Distillation (SRD) method to enhance scenegraph datasets. SRD leverages human-annotated relations alongside object-to-object and predicate-to-predicate similarities to reinforce the existence likelihood of scene graph relations. Moreover SRD can augment relational frequency using relations of non-selected object and predicate categories that are usually omitted by scene graph generation (SGG) tasks. The output from SRD derives the prior probability which is combined with model-predicted probabilities to annotate missing relations in training images and sub-sequently re-train SGG models on the augmented dataset. We evaluate our proposed method on Visual Genome and GQA-200 datasets. Experimental results show that training on the augmented dataset enhances the performance of prominent scene-graph generation models. The implementation code is at https://github.com/LUNAProject22/SRD.	https://openaccess.thecvf.com//content/WACV2025/html/Nguyen_Effective_Scene_Graph_Generation_by_Statistical_Relation_Distillation_WACV_2025_paper.html	Thanh-Son Nguyen, Hong Yang, Basura Fernando
Effective and Efficient Medical Image Segmentation with Hierarchical Context Interaction	The U-Net models have become the predominant architecture within the domain of medical image segmentation. Recent advancements have showcased the potential of incorporating attention-based techniques into U-Net structures. Nevertheless the inclusion of attention mechanisms often leads to a substantial increase in both computational demands and the number of parameters with only a marginal improvement in the performance. This observation raises a critical evaluation of the efficiency associated with the integration of attention modules. In this paper we propose a novel methodology termed Hierarchical Context Interaction (HCI) a parameter-efficient attention-free enhancement that can be seamlessly incorporated into U-Net-based models. Experimental results demonstrate that our proposed HCI module attains state-of-the-art performance on two widely used benchmarks i.e. Medical Segmentation Decathlon Datasets and Synapse Datasets while concurrently sustaining a computationally efficient profile comparable to conventional U-Net configurations.	https://openaccess.thecvf.com//content/WACV2025/html/Cheng_Effective_and_Efficient_Medical_Image_Segmentation_with_Hierarchical_Context_Interaction_WACV_2025_paper.html	Zehua Cheng, Di Yuan, Wenhu Zhang, Thomas Lukasiewicz
Efficient Progressive Image Compression with Variance-Aware Masking	Learned progressive image compression is gaining momentum as it allows improved image reconstruction as more bits are decoded at the receiver. We propose a progressive image compression method in which an image is first represented as a pair of base-quality and top-quality latent representations. Next a residual latent representation is encoded as the element-wise difference between the top and base representations. Our scheme enables progressive image compression with element-wise granularity by introducing a masking system that ranks each element of the residual latent representation from most to least important dividing it into complementary components which can be transmitted separately to the decoder in order to obtain different reconstruction quality. The masking system does not add further parameters nor complexity. At the receiver any elements of the top latent representation excluded from the transmitted components can be independently replaced with the mean predicted by the hyperprior architecture ensuring reliable reconstructions at any intermediate quality level. We also introduced Rate Enhancement Modules (REMs) which refine the estimation of entropy parameters using already decoded components. We obtain results competitive with state-of-the-art competitors while significantly reducing computational complexity decoding time and number of parameters.	https://openaccess.thecvf.com//content/WACV2025/html/Presta_Efficient_Progressive_Image_Compression_with_Variance-Aware_Masking_WACV_2025_paper.html	Alberto Presta, Enzo Tartaglione, Attilio Fiandrotti, Marco Grangetto, Pamela Cosman
Efficient Video Object Segmentation via Modulated Cross-Attention Memory	Recently transformer-based approaches have shown promising results for semi-supervised video object segmentation. However these approaches typically struggle on long videos due to increased GPU memory demands as they frequently expand the memory bank every few frames. We propose a transformer-based approach named MAVOS that introduces an optimized and dynamic long-term modulated cross-attention (MCA) memory to model temporal smoothness without requiring frequent memory expansion. The proposed MCA effectively encodes both local and global features at various levels of granularity while efficiently maintaining consistent speed regardless of the video length. Extensive experiments on multiple benchmarks LVOS Long-Time Video and DAVIS 2017 demonstrate the effectiveness of our proposed contributions leading to real-time inference and markedly reduced memory demands without any degradation in segmentation accuracy on long videos. Compared to the best existing transformer-based approach our MAVOS increases the speed by 7.6x while significantly reducing the GPU memory by 87% with comparable segmentation performance on short and long video datasets. Notably on the LVOS dataset our MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second (FPS) on a single V100 GPU. Our code and models will be publicly released at https://github.com/Amshaker/MAVOS.	https://openaccess.thecvf.com//content/WACV2025/html/Shaker_Efficient_Video_Object_Segmentation_via_Modulated_Cross-Attention_Memory_WACV_2025_paper.html	Abdelrahman Shaker, Syed Talal Wasim, Martin Danelljan, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan
EfficientCrackNet: A Lightweight Model for Crack Segmentation	Crack detection particularly from pavement images presents a formidable challenge in computer vision due to inherent complexities such as intensity inhomogeneity intricate topologies low contrast and noisy backgrounds. Automated crack detection is crucial for maintaining the structural integrity of essential infrastructures including buildings pavements and bridges. Existing lightweight methods often face challenges including computational inefficiency complex crack patterns and difficult backgrounds leading to inaccurate detection and impracticality for real-world applications. We propose EfficientCrackNet a lightweight hybrid model combining Convolutional Neural Networks (CNNs) and transformers for precise crack segmentation to address these limitations. EfficientCrackNet integrates depthwise separable convolutions (DSC) layers and MobileViT block to capture global and local features. The model employs an Edge Extraction Method (EEM) for efficient crack edge detection without pretraining and an Ultra-Lightweight Subspace Attention Module (ULSAM) to enhance feature extraction. Extensive experiments on three benchmark datasets Crack500 DeepCrack and GAPs384 demonstrate that EfficientCrackNet achieves superior performance compared to existing lightweight models requiring only 0.26M parameters and 0.483 GFLOPs. The proposed model offers an optimal balance between accuracy and computational efficiency outperforming state-of-the-art lightweight models and providing a robust and adaptable solution for real-world crack segmentation.	https://openaccess.thecvf.com//content/WACV2025/html/Zim_EfficientCrackNet_A_Lightweight_Model_for_Crack_Segmentation_WACV_2025_paper.html	Abid Hasan Zim, Aquib Iqbal, Zaid Al-Huda, Asad Malik, Minoru Kuribayashi
EfficientMorph: Parameter-Efficient Transformer-Based Architecture for 3D Image Registration	Transformers have emerged as the state-of-the-art architecture in medical image registration outperforming convolutional neural networks (CNNs) by addressing their limited receptive fields and overcoming gradient instability in deeper models. Despite their success transformer-based models require substantial resources for training including data memory and computational power which may restrict their applicability for end users with limited resources. In particular existing transformer-based 3D image registration architectures face two critical gaps that challenge their efficiency and effectiveness. Firstly although window-based attention mechanisms reduce the quadratic complexity of full attention by focusing on local regions they often struggle to effectively integrate both local and global information. Secondly the granularity of tokenization a crucial factor in registration accuracy presents a performance trade-off: smaller voxel-size tokens enhance detail capture but come with increased computational complexity higher memory usage and a greater risk of overfitting. We present EFFICIENTMORPH a transformerbased architecture for unsupervised 3D image registration that balances local and global attention in 3D volumes through a plane-based attention mechanism and employs a Hi-Res tokenization strategy with merging operations thus capturing finer details without compromising computational efficiency. Notably EFFICIENTMORPH sets a new benchmark for performance on the OASIS dataset with 16-27x fewer parameters. https://github.com/MedVICLab/Efficient_Morph_Registration	https://openaccess.thecvf.com//content/WACV2025/html/Bin_Aziz_EfficientMorph_Parameter-Efficient_Transformer-Based_Architecture_for_3D_Image_Registration_WACV_2025_paper.html	Abu Zahid Bin Aziz, Mokshagna Sai Teja Karanam, Tushar Kataria, Shireen Y. Elhabian
Ego-VPA: Egocentric Video Understanding with Parameter-Efficient Adaptation	Video understanding typically requires fine-tuning the large backbone when adapting to new domains. In this paper we leverage the egocentric video foundation models (Ego-VFMs) based on video-language pre-training and propose a parameter-efficient adaptation for egocentric video tasks namely Ego-VPA. It employs a local sparse approximation for each video frame/text feature using the basis prompts and the selected basis prompts are used to synthesize video/text prompts. Since the basis prompts are shared across frames and modalities it models context fusion and cross-modal transfer in an efficient fashion. Experiments show that Ego-VPA excels in lightweight adaptation (with only 0.84% learnable parameters) largely improving over baselines and reaching the performance of full fine-tuning.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_Ego-VPA_Egocentric_Video_Understanding_with_Parameter-Efficient_Adaptation_WACV_2025_paper.html	Tz-Ying Wu, Kyle Min, Subarna Tripathi, Nuno Vasconcelos
EgoCast: Forecasting Egocentric Human Pose in the Wild	Accurately estimating and forecasting human body pose is important for enhancing the user's sense of immersion in Augmented Reality. Addressing this need our paper introduces EgoCast a bimodal method for 3D human pose forecasting using egocentric videos and proprioceptive data. We study the task of human pose forecasting in a realistic setting extending the boundaries of temporal forecasting in dynamic scenes and building on the current framework for current pose estimation in the wild. We introduce a current-frame estimation module that generates pseudo-groundtruth poses for inference eliminating the need for past groundtruth poses typically required by current methods during forecasting. Our experimental results on the recent Ego-Exo4D and Aria Digital Twin datasets validate EgoCast for real-life motion estimation. On the Ego-Exo4D Body Pose 2024 Challenge our method significantly outperforms the state-of-the-art approaches laying the groundwork for future research in human pose estimation and forecasting in unscripted activities with egocentric inputs.	https://openaccess.thecvf.com//content/WACV2025/html/Escobar_EgoCast_Forecasting_Egocentric_Human_Pose_in_the_Wild_WACV_2025_paper.html	Maria Escobar, Juanita Puentes, Cristhian Forigua, Jordi Pont-Tuset, Kevis-Kokitsi Maninis, Pablo Arbelaez
EgoPoints: Advancing Point Tracking for Egocentric Videos	We introduce EgoPoints a benchmark for point tracking in egocentric videos. We annotate 4.7K challenging tracks in egocentric sequences. Compared to the popular TAP-Vid-DAVIS evaluation benchmark we include 9x more points that go out-of-view and 59x more points that require re-identification (ReID) after returning to view. To measure the performance of models on these challenging points we introduce evaluation metrics that specifically monitor tracking performance on points in-view out-of-view and points that require re-identification. We then propose a pipeline to create semi-real sequences with automatic ground truth. We generate 11K such sequences by combining dynamic Kubric objects with scene points from EPIC Fields. When fine-tuning point tracking methods on these sequences and evaluating on our annotated EgoPoints sequences we improve CoTracker across all metrics including the tracking accuracy d^*_avg by 2.7 percentage points and accuracy on ReID sequences (ReIDd_avg) by 2.4 points. We also improve d^*_avg and ReIDd_avg of PIPs++ by 0.3 and 2.8 respectively.	https://openaccess.thecvf.com//content/WACV2025/html/Darkhalil_EgoPoints_Advancing_Point_Tracking_for_Egocentric_Videos_WACV_2025_paper.html	Ahmad Darkhalil, Rhodri Guerrier, Adam W. Harley, Dima Damen
EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos	We introduce EgoSonics a method to generate semantically meaningful and synchronized audio tracks conditioned on silent egocentric videos. Generating audio for silent egocentric videos could open new applications in virtual reality assistive technologies or for augmenting existing datasets. Existing work has been limited to domains like speech music or impact sounds and cannot easily capture the broad range of audio frequencies found in egocentric videos. EgoSonics addresses these limitations by building on the strength of latent diffusion models for conditioned audio synthesis. We first encode and process audio and video data into a form that is suitable for generation. The encoded data is used to train our model to generate audio tracks that capture the semantics of the input video. Our proposed SyncroNet builds on top of ControlNet to provide control signals that enables temporal synchronization to the synthesized audio. Extensive evaluations show that our model outperforms existing work in audio quality and in our newly proposed synchronization evaluation method. Furthermore we demonstrate downstream applications of our model in improving video summarization.	https://openaccess.thecvf.com//content/WACV2025/html/Rai_EgoSonics_Generating_Synchronized_Audio_for_Silent_Egocentric_Videos_WACV_2025_paper.html	Aashish Rai, Srinath Sridhar
ElasticLaneNet: An Efficient Geometry-Flexible Lane Detection Framework	The task of lane detection involves identifying the boundaries of driving areas in real-time. Recognizing lanes with variable and complex geometric structures remains a challenge. In this paper we explore a novel and flexible way of implicit lanes representation named Elastic Lane map (ELM) and introduce an efficient physics-informed end-to-end lane detection framework namely ElasticLaneNet (Elastic interaction energy-informed Lane detection Network). The approach considers predicted lanes as moving zero-contours on the flexibly shaped ELM that are attracted to the ground truth guided by an elastic interaction energy-loss function (EIE loss). Our framework well integrates the global information and low-level features. The method performs well in complex lane scenarios including those with large curvature turns intersections various crossing lanes Y-shapes lanes dense lanes etc. We apply our approach on three datasets: SDLane TuSimple and CULane. The results demonstrate exceptional performance of our method with the state-of-the-art results on the structurally diverse SDLane achieving F1-score of 89.51 Recall of 87.50 and Precision of 91.61 with fast inference speed.	https://openaccess.thecvf.com//content/WACV2025/html/Feng_ElasticLaneNet_An_Efficient_Geometry-Flexible_Lane_Detection_Framework_WACV_2025_paper.html	Yaxin Feng, Yuan Lan, Luchan Zhang, Yang Xiang
Elemental Composite Prototypical Network: Few-Shot Object Detection on Outdoor 3D Point Cloud Scenes	This paper introduces the Elemental Composite Prototypical Network (ECPN) a novel approach to few-shot learning (FSL) in outdoor 3D point cloud object detection. Such point clouds are inherently non-uniformly packed and show marked intra-class variations due to aberrations in lidar scanning methods. Due to the limited availability of examples in the FSL setting the intra-class variations serve as a much more formidable challenge to traditional detection algorithms. ECPN employs a novel prototypical learning method that solves the issues mentioned above. We generate and leverage multiple elemental prototypes for each class to capture essential geometric features from limited examples. These elemental prototypes are then combined in a weighted manner to arrive at composite prototypes that score relevant and irrelevant features in the elemental prototypes with respect to the query point cloud scene. Moreover we introduce a novel feature-similarity-discrimination loss to refine the model's ability to distinguish between relevant objects and their background significantly improving object detection accuracy in FSL scenarios. Our extensive testing on the nuScenes dataset demonstrates that ECPN significantly outperforms existing baselines offering a robust solution to the complexities of outdoor few-shot 3D object detection (O-FS3D) and setting a new standard for future research.	https://openaccess.thecvf.com//content/WACV2025/html/De_Elemental_Composite_Prototypical_Network_Few-Shot_Object_Detection_on_Outdoor_3D_WACV_2025_paper.html	Arkadipta De, Vartika Sengar, Daksh Thapar, Mahesh Chandran, Manohar Kaul
Elucidating Optimal Reward-Diversity Tradeoffs in Text-to-Image Diffusion Models	Text-to-image (T2I) diffusion models have become prominent tools for generating high-fidelity images from text prompts. However when trained on unfiltered internet data these models can produce unsafe incorrect or stylistically undesirable images that are not aligned with human preferences. To address this recent approaches have incorporated human preference datasets to fine-tune T2I models or to optimize reward functions that capture these preferences. Although effective these methods are vulnerable to reward hacking where the model overfits to the reward function leading to a loss of diversity in the generated images. In this paper we prove the inevitability of reward hacking and study natural regularization techniques like KL divergence and LoRA scaling and their limitations for diffusion models. We also introduce Annealed Importance Guidance (AIG) an inference-time regularization inspired by Annealed Importance Sampling which retains the diversity of the base model while achieving Pareto-Optimal reward-diversity tradeoffs. Our experiments demonstrate the benefits of AIG for Stable Diffusion models striking the optimal balance between reward optimization and image diversity. Furthermore a user study confirms that AIG improves diversity and quality of generated images across different model architectures and reward functions.	https://openaccess.thecvf.com//content/WACV2025/html/Jena_Elucidating_Optimal_Reward-Diversity_Tradeoffs_in_Text-to-Image_Diffusion_Models_WACV_2025_paper.html	Rohit Jena, Ali Taghibakhshi, Sahil Jain, Gerald Shen, Nima Tajbakhsh, Arash Vahdat
Elucidating the Solution Space of Extended Reverse-Time SDE for Diffusion Models	Sampling from Diffusion Models can alternatively be seen as solving differential equations where there is a challenge in balancing speed and image visual quality. ODE-based samplers offer rapid sampling time but reach a performance limit whereas SDE-based samplers achieve superior quality albeit with longer iterations. In this work we formulate the sampling process as an Extended Reverse-Time SDE (ER SDE) unifying prior explorations into ODEs and SDEs. Theoretically leveraging the semi-linear structure of ER SDE solutions we offer exact solutions and approximate solutions for VP SDE and VE SDE respectively. Based on the approximate solution space of the ER SDE referred to as one-step prediction errors we yield mathematical insights elucidating the rapid sampling capability of ODE solvers and the high-quality sampling ability of SDE solvers. Additionally we unveil that VP SDE solvers stand on par with their VE SDE counterparts. Based on these findings leveraging the dual advantages of ODE solvers and SDE solvers we devise efficient high-quality samplers namely ER-SDE-Solvers. Experimental results demonstrate that ER-SDE-Solvers achieve state-of-the-art performance across all stochastic samplers while maintaining efficiency of deterministic samplers. Specifically on the ImageNet 128 x 128 dataset ER-SDE-Solvers obtain 8.33 FID in only 20 function evaluations. Code is available at https://github.com/QinpengCui/ER-SDE-Solver	https://openaccess.thecvf.com//content/WACV2025/html/Cui_Elucidating_the_Solution_Space_of_Extended_Reverse-Time_SDE_for_Diffusion_WACV_2025_paper.html	Qinpeng Cui, Xinyi Zhang, Qiqi Bao, Qingmin Liao
EmoVOCA: Speech-Driven Emotional 3D Talking Heads	A notable challenge in 3D talking head generation consists in blending speech-related motions with expression dynamics. This is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Some literature works attempted to overcome such lack of data by fitting parametric 3D models (3DMMs) to 2D videos and using the reconstructed 3D faces as replacement. However their underlying parametric space limits the precision required to accurately reproduce convincing lip motions and synching which is crucial for the application at hand. In this work we look at the problem from a different perspective and developed a data-driven technique to combine inexpressive 3D talking heads with a set of 3D expressive sequences which we used for creating a synthetic dataset called EmoVOCA. We then designed and trained an emotional 3D talking head generator that accepts a 3D face an audio file an emotion label and an intensity value as inputs and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments both quantitative and qualitative using our data and generator evidence superior ability in synthesizing convincing animations when compared with the best performing methods in the literature. Our code and pre-trained models are available at https://github.com/miccunifi/EmoVOCA.	https://openaccess.thecvf.com//content/WACV2025/html/Nocentini_EmoVOCA_Speech-Driven_Emotional_3D_Talking_Heads_WACV_2025_paper.html	Federico Nocentini, Claudio Ferrari, Stefano Berretti
Endoscopic Scoring and Localization in Unconstrained Clinical Trial Videos	Endoscopic assessment using the Mayo clinic score (or Mayo score 4 categories) is currently the standard for diagnosing and evaluating mucosal disease activities. However annotating Mayo scores is time-consuming and often relies on weakly labeled evaluations from central and local readers (doctors) leading to a large number of unlabeled or mislabeled video clips. Additionally such labels also suffer from data imbalance due to patient distributions and varying disease severity levels. This gap underscores the need for more customizable and refined methods for endoscopic scoring and localization. To address these challenges we introduce an end-to-end pipeline with a new dataset for endoscopic scoring and localization in unconstrained clinical trial videos. Specifically we propose an automated scoring system that includes an active learning-based pre-processing stage for cleaning the raw videos which are then weakly labeled by doctors. Therefore we obtain a comprehensive dataset for endoscopic Mayo scoring comprising approximately 49.9 hours of video and 86423 clips providing a solid foundation for developing endoscopic scoring and localization models. Then we propose a video classification model for Mayo score classifications. For personalized disease quantification in localization we introduce a novel 1D trajectory model with a novel cumulative disease score that addresses the limitations of previous 3D trajectory projection methods. Our dataset and end-to-end pipeline offer a valuable foundation for advancing endoscopic clinical trial research.	https://openaccess.thecvf.com//content/WACV2025/html/Xiang_Endoscopic_Scoring_and_Localization_in_Unconstrained_Clinical_Trial_Videos_WACV_2025_paper.html	Jinlin Xiang, Hillol Sarker, Bozhao Qi, Ruisu Zhang, Roger Trullo, Salvatore Badalamenti, Maria Wiekowski, Annie Kruger, Etienne Pochet, Qi Tang, Wei Zhao
Enhancing Embodied Object Detection with Spatial Feature Memory	Deep-learning and large scale language-image training have produced image object detectors that generalise well to diverse environments and semantic classes. However existing object detection paradigms are not optimally tailored for the embodied conditions inherent in robotics where the same objects are repeatedly observed over time. In this setting detectors that operate on single images or short sequences are likely to produce inconsistent predictions. Motivated by this we explore if the embodiment of the detector can be utilised to generate more consistent and reliable detections during repeat observation of a scene. We propose a novel framework that incrementally updates a spatial feature memory while using it as a prior to perform image object detection. By leveraging the embodiment of the robot in this way raw object detection performance is enhanced by up to 4.12 mAP and downstream robotic tasks such as semantic mapping and object recall are improved. We also investigate the structure this spatial memory should take leading to an implementation that aggregates features from the shared language-image embedding space. This approach allows the detector to effectively balance the use of memory and image features while ensuring that the benefits of language-image pre-training can be enjoyed alongside our spatial memory.	https://openaccess.thecvf.com//content/WACV2025/html/Chapman_Enhancing_Embodied_Object_Detection_with_Spatial_Feature_Memory_WACV_2025_paper.html	Nicolas Harvey Chapman, Christopher Lehnert, Will Browne, Feras Dayoub
Enhancing Image Layout Control with Loss-Guided Diffusion Models	Diffusion models are a powerful class of generative models capable of producing high-quality images from pure noise using a simple text prompt. While most methods which introduce additional spatial constraints into the generated images (e.g. bounding boxes) require fine-tuning a smaller and more recent subset of these methods take advantage of the models' attention mechanism and are training-free. These methods generally fall into one of two categories. The first entails modifying the cross-attention maps of specific tokens directly to enhance the signal in certain regions of the image. The second works by defining a loss function over the cross-attention maps and using the gradient of this loss to guide the latent. While previous work explores these as alternative strategies we provide an interpretation for these methods which highlights their complimentary features and demonstrate that it is possible to obtain superior performance when both methods are used in concert.	https://openaccess.thecvf.com//content/WACV2025/html/Patel_Enhancing_Image_Layout_Control_with_Loss-Guided_Diffusion_Models_WACV_2025_paper.html	Zakaria Patel, Kirill Serkh
Enhancing Monocular Depth Estimation with Multi-Source Auxiliary Tasks	Monocular depth estimation (MDE) is a challenging task in computer vision often hindered by the cost and scarcity of high-quality labeled datasets. We tackle this challenge using auxiliary datasets from related vision tasks for an alternating training scheme with a shared decoder built on top of a pre-trained vision foundation model while giving a higher weight to MDE. Through extensive experiments we demonstrate the benefits of incorporating various in-domain auxiliary datasets and tasks to improve MDE quality on average by 11%. Our experimental analysis shows that auxiliary tasks have different impacts confirming the importance of task selection highlighting that quality gains are not achieved by merely adding data. Remarkably our study reveals that using semantic segmentation datasets as Multi-Label Dense Classification (MLDC) often results in additional quality gains. Lastly our method significantly improves the data efficiency for the considered MDE datasets enhancing their quality while reducing their size by at least 80%. This paves the way for using auxiliary data from related tasks to improve MDE quality despite limited availability of high-quality labeled data. Code is available at https://jugit.fz-juelich.de/ias-8/mdeaux.	https://openaccess.thecvf.com//content/WACV2025/html/Quercia_Enhancing_Monocular_Depth_Estimation_with_Multi-Source_Auxiliary_Tasks_WACV_2025_paper.html	Alessio Quercia, Erenus Yildiz, Zhuo Cao, Kai Krajsek, Abigail Morrison, Ira Assent, Hanno Scharr
Enhancing Novel Object Detection via Cooperative Foundational Models	In this work we address the challenging and emergent problem of novel object detection (NOD) focusing on the accurate detection of both known and novel object categories during inference. Traditional object detection algorithms are inherently closed-set limiting their capability to handle NOD. We present a novel approach to transform existing closed-set detectors into open-set detectors. This transformation is achieved by leveraging the complementary strengths of pre-trained foundational models specifically CLIP and SAM through our cooperative mechanism. Furthermore by integrating this mechanism with state-of-the-art open-set detectors such as GDINO we establish new benchmarks in object detection performance. Our method achieves 17.42 mAP in novel object detection and 42.08 mAP for known objects on the challenging LVIS dataset. Adapting our approach to the COCO OVD split we obtain an impressive result of 49.6 Novel AP50 which outperforms existing SOTA methods with similar backbone. Our code is available at: https://rohit901.github.io/coop-foundation-models/	https://openaccess.thecvf.com//content/WACV2025/html/Bharadwaj_Enhancing_Novel_Object_Detection_via_Cooperative_Foundational_Models_WACV_2025_paper.html	Rohit Bharadwaj, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
Enhancing Predictive Imaging Biomarker Discovery through Treatment Effect Analysis	Identifying predictive covariates which forecast individual treatment effectiveness is crucial for decision-making across different disciplines such as personalized medicine. These covariates referred to as biomarkers are extracted from pre-treatment data often within randomized controlled trials and should be distinguished from prognostic biomarkers which are independent of treatment assignment. Our study focuses on discovering predictive imaging biomarkers specific image features by leveraging pre-treatment images to uncover new causal relationships. Unlike labor-intensive approaches relying on handcrafted features prone to bias we present a novel task of directly learning predictive features from images. We propose an evaluation protocol to assess a model's ability to identify predictive imaging biomarkers and differentiate them from purely prognostic ones by employing statistical testing and a comprehensive analysis of image feature attribution. We explore the suitability of deep learning models originally developed for estimating the conditional average treatment effect (CATE) for this task which have been assessed primarily for their precision of CATE estimation while overlooking the evaluation of imaging biomarker discovery. Our proof-of-concept analysis demonstrates the feasibility and potential of our approach in discovering and validating predictive imaging biomarkers from synthetic outcomes and real-world image datasets. Our code is available at https://github.com/MIC-DKFZ/predictive_image_biomarker_analysis.	https://openaccess.thecvf.com//content/WACV2025/html/Xiao_Enhancing_Predictive_Imaging_Biomarker_Discovery_through_Treatment_Effect_Analysis_WACV_2025_paper.html	Shuhan Xiao, Lukas Klein, Jens Petersen, Philipp Vollmuth, Paul F. Jaeger, Klaus H. Maier-Hein
Enhancing Scene Graph Generation with Hierarchical Relationships and Commonsense Knowledge	This work introduces an enhanced approach to generating scene graphs by incorporating both a relationship hierarchy and commonsense knowledge. Specifically we begin by proposing a hierarchical relation head that exploits an informative hierarchical structure. It jointly predicts the relation super-category between object pairs in an image along with detailed relations under each super-category. Following this we implement a robust commonsense validation pipeline that harnesses foundation models to critique the results from the scene graph prediction system removing nonsensical predicates even with a small language-only model. Extensive experiments on Visual Genome and OpenImage V6 datasets demonstrate that the proposed modules can be seamlessly integrated as plug-and-play enhancements to existing scene graph generation algorithms. The results show significant improvements with an extensive set of reasonable predictions beyond dataset annotations. Codes are available at https://github.com/bowenupenn/scene graph_commonsense.	https://openaccess.thecvf.com//content/WACV2025/html/Jiang_Enhancing_Scene_Graph_Generation_with_Hierarchical_Relationships_and_Commonsense_Knowledge_WACV_2025_paper.html	Bowen Jiang, Zhijun Zhuang, Shreyas S. Shivakumar, Camillo J. Taylor
Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM	Current AI-assisted skin image diagnosis has achieved dermatologist-level performance in classifying skin cancer driven by rapid advancements in deep learning architectures. However unlike traditional vision tasks skin images in general present unique challenges due to the limited availability of well-annotated datasets complex variations in conditions and the necessity for detailed interpretations to ensure patient safety. Previous segmentation methods have sought to reduce image noise and enhance diagnostic performance but these techniques require fine-grained pixel-level ground truth masks for training. In contrast with the rise of foundation models the Segment Anything Model (SAM) has been introduced to facilitate promptable segmentation enabling the automation of the segmentation process with simple yet effective prompts. Efforts applying SAM predominantly focus on dermatoscopy images which present more easily identifiable lesion boundaries than clinical photos taken with smartphones. This limitation constrains the practicality of these approaches to real-world applications. To overcome the challenges posed by noisy clinical photos acquired via non-standardized protocols and to improve diagnostic accessibility we propose a novel Cross-Attentive Fusion framework for interpretable skin lesion diagnosis. Our method leverages SAM to generate visual concepts for skin diseases using prompts integrating local visual concepts with global image features to enhance model performance. Extensive evaluation on two skin disease datasets demonstrates our proposed method's effectiveness on lesion diagnosis and interpretability.	https://openaccess.thecvf.com//content/WACV2025/html/Hu_Enhancing_Skin_Disease_Diagnosis_Interpretable_Visual_Concept_Discovery_with_SAM_WACV_2025_paper.html	Xin Hu, Janet Wang, Jihun Hamm, Rie R Yotsu, Zhengming Ding
Enhancing Vision-Language Few-Shot Adaptation with Negative Learning	"Large-scale pre-trained Vision-Language Models (VLMs) have exhibited impressive zero-shot performance and transferability allowing them to adapt to downstream tasks in a data-efficient manner. However when only a few labeled samples are available adapting VLMs to distinguish subtle differences between similar classes in specific downstream tasks remains challenging. In this work we propose a Simple yet effective Negative Learning approach SimNL to more efficiently exploit the task-specific knowledge from few-shot labeled samples. Unlike previous methods that focus on identifying a set of representative positive features defining ""what is a CLASS "" SimNL discovers a complementary set of negative features that define ""what is not a CLASS "" providing additional insights that supplement the positive features to enhance task-specific recognition capability. Further we identify that current adaptation approaches are particularly vulnerable to potential noise in the few-shot sample set. To mitigate this issue we introduce a plug-and-play few-shot instance reweighting technique to suppress noisy outliers and amplify clean samples for more stable adaptation. Our extensive experimental results across 15 datasets validate that the proposed SimNL outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency. Code is available at https://github.com/zhangce01/SimNL."	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_Enhancing_Vision-Language_Few-Shot_Adaptation_with_Negative_Learning_WACV_2025_paper.html	Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie
Enhancing Visual Classification using Comparative Descriptors	The performance of vision-language models (VLMs) such as CLIP in visual classification tasks has been enhanced by leveraging semantic knowledge from large language models (LLMs) including GPT. Recent studies have shown that in zero-shot classification tasks descriptors incorporating additional cues high-level concepts or even random characters often outperform those using only category names. In many classification tasks while the top-1 accuracy may be relatively low the top-5 accuracy is often significantly higher. This gap implies that most misclassifications occur among a few similar classes highlighting the model's difficulty in distinguishing between classes with subtle differences. To address this challenge we introduce a novel concept of comparative descriptors. These descriptors emphasize the unique features of a target class against its most similar classes enhancing differentiation. By generating and integrating these comparative descriptors into the classification framework we refine the semantic focus and improve classification accuracy. An additional filtering process ensures that these descriptors are closer to the image embeddings in the CLIP space further enhancing performance. Our approach demonstrates improved accuracy and robustness in visual classification tasks by addressing the specific challenge of subtle inter-class differences. Code is available at https://github.com/hk1ee/Comparative-CLIP	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Enhancing_Visual_Classification_using_Comparative_Descriptors_WACV_2025_paper.html	Hankyeol Lee, Gawon Seo, Wonseok Choi, Geunyoung Jung, Kyungwoo Song, Jiyoung Jung
Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge Transfer	Current facial expression recognition (FER) models are often designed in a supervised learning manner and thus are constrained by the lack of large-scale facial expression images with high-quality annotations. Consequently these models often fail to generalize well performing poorly on unseen images in inference. Vision-language-based zero-shot models demonstrate a promising potential for addressing such challenges. However these models lack task-specific knowledge and therefore are not optimized for the nuances of recognizing facial expressions. To bridge this gap this work proposes a novel method Exp-CLIP to enhance zero-shot FER by transferring the task knowledge from large language models (LLMs). Specifically based on the pre-trained vision-language encoders we incorporate a projection head designed to map the initial joint vision-language space into a space that captures representations of facial actions. To train this projection head for subsequent zero-shot predictions we propose to align the projected visual representations with task-specific semantic meanings derived from the LLM encoder and the text instruction-based strategy is employed to customize the LLM knowledge. Given unlabelled facial data and efficient training of the projection head Exp-CLIP achieves superior zero-shot results to the CLIP models and several other large vision-language models (LVLMs) on seven in-the-wild FER datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Zhao_Enhancing_Zero-Shot_Facial_Expression_Recognition_by_LLM_Knowledge_Transfer_WACV_2025_paper.html	Zengqun Zhao, Yu Cao, Shaogang Gong, Ioannis Patras
Enriching Local Patterns with Multi-Token Attention for Broad-Sight Neural Networks	In neural networks recognizing visual patterns is challenging because global average pooling disregards local patterns and solely relies on over-concentrated activation. Global average pooling enforces the network to learn objects regardless of their location so features tend to be activated only in specific regions. To support this claim we provide a novel analysis of the problems that over-concentration brings about in networks with extensive experiments. We analyze the over-concentration through problems arising from feature variance and dead neurons that are not activated. Based on our analysis we introduce a multi-token attention pooling layer to alleviate the over-concentration problem. Our attention-pooling layer captures broad-sight local patterns by learning multiple tokens with the proposed distillation algorithm. It resolves the high bias and high variance errors of learned multi-tokens which is crucial when aggregating local patterns with multi-tokens. Our method applies to various vision tasks and network architectures such as CNN ViT and MLP-Mixer. The proposed method improves baselines with few extra resources and a network employing our pooling method works favorably against state-of-the-art networks. We open-source the code at https://github.com/Lab-LVM/imagenet-models.	https://openaccess.thecvf.com//content/WACV2025/html/Kang_Enriching_Local_Patterns_with_Multi-Token_Attention_for_Broad-Sight_Neural_Networks_WACV_2025_paper.html	Hankyul Kang, Jongbin Ryu
Epipolar Attention Field Transformers for Bird's Eye View Semantic Segmentation	Spatial understanding of the semantics of the surroundings is a key capability needed by autonomous cars to enable safe driving decisions. Recently purely vision-based solutions have gained increasing research interest. In particular approaches extracting a bird's eye view (BEV) from multiple cameras have demonstrated great performance for spatial understanding. This paper addresses the dependency on learned positional encodings to correlate image and BEV feature map elements for transformer-based methods. We propose leveraging epipolar geometric constraints to model the relationship between cameras and the BEV by Epipolar Attention Fields. They are incorporated into the attention mechanism as a novel attribution term serving as an alternative to learned positional encodings. Experiments show that our method EAFormer outperforms previous BEV approaches by 2% mIoU for map semantic segmentation and exhibits superior generalization capabilities compared to implicitly learning the camera configuration.	https://openaccess.thecvf.com//content/WACV2025/html/Witte_Epipolar_Attention_Field_Transformers_for_Birds_Eye_View_Semantic_Segmentation_WACV_2025_paper.html	Christian Witte, Jens Behley, Cyrill Stachniss, Marvin Raaijmakers
Evaluating Sensitivity Consistency of Explanations	While the performance of deep neural networks is rapidly developing their reliability is increasingly receiving more attention. Explainability methods are one of the most relevant tools to enhance reliability mainly by highlighting important input features for the explanation purpose. Although numerous explainability methods have been proposed their assessment remains challenging due to the absence of ground truth. Several existing studies propose evaluation methods from a certain aspect e.g. fidelity robustness etc. However they typically address only one property of explanations and thus more assessing perspectives contribute to a better explanation evaluating system. This work proposes an evaluation method from a novel perspective called sensitivity consistency where the intuition behind is that features and parameters that strongly impact the predictions and explanations should be highly consistent and vise versa. Extensive experiments on different datasets and models evaluate popular explainability methods while providing qualitative and quantitative results. Our approach further complements the existing evaluation systems and aims to facilitate the proposal of an acknowledged explanation evaluation methodology.	https://openaccess.thecvf.com//content/WACV2025/html/Tan_Evaluating_Sensitivity_Consistency_of_Explanations_WACV_2025_paper.html	Hanxiao Tan
Event-Guided Fusion-Mamba for Context-Aware 3D Human Pose Estimation	3D human pose estimation (3D HPE) is an important computer vision task with various practical applications. Researchers have proposed various deep learning-based methods for 3D HPE. However the majority of such methods rely on lifting 2D pose sequence to 3D which do not perform well in challenging scenarios and are often computationally expensive. Such methods typically rely on 2D joint coordinates which do not provide much spatial context to solve ambiguity problem. In addition merely relying on information extracted from RGB frames may miss temporal information and structural context. Thus in this paper we propose a framework that incorporates event stream as an additional input since event features provide such useful information. Moreover instead of using 2D joint coordinates in pose sequence our framework uses intermediate visual representations produced by off-the-shelf 2D pose detectors to implicitly encode joint-centric spatial context. Our new framework is a novel state space model (SSM)-based solution called Event-Guided Context Aware MambaPose (CA-MambaPose). In CA-MambaPose framework we design a novel cross modality fusion mamba module to skillfully fuse the RGB and Event features. CA-MambaPose has lower computational cost due to the efficiency of Mamba blocks. We conduct extensive experiments to evaluate CA-MambaPose using two existing datasets. Our experimental results show that CA-MambaPose achieves better performance than SOTA methods.	https://openaccess.thecvf.com//content/WACV2025/html/Lang_Event-Guided_Fusion-Mamba_for_Context-Aware_3D_Human_Pose_Estimation_WACV_2025_paper.html	Bo Lang, Mooi Choo Chuah
Event-Guided Low-Light Video Semantic Segmentation	Recent video semantic segmentation (VSS) methods have demonstrated promising results in well-lit environments. However their performance significantly drops in low-light scenarios due to limited visibility and reduced contextual details. In addition unfavorable low-light conditions make it harder to incorporate temporal consistency across video frames and thus lead to video flickering effects. Compared with conventional cameras event cameras can capture motion dynamics filter out temporal-redundant information and are robust to lighting conditions. To this end we propose EVSNet a lightweight framework that leverages event modality to guide the learning of a unified illumination-invariant representation. Specifically we leverage a Motion Extraction Module to extract short-term and long-term temporal motions from event modality and a Motion Fusion Module to integrate image features and motion features adaptively. Furthermore we use a Temporal Decoder to exploit video contexts and generate segmentation predictions. Such designs in EVSNet result in a lightweight architecture while achieving SOTA performance. Experimental results on 3 large-scale datasets demonstrate our proposed EVSNet outperforms SOTA methods with up to 11x higher parameter efficiency.	https://openaccess.thecvf.com//content/WACV2025/html/Yao_Event-Guided_Low-Light_Video_Semantic_Segmentation_WACV_2025_paper.html	Zhen Yao, Mooi Choo Chuah
Event-Guided Video Transformer for End-to-End 3D Human Pose Estimation	3D human pose estimation (3D HPE) is an important computer vision task with various practical applications. However 3D pose estimation for multi-person from a monocular video (3DMPPE) is particularly challenging. Recent transformer-based approaches focus on capturing the spatial-temporal information from sequential 2D poses which unfortunately loses the visual feature relevant for 3D pose estimation. In this paper we propose an end-to-end framework called Event Guided Video Transformer (EVT) which predicts 3D poses directly from video frames by learning spatial-temporal contextual information from visual features effectively. In addition our design is the first that incorporates event features to help guide 3D pose estimation. EVT first decouples persons into different instance-aware feature maps from video frames. These features containing specific clues of body structure information are then fed together with event features into an attention based Event-Aware Embedding Module. Next the fused features for each instance are then fed into an intra-human relation extraction module and subsequently to a temporal transformer to extract inter-frame relationship. Finally the extracted features are fed into a decoder for 3D pose estimation. Experiments using three widely used 3D pose estimation benchmarks show that our proposed EVT achieves better performance than state-of-the-art models.	https://openaccess.thecvf.com//content/WACV2025/html/Lang_Event-Guided_Video_Transformer_for_End-to-End_3D_Human_Pose_Estimation_WACV_2025_paper.html	Bo Lang, Mooi Choo Chuah
EvoCL: Continual Learning over Evolving Domains	Continual Learning aspires to build models capable of learning new tasks without forgetting previously learnt tasks. In real-world settings the distributions underlying the tasks are prone to shift. This necessitates a model capable of observing how the task distributions drift with time and adapt proactively. We present a novel framework of continual learning under evolving domains. Our approach employs a hypernetwork with separate embeddings conditioned on both domain and task to address this problem. The hypernetwork generates customised classifier weights corresponding to any domain-task pair. We employ a separate network that is trained end to end along with the hypernetwork to predict the next domain embedding which in turn helps to generate classifier parameters corresponding to the next future domain in the evolution. We conduct extensive experiments on various datasets with a wide variety of distribution shifts to demonstrate the efficacy of our model in generalizing to future domains across all the tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Kumaravelu_EvoCL_Continual_Learning_over_Evolving_Domains_WACV_2025_paper.html	Vishnuprasadh Kumaravelu, P.K. Srijith, Sunil Gupta
Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities using Web Instructional Videos	We propose a novel benchmark for cross-view knowledge transfer of dense video captioning adapting models from web instructional videos with exocentric views to an egocentric view. While dense video captioning (predicting time segments and their captions) is primarily studied with exocentric videos (e.g. YouCook2) benchmarks with egocentric videos are restricted due to data scarcity. To overcome the limited video availability transferring knowledge from abundant exocentric web videos is demanded as a practical approach. However learning the correspondence between exocentric and egocentric views is difficult due to their dynamic view changes. The web videos contain shots showing either full-body or hand regions while the egocentric view is constantly shifting. This necessitates the in-depth study of cross-view transfer under complex view changes. To this end we first create a real-life egocentric dataset (EgoYC2) whose captions follow the definition of YouCook2 captions enabling transfer learning between these datasets with access to their ground-truth. To bridge the view gaps we propose a view-invariant learning method using adversarial training which consists of pre-training and fine-tuning stages. Our experiments confirm the effectiveness of overcoming the view change problem and knowledge transfer to egocentric views. Our benchmark pushes the study of cross-view transfer into a new task domain of dense video captioning and envisions methodologies that describe egocentric videos in natural language.	https://openaccess.thecvf.com//content/WACV2025/html/Ohkawa_Exo2EgoDVC_Dense_Video_Captioning_of_Egocentric_Procedural_Activities_using_Web_WACV_2025_paper.html	Takehiko Ohkawa, Takuma Yagi, Taichi Nishimura, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato
Explicit Guidance for Robust Video Frame Interpolation against Discontinuous Motions	Nowadays many videos contain graphic elements such as logos subtitles and user interfaces. These overlayed elements exhibit discontinuous motions characterized by static or instantaneous motions that are neither spatially nor temporally coherent. As existing Video Frame Interpolation (VFI) methods rely on motion-compensation techniques they work best for videos with continuous motions but face limitations against videos with discontinuous motion. In this paper we propose a simple framework to enhance the robustness of existing VFI models against discontinuous motion. We first identify key properties that distinguish discontinuous from continuous motion. They are then leveraged by the Discontinuity map (D-map) estimator to explicitly guide the classification of continuous and discontinuous areas through a coherence mask and an additional supervisory signal. Our framework separately interpolates the predicted continuous and discontinuous regions to achieve state-of-the-art performance against synthetic discontinuous motions while also generalizing well to real-world discontinuous motions. Moreover our framework's `plug-and-play' design enables easy application to existing VFI models without the need for retraining and maintains strong performance on continuous motion.	https://openaccess.thecvf.com//content/WACV2025/html/Park_Explicit_Guidance_for_Robust_Video_Frame_Interpolation_against_Discontinuous_Motions_WACV_2025_paper.html	JaeHyun Park, Nam Ik Cho
Exploiting Inter-Sample Information for Long-Tailed Out-of-Distribution Detection	Detecting out-of-distribution (OOD) data is essential for safe deployment of deep neural networks (DNNs). This problem becomes particularly challenging in the presence of long-tailed in-distribution (ID) datasets often leading to high false positive rates (FPR) and low tail-class ID classification accuracy. In this paper we demonstrate that exploiting inter-sample relationships using a graph-based representation can significantly improve OOD detection in long-tailed recognition of vision datasets. To this end we use the feature space of a pre-trained model to initialize our graph structure. We account for the differences between the activation layer distribution of the pre-training vs. training data and actively introduce Gaussianization to alleviate any deviations from a standard normal distribution in the activation layers of the pre-trained model. We then refine this initial graph representation using graph convolutional networks (GCNs) to arrive at a feature space suitable for long-tailed OOD detection. This leads us to address the inferior performance observed in ID tail-classes within existing OOD detection methods. Experiments over three benchmarks CIFAR10-LT CIFAR100-LT and ImageNet-LT demonstrate that our method outperforms the state-of-the-art approaches by a large margin in terms of FPR and tail-class ID classification accuracy.	https://openaccess.thecvf.com//content/WACV2025/html/Udayangani_Exploiting_Inter-Sample_Information_for_Long-Tailed_Out-of-Distribution_Detection_WACV_2025_paper.html	Nimeshika Udayangani, Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher Leckie
Exploiting VLM Localizability and Semantics for Open Vocabulary Action Detection	Action detection aims to detect (recognize and localize) human actions spatially and temporally in videos. Existing approaches focus on the closed-set setting where an action detector is trained and tested on videos from a fixed set of action categories. However this constrained setting is not viable in an open world where test videos inevitably come beyond the trained action categories. In this paper we address the practical yet challenging Open-Vocabulary Action Detection (OVAD) problem. It aims to detect any action in test videos while training a model on a fixed set of action categories. To achieve such an open-vocabulary capability we propose a novel method OpenMixer that exploits the inherent semantics and localizability of large vision-language models (VLM) within the family of query-based detection transformers (DETR). Specifically the OpenMixer is developed by spatial and temporal OpenMixer blocks (S-OMB and T-OMB) and a dynamically fused alignment (DFA) module. The three components collectively enjoy the merits of strong generalization from pre-trained VLMs and end-to-end learning from DETR design. Moreover we established OVAD benchmarks under various settings and the experimental results show that the OpenMixer performs the best over baselines for detecting seen and unseen actions. We release the codes models and dataset splits at: https://github.com/Cogito2012/OpenMixer.	https://openaccess.thecvf.com//content/WACV2025/html/Bao_Exploiting_VLM_Localizability_and_Semantics_for_Open_Vocabulary_Action_Detection_WACV_2025_paper.html	Wentao Bao, Kai Li, Yuxiao Chen, Deep A Patel, Martin Renqiang Min, Yu Kong
Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization	The vocabulary size in temporal action localization (TAL) is limited by the scarcity of large-scale annotated datasets. To overcome this recent works integrate vision-language models (VLMs) such as CLIP for open-vocabulary TAL (OV-TAL). However despite the success of VLMs trained on extensive datasets existing OV-TAL methods still rely on human-labeled TAL datasets of limited size to train action localizers limiting their generalizability. In this paper we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic action localizer is trained on a human-labeled TAL dataset to generate pseudo-labels for unlabeled videos and (2) the large-scale pseudo-labeled dataset is then used to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally we identify limitations in existing OV-TAL evaluation schemes and propose a new benchmark for thorough assessment. Finally we showcase the TAL performance of the large multimodal model Gemini-1.5 on our new benchmark. Code is released at https://github.com/HYUNJS/STOV-TAL.	https://openaccess.thecvf.com//content/WACV2025/html/Hyun_Exploring_Scalability_of_Self-Training_for_Open-Vocabulary_Temporal_Action_Localization_WACV_2025_paper.html	Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim
Exploring the Stability Gap in Continual Learning: The Role of the Classification Head	Continual learning (CL) has emerged as a critical area in machine learning enabling neural networks to learn from evolving data distributions while mitigating catastrophic forgetting. However recent research has identified the stability gap - a phenomenon where models initially lose performance on previously learned tasks before partially recovering during training. Such learning dynamics are contradictory to the intuitive understanding of stability in continual learning where one would expect the performance to degrade gradually instead of rapidly decreasing and then partially recovering later. To better understand and alleviate the stability gap we investigate it at different levels of the neural network architecture particularly focusing on the role of the classification head. We introduce the nearest-mean classifier (NMC) as a tool to attribute the influence of the backbone and the classification head on the stability gap. Our experiments demonstrate that NMC not only improves final performance but also significantly enhances training stability across various continual learning benchmarks including CIFAR100 ImageNet100 CUB-200 and FGVC Aircrafts. Moreover we find that NMC also reduces task-recency bias. Our analysis provides new insights into the stability gap and suggests that the primary contributor to this phenomenon is the linear head rather than the insufficient representation learning.	https://openaccess.thecvf.com//content/WACV2025/html/Lapacz_Exploring_the_Stability_Gap_in_Continual_Learning_The_Role_of_WACV_2025_paper.html	Wojciech Åapacz, Daniel Marczak, Filip Szatkowski, Tomasz TrzciÅski
F2FLDM: Latent Diffusion Models with Histopathology Pre-Trained Embeddings for Unpaired Frozen Section to FFPE Translation	Frozen Section (FS) technique is a rapid method taking only 15-30 minutes to prepare slides for pathologists' evaluation during surgery enabling immediate surgical decisions. However the FS process often introduces artifacts and distortions like folds and ice-crystal effects. In contrast these artifacts are absent in higher-quality formalin-fixed paraffin-embedded (FFPE) slides which take 2-3 days to prepare. Generative Adversarial Network (GAN)-based methods have been used to translate FS to FFPE images but they may leave morphological inaccuracies or introduce new artifacts reducing translation quality for clinical assessments. In this study we benchmark recent generative models focusing on GANs and Latent Diffusion Models (LDMs) to address these limitations. We introduce a novel approach combining LDMs with Histopathology Pre-Trained Embeddings to enhance FS image restoration. Our framework uses LDMs conditioned by both text and pre-trained embeddings to learn meaningful features of FS and FFPE images. Through diffusion and denoising processes our approach preserves essential diagnostic attributes like color staining and tissue morphology using DDIM Inversion with FS representation. Additionally it enhances the histological quality of translated images by predicting the targeted FFPE representation via an innovative embedding translation mechanism improving morphological details and reducing FS artifacts. As a result this work significantly improves classification performance with the Area Under the Curve rising from 81.99% to 94.64% and achieves the best results in user study. This work establishes a new benchmark for FS to FFPE image translation quality promising enhanced reliability and accuracy in histopathology FS image analysis. Our work is available at https://minhmanho.github.io/f2f_ldm/.	https://openaccess.thecvf.com//content/WACV2025/html/Ho_F2FLDM_Latent_Diffusion_Models_with_Histopathology_Pre-Trained_Embeddings_for_Unpaired_WACV_2025_paper.html	Man M. Ho, Shikha Dubey, Yosep Chong, Beatrice Knudsen, Tolga Tasdizen
F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring	Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper we propose a novel approach based on the Fractional Fourier Transform (FRFT) a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously making it ideal for processing non-stationary signals like images. Specifically we introduce a Fractional Fourier Transformer (F2former) where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.	https://openaccess.thecvf.com//content/WACV2025/html/Paul_F2former_When_Fractional_Fourier_Meets_Deep_Wiener_Deconvolution_and_Selective_WACV_2025_paper.html	Subhajit Paul, Sahil Kumawat, Ashutosh Gupta, Deepak Mishra
FAIR-TAT: Improving Model Fairness using Targeted Adversarial Training	Deep neural networks are susceptible to adversarial attacks and common corruptions which undermine their robustness. In order to enhance model resilience against such challenges Adversarial Training (AT) has emerged as a prominent solution. Nevertheless adversarial robustness is often attained at the expense of model fairness during AT i.e. disparity in class-wise robustness of the model. While distinctive classes become more robust towards such adversaries hard to detect classes suffer. Recently research has focused on improving model fairness specifically for perturbed images overlooking the accuracy of the most likely non-perturbed data. Additionally despite their robustness against the adversaries encountered during model training state-of-the-art adversarial trained models have difficulty maintaining robustness and fairness when confronted with diverse adversarial threats or common corruptions. In this work we address the above concerns by introducing a novel approach called Fair Targeted Adversarial Training (FAIR-TAT). We show that using targeted adversarial attacks for adversarial training (instead of untargeted attacks) can allow for more favorable trade-offs with respect to adversarial fairness. Empirical results validate the efficacy of our approach.	https://openaccess.thecvf.com//content/WACV2025/html/Medi_FAIR-TAT_Improving_Model_Fairness_using_Targeted_Adversarial_Training_WACV_2025_paper.html	Tejaswini Medi, Steffen Jung, Margret Keuper
FALCON: Fair Face Recognition via Local Optimal Feature Normalization	Face recognition systems are widely used for identity verification in various fields. However recent studies have highlighted bias issues related to demographic and non-demographic attributes such as accessories haircolor ethnicity or gender. These biases lead to higher error rates for specific attribute subgroups. This is especially problematic in critical areas like forensics where these systems are deployed. Addressing this issue requires a solution that reduces bias without compromising accuracy. Existing methods focus on learning less biased face representations but they are often difficult to integrate into current systems or negatively impact overall recognition performance. This work introduces FALCON (Fair Adaptation Through Local Optimal Normalization) an effective method to increase fairness in face recognition systems. FALCON operates in an unsupervised manner addressing bias without requiring demographic labels and can be easily integrated as a post-processing step. It treats individuals with similar traits similarly reducing bias in face recognition by processing each image individually. The proposed method is rigorously tested across various face recognition models and datasets and compared with four other fairness post-processing methods. Results show that FALCON significantly enhances both fairness and accuracy. Unlike other methods it allows seamlessly adjusting the fairness-accuracy trade-off while effectively addressing bias.	https://openaccess.thecvf.com//content/WACV2025/html/Al-Refai_FALCON_Fair_Face_Recognition_via_Local_Optimal_Feature_Normalization_WACV_2025_paper.html	Rouqaiah Al-Refai, Philipp Hempel, Clara Biagi, Philipp TerhÃ¶rst
FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework	Scene Text Editing (STE) is a challenging research problem that primarily aims towards modifying existing texts in an image while preserving the background and the font style of the original text. Despite its utility in numerous real-world applications existing style-transfer-based approaches have shown sub-par editing performance due to (1) complex image backgrounds (2) diverse font attributes and (3) varying word lengths within the text. To address such limitations in this paper we propose a novel font-agnostic scene text editing and rendering framework named FASTER for simultaneously generating text in arbitrary styles and locations while preserving a natural and realistic appearance and structure. A combined fusion of target mask generation and style transfer units with a cascaded self-attention mechanism has been proposed to focus on multi-level text region edits to handle varying word lengths. Extensive evaluation on a real-world database with further subjective human evaluation study indicates the superiority of FASTER in both scene text editing and rendering tasks in terms of model performance and efficiency. The code and pre-trained models have been released in our Github repo.	https://openaccess.thecvf.com//content/WACV2025/html/Das_FASTER_A_Font-Agnostic_Scene_Text_Editing_and_Rendering_Framework_WACV_2025_paper.html	Alloy Das, Sanket Biswas, Prasun Roy, Subhankar Ghosh, Umapada Pal, Michael Blumenstein, Josep LladÃ³s, Saumik Bhattacharya
FDS: Feedback-Guided Domain Synthesis with Multi-Source Conditional Diffusion Models for Domain Generalization	Domain Generalization techniques aim to enhance model robustness by simulating novel data distributions during training typically through various augmentation or stylization strategies. However these methods frequently suffer from limited control over the diversity of generated images and lack assurance that these images span distinct distributions. To address these challenges we propose FDS Feedback-guided Domain Synthesis a novel strategy that employs diffusion models to synthesize novel pseudo-domains by training a single model on all source domains and performing domain mixing based on learned features. By incorporating images that pose classification challenges to models trained on original samples alongside the original dataset we ensure the generation of a training set that spans a broad distribution spectrum. Our comprehensive evaluations demonstrate that this methodology sets new benchmarks in domain generalization performance across a range of challenging datasets effectively managing diverse types of domain shifts. The code can be found at https://github.com/Mehrdad-Noori/FDS.	https://openaccess.thecvf.com//content/WACV2025/html/Noori_FDS_Feedback-Guided_Domain_Synthesis_with_Multi-Source_Conditional_Diffusion_Models_for_WACV_2025_paper.html	Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo A Vargas Hakim, David Osowiechi, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers
FLAIR: A Conditional Diffusion Framework with Applications to Face Video Restoration	Face video restoration (FVR) is a challenging but important problem where one seeks to recover a perceptually realistic face videos from a low-quality input. While diffusion probabilistic models (DPMs) have been shown to achieve remarkable performance for face image restoration they often fail to preserve temporally coherent high-quality videos compromising the fidelity of reconstructed faces. We present a new conditional diffusion framework called FLAIR for FVR. FLAIR ensures improved temporal alignments across frames in a computationally efficient fashion by converting a traditional image DPM into a video DPM. The proposed conversion uses a recurrent video refinement layer and a temporal self-attention at different scales. FLAIR also uses a conditional iterative refinement process to balance the perceptual and distortion quality during inference. This process consists of two key components: a data-consistency module that analytically ensures that the generated video precisely matches its degraded observation and a coarse-to-fine image enhancement module specifically for facial regions. Our extensive experiments show superiority of FLAIR over the current state-of-the-art (SOTA) for video super-resolution deblurring JPEG restoration and space-time frame interpolation on two high-quality face video datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Zou_FLAIR_A_Conditional_Diffusion_Framework_with_Applications_to_Face_Video_WACV_2025_paper.html	Zihao Zou, Jiaming Liu, Shirin Shoushtari, Yubo Wang, Ulugbek S. Kamilov
FMD: Comprehensive Data Compression in Medical Domain via Fused Matching Distillation	Medical datasets are often large and contain sensitive information presenting significant challenges for data sharing and storage. To address these issues this paper introduces a novel method called Fused Matching Distillation (FMD) which combines multiple dataset distillation techniques to achieve both data compression and enhanced privacy. FMD synthesizes representative subsets that capture the essential information from the original dataset while anonymizing sensitive details during the distillation process. The proposed approach integrates two complementary methods: parameter matching a technique that aligns the training trajectories of a teacher network trained on real data with those of a student network trained on synthetic data and feature distribution matching which ensures that the synthetic dataset closely approximates the feature distribution of the original data. By fusing these techniques FMD maximizes the information density within each pixel of the distilled dataset achieving a balance between compression and performance. Experimental evaluations on medical datasets including COVID chest X-ray and Pancreas cancer CT demonstrate that FMD achieves superior accuracy and privacy compared to existing methods. Furthermore the proposed method is evaluated using metrics for both model performance and anonymity showing that FMD not only maintains high diagnostic accuracy but also effectively anonymizes the data. This makes FMD a promising tool for secure and efficient medical data sharing.	https://openaccess.thecvf.com//content/WACV2025/html/Son_FMD_Comprehensive_Data_Compression_in_Medical_Domain_via_Fused_Matching_WACV_2025_paper.html	Ju Heon Son, Jang-Hwan Choi
FOR: Finetuning for Object Level Open Vocabulary Image Retrieval	As working with large datasets becomes standard the task of accurately retrieving images containing objects of interest by an open set textual query gains practical importance. The current leading approach utilizes a pre-trained CLIP model without any adaptation to the target domain balancing accuracy and efficiency through additional post-processing. In this work we propose FOR: Finetuning for Object-centric Open-vocabulary Image Retrieval which allows finetuning on a target dataset using closed-set labels while keeping the visual-language association crucial for open vocabulary retrieval. FOR is based on two design elements: a specialized decoder variant of the CLIP head customized for the intended task and its coupling within a multi-objective training framework. Together these design choices result in a significant increase in accuracy show-casing improvements of up to 8 mAP@50 points over SoTA across three datasets. Additionally we demonstrate that FOR is also effective in a semi-supervised setting achieving impressive results even when only a small portion of the dataset is labeled.	https://openaccess.thecvf.com//content/WACV2025/html/Levi_FOR_Finetuning_for_Object_Level_Open_Vocabulary_Image_Retrieval_WACV_2025_paper.html	Hila Levi, Guy Heller, Dan Levi
FRAUD-Net: Fraud News Detection using Sample Uncertainty & Domain Aware Generalized Network	Due to the widespread impact of social media efficiently detecting out-of-context misinformation where a real image is paired with a fake caption has become imperative. Towards this goal we propose a novel framework FRAUD-Net which incorporates several unexplored aspects of this task in the model formulation. Keeping in mind that the image and textual evidences collected using the input image-text pair by web search play a crucial role in this task we build a generalized model which can handle missing or variable number of evidences as expected in real-world scenarios. We achieve this by effectively utilizing the common semantic latent space of Visual Language Models and transformer attention blocks. In addition we observe that explicit domain information not only allows the model to learn better but also dynamically adjusts to the diverse domains (like politics healthcare etc.) during testing. We also propose a mechanism to handle noisy training data by analyzing prediction consistency which helps in model training. Extensive experiments on the large-scale NewsClippings and Verite benchmark datasets showcase the effectiveness of the proposed framework compared to the state-of-the-art techniques for this challenging task.	https://openaccess.thecvf.com//content/WACV2025/html/Patel_FRAUD-Net_Fraud_News_Detection_using_Sample_Uncertainty__Domain_Aware_WACV_2025_paper.html	Devendra Patel, Vikas Verma, Shreyas Kumar Tah, Shwetabh Biswas, Soma Biswas
FT2TF: First-Person Statement Text-To-Talking Face Generation	Talking face generation has gained immense popularity in the computer vision community with various applications including AR VR teleconferencing digital assistants and avatars. Traditional methods are mainly audio-driven which have to deal with the inevitable resource-intensive nature of audio storage and processing. To address such a challenge we propose FT2TF - First-Person Statement Text-To-Talking Face Generation a novel one-stage end-to-end pipeline for talking face generation driven by first-person statement text. Different from previous work our model only leverages visual and textual information without any other sources (e.g. audio/landmark/pose) during inference. Extensive experiments are conducted on LRS2 and LRS3 datasets and results on multi-dimensional evaluation metrics are reported. Both quantitative and qualitative results showcase that FT2TF outperforms existing relevant methods and reaches the state-of-the-art. This achievement highlights our model's capability to bridge first-person statements and dynamic face generation providing insightful guidance for future work.	https://openaccess.thecvf.com//content/WACV2025/html/Diao_FT2TF_First-Person_Statement_Text-To-Talking_Face_Generation_WACV_2025_paper.html	Xingjian Diao, Ming Cheng, Wayner Barrios, SouYoung Jin
FUN-AD: Fully Unsupervised Learning for Anomaly Detection with Noisy Training Data	While the mainstream research in anomaly detection has mainly followed the one-class classification practical industrial environments often incur noisy training data due to annotation errors or lack of labels for new or refurbished products. To address these issues we propose a novel learning-based approach for fully unsupervised anomaly detection with unlabeled and potentially contaminated training data. Our method is motivated by two observations that i) the pairwise feature distances between the normal samples are on average likely to be smaller than those between the anomaly samples or heterogeneous samples and ii) pairs of features mutually closest to each other are likely to be homogeneous pairs which hold if the normal data has smaller variance than the anomaly data. Building on the first observation that nearest-neighbor distances can distinguish between confident normal samples and anomalies we propose a pseudo-labeling strategy using an iteratively reconstructed memory bank (IRMB). The second observation is utilized as a new loss function to promote class-homogeneity between mutually closest pairs thereby reducing the ill-posedness of the task. Experimental results on two public industrial anomaly benchmarks and semantic anomaly examples validate the effectiveness of FUN-AD across different scenarios and anomaly-to-normal ratios. Our code is available at https://github.com/HY-Vision-Lab/FUNAD.	https://openaccess.thecvf.com//content/WACV2025/html/Im_FUN-AD_Fully_Unsupervised_Learning_for_Anomaly_Detection_with_Noisy_Training_WACV_2025_paper.html	Jiin Im, Yongho Son, Je Hyeong Hong
FaVoR: Features via Voxel Rendering for Camera Relocalization	Camera relocalization methods range from dense image alignment to direct camera pose regression from a query image. Among these sparse feature matching stands out as an efficient versatile and generally lightweight approach with numerous applications. However feature-based methods often struggle with significant viewpoint and appearance changes leading to matching failures and inaccurate pose estimates. To overcome these limitations we propose a novel approach that leverages a globally sparse but locally dense 3D representation of 2D features. By tracking and triangulating landmarks over a sequence of frames we construct a sparse voxel map optimized to render image patch descriptors observed during tracking. Given an initial pose estimate we first synthesize descriptors from the voxels using volumetric rendering and then perform feature matching to estimate the camera pose. This method enables the generation of descriptors for unseen views enhancing robustness to viewpoint changes. We evaluate our method on the 7-Scenes and Cambridge Landmarks datasets. Our results show that our approach significantly outperforms existing state-of-the-art feature representation techniques in indoor environments achieving up to a 39% improvement in median translation error. Additionally our approach yields comparable results to other methods for outdoor scenes but with lower computational and memory footprints.	https://openaccess.thecvf.com//content/WACV2025/html/Polizzi_FaVoR_Features_via_Voxel_Rendering_for_Camera_Relocalization_WACV_2025_paper.html	Vincenzo Polizzi, Marco Cannici, Davide Scaramuzza, Jonathan Kelly
Face Anonymization Made Simple	Current face anonymization techniques often depend on identity loss calculated by face recognition models which can be inaccurate and unreliable. Additionally many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast our approach uses diffusion models with only a reconstruction loss eliminating the need for facial landmarks or masks while still producing images with intricate fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization facial attribute preservation and image quality. Beyond its primary function of anonymization our model can also perform face swapping tasks by incorporating an additional facial image as input demonstrating its versatility and potential for diverse applications. Our code and models are available at https://github.com/hanweikung/face_anon_simple.	https://openaccess.thecvf.com//content/WACV2025/html/Kung_Face_Anonymization_Made_Simple_WACV_2025_paper.html	Han-Wei Kung, Tuomas Varanka, Sanjay Saha, Terence Sim, Nicu Sebe
Facial Expression Recognition with Controlled Privacy Preservation and Feature Compensation	Facial expression recognition (FER) systems raise significant privacy concerns due to the potential exposure of sensitive identity information. This paper presents a study on removing identity information while preserving FER capabilities. Drawing on the observation that low-frequency components predominantly contain identity information and high-frequency components capture expression we propose a novel two-stream framework that applies privacy enhancement to each component separately. We introduce a controlled privacy enhancement mechanism to optimize performance and a feature compensator to enhance task-relevant features without compromising privacy. Furthermore we propose a novel privacy-utility trade-off providing a quantifiable measure of privacy preservation efficacy in closed-set FER tasks. Extensive experiments on the benchmark CREMA-D dataset demonstrate that our framework achieves 78.84% recognition accuracy with a privacy (facial identity) leakage ratio of only 2.01% highlighting its potential for secure and reliable video-based FER applications.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Facial_Expression_Recognition_with_Controlled_Privacy_Preservation_and_Feature_Compensation_WACV_2025_paper.html	Feng Xu, David Ahmedt-Aristizabal, Lars Petersson, Dadong Wang, Xun Li
Fair Domain Generalization with Heterogeneous Sensitive Attributes Across Domains	Domain generalization(DG) techniques classify data from unseen domains by capitalizing on data from multiple source domains. Most methods in DG focus on improving predictive performance measures on the unseen domain. Recent studies have also attempted to enforce fairness measures on the unseen domain. However these studies assume that every domain has the same sensitive attribute including the unseen domain. In practice each domain may be required to satisfy fairness with respect to its own set of multiple sensitive attributes. Given a set of sensitive attributes (S) current methods need to train 2^n models to ensure fairness with respect to any subset of S where n = |S|. We propose a single-model solution to address this new problem setting. We learn two feature representations one to generalize the model's predictive performance and another to generalize the model's fairness. The first representation is made invariant across domains to generalize predictive performance. The second representation is kept selectively invariant i.e. invariant only across domains having the same sensitive attributes. Our single model exhibits superior predictive performance and fairness measures against the current alternative of 2^n models on unseen domains on multiple real-world vision datasets. Our code is available at https://github.com/ragjapk/SISA.	https://openaccess.thecvf.com//content/WACV2025/html/Palakkadavath_Fair_Domain_Generalization_with_Heterogeneous_Sensitive_Attributes_Across_Domains_WACV_2025_paper.html	Ragja Palakkadavath, Hung Le, Thanh Nguyen-Tang, Sunil Gupta, Svetha Venkatesh
Fairer Analysis and Demographically Balanced Face Generation for Fairer Face Verification	Face recognition and verification are two computer vision tasks whose performances have advanced with the introduction of deep representations. However ethical legal and technical challenges due to the sensitive nature of face data and biases in real-world training datasets hinder their development. Generative AI addresses privacy by creating fictitious identities but fairness problems remain. Using the existing DCFace SOTA framework we introduce a new controlled generation pipeline that improves fairness. Through classical fairness metrics and a proposed in-depth statistical analysis based on logit models and ANOVA we show that our generation pipeline improves fairness more than other bias mitigation approaches while slightly improving raw performance.	https://openaccess.thecvf.com//content/WACV2025/html/Fournier-Montgieux_Fairer_Analysis_and_Demographically_Balanced_Face_Generation_for_Fairer_Face_WACV_2025_paper.html	Alexandre Fournier-Montgieux, MichaÃ«l Soumm, Adrian Popescu, Bertrand Luvison, HervÃ© Le Borgne
FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing	Diffusion models have demonstrated remarkable capabilities in text-to-image and text-to-video generation opening up possibilities for video editing based on textual input. However the computational cost associated with sequential sampling in diffusion models poses challenges for efficient video editing. Existing approaches relying on image generation models for video editing suffer from time-consuming one-shot fine-tuning additional condition extraction or DDIM inversion making real-time applications impractical. In this work we propose FastVideoEdit an efficient zero-shot video editing approach inspired by Consistency Models (CMs). By leveraging the self-consistency property of CMs we eliminate the need for time-consuming inversion or additional condition extraction reducing editing time. Our method enables direct mapping from source video to target video with strong preservation ability through attention control. This results in improved speed advantages as fewer sampling steps can be used while maintaining comparable generation quality. Experimental results validate the state-of-the-art performance and speed advantages of FastVideoEdit across evaluation metrics encompassing editing speed temporal consistency and text-video alignment. The source code is available at github.com/youyuan-zhang/FastVideoEdit.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_FastVideoEdit_Leveraging_Consistency_Models_for_Efficient_Text-to-Video_Editing_WACV_2025_paper.html	Youyuan Zhang, Xuan Ju, James J. Clark
Feasibility of Federated Learning from Client Databases with Different Brain Diseases and MRI Modalities	Segmentation models for brain lesions in MRI are commonly developed for a specific disease and trained on data with a predefined set of MRI modalities. Such models cannot segment the disease using data with a different set of MRI modalities nor can they segment other types of diseases. Moreover this training paradigm prevents a model from using the advantages of learning from heterogeneous databases that may contain scans and segmentation labels for different brain pathologies and diverse sets of MRI modalities. Additionally the confidentiality of patient data often prevents central data aggregation necessitating a decentralized approach. Is it feasible to use Federated Learning (FL) to train a single model on client databases that contain scans and labels of different brain pathologies and diverse sets of MRI modalities? We demonstrate promising results by combining appropriate simple and practical modifications to the model and training strategy: Designing a model with input channels that cover the whole set of modalities available across clients training with random modality drop and exploring the effects of feature normalization methods. Evaluation on 7 brain MRI databases with 5 different diseases shows that this FL framework can train a single model that achieves very promising results in segmenting all disease types seen during training. Importantly it can segment these diseases in new databases that contain sets of modalities different from those in training clients. These results demonstrate for the first time the feasibility and effectiveness of using FL to train a single 3D segmentation model on decentralised data with diverse brain diseases and MRI modalities a necessary step towards leveraging heterogeneous real-world databases. Code: https://github.com/FelixWag/FedUniBrain	https://openaccess.thecvf.com//content/WACV2025/html/Wagner_Feasibility_of_Federated_Learning_from_Client_Databases_with_Different_Brain_WACV_2025_paper.html	Felix Wagner, Wentian Xu, Pramit Saha, Ziyun Liang, Daniel Whitehouse, David Menon, Virginia Newcombe, Natalie Voets, J. Alison Noble, Konstantinos Kamnitsas
Feature Augmentation Based Test-Time Adaptation	Test-time adaptation (TTA) allows a model to be adapted to an unseen domain without accessing the source data. Due to the nature of practical environments TTA has a limited amount of data for adaptation. Recent TTA methods further restrict this by filtering input data for reliability making the effective data size even smaller and limiting adaptation potential. To address this issue We propose Feature Augmentation based Test-time Adaptation (FATA) a simple method that fully utilizes the limited amount of input data through feature augmentation. FATA employs Normalization Perturbation to augment features and adapts the model using the FATA loss which makes the outputs of the augmented and original features similar. FATA is model-agnostic and can be seamlessly integrated into existing models without altering the model architecture. We demonstrate the effectiveness of FATA on various models and scenarios on ImageNet-C and Office-Home validating its superiority in diverse real-world conditions. Code is available at https://github.com/RangeWING/FATA.	https://openaccess.thecvf.com//content/WACV2025/html/Cho_Feature_Augmentation_Based_Test-Time_Adaptation_WACV_2025_paper.html	Younggeol Cho, Youngrae Kim, Junho Yoon, Seunghoon Hong, Dongman Lee
Feature Design for Bridging SAM and CLIP toward Referring Image Segmentation	Referring Image Segmentation (RIS) is a task aimed at segmenting objects expressed in natural language within an image. This task requires an understanding of the relationship between vision and language along with precise segmentation capabilities. In the field of computer vision CLIP and Segment anything model (SAM) have gained significant attention for their classification and the segmentation capabilities. Given that both models possess essential skills for RIS combining them seems to be an effective strategy. In this paper we propose a model that integrates CLIP and SAM to enhance RIS. Since SAM lacks classification capabilities we developed a module that supplies the SAM mask decoder with features that specify the target object. We introduce a new module which is trained on additional instance segmentation tasks. The features utilized and derived from this module serve as inputs for the SAM decoder. With these inputs SAM is expected to effectively segment areas corresponding to the given natural language expressions. We conducted experiments using the traditional RefCOCO/+/g as well as the recently introduced gRefCOCO and Ref-zom datasets demonstrating the advantages of our approach. Code will be available on https://github.com/hitachi-rd-cv/dfam.	https://openaccess.thecvf.com//content/WACV2025/html/Ito_Feature_Design_for_Bridging_SAM_and_CLIP_toward_Referring_Image_WACV_2025_paper.html	Koichiro Ito
Feature Fusion Transferability Aware Transformer for Unsupervised Domain Adaptation	Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from labeled source domains to improve performance on the unlabeled target domains. While Convolutional Neural Networks (CNNs) have been dominant in previous UDA methods recent research has shown promise in applying Vision Transformers (ViTs) to this task. In this study we propose a novel Feature Fusion Transferability Aware Transformer (FFTAT) to enhance ViT performance in UDA tasks. Our method introduces two key innovations: First we introduce a patch discriminator to evaluate the transferability of patches generating a transferability matrix. We integrate this matrix into self-attention directing the model to focus on transferable patches. Second we propose a feature fusion technique to fuse embeddings in the latent space enabling each embedding to incorporate information from all others thereby improving generalization. These two components work in synergy to enhance feature representation learning. Extensive experiments on widely used benchmarks demonstrate that our method significantly improves UDA performance achieving state-of-the-art (SOTA) results.	https://openaccess.thecvf.com//content/WACV2025/html/Yu_Feature_Fusion_Transferability_Aware_Transformer_for_Unsupervised_Domain_Adaptation_WACV_2025_paper.html	Xiaowei Yu, Zhe Huang, Zao Zhang
Feature Space Perturbation: A Panacea to Enhanced Transferability Estimation	Leveraging a transferability estimation metric facilitates the non-trivial challenge of selecting the optimal model for the downstream task from a pool of pre-trained models. Most existing metrics primarily focus on identifying the statistical relationship between feature embeddings and the corresponding labels within the target dataset but overlook crucial aspect of model robustness. This oversight may limit their effectiveness in accurately ranking pre-trained models. To address this limitation we introduce a feature perturbation method that enhances the transferability estimation process by systematically altering the feature space. Our method includes a Spread operation that increases intra-class variability adding complexity within classes and an Attract operation that minimizes the distances between different classes thereby blurring the class boundaries. Through extensive experimentation we demonstrate the efficacy of our feature perturbation method in providing a more precise and robust estimation of model transferability. Notably the existing LogMe method exhibited a significant improvement showing a 28.84% increase in performance after applying our feature perturbation method. The implementation is available at https://github.com/prafful-kumar/enhancing_TE.git	https://openaccess.thecvf.com//content/WACV2025/html/Khoba_Feature_Space_Perturbation_A_Panacea_to_Enhanced_Transferability_Estimation_WACV_2025_paper.html	Prafful Kumar Khoba, Zijian Wang, Chetan Arora, Mahsa Baktashmotlagh
Feature-Level and Spatial-Level Activation Expansion for Weakly-Supervised Semantic Segmentation	Weakly-supervised Semantic Segmentation (WSSS) aims to provide a precise semantic segmentation results without expensive pixel-wise segmentation labels. With the supervision gap between classification and segmentation Image-level WSSS mainly relies on Class Activation Maps (CAMs) from the classification model to emulate the pixel-wise annotations. However CAMs often fail to cover the entire object region because classification models tend to focus on narrow discriminative regions in an object. Towards accurate CAM coverage Existing WSSS methods have tried to boost feature representation learning or impose consistency regularization to the classification models but still there are limitation in activating non-discriminative area where the focus of the models is weak. To tackle this issue we propose FSAE framework which provides explicit supervision of non-discriminative area encouraging the CAMs to activate on various object features. We leverage weak-strong consistency with pseudo-label expansion strategy for reliable supervision and enhance learning of non-discriminative object boundaries. Specifically we use strong perturbation to make challenging inference target and focus on generating reliable pixel-wise supervision signal for broad object regions. Extensive experiments on the WSSS benchmark datasets show that our method boosts initial seed quality and segmentation performance by large margin achieving new state-of-the-art performance on benchmark WSSS datasets. Our public code is available at https://github.com/obeychoi0120/FSAE.	https://openaccess.thecvf.com//content/WACV2025/html/Choi_Feature-Level_and_Spatial-Level_Activation_Expansion_for_Weakly-Supervised_Semantic_Segmentation_WACV_2025_paper.html	Junsu Choi, Jin-Seop Lee, Noo-ri Kim, SuHyun Yoon, Jee-Hyong Lee
Federated Source-Free Domain Adaptation for Classification: Weighted Cluster Aggregation for Unlabeled Data	Federated learning (FL) commonly assumes that the server or some clients have labeled data which is often impractical due to annotation costs and privacy concerns. Addressing this problem we focus on a source-free domain adaptation task where (1) the server holds a pre-trained model on labeled source domain data (2) clients possess only unlabeled data from various target domains and (3) the server and clients cannot access the source data in the adaptation phase. This task is known as Federated source-Free Domain Adaptation (FFREEDA). Specifically we focus on classification tasks while the previous work solely studies semantic segmentation. Our contribution is the novel Federated learning with Weighted Cluster Aggregation (FedWCA) method designed to mitigate both domain shifts and privacy concerns with only unlabeled data. Fed-WCA comprises three phases: private and parameter-free clustering of clients to obtain domain-specific global models on the server weighted aggregation of the global models for the clustered clients and local domain adaptation with pseudo-labeling. Experimental results show that Fed-WCA surpasses several existing methods and baselines in FFREEDA establishing its effectiveness and practicality.	https://openaccess.thecvf.com//content/WACV2025/html/Mori_Federated_Source-Free_Domain_Adaptation_for_Classification_Weighted_Cluster_Aggregation_for_WACV_2025_paper.html	Junki Mori, Kosuke Kihara, Taiki Miyagawa, Akinori F. Ebihara, Isamu Teranishi, Hisashi Kashima
Federated Voxel Scene Graph for Intracranial Hemorrhage	Intracranial Hemorrhage is a potentially lethal condition whose manifestation is vastly diverse and shifts across clinical centers worldwide. Deep-learning-based solutions are starting to model complex relations between brain structures but still struggle to generalize. While gathering more diverse data is the most natural approach privacy regulations often limit the sharing of medical data. We propose the first application of Federated Scene Graph Generation. We show that our models can leverage the increased training data diversity. For Scene Graph Generation they can recall up to 20% more clinically relevant relations across datasets compared to models trained on a single centralized dataset. Learning structured data representation in a federated setting can open the way to the development of new methods that can leverage this finer information to regularize across clients more effectively.	https://openaccess.thecvf.com//content/WACV2025/html/Sanner_Federated_Voxel_Scene_Graph_for_Intracranial_Hemorrhage_WACV_2025_paper.html	Antoine P. Sanner, Jonathan Stieber, Nils F. Grauhan, Suam Kim, Marc A. Brockmann, Ahmed E. Othman, Anirban Mukhopadhyay
Federated-Continual Dynamic Segmentation of Histopathology Guided by Barlow Continuity	Federated- and Continual Learning have been established as approaches to enable privacy-aware learning on continuously changing data as required for deploying AI systems in histopathology images. However data shifts can occur in a dynamic world spatially between institutions and temporally due to changing data over time. This leads to two issues: Client Drift where the central model degrades from aggregating data from clients trained on shifted data and Catastrophic Forgetting from temporal shifts such as changes in patient populations. Both tend to degrade the model's performance of previously seen data or spatially distributed training. Despite both problems arising from the same underlying problem of data shifts existing research addresses them only individually. In this work we introduce a method that can jointly alleviate Client Drift and Catastrophic Forgetting by using our proposed Dynamic Barlow Continuity that evaluates client updates on a public reference dataset and uses this to guide the training process to a spatially and temporally shift-invariant model. We evaluate our approach on the histopathology datasets BCSS and Semicol and prove our method to be highly effective by jointly improving the dice score as much as from 15.8% to 71.6% in Client Drift and from 42.5% to 62.8% in Catastrophic Forgetting. This enables Dynamic Learning by establishing spatio-temporal shift-invariance.	https://openaccess.thecvf.com//content/WACV2025/html/Babendererde_Federated-Continual_Dynamic_Segmentation_of_Histopathology_Guided_by_Barlow_Continuity_WACV_2025_paper.html	Niklas Babendererde, Haozhe Zhu, Moritz Fuchs, Jonathan Stieber, Anirban Mukhopadhyay
Few-Shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks	This paper proposes a novel approach to few-shot semantic segmentation for machinery with multiple parts that exhibit spatial and hierarchical relationships. Our method integrates the foundation models CLIPSeg and Segment Anything Model (SAM) with the interest point detector SuperPoint and a graph convolutional network (GCN) to accurately segment machinery parts. By providing 1 to 25 annotated samples our model evaluated on a purely synthetic dataset depicting a truck-mounted loading crane achieves effective segmentation across various levels of detail. Training times are kept under five minutes on consumer GPUs. The model demonstrates robust generalization to real data achieving a qualitative synthetic-to-real generalization with a J&F score of 92.2 on real data using 10 synthetic support samples. When benchmarked on the DAVIS 2017 dataset it achieves a J&F score of 71.5 in semi-supervised video segmentation with three support samples. This method's fast training times and effective generalization to real data make it a valuable tool for autonomous systems interacting with machinery and infrastructure and illustrate the potential of combined and orchestrated foundation models for few-shot segmentation tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Schwingshackl_Few-Shot_Structure-Informed_Machinery_Part_Segmentation_with_Foundation_Models_and_Graph_WACV_2025_paper.html	Michael Schwingshackl, Fabio F. Oberweger, Markus Murschitz
Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of Out-of-Distribution Objects using Prototypes	Detecting and localising unknown or Out-of-distribution (OOD) objects in any scene can be a challenging task in vision particularly in safety-critical cases involving autonomous systems like automated vehicles or trains. Supervised anomaly segmentation or open-world object detection models depend on training on exhaustively annotated datasets for every domain and still struggle in distinguishing between background and OOD objects. In this work we present a plug-and-play framework - PRototype based OOD detection Without Labels (PROWL). It is an inference-based method that does not require training on the domain dataset and relies on extracting relevant features from self-supervised pre-trained models. PROWL can be easily adapted to detect in-domain objects in any operational design domain (ODD) in a zero-shot manner by specifying a list of known classes from this domain. PROWL as a first zero-shot unsupervised method achieves state-of-the-art results on the RoadAnomaly and RoadObstacle datasets provided in road driving benchmarks - SegmentMeIfYouCan (SMIYC) and Fishyscapes as well as comparable performance against existing supervised methods trained without auxiliary OOD data. We also demonstrate its generalisability to other domains such as rail and maritime.	https://openaccess.thecvf.com//content/WACV2025/html/Sinhamahapatra_Finding_Dino_A_Plug-and-Play_Framework_for_Zero-Shot_Detection_of_Out-of-Distribution_WACV_2025_paper.html	Poulami Sinhamahapatra, Franziska Schwaiger, Shirsha Bose, Huiyu Wang, Karsten Roscher, Stephan GÃ¼nnemann
Fine-Grained Controllable Video Generation via Object Appearance and Context	While text-to-video generation shows state-of-the-art results fine-grained output control remains challenging for users relying solely on natural language prompts. In this work we present FACTOR for fine-grained controllable video generation. FACTOR provides an intuitive interface where users can manipulate the trajectory and appearance of individual objects in conjunction with a text prompt. We propose a unified framework to integrate these control signals into an existing text-to-video model. Our approach involves a multimodal condition module with a joint encoder control-attention layers and an appearance augmentation mechanism. This design enables FACTOR to generate videos that closely align with detailed user specifications. Extensive experiments on standard benchmarks and user-provided inputs demonstrate a notable improvement in controllability by FACTOR over competitive baselines.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_Fine-Grained_Controllable_Video_Generation_via_Object_Appearance_and_Context_WACV_2025_paper.html	Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu Jiang, Xuhui Jia, Yukun Zhu, Ming-Hsuan Yang
Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding	3D visual grounding consists of identifying the instance in a 3D scene which is referred to by an accompanying language description. While several architectures have been proposed within the commonly employed grounding-by-selection framework the utilized losses are comparatively under-explored. In particular most methods rely on a basic supervised cross-entropy loss on the predicted distribution over candidate instances which fails to model both spatial relations between instances and the internal fine-grained word-level structure of the verbal referral. Sparse attempts to additionally supervise verbal embeddings globally by learning the class of the referred instance from the description or employing verbo-visual contrast to better separate instance embeddings do not fundamentally lift the aforementioned limitations. Responding to these shortcomings we introduce two novel losses for 3D visual grounding: a visual-level offset loss on regressed vector offsets from each instance to the ground-truth referred instance and a language-related span loss on predictions for the word-level span of the referred instance in the description. In addition we equip the verbo-visual fusion module of our new 3D visual grounding architecture AsphaltNet with a top-down bidirectional attentive fusion block which enables the supervisory signals from our two losses to propagate to the respective converse branches of the network and thus aid the latter to learn context-aware instance embeddings and grounding-aware verbal embeddings. AsphaltNet proposes novel auxiliary losses to aid 3D visual grounding with competitive results compared to the state-of-the-art on the ReferIt3D benchmark.	https://openaccess.thecvf.com//content/WACV2025/html/Dey_Fine-Grained_Spatial_and_Verbal_Losses_for_3D_Visual_Grounding_WACV_2025_paper.html	Sombit Dey, Ozan Unal, Christos Sakaridis, Luc Van Gool
Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think	Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results high computational demands due to multi-step inference limited its use in many scenarios. In this paper we show that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200x faster. To optimize for downstream task performance we perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. We surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models calling into question some of the conclusions drawn from prior works.	https://openaccess.thecvf.com//content/WACV2025/html/Garcia_Fine-Tuning_Image-Conditional_Diffusion_Models_is_Easier_than_You_Think_WACV_2025_paper.html	Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, Bastian Leibe
FineControlNet: Fine-Level Text Control for Image Generation with Spatially Aligned Text Control Injection	Recently introduced ControlNet has the ability to steer the text-driven image generation process with geometric input such as human 2D pose or edge representations. While ControlNet provides control over the geometric form of the instances in the generated image it lacks the capability to dictate the visual appearance of each instance. We present FineControlNet to provide fine control over each instance's appearance while maintaining the pose control capability. Specifically we develop and demonstrate FineControlNet with geometric control via human pose images and appearance control via instance-level text prompts. The spatial alignment of instance-specific text prompts and 2D poses in latent space enables the fine control capabilities of FineControlNet. We evaluate the performance of FineControlNet with rigorous comparison against state-of-the-art pose-conditioned text-to-image diffusion models. FineControlNet achieves superior performance in generating high quality images that follow instance-specific controls. We will release the code and the dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Choi_FineControlNet_Fine-Level_Text_Control_for_Image_Generation_with_Spatially_Aligned_WACV_2025_paper.html	Hongsuk Choi, Isaac Kasahara, Selim Engin, Moritz A. Graule, Nikhil Chavan-Dafle, Volkan Isler
FitDiff: Robust Monocular 3D Facial Shape and Reflectance Estimation using Diffusion Models	"The remarkable progress in 3D face reconstruction has resulted in high-detail and photorealistic facial representations. Recently Diffusion Models have revolutionized the capabilities of generative methods by surpassing the performance of GANs. In this work we present FitDiff a diffusion-based 3D facial avatar generative model. Leveraging diffusion principles our model accurately generates relightable facial avatars utilizing an identity embedding extracted from an ""in-the-wild"" 2D facial image. The introduced multi-modal diffusion model concurrently outputs facial reflectance maps (diffuse and specular albedo and normals) and shapes showcasing great generalization capabilities. It is solely trained on an annotated subset of a public facial dataset paired with 3D reconstructions. We revisit the typical 3D facial fitting approach by guiding a reverse diffusion process using perceptual and face recognition losses. Being the first LDM conditioned on face recognition embeddings FitDiff reconstructs relightable human avatars that can be used as-is in common rendering engines starting only from an unconstrained facial image and achieving state-of-the-art performance."	https://openaccess.thecvf.com//content/WACV2025/html/Galanakis_FitDiff_Robust_Monocular_3D_Facial_Shape_and_Reflectance_Estimation_using_WACV_2025_paper.html	Stathis Galanakis, Alexandros Lattas, Stylianos Moschoglou, Stefanos Zafeiriou
FlashMix: Fast Map-Free LiDAR Localization via Feature Mixing and Contrastive-Constrained Accelerated Training	Map-free LiDAR localization systems accurately localize within known environments by predicting sensor position and orientation directly from raw point clouds eliminating the need for large maps and descriptors. However their long training times hinder rapid adaptation to new environments. To address this we propose FlashMix which uses a frozen scene-agnostic backbone to extract local point descriptors aggregated with an MLP mixer to predict sensor pose. A buffer of local descriptors is used to accelerate training by orders of magnitude combined with metric learning or contrastive loss regularization of aggregated descriptors to improve performance and convergence. We evaluate FlashMix on various LiDAR localization benchmarks examining different regularizations and aggregators and demonstrating its effectiveness for rapid and accurate LiDAR localization in real-world scenarios. The code is available at https://github.com/raktimgg/FlashMix.	https://openaccess.thecvf.com//content/WACV2025/html/Goswami_FlashMix_Fast_Map-Free_LiDAR_Localization_via_Feature_Mixing_and_Contrastive-Constrained_WACV_2025_paper.html	Raktim Gautam Goswami, Naman Patel, Prashanth Krishnamurthy, Farshad Khorrami
FlashVTG: Feature Layering and Adaptive Score Handling Network for Video Temporal Grounding	Text-guided Video Temporal Grounding (VTG) aims to localize relevant segments in untrimmed videos based on textual descriptions encompassing two subtasks: Moment Retrieval (MR) and Highlight Detection (HD). Although previous typical methods have achieved commendable results it is still challenging to retrieve short video moments. This is primarily due to the reliance on sparse and limited decoder queries which significantly constrain the accuracy of predictions. Furthermore suboptimal outcomes often arise because previous methods rank predictions based on isolated predictions neglecting the broader video context. To tackle these issues we introduce FlashVTG a framework featuring a Temporal Feature Layering (TFL) module and an Adaptive Score Refinement (ASR) module. The TFL module replaces the traditional decoder structure to capture nuanced video content variations across multiple temporal scales while the ASR module improves prediction ranking by integrating context from adjacent moments and multi-temporal-scale features. Extensive experiments demonstrate that FlashVTG achieves state-of-the-art performance on four widely adopted datasets in both MR and HD. Specifically on the QVHighlights dataset it boosts mAP by 5.8% for MR and 3.3% for HD. For short-moment retrieval FlashVTG increases mAP to 125% of previous SOTA performance. All these improvements are made without adding training burdens underscoring its effectiveness.	https://openaccess.thecvf.com//content/WACV2025/html/Cao_FlashVTG_Feature_Layering_and_Adaptive_Score_Handling_Network_for_Video_WACV_2025_paper.html	Zhuo Cao, Bingqing Zhang, Heming Du, Xin Yu, Xue Li, Sen Wang
Flatness Improves Backbone Generalisation in Few-Shot Classification	Deployment of deep neural networks in real-world settings typically requires adaptation to new tasks with few examples. Few-shot classification (FSC) provides a solution to this problem by leveraging pre-trained backbones for fast adaptation to new classes. However approaches for multi-domain FSC typically result in complex pipelines aimed at information fusion and task-specific adaptation without consideration of the importance of backbone training. In this work we introduce an effective strategy for backbone training and selection in multi-domain FSC by utilizing flatness-aware training and fine-tuning. Our work is theoretically grounded and empirically performs on par or better than state-of-the-art methods despite being simpler. Further our results indicate that backbone training is crucial for good generalisation in FSC across different adaptation methods.	https://openaccess.thecvf.com//content/WACV2025/html/Li_Flatness_Improves_Backbone_Generalisation_in_Few-Shot_Classification_WACV_2025_paper.html	Rui Li, Martin Trapp, Marcus Klasson, Arno Solin
Flowering Time Prediction of Wheat from DIA-MS Data	Traditional methods utilising data-independent acquisition mass spectrometry (DIA-MS) data for predictions depend on database searches against predefined spectral libraries for characterisation and quantification of the proteomes limiting scalability and adaptability across various applications. However directly applying existing networks on DIA-MS data represented as images for end-to-end predictions struggles to mine a predictive pattern due to non-uniform region importance across the image and divergences exhibited among different regions of the image. To overcome these limitations we propose a new framework with two modules: i) a dynamic sampling module that identifies regions of interest from the DIA-MS image constraining the network to focus on the most informative regions of the image only; ii) a mixture of experts module that sparsely routes the regions of interest to related expert networks facilitating adaptive computation of region features. The region features are then fused for predictions. Experimentally to benchmark our method we collected a large DIA-MS dataset of wheat for flowering time prediction and our approach significantly outperforms previous end-to-end methods i. e. 0.171 R2 improvements.	https://openaccess.thecvf.com//content/WACV2025/html/Yang_Flowering_Time_Prediction_of_Wheat_from_DIA-MS_Data_WACV_2025_paper.html	Yan Yang, Utpal Bose, James Broadbent, Sally Stockwell, Keren A Byrne, Md Zakir Hossain, Eric A Stone, Shannon Dillon
FluoNeRF: Fluorescent Novel-View Synthesis under Novel Light Source Colors	Synthesizing photo-realistic images of a scene from arbitrary viewpoints and under arbitrary lighting environments is one of the important research topics in computer vision and graphics. In this paper we propose a method for synthesizing photo-realistic images of a scene with fluorescent objects from novel viewpoints and under novel lighting colors. In general fluorescent materials absorb light with certain wavelengths and then emit light with longer wavelengths than the absorbed ones in contrast to reflective materials which preserve wavelengths of light. Therefore we cannot reproduce the colors of fluorescent objects under arbitrary lighting colors by combining conventional view synthesis techniques with the white balance adjustment of the RGB channels. Accordingly we extend the novel view synthesis based on the neural radiance fields by incorporating the superposition principle of light; our proposed method captures a sparse set of images of a scene from varying viewpoints and under varying light source colors by using a display-camera system and then synthesize photo-realistic images of the scene without explicitly modeling the geometric and photometric models of the scene. We conduct a number of experiments using real images and confirm the effectiveness of our method.	https://openaccess.thecvf.com//content/WACV2025/html/Shi_FluoNeRF_Fluorescent_Novel-View_Synthesis_under_Novel_Light_Source_Colors_WACV_2025_paper.html	Lin Shi, Kengo Matsufuji, Ryo Kawahara, Takahiro Okabe
Focusing on What to Decode and What to Train: SOV Decoding with Specific Target Guided DeNoising and Vision Language Advisor	Recent transformer-based methods achieve notable gains in the Human-object Interaction Detection (HOID) task by leveraging the detection of DETR and the prior knowledge of Vision-Language Model (VLM). However these methods suffer from extended training times and complex optimization due to the entanglement of object detection and HOI recognition during the decoding process. Especially the query embeddings used to predict both labels and boxes suffer from ambiguous representations and the gap between the prediction of HOI labels and verb labels is not considered. To address these challenges we introduce SOV-STG-VLA with three key components: Subject-Object-Verb (SOV) decoding Specific Target Guided (STG) denoising and a Vision-Language Advisor (VLA). Our SOV decoders disentangle object detection and verb recognition with a novel interaction region representation. The STG denoising strategy learns label embeddings with ground-truth information to guide the training and inference. Our SOV-STG achieves a fast convergence speed and high accuracy and builds a foundation for the VLA to incorporate the prior knowledge of the VLM. We introduce a vision advisor decoder to fuse both the interaction region information and the VLM's vision knowledge and a Verb-HOI prediction bridge to promote interaction representation learning. Our VLA notably improves our SOV-STG and achieves SOTA performance with one-sixth of training epochs compared to recent SOTA. Code and models are available at https://github.com/cjw2021/SOV-STG-VLA.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_Focusing_on_What_to_Decode_and_What_to_Train_SOV_WACV_2025_paper.html	Junwen Chen, Yingcheng Wang, Keiji Yanai
Forensic Iris Image-Based Post-Mortem Interval Estimation	Post-mortem iris recognition is an emerging application of iris-based human identification in a forensic setup. One factor that may be useful in conditioning iris recognition methods is the tissue decomposition level which is correlated with the post-mortem interval (PMI) i.e. the number of hours that have elapsed since death. PMI however is not always available and its precise estimation remains one of the core challenges in forensic examination. This paper presents the first known to us method of the PMI estimation directly from iris images captured after death. To assess the feasibility of the iris-based PMI estimation we designed models predicting the PMI from (a) near-infrared (NIR) (b) visible (RGB) and (c) multispectral (RGB+NIR) forensic iris images. Models were evaluated following a 10-fold cross-validation in (S1) sample-disjoint (S2) subject-disjoint and (S3) cross-dataset scenarios. We explore two data balancing techniques for S3: resampling-based balancing (S3-real) and synthetic data-supplemented balancing (S3-synthetic). We found that using the multispectral data offers a spectacularly low mean absolute error (MAE) of = 3.5 hours in the scenario (S1) a bit worse MAE = 17.5 hours in the scenario (S2) and MAE = 45.77 hours in the scenario (S3). Additionally supplementing the training set with synthetically-generated forensic iris images (S3-synthetic) significantly enhances the models' ability to generalize to new NIR RGB and multispectral data collected in a different lab. This suggests that if the environmental conditions are favorable (e.g. bodies are kept in low temperatures) forensic iris images provide features that are indicative of the PMI and can be automatically estimated.	https://openaccess.thecvf.com//content/WACV2025/html/Bhuiyan_Forensic_Iris_Image-Based_Post-Mortem_Interval_Estimation_WACV_2025_paper.html	Rasel Ahmed Bhuiyan, Adam Czajka
Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering	This paper tackles the intricate challenge of video question-answering (VideoQA). Despite notable progress current methods fall short of effectively integrating questions with video frames and semantic object-level abstractions to create question-aware video representations. We introduce Local - Global Question Aware Video Embedding (LGQAVE) which incorporates three major innovations to integrate multi-modal knowledge better and emphasize semantic visual concepts relevant to specific questions. LGQAVE moves beyond traditional ad-hoc frame sampling by utilizing a cross-attention mechanism that precisely identifies the most relevant frames concerning the questions. It captures the dynamics of objects within these frames using distinct graphs grounding them in question semantics with the miniGPT model. These graphs are processed by a question-aware dynamic graph transformer (Q-DGT) which refines the outputs to develop nuanced global and local video representations. An additional cross-attention module integrates these local and global embeddings to generate the final video embeddings which a language model uses to generate answers. Extensive evaluations across multiple benchmarks demonstrate that LGQAVE significantly outperforms existing models in delivering accurate multi-choice and open-ended answers.	https://openaccess.thecvf.com//content/WACV2025/html/Rongali_Foundation_Models_and_Adaptive_Feature_Selection_A_Synergistic_Approach_to_WACV_2025_paper.html	Sai Bhargav Rongali, Mohamad Hassan N C, Ankit Jha, Neha Bhargava, Saurabh Prasad, Biplab Banerjee
Foundation X: Integrating Classification Localization and Segmentation through Lock-Release Pretraining Strategy for Chest X-ray Analysis	Developing robust and versatile deep-learning models is essential for enhancing diagnostic accuracy and guiding clinical interventions in medical imaging but it requires a large amount of annotated data. The advancement of deep learning has facilitated the creation of numerous medical datasets with diverse expert-level annotations. Aggregating these datasets can maximize data utilization and address the inadequacy of labeled data. However the heterogeneity of expert-level annotations across tasks such as classification localization and segmentation presents a significant challenge for learning from these datasets. To this end we introduce Foundation X an end-to-end framework that utilizes diverse expert-level annotations from numerous public datasets to train a foundation model capable of multiple tasks including classification localization and segmentation. To address the challenges of annotation and task heterogeneity we propose a Lock-Release pretraining strategy to enhance the cyclic learning from multiple datasets combined with the student-teacher learning paradigm ensuring the model retains general knowledge for all tasks while preventing overfitting to any single task. To demonstrate the effectiveness of Foundation X we trained a model using 11 chest X-ray datasets covering annotations for classification localization and segmentation tasks. Our experimental results show that Foundation X achieves notable performance gains through extensive annotation utilization excels in cross-dataset and cross-task learning and further enhances performance in organ localization and segmentation tasks. All code and pretrained models are publicly accessible at GitHub.com/JLiangLab/Foundation_X.	https://openaccess.thecvf.com//content/WACV2025/html/Islam_Foundation_X_Integrating_Classification_Localization_and_Segmentation_through_Lock-Release_Pretraining_WACV_2025_paper.html	Nahid Ul Islam, DongAo Ma, Jiaxuan Pang, Shivasakthi Senthil Velan, Michael Gotway, Jianming Liang
Frame by Familiar Frame: Understanding Replication in Video Diffusion Models	Building on the momentum of image generation diffusion models there is an increasing interest in video-based diffusion models. However video generation poses greater challenges due to its higher-dimensional nature the scarcity of training data and the complex spatiotemporal relationships involved. Image generation models due to their extensive data requirements have already strained computational resources to their limits. There have been instances of these models reproducing elements from the training samples leading to concerns and even legal disputes over sample replication. Video diffusion models which operate with even more constrained datasets and are tasked with generating both spatial and temporal content may be more prone to replicating samples from their training sets. Compounding the issue these models are often evaluated using metrics that inadvertently reward replication. In our paper we present a systematic investigation into the phenomenon of sample replication in video diffusion models. We scrutinize various recent diffusion models for video synthesis assessing their tendency to replicate spatial and temporal content in both unconditional and conditional generation scenarios. Our study identifies strategies that are less likely to lead to replication. Furthermore we propose new evaluation strategies that take replication into account offering a more accurate measure of a model's ability to generate the original content.	https://openaccess.thecvf.com//content/WACV2025/html/Rahman_Frame_by_Familiar_Frame_Understanding_Replication_in_Video_Diffusion_Models_WACV_2025_paper.html	Aimon Rahman, Malsha V. Perera, Vishal M. Patel
Frequency-Domain Refinement of Vision Transformers for Robust Medical Image Segmentation under Degradation	Medical image segmentation is crucial for precise diagnosis treatment planning and disease monitoring in clinical settings. While convolutional neural networks (CNNs) have achieved remarkable success they struggle with modeling long-range dependencies. Vision Transformers (ViTs) address this limitation by leveraging self-attention mechanisms to capture global contextual information. However ViTs often fall short in local feature description which is crucial for precise segmentation. To address this issue we reformulate self-attention in the frequency domain to enhance both local and global feature representation. Our approach the Enhanced Wave Vision Transformer (EW-ViT) incorporates wavelet decomposition within the self-attention block to adaptively refine feature representation in low and high-frequency components. We also introduce the Prompt-Guided High-Frequency Refiner (PGHFR) module to handle image degradation which mainly affects high-frequency components. This module uses implicit prompts to encode degradation-specific information and adjust high-frequency representations accordingly. Additionally we apply a contrastive learning strategy to maintain feature consistency and ensure robustness against noise leading to state-of-the-art (SOTA) performance in medical image segmentation especially under various conditions of degradation. Source code is available at GitHub.	https://openaccess.thecvf.com//content/WACV2025/html/Karimijafarbigloo_Frequency-Domain_Refinement_of_Vision_Transformers_for_Robust_Medical_Image_Segmentation_WACV_2025_paper.html	Sanaz Karimijafarbigloo, Sina Ghorbani Kolahi, Reza Azad, Ulas Bagci, Dorit Merhof
From Visual Explanations to Counterfactual Explanations with Latent Diffusion	"Visual counterfactual explanations are ideal hypothetical images that change the decision-making of the classifier with high confidence toward the desired class while remaining visually plausible and close to the initial image. In this paper we propose a new approach to tackle two key challenges in recent prominent works: i) determining which specific counterfactual features are crucial for distinguishing the ""concept"" of the target class from the original class and ii) supplying valuable explanations for the non-robust classifier without relying on the support of an adversarially robust model. Our method identifies the essential region for modification through algorithms that provide visual explanations and then our framework generates realistic counterfactual explanations by combining adversarial attacks based on pruning the adversarial gradient of the target classifier and the latent diffusion model. The proposed method outperforms previous state-of-the-art results on various evaluation criteria on ImageNet and CelebA-HQ datasets. In general our method can be applied to arbitrary classifiers highlight the strong association between visual and counterfactual explanations make semantically meaningful changes from the target classifier and provide observers with subtle counterfactual images."	https://openaccess.thecvf.com//content/WACV2025/html/Luu_From_Visual_Explanations_to_Counterfactual_Explanations_with_Latent_Diffusion_WACV_2025_paper.html	Tung Luu, Nam Le, Duc Le, Bac Le
GANESH: Generalizable NeRF for Lensless Imaging	Lensless imaging offers a significant opportunity to develop ultra-compact cameras by removing the conventional bulky lens system. However without a focusing element the sensor's output is no longer a direct image but a complex multiplexed scene representation. Traditional methods have attempted to address this challenge by employing learnable inversions and refinement models but these methods are primarily designed for 2D reconstruction and do not generalize well to 3D reconstruction. We introduce GANESH a novel framework designed to enable simultaneous refinement and novel view synthesis from multi-view lensless images. Unlike existing methods that require scene-specific training our approach supports on-the-fly inference without retraining on each scene. Moreover our framework allows us to tune our model to specific scenes enhancing the rendering and refinement quality. To facilitate research in this area we also present the first multi-view lens- less dataset LenslessScenes. Extensive experiments demonstrate that our method outperforms current approaches in reconstruction accuracy and refinement quality. Code and video results are available here.	https://openaccess.thecvf.com//content/WACV2025/html/Madhavan_GANESH_Generalizable_NeRF_for_Lensless_Imaging_WACV_2025_paper.html	Rakesh Raj Madhavan, Akshat Kaimal, Badhrinarayanan K.V, Vinayak Gupta, Rohit Choudhary, Chandrakala Shanmuganathan, Kaushik Mitra
GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space	We train a feed-forward text-to-3D diffusion generator for human characters using only single-view 2D data for supervision. Existing 3D generative models cannot yet match the fidelity of image and/or video generative models. State-of-the-art 3D generators are either limited by the volume and diversity of existing 3D data available for supervision or those that can be trained with only 2D data as supervision produce coarser results cannot be text-conditioned and/or must revert to test-time optimization. We observe that GAN- and diffusion-based generators have complementary qualities: GANs can be trained efficiently with 2D supervision to produce high-quality 3D objects but are hard to condition on text. In contrast diffusion models can be conditioned efficiently but tend to be hard to train with only 2D supervision. We introduce GANFusion that starts by generating unconditional triplane features for 3D data using a GAN architecture trained with only single-view 2D data. We then generate random samples from the GAN caption them and train a text-conditioned diffusion model that directly learns to sample from the space of good triplane features that can be decoded into 3D objects. We evaluate the proposed method in the context of text-conditioned full-body human generation and show improvements over possible alternatives.	https://openaccess.thecvf.com//content/WACV2025/html/Attaiki_GANFusion_Feed-Forward_Text-to-3D_with_Diffusion_in_GAN_Space_WACV_2025_paper.html	Souhaib Attaiki, Paul Guerrero, Duygu Ceylan, Niloy Mitra, Maks Ovsjanikov
GAUDA: Generative Adaptive Uncertainty-Guided Diffusion-Based Augmentation for Surgical Segmentation	Augmentation by generative modelling yields a promising alternative to the accumulation of surgical data where ethical organisational and regulatory aspects must be considered. Yet the joint synthesis of (image mask) pairs for segmentation a major application in surgery is rather unexplored. We propose to learn semantically comprehensive yet compact latent representations of the (image mask) space which we jointly model with a Latent Diffusion Model. We show that our approach can effectively synthesise unseen high-quality paired segmentation data of remarkable semantic coherence. Generative augmentation is typically applied pre-training by synthesising a fixed number of additional training samples to improve downstream task models. To enhance this approach we further propose Generative Adaptive Uncertainty-guided Diffusion-based Augmentation (GAUDA) leveraging the epistemic uncertainty of a Bayesian downstream model for targeted online synthesis. We condition the generative model on classes with high estimated uncertainty during training to produce additional unseen samples for these classes. By adaptively utilising the generative model online we can minimise the number of additional training samples and centre them around the currently most uncertain parts of the data distribution. GAUDA effectively improves downstream segmentation results over comparable methods by an average absolute IoU of 1.6% on CaDISv2 and 1.5% on CholecSeg8k two prominent surgical datasets for semantic segmentation.	https://openaccess.thecvf.com//content/WACV2025/html/Frisch_GAUDA_Generative_Adaptive_Uncertainty-Guided_Diffusion-Based_Augmentation_for_Surgical_Segmentation_WACV_2025_paper.html	Yannik Frisch, Christina Bornberg, Moritz Fuchs, Anirban Mukhopadhyay
GET-UP: GEomeTric-Aware Depth Estimation with Radar Points UPsampling	Depth estimation plays a pivotal role in autonomous driving facilitating a comprehensive understanding of the vehicle's 3D surroundings. Radar with its robustness to adverse weather conditions and capability to measure distances has drawn significant interest for radar-camera depth estimation. However existing algorithms process the inherently noisy and sparse radar data by projecting 3D points onto the image plane for pixel-level feature extraction overlooking the valuable geometric information contained within the radar point cloud. To address this gap we propose GET-UP leveraging attention-enhanced Graph Neural Networks (GNN) to exchange and aggregate both 2D and 3D information from radar data. This approach effectively enriches the feature representation by incorporating spatial relationships compared to traditional methods that rely only on 2D feature extraction. Furthermore we incorporate a point cloud upsampling task to densify the radar point cloud rectify point positions and derive additional 3D features under the guidance of lidar data. Finally we fuse radar and camera features during the decoding phase for depth estimation. We benchmark our proposed GET-UP on the nuScenes dataset achieving state-of-the-art performance with a 15.3% and 14.7% improvement in MAE and RMSE over the previously best-performing model. Code: https://github.com/ harborsarah/GET-UP	https://openaccess.thecvf.com//content/WACV2025/html/Sun_GET-UP_GEomeTric-Aware_Depth_Estimation_with_Radar_Points_UPsampling_WACV_2025_paper.html	Huawei Sun, Zixu Wang, Hao Feng, Julius Ott, Lorenzo Servadei, Robert Wille
GEXIA: Granularity Expansion and Iterative Approximation for Scalable Multi-Grained Video-Language Learning	In various video-language learning tasks the challenge of achieving cross-modality alignment with multi-grained data persists. We propose a method to tackle this challenge from two crucial perspectives: data and modeling. Given the absence of a multi-grained video-text pretraining dataset we introduce a Granularity EXpansion (GEX) method with Integration and Compression operations to expand the granularity of a single-grained dataset. To better model multi-grained data we introduce an Iterative Approximation Module (IAM) which embeds multi-grained videos and texts into a unified low-dimensional semantic space while preserving essential information for cross-modal alignment. Furthermore GEXIA is highly scalable with no restrictions on the number of video-text granularities for alignment. We evaluate our work on three categories of video tasks across seven benchmark datasets showcasing state-of-the-art or comparable performance. Remarkably our model excels in tasks involving long-form video understanding even though the pretraining dataset only contains short video clips.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_GEXIA_Granularity_Expansion_and_Iterative_Approximation_for_Scalable_Multi-Grained_Video-Language_WACV_2025_paper.html	Yicheng Wang, Zhikang Zhang, Jue Wang, David Fan, Zhenlin Xu, Linda Liu, Xiang Hao, Vimal Bhat, Xinyu Li
GHOST: Grounded Human Motion Generation with Open Vocabulary Scene-and-Text Contexts	The connection between our 3D surroundings and the descriptive language that characterizes them would be well-suited for localizing and generating human motion in context but for one problem. The complexity introduced by multiple modalities makes capturing this connection challenging with a fixed set of descriptors. Specifically closed vocabulary scene encoders which require learning text-scene associations from scratch have been favored in the literature often resulting in inaccurate motion grounding. In this paper we propose a method that integrates an open vocabulary scene encoder into the architecture establishing a robust connection between text and scene. Our two-step approach starts with pretraining the scene encoder through knowledge distillation from an existing open vocabulary semantic image segmentation model ensuring a shared text-scene feature space. Subsequently the scene encoder is fine-tuned for conditional motion generation incorporating two novel regularization losses that regress the category and size of the goal object. Our methodology achieves up to a 30% reduction in the goal object distance metric compared to the prior state-of-the-art baseline model on the HUMANISE dataset. This improvement is demonstrated through evaluations conducted using three implementations of our framework a perceptual study and an open vocabulary experiment. Additionally our method is designed to accommodate future 2D open vocabulary segmentation methods for distillation in a plug-and-play manner.	https://openaccess.thecvf.com//content/WACV2025/html/Milacski_GHOST_Grounded_Human_Motion_Generation_with_Open_Vocabulary_Scene-and-Text_Contexts_WACV_2025_paper.html	ZoltÃ¡n Ã. Milacski, Koichiro Niinuma, Ryosuke Kawamura, Fernando de la Torre, LÃ¡szlÃ³ A. Jeni
GMT: Guided Mask Transformer for Leaf Instance Segmentation	Leaf instance segmentation is a challenging multi-instance segmentation task aiming to separate and delineate each leaf in an image of a plant. Accurate segmentation of each leaf is crucial for plant-related applications such as the fine-grained monitoring of plant growth and crop yield estimation. This task is challenging because of the high similarity (in shape and colour) great size variation and heavy occlusions among leaf instances. Furthermore the typically small size of annotated leaf datasets makes it more difficult to learn the distinctive features needed for precise segmentation. We hypothesise that the key to overcoming the these challenges lies in the specific spatial patterns of leaf distribution. In this paper we propose the Guided Mask Transformer (GMT) which leverages and integrates leaf spatial distribution priors into a Transformer-based segmentor. These spatial priors are embedded in a set of guide functions that map leaves at different positions into a more separable embedding space. Our GMT consistently outperforms the state-of-the-art on three public plant datasets. Our code is available at https://github.com/vios-s/gmt-leaf-ins-seg.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_GMT_Guided_Mask_Transformer_for_Leaf_Instance_Segmentation_WACV_2025_paper.html	Feng Chen, Sotirios A. Tsaftaris, Mario Valerio Giuffrida
GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled Appearance and Geometry Modeling	Gaussian splatting has demonstrated excellent performance for view synthesis and scene reconstruction. The representation achieves photorealistic quality by optimizing the position scale color and opacity of thousands to millions of 2D or 3D Gaussian primitives within a scene. However since each Gaussian primitive encodes both appearance and geometry these attributes are strongly coupled - thus high-fidelity appearance modeling requires a large number of Gaussian primitives even when the scene geometry is simple (e.g. for a textured planar surface). We propose to texture each 2D Gaussian primitive so that even a single Gaussian can be used to capture appearance details. By employing per-primitive texturing our appearance representation is agnostic to the topology and complexity of the scene's geometry. We show that our approach GStex yields improved visual quality over prior work in texturing Gaussian splats. Furthermore we demonstrate that our decoupling enables improved novel view synthesis performance compared to 2D Gaussian splatting when reducing the number of Gaussian primitives and that GStex can be used for scene appearance editing and re-texturing.	https://openaccess.thecvf.com//content/WACV2025/html/Rong_GStex_Per-Primitive_Texturing_of_2D_Gaussian_Splatting_for_Decoupled_Appearance_WACV_2025_paper.html	Victor Rong, Jingxiang Chen, Sherwin Bahmani, Kiriakos Kutulakos, David Lindell
GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction	High Dynamic Range (HDR) content (i.e. images and videos) has a broad range of applications. However capturing HDR content from real-world scenes is expensive and time-consuming. Therefore the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community. A major challenge is the lack of datasets which capture diverse scene conditions (e.g. lighting weather locations) and various image features (e.g. color contrast saturation). To address this gap we introduce GTA-HDR a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game. We perform thorough evaluation of the proposed dataset which enables significant qualitative and quantitative improvements of the state-of-the-art HDR image reconstruction methods. Furthermore we demonstrate the effectiveness of the proposed dataset and its impact on additional computer vision tasks including 3D human pose estimation human body part segmentation and holistic scene segmentation. The dataset data collection pipeline and evaluation code are available at: https://github.com/HrishavBakulBarua/GTA-HDR.	https://openaccess.thecvf.com//content/WACV2025/html/Barua_GTA-HDR_A_Large-Scale_Synthetic_Dataset_for_HDR_Image_Reconstruction_WACV_2025_paper.html	Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
GaitCloud: Leveraging Spatial-Temporal Information for LiDAR-Base Gait Recognition with A True-3D Gait Representation	Gait recognition using point clouds captured by LiDAR (Light Detection And Ranging) sensors offers better adaptability to variations in walking conditions compared to camera-based methods due to the precise spatial information captured. However existing methods typically project the point clouds into a sequence of 2D depth images extended along the time dimension and adopt gait recognition networks optimized for camera-based approaches. This planar projection compromises the integrity of the 3D coordinates (length width and depth) and results in severe silhouette deformations with varied observation viewpoints similar to the camera-based methods. To better utilize the spatial information in gait point clouds we propose a true 3D gait representation using eff icient point cloud voxelization termed GaitCloud. Additionally we explore the unique nature of LiDAR-captured point clouds and present two improved modules adapted to our method called Layer Encoder (LE) and Horizontal Convolutional Pooling (HCP). Evaluation results using the open-access gait dataset SUSTech1K show that our method outperforms the state-of-the-art achieving recognition accuracies of 93.1% and 89.2% in cross-view and variance experiments respectively. These results demonstrate that 3D gait representation based on point cloud voxelization more effectively utilizes spatial information than depth images offering new possibilities for high-performance LiDAR-based gait recognition. The source code is available at https://github.com/seagrgz/GaitCloud-master.git.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_GaitCloud_Leveraging_Spatial-Temporal_Information_for_LiDAR-Base_Gait_Recognition_with_A_WACV_2025_paper.html	Shaoxiong Zhang, Hiromitsu Awano, Takashi Sato
GaitContour: Efficient Gait Recognition Based on a Contour-Pose Representation	Gait recognition holds the promise to robustly identify subjects based on walking patterns instead of appearance information. In recent years this field has been dominated by learning methods based on two input formats: silhouette images and sparse keypoints. Compared to image-based approaches keypoint-based methods can achieve significantly higher efficiency due to their sparsity. However sparsity also results in information loss thereby reducing performance. In this work we propose a novel keypoint-based Contour-Pose representation which compactly encodes both body shape and parts information. We further propose a local-to-global architecture called GaitContour to leverage this novel representation and efficiently compute subject embedding in two stages. The first stage consists of a local transformer that extracts features from five different body regions. The second stage then aggregates the regional features to estimate a global human gait representation. Such a design significantly reduces the complexity of the attention operation and improves both efficiency and performance. Through large scale experiments GaitContour is shown to perform significantly better than previous keypoint-based methods. Furthermore the Contour-Pose representation also achieves new SoTA performances on fusion-based gait recognition methods.	https://openaccess.thecvf.com//content/WACV2025/html/Guo_GaitContour_Efficient_Gait_Recognition_Based_on_a_Contour-Pose_Representation_WACV_2025_paper.html	Yuxiang Guo, Anshul Shah, Jiang Liu, Ayush Gupta, Rama Chellappa, Cheng Peng
GauFRe: Gaussian Deformation Fields for Real-Time Dynamic Novel View Synthesis	We propose a method that achieves state-of-the-art rendering quality and efficiency on monocular dynamic scene reconstruction using deformable 3D Gaussians. Implicit deformable representations commonly model motion with a canonical space and time-dependent backward-warping deformation field. Our method GauFRe uses a forward-warping deformation to explicitly model non-rigid transformations of scene geometry. Specifically we propose a template set of 3D Gaussians residing in a canonical space and a time-dependent forward-warping deformation field to model dynamic objects. Additionally we tailor a 3D Gaussian-specific static component supported by an inductive bias-aware initialization approach which allows the deformation field to focus on moving scene regions improving the rendering of complex real-world motion. The differentiable pipeline is optimized end-to-end with a self-supervised rendering loss. Experiments show our method achieves competitive results and higher efficiency than both previous state-of-the-art NeRF and Gaussian-based methods. For real-world scenes GauFRe can train in 20 mins and offer 96 FPS real-time rendering on an RTX 3090 GPU.	https://openaccess.thecvf.com//content/WACV2025/html/Liang_GauFRe_Gaussian_Deformation_Fields_for_Real-Time_Dynamic_Novel_View_Synthesis_WACV_2025_paper.html	Yiqing Liang, Numair Khan, Zhengqin Li, Thu H Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao
Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities	"Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant potential for modeling 3D head avatars providing greater flexibility than mesh-based methods and more efficient rendering compared to NeRF-based approaches. Despite these advancements the creation of controllable 3DGS-based head avatars remains time-intensive often requiring tens of minutes to hours. To expedite this process we here introduce the ""Gaussian Deja-vu"" framework which first obtains a generalized model of the head avatar and then personalizes the result. The generalized model is trained on large 2D (synthetic and real) image datasets. This model provides a well-initialized 3D Gaussian head that is further refined using a monocular video to achieve the personalized head avatar. For personalizing we propose learnable expression-aware rectification blendmaps to correct the initial 3D Gaussians ensuring rapid convergence without the reliance on neural networks. Experiments demonstrate that the proposed method meets its objectives. It outperforms state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as well as reduces training time consumption to at least a quarter of the existing methods producing the avatar in minutes. Project homepage: https://peizhiyan.github.io/docs/dejavu"	https://openaccess.thecvf.com//content/WACV2025/html/Yan_Gaussian_Deja-vu_Creating_Controllable_3D_Gaussian_Head-Avatars_with_Enhanced_Generalization_WACV_2025_paper.html	Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du
GaussianBeV : 3D Gaussian Representation Meets Perception Models for BeV Segmentation	The Bird's-eye View (BeV) representation is widely used for 3D perception from multi-view camera images. It allows to merge features from different cameras into a common space providing a unified representation of the 3D scene. The key component is the view transformer which transforms image views into the BeV. However actual view transformer methods based on geometry or cross-attention do not provide a sufficiently detailed representation of the scene as they use a sub-sampling of the 3D space that is non-optimal for modeling the fine structures of the environment. In this paper we propose GaussianBeV a novel method for transforming image features to BeV by finely representing the scene using a set of 3D gaussians located and oriented in 3D space. This representation is then splattered to produce the BeV feature map by adapting recent advances in 3D representation rendering based on gaussian splatting. GaussianBeV is the first approach to use this 3D gaussian modeling and 3D scene rendering process in an optimization free manner i.e. without optimizing it on a specific scene and directly integrated into a single stage model for BeV scene understanding. Experiments show that the proposed representation is highly effective and place GaussianBeV as the new state-of-the-art on the BeV semantic segmentation task on the nuScenes dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Chabot_GaussianBeV__3D_Gaussian_Representation_Meets_Perception_Models_for_BeV_WACV_2025_paper.html	Florian Chabot, Nicolas Granger, Guillaume Lapouge
GazeSearch: Radiology Findings Search Benchmark	Medical eye-tracking data is an important information source for understanding how radiologists visually interpret medical images. This information not only improves the accuracy of deep learning models for X-ray analysis but also their interpretability enhancing transparency in decision-making. However the current eye-tracking data is dispersed unprocessed and ambiguous making it difficult to derive meaningful insights. Therefore there is a need to create a new dataset with more focus and purposeful eyetracking data improving its utility for diagnostic applications. In this work we propose a refinement method inspired by the target-present visual search challenge: there is a specific finding and fixations are guided to locate it. After refining the existing eye-tracking datasets we transform them into a curated visual search dataset called GazeSearch specifically for radiology findings where each fixation sequence is purposefully aligned to the task of locating a particular finding. Subsequently we introduce a scan path prediction baseline called ChestSearch specifically tailored to GazeSearch. Finally we employ the newly introduced GazeSearch as a benchmark to evaluate the performance of current state-of-the-art methods offering a comprehensive assessment for visual search in the medical imaging domain. Code is available at https://github.com/ UARK-AICV/GazeSearch.	https://openaccess.thecvf.com//content/WACV2025/html/Pham_GazeSearch_Radiology_Findings_Search_Benchmark_WACV_2025_paper.html	Trong Thang Pham, Tien-Phat Nguyen, Yuki Ikebe, Akash Awasthi, Zhigang Deng, Carol C. Wu, Hien Nguyen, Ngan Le
Generalist YOLO: Towards Real-Time End-to-End Multi-Task Visual Language Models	Generalist models capable of handling multiple modalities and tasks simultaneously are currently one of the hottest research topics. However due to interference between different tasks during the training process existing generalist models require a very large decoder to achieve good results in various tasks which makes real-time prediction difficult for current generalist models. This paper introduces Generalist YOLO which takes a significant step towards real-time prediction systems for visual language generalist models. The proposed Generalist YOLO uses a unified encoder to reduce conflicts between different tasks thereby decreasing the complexity required by the decoder. It also introduces a primary-secondary co-attention mechanism that allows different tasks to learn together more effectively achieving high efficiency and high accuracy. We propose a semantically consistent asymmetric training strategy allowing various tasks to benefit from performance improvements brought by the latest research results in various fields. The proposed Generalist YOLO achieves excellent results on various vision and language tasks based on MS COCO. While maintaining high accuracy across all tasks it is 135 times faster than existing generalist models. The source code is released on GitHub at https://github.com/WongKinYiu/GeneralistYOLO.	https://openaccess.thecvf.com//content/WACV2025/html/Chang_Generalist_YOLO_Towards_Real-Time_End-to-End_Multi-Task_Visual_Language_Models_WACV_2025_paper.html	Hung-Shuo Chang, Chien-Yao Wang, Richard Robert Wang, Gene Chou, Hong-Yuan Mark Liao
Generalizable Single-Source Cross-Modality Medical Image Segmentation via Invariant Causal Mechanisms	"Single-source domain generalization (SDG) aims to learn a model from a single source domain that can generalize well on unseen target domains. This is an important task in computer vision particularly relevant to medical imaging where domain shifts are common. In this work we consider a challenging yet practical setting: SDG for cross-modality medical image segmentation. We combine causality-inspired theoretical insights on learning domain-invariant representations with recent advancements in diffusion-based augmentation to improve generalization across diverse imaging modalities. Guided by the ""intervention-augmentation equivariant"" principle we use controlled diffusion models (DMs) to simulate diverse imaging styles while preserving the content leveraging rich generative priors in large-scale pretrained DMs to comprehensively perturb the multidimensional style variable. Extensive experiments on challenging cross-modality segmentation tasks demonstrate that our approach consistently outperforms state-of-the-art SDG methods across three distinct anatomies and imaging modalities. The source code is available at https://github.com/ratschlab/ICMSeg."	https://openaccess.thecvf.com//content/WACV2025/html/Chen_Generalizable_Single-Source_Cross-Modality_Medical_Image_Segmentation_via_Invariant_Causal_Mechanisms_WACV_2025_paper.html	Boqi Chen, Yuanzhi Zhu, Yunke Ao, Sebastiano Caprara, Reto Sutter, Gunnar RÃ¤tsch, Ender Konukoglu, Anna Susmelj
Generalizable Single-View Object Pose Estimation by Two-Side Generating and Matching	In this paper we present a novel generalizable object pose estimation method to determine the object pose using only one RGB image. Unlike traditional approaches that rely on instance-level object pose estimation and necessitate extensive training data our method offers generalization to unseen objects without extensive training operates with a single reference image of the object and eliminates the need for 3D object models or multiple views of the object. These characteristics are achieved by utilizing a diffusion model to generate novel-view images and conducting a two-sided matching on these generated images. Quantitative experiments demonstrate the superiority of our method over existing pose estimation techniques across both synthetic and real-world datasets. Remarkably our approach maintains strong performance even in scenarios with significant viewpoint changes highlighting its robustness and versatility in challenging conditions. The code will be released at https://github.com/scy639/Gen2SM	https://openaccess.thecvf.com//content/WACV2025/html/Sun_Generalizable_Single-View_Object_Pose_Estimation_by_Two-Side_Generating_and_Matching_WACV_2025_paper.html	Yujing Sun, Caiyi Sun, Yuan Liu, Yuexin Ma, Siu Ming Yiu
GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time Distribution Shifts	We consider the problem of test-time domain generalization where a model is trained on several source domains and adjusted on target domains never seen during training. Different from the common methods that fine-tune the model or adjust the classifier parameters online we propose to generate multiple layer parameters on the fly during inference by a lightweight meta-learned transformer which we call GeneralizeFormer. The layer-wise parameters are generated per target batch without fine-tuning or online adjustment. By doing so our method is more effective in dynamic scenarios with multiple target distributions and also avoids forgetting valuable source distribution characteristics. Moreover by considering layer-wise gradients the proposed method adapts itself to various distribution shifts. To reduce the computational and time cost we fix the convolutional parameters while only generating parameters of the Batch Normalization layers and the linear classifier. Experiments on six widely used domain generalization datasets demonstrate the benefits and abilities of the proposed method to efficiently handle various distribution shifts generalize in dynamic scenarios and avoid forgetting.	https://openaccess.thecvf.com//content/WACV2025/html/Ambekar_GeneralizeFormer_Layer-Adaptive_Model_Generation_across_Test-Time_Distribution_Shifts_WACV_2025_paper.html	Sameer Ambekar, Zehao Xiao, Xiantong Zhen, Cees Snoek
Generating Long-Take Videos via Effective Keyframes and Guidance	We tackle the challenge of generating long-take videos encompassing multiple non-repetitive yet coherent events. Existing approaches generate long videos conditioned on single input guidance often leading to repetitive content. To address this problem we develop a framework that uses multiple guidance sources to enhance long video generation. The main idea of our approach is to decouple video generation into keyframe generation and frame interpolation. In this process keyframe generation focuses on creating multiple coherent events while the frame interpolation stage generates smooth intermediate frames between keyframes using existing video generation models. A novel mask attention module is further introduced to improve coherence and efficiency. Experiments on challenging real-world videos demonstrate that the proposed method outperforms prior methods by up to 9.5% in objective metrics.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_Generating_Long-Take_Videos_via_Effective_Keyframes_and_Guidance_WACV_2025_paper.html	Hsin-Ping Huang, Yu-Chuan Su, Ming-Hsuan Yang
Generating Visual Explanations from Deep Networks using Implicit Neural Representations	Explaining deep learning models in a way that humans can easily understand is essential for responsible artificial intelligence applications. Attribution methods constitute an important area of explainable deep learning. The attribution problem involves finding parts of the network's input that are the most responsible for the model's output. In this work we demonstrate that implicit neural representations (INRs) constitute a good framework for generating visual explanations. Firstly we utilize coordinate-based implicit networks to reformulate and extend the extremal perturbations technique and generate attribution masks. Experimental results confirm the usefulness of our method. For instance by proper conditioning of the implicit network we obtain attribution masks that are well-behaved with respect to the imposed area constraints. Secondly we present an iterative INR-based method that can be used to generate multiple non-overlapping attribution masks for the same image. We depict that a deep learning model may associate the image label with both the appearance of the object of interest as well as with areas and textures usually accompanying the object. Our study demonstrates that implicit networks are well-suited for the generation of attribution masks and can provide interesting insights about the performance of deep learning models.	https://openaccess.thecvf.com//content/WACV2025/html/Byra_Generating_Visual_Explanations_from_Deep_Networks_using_Implicit_Neural_Representations_WACV_2025_paper.html	Michal Byra, Henrik Skibbe
Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models	In this paper we address the challenge of generating realistic 3D human motions for action classes that were never seen during the training phase. Our approach involves decomposing complex actions into simpler movements specifically those observed during training by leveraging the knowledge of human motion contained in GPTs models. These simpler movements are then combined into a single realistic animation using the properties of diffusion models. Our claim is that this decomposition and subsequent recombination of simple movements can synthesize an animation that accurately represents the complex input action. This method operates during the inference phase and can be integrated with any pre-trained diffusion model enabling the synthesis of motion classes not present in the training data. We evaluate our method by dividing two benchmark human motion datasets into basic and complex actions and then compare its performance against the state-of-the-art. Our code and models are publicly available at our https://github.com/divanoLetto/MotionCompositionDiffusion.	https://openaccess.thecvf.com//content/WACV2025/html/Mandelli_Generation_of_Complex_3D_Human_Motion_by_Temporal_and_Spatial_WACV_2025_paper.html	Lorenzo Mandelli, Stefano Berretti
Generative Model-Based Fusion for Improved Few-Shot Semantic Segmentation of Infrared Images	Infrared (IR) imaging is commonly used in various scenarios including autonomous driving fire safety and defense applications. Thus semantic segmentation of such images is of great interest. However this task faces several challenges including data scarcity differing contrast and input channel number compared to natural images and emergence of classes not represented in databases in certain scenarios such as defense applications. Few-shot segmentation (FSS) provides a framework to overcome these issues by segmenting query images using a few labeled support samples. However existing FSS models for IR images require paired visible RGB images which is a major limitation since acquiring such paired data is difficult or impossible in some applications. In this work we develop new strategies for FSS of IR images by using generative modeling and fusion techniques. To this end we propose to synthesize auxiliary data to provide additional channel information to complement the limited contrast in the IR images as well as IR data synthesis for data augmentation. Here the former helps the FSS model to better capture the relationship between the support and query sets while the latter addresses the issue of data scarcity. Finally to further improve the former aspect we propose a novel fusion ensemble module for integrating the two different modalities. Our methods are evaluated on different IR datasets and improve upon the state-of-the-art (SOTA) FSS models.	https://openaccess.thecvf.com//content/WACV2025/html/Yun_Generative_Model-Based_Fusion_for_Improved_Few-Shot_Semantic_Segmentation_of_Infrared_WACV_2025_paper.html	Junno Yun, Mehmet AkÃ§akaya
GeoDiffuser: Geometry-Based Image Editing with Diffusion Models	The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However these methods are bespoke imprecise require additional information or are limited to only 2D image edits. We present GeoDiffuser a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation 3D rotation and removal. We present quantitative results including a perceptual study that shows how our approach is better than existing methods. The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However these methods are bespoke imprecise require additional information or are limited to only 2D image edits. We present GeoDiffuser a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation 3D rotation and removal. We present quantitative results including a perceptual study that shows how our approach is better than existing methods.	https://openaccess.thecvf.com//content/WACV2025/html/Sajnani_GeoDiffuser_Geometry-Based_Image_Editing_with_Diffusion_Models_WACV_2025_paper.html	Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil D Katyal, Srinath Sridhar
GeoGuide: Geometric Guidance of Diffusion Models	Diffusion models are currently one of the most effective tools in image generation. This is in particular due to the fact that contrary to GANs during training they can be easily conditioned. However given a pretrained diffusion guiding it to obtain desired result is typically a more delicate task. A typical technique based on the probabilistic derivation lies in adding the rescaled gradient of the classifier during backward propagation. In this paper we switch the perspective from probabilistic to metric. By studying the distance of the trajectory of the diffusion model from the data manifold we introduce a new guideance model GeoGuide. GeoGuide is not only easy to apply as it is based on classifier gradient normalization but it outperforms the probabilistic approach both with respect to FID and the quality of generated images.	https://openaccess.thecvf.com//content/WACV2025/html/Poleski_GeoGuide_Geometric_Guidance_of_Diffusion_Models_WACV_2025_paper.html	Mateusz Poleski, Jacek Tabor, Przemyslaw Spurek
GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details in Image Synthesis using Convolutional Neural Networks	The enduring inability of image generative models to recreate intricate geometric features such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets this issue remains prevalent across all models from denoising diffusion models to Generative Adversarial Networks (GAN) pointing to a fundamental shortcoming in the underlying architectures. In this paper we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative n-dimensional Cartesian coordinate system. We show this drastically improves quality of hand and face images generated by GANs and Variational AutoEncoders (VAE).	https://openaccess.thecvf.com//content/WACV2025/html/Hosseini_GeoPos_A_Minimal_Positional_Encoding_for_Enhanced_Fine-Grained_Details_in_WACV_2025_paper.html	Mehran Hosseini, Peyman Hosseini
Geometry-Aware Deep Learning for 3D Skeleton-Based Motion Prediction	The field of human motion prediction in computer vision faces challenges especially in 3D Skeleton-based Human Motion. Deep learning models albeit successful in most vision tasks were designed for data characterized by an underlying Euclidean structure which is not always fulfilled as pre-processed data may often reside in a non-linear space. Conventional RNNs struggle with capturing long-term dependencies in motion contexts. Our novel approach focuses on geometry-aware deep learning to predict the motion. We use a compact manifold-valued representation of 3D human skeleton motion integrating self-attention in transformer networks. This representation maps motions to points on a manifold ensuring smooth and coherent long-term predictions. Combining Kendall's shape space for non-rigid deformation and Lie group for rigid deformation provides a complete transformation. Experiments on various datasets demonstrate superiority over state-of-the-art methods in both short and long-term horizons.	https://openaccess.thecvf.com//content/WACV2025/html/Zaier_Geometry-Aware_Deep_Learning_for_3D_Skeleton-Based_Motion_Prediction_WACV_2025_paper.html	Mayssa Zaier, Hazem Wannous, Hassen Drira
Global-Guided Focal Neural Radiance Field for Large-Scale Scene Rendering	Neural radiance fields (NeRF) have recently been applied to render large-scale scenes. However their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks which are subsequently handled by separate sub-NeRFs. These sub-NeRFs trained from scratch and processed independently lead to inconsistencies in geometry and appearance across the scene. Consequently the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture sub-encoders only need fine-tuning based on the global encoder thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably our approach does not rely on any prior knowledge about the target scene attributing GF-NeRF adaptable to various large-scale scene types including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity natural rendering results on various types of large-scale datasets. Our project page: https://shaomq2187.github.io/GF-NeRF/	https://openaccess.thecvf.com//content/WACV2025/html/Shao_Global-Guided_Focal_Neural_Radiance_Field_for_Large-Scale_Scene_Rendering_WACV_2025_paper.html	Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang
GlobalDoc: A Cross-Modal Vision-Language Framework for Real-World Document Image Retrieval and Classification	Visual document understanding (VDU) has rapidly advanced with the development of powerful multi-modal language models. However these models typically require extensive document pre-training data to learn intermediate representations and often suffer a significant performance drop in real-world online industrial settings. A primary issue is their heavy reliance on OCR engines to extract local positional information within document pages which limits the models' ability to capture global information and hinders their generalizability flexibility and robustness. In this paper we introduce GlobalDoc a cross-modal transformer-based architecture pre-trained in a self-supervised manner using three novel pretext objective tasks. GlobalDoc improves the learning of richer semantic concepts by unifying language and visual representations resulting in more transferable models. For proper evaluation we also propose two novel document-level downstream VDU tasks Few-Shot Document Image Classification (DIC) and Content-based Document Image Retrieval (DIR) designed to simulate industrial scenarios more closely. Extensive experimentation has been conducted to demonstrate GlobalDoc's effectiveness in practical settings.	https://openaccess.thecvf.com//content/WACV2025/html/Bakkali_GlobalDoc_A_Cross-Modal_Vision-Language_Framework_for_Real-World_Document_Image_Retrieval_WACV_2025_paper.html	Souhail Bakkali, Sanket Biswas, Zuheng Ming, MickaÃ«l Coustaty, MarÃ§al RusiÃ±ol, Oriol Ramos Terrades, Josep LladÃ³s
Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models	Recent advances in text-to-image (T2I) diffusion models have facilitated creative and photorealistic image synthesis. By varying the random seeds we can generate many images for a fixed text prompt. Technically the seed controls the initial noise and in multi-step diffusion inference the noise used for reparameterization at intermediate timesteps in the reverse diffusion process. However the specific impact of the random seed on the generated images remains relatively unexplored. In this work we conduct a large-scale scientific study into the impact of random seeds during diffusion inference. Remarkably we reveal that the best 'golden' seed achieved an impressive FID of 21.60 compared to the worst 'inferior' seed's FID of 31.97. Additionally a classifier can predict the seed number used to generate an image with over 99.9% accuracy in just a few epochs establishing that seeds are highly distinguishable based on generated images. Encouraged by these findings we examined the influence of seeds on interpretable visual dimensions. We find that certain seeds consistently produce grayscale images prominent sky regions or image borders. Seeds also affect image composition including object location size and depth. Moreover by leveraging these 'golden' seeds we demonstrate improved image generation such as high-fidelity inference and diversified sampling. Our investigation extends to inpainting tasks where we uncover some seeds that tend to insert unwanted text artifacts. Overall our extensive analyses highlight the importance of selecting good seeds and offer practical utility for image generation.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Good_Seed_Makes_a_Good_Crop_Discovering_Secret_Seeds_in_WACV_2025_paper.html	Katherine Xu, Lingzhi Zhang, Jianbo Shi
Graph-Jigsaw Conditioned Diffusion Model for Skeleton-Based Video Anomaly Detection	Skeleton-based video anomaly detection (SVAD) is a crucial task in computer vision. Accurately identifying abnormal patterns or events enables operators to promptly detect suspicious activities thereby enhancing safety. Achieving this demands a comprehensive understanding of human motions both at body and region levels while also accounting for the wide variations of performing a single action. However existing studies fail to simultaneously address these crucial properties. This paper introduces a novel practical and lightweight framework namely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD. GiCiSAD consists of three novel modules: the Graph Attention-based Forecasting module to capture the spatio-temporal dependencies inherent in the data the Graph-level Jigsaw Puzzle Maker module to distinguish subtle region-level discrepancies between normal and abnormal motions and the Graph-based Conditional Diffusion model to generate a wide spectrum of human motions. Extensive experiments on four widely used skeleton-based video datasets show that GiCiSAD outperforms existing methods with significantly fewer training parameters establishing it as the new state-of-the-art.	https://openaccess.thecvf.com//content/WACV2025/html/Karami_Graph-Jigsaw_Conditioned_Diffusion_Model_for_Skeleton-Based_Video_Anomaly_Detection_WACV_2025_paper.html	Ali Karami, Thi Kieu Khanh Ho, Narges Armanfard
GroundingMate: Aiding Object Grounding for Goal-Oriented Vision-and-Language Navigation	Goal-Oriented Vision-and-Language Navigation (VLN) aims to enable agents to navigate to specified locations and identify designated target objects following natural language instruction. This approach has gained popularity due to its close alignment with real-world scenarios. However existing studies have predominantly focused on enhancing navigation performance neglecting the ability to locate objects at the navigation endpoint. This oversight has resulted in a significant discrepancy between the success rates of navigation and object grounding. The challenge is compounded by the complex reasoning required by the instructions and the necessity to synthesize multi-perspective images of objects which overwhelms traditional object grounding methods. We leverage the Multi-Modal Large Language Model (MLLM) to bridge this gap allowing agents to seek assistance from these models when struggling to locate the target object. The agent conducts a multi-stage evaluation to discern the cause of its confusion and promptly extracts and updates the most relevant information for MLLM to assess. Our method is plug-and-play and model-agnostic facilitating integration with numerous existing VLN strategies without the need for retraining. Implementing our approach across four distinct methods has improved performance on the REVERIE and SOON datasets demonstrating the effectiveness and generalizability of our technique.	https://openaccess.thecvf.com//content/WACV2025/html/Liu_GroundingMate_Aiding_Object_Grounding_for_Goal-Oriented_Vision-and-Language_Navigation_WACV_2025_paper.html	Qianyi Liu, Siqi Zhang, Yanyuan Qiao, Junyou Zhu, Xiang Li, Longteng Guo, Qunbo Wang, Xingjian He, Qi Wu, Jing Liu
Guardian of the Ensembles: Introducing Pairwise Adversarially Robust Loss for Resisting Adversarial Attacks in DNN Ensembles	Adversarial attacks rely on transferability where an adversarial example (AE) crafted on a surrogate classifier tends to mislead a target classifier. Recent ensemble methods demonstrate that AEs are less likely to mislead multiple classifiers in an ensemble. This paper proposes a new ensemble training using a Pairwise Adversarially Robust Loss (PARL) that by construction produces an ensemble of classifiers with diverse decision boundaries. PARL utilizes outputs and gradients of each layer with respect to network parameters in every classifier within the ensemble simultaneously. PARL is demonstrated to achieve higher robustness against black-box transfer attacks than previous ensemble methods as well as adversarial training without adversely affecting clean example accuracy. Extensive experiments using standard Resnet20 WideResnet28-10 classifiers demonstrate the robustness of PARL against state-of-the-art adversarial attacks. While maintaining similar clean accuracy and lesser training time the proposed architecture has a 24.8% increase in robust accuracy (e = 0.07) from the state-of-the art method. Code is available at: https://github.com/shubhishukla10/PARL	https://openaccess.thecvf.com//content/WACV2025/html/Shukla_Guardian_of_the_Ensembles_Introducing_Pairwise_Adversarially_Robust_Loss_for_WACV_2025_paper.html	Shubhi Shukla, Subhadeep Dalui, Manaar Alam, Shubhajit Datta, Arijit Mondal, Debdeep Mukhopadhyay, Partha Pratim Chakrabarti
Guess Future Anomalies from Normalcy: Forecasting Abnormal Behavior in Real-World Videos	Forecasting Abnormal Human Behavior (AHB) aims to predict unusual behavior in advance by analyzing early patterns of normal human interactions. Unlike typical action prediction methods this task focuses on observing only normal interactions to predict both short and long term future abnormal behavior. Despite its affirmative impact on society AHB prediction remains under-explored in current research. This is primarily due to the challenges involved in anticipating complex human behaviors and interactions with surrounding agents in real-world situations. Further there exists an underlying uncertainty between the early normal patterns and the future abnormal behavior thereby making the prediction harder. To address these challenges we introduce a novel transformer model that improves early interaction modeling by accounting for uncertainties in both observations and future outcomes. To the best of our knowledge we are the first to explore the task. Therefore we present a new comprehensive dataset referred to as AHB-F which features real-world scenarios with complex human interactions. The AHB-F has a deterministic evaluation protocol that ensures only normal frames to be observed for long and short term future prediction. We extensively evaluate and compare competitive action anticipation methods on our benchmark. Our results show that our method consistently outperforms existing action anticipation approaches both in quantitative and qualitative evaluations.	https://openaccess.thecvf.com//content/WACV2025/html/Majhi_Guess_Future_Anomalies_from_Normalcy_Forecasting_Abnormal_Behavior_in_Real-World_WACV_2025_paper.html	Snehashis Majhi, Mohammed Guermal, Antitza Dantcheva, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, FranÃ§ois BrÃ©mond
HDPNet: Hourglass Vision Transformer with Dual-Path Feature Pyramid for Camouflaged Object Detection	Existing camouflaged object detection methods often struggle with detecting small objects and fine object boundaries. To alleviate these issues we propose a novel hourglass vision Transformer with Dual-path Feature Pyramid (HDPNet). Specifically we construct an hourglass Transformer encoder that effectively captures the global semantic cues while extracting detailed feature maps at various scales preserving the spatial details and fine-grained boundaries of the camouflaged object. To ensure the preservation of essential cues of hourglass features we introduce a dual-path feature pyramid decoder (DPFD). This decoder performs coarse-to-fine feature fusion laterally mitigating the dilution of essential feature cues caused by the semantic gaps. In addition to further facilitate the local feature modeling in the encoder to mine the correlation between local features and global semantic cues from the camouflaged region we design a feature interaction enhancement module (FIEM). This module adopts a symmetric structure enables detailed appearance features and global semantic features to complement each other enhancing the model's ability to capture a wide range of fine-grained details. Extensive quantitative and qualitative experiments demonstrate that the proposed model significantly outperforms 25 existing methods across three challenging COD benchmark datasets particularly excelling in the detection of small objects and fine boundaries. The code is available at https://github.com/LittleGrey-hjp/HDPNet.	https://openaccess.thecvf.com//content/WACV2025/html/He_HDPNet_Hourglass_Vision_Transformer_with_Dual-Path_Feature_Pyramid_for_Camouflaged_WACV_2025_paper.html	Jinpeng He, Biyuan Liu, Huaixin Chen
HEX: Hierarchical Emergence Exploitation in Self-Supervised Algorithms	In this paper we propose an algorithm that can be used on top of a wide variety of self-supervised (SSL) approaches to take advantage of hierarchical structures that emerge during training. SSL approaches typically work through some invariance term to ensure consistency between similar samples and a regularization term to prevent global dimensional collapse. Dimensional collapse refers to data representations spanning a lower-dimensional subspace. Recent work has demonstrated that the representation space of these algorithms gradually reflects a semantic hierarchical structure as training progresses. Data samples of the same hierarchical grouping tend to exhibit greater dimensional collapse locally compared to the dataset as a whole due to sharing features in common with each other. Ideally SSL algorithms would take advantage of this hierarchical emergence to have an additional regularization term to account for this local dimensional collapse effect. However the construction of existing SSL algorithms does not account for this property. To address this we propose an adaptive algorithm that performs a weighted decomposition of the denominator of the InfoNCE loss into two terms: local hierarchical and global collapse regularization respectively. This decomposition is based on an adaptive threshold that gradually lowers to reflect the emerging hierarchical structure of the representation space throughout training. It is based on an analysis of the cosine similarity distribution of samples in a batch. We demonstrate that this hierarchical emergence exploitation (HEX) approach can be integrated across a wide variety of SSL algorithms. Empirically we show performance improvements of up to 5.6% relative improvement over baseline SSL approaches on classification accuracy on Imagenet with 100 epochs of training.	https://openaccess.thecvf.com//content/WACV2025/html/Kokilepersaud_HEX_Hierarchical_Emergence_Exploitation_in_Self-Supervised_Algorithms_WACV_2025_paper.html	Kiran Kokilepersaud, Seulgi Kim, Mohit Prabhushankar, Ghassan AlRegib
HOPE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts	Compositional Zero-Shot Learning (CZSL) has emerged as an essential paradigm in machine learning aiming to overcome the constraints of traditional zero-shot learning by incorporating compositional thinking into its methodology. Conventional zero-shot learning has difficulty managing unfamiliar combinations of seen and unseen classes because it depends on pre-defined class embeddings. In contrast Compositional Zero-Shot Learning leverages the inherent hierarchies and structural connections among classes creating new class representations by combining attributes components or other semantic elements. In our paper we propose a novel framework that for the first time combines the Modern Hopfield Network with a Mixture of Experts (HOPE) to classify the compositions of previously unseen objects. Specifically the Modern Hopfield Network creates a memory that stores label prototypes and identifies relevant labels for a given input image. Subsequently the Mixture of Expert models integrates the image with the appropriate prototype to produce the final composition classification. Our approach achieves SOTA performance on several benchmarks including MIT-States and UT-Zappos. We also examine how each component contributes to improved generalization.	https://openaccess.thecvf.com//content/WACV2025/html/Dat_HOPE_A_Memory-Based_and_Composition-Aware_Framework_for_Zero-Shot_Learning_with_WACV_2025_paper.html	Do Huu Dat, Po-Yuan Mao, Tien Hoang Nguyen, Wray Buntine, Mohammed Bennamoun
HSDA: High-Frequency Shuffle Data Augmentation for Bird's-Eye-View Map Segmentation	Autonomous driving has garnered significant attention in recent research and Bird's-Eye-View (BEV) map segmentation plays a vital role in the field providing the basis for safe and reliable operation. While data augmentation is a commonly used technique for improving BEV map segmentation networks existing approaches predominantly focus on manipulating spatial domain representations. In this work we investigate the potential of frequency domain data augmentation for camera-based BEV map segmentation. We observe that high-frequency information in camera images is particularly crucial for accurate segmentation. Based on this insight we propose High-frequency Shuffle Data Augmentation (HSDA) a novel data augmentation strategy that enhances a network's ability to interpret high-frequency image content. This approach encourages the network to distinguish relevant high-frequency information from noise leading to improved segmentation results for small and intricate image regions as well as sharper edge and detail perception. Evaluated on the nuScenes dataset our method demonstrates broad applicability across various BEV map segmentation networks achieving a new state-of-the-art mean Intersection over Union (mIoU) of 61.3% for camera-only systems. This significant improvement underscores the potential of frequency domain data augmentation for advancing the field of autonomous driving perception. Code has been released: https://github.com/Zarhult/HSDA	https://openaccess.thecvf.com//content/WACV2025/html/Glisson_HSDA_High-Frequency_Shuffle_Data_Augmentation_for_Birds-Eye-View_Map_Segmentation_WACV_2025_paper.html	Calvin Glisson, Qiuxiao Chen
HandCraft: Anatomically Correct Restoration of Malformed Hands in Diffusion Generated Images	"Generative text-to-image models such as Stable Diffusion have demonstrated a remarkable ability to generate diverse high-quality images. However they are surprisingly inept when it comes to rendering human hands which are often anatomically incorrect or reside in the ""uncanny valley"". In this paper we propose a method HandCraft for restoring such malformed hands. This is achieved by automatically constructing masks and depth images for hands as conditioning signals using a parametric model allowing a diffusion-based image editor to fix the hand's anatomy and adjust its pose while seamlessly integrating the changes into the original image preserving pose color and style. Our plug-and-play hand restoration solution is compatible with existing pretrained diffusion models and the restoration process facilitates adoption by eschewing any fine-tuning or training requirements for the diffusion models. We also contribute MalHand datasets that contain generated images with a wide variety of malformed hands in several styles for hand detector training and hand restoration benchmarking and demonstrate through qualitative and quantitative evaluation that HandCraft not only restores anatomical correctness but also maintains the integrity of the overall image."	https://openaccess.thecvf.com//content/WACV2025/html/Qin_HandCraft_Anatomically_Correct_Restoration_of_Malformed_Hands_in_Diffusion_Generated_WACV_2025_paper.html	Zhenyue Qin, Yiqun Zhang, Yang Liu, Dylan Campbell
Harmonizing Attention: Training-Free Texture-Aware Geometry Transfer	Creating images where surface patterns of one object - such as cracks holes or grooves - are precisely transferred onto objects made of different materials remains a challenging task in computer graphics. For example recreating the exact pattern of wood grain cracks on a metallic surface while maintaining the realistic metallic texture requires sophisticated technical solutions. In this study we introduce Harmonizing Attention a new method that can automatically extract these surface patterns from photographs and recreate them with different materials while preserving natural-looking textures. Our approach achieves this through a novel attention mechanism that can process multiple reference images simultaneously without requiring additional training. This makes the method both practical and efficient for real-world applications opening up new possibilities in augmented reality image editing and beyond.	https://openaccess.thecvf.com//content/WACV2025/html/Ikuta_Harmonizing_Attention_Training-Free_Texture-Aware_Geometry_Transfer_WACV_2025_paper.html	Eito Ikuta, Yohan Lee, Akihiro Iohara, Yu Saito, Toshiyuki Tanaka
Hausdorff Distance Matching with Adaptive Query Denoising for Rotated Detection Transformer	Detection Transformers (DETR) have recently set new benchmarks in object detection. However their performance in detecting rotated objects lags behind established oriented object detectors. Our analysis identifies a key observation: the boundary discontinuity and square-like problem in bipartite matching poses an issue with assigning appropriate ground truths to predictions leading to du plicate low-confidence predictions. To address this we introduce a Hausdorff distance-based cost for bipartite matching which more accurately quantifies the discrepancy between predictions and ground truths. Additionally we find that a static denoising approach impedes the training of rotated DETR especially as the quality of the detector's predictions begins to exceed that of the noised ground truths. To overcome this we propose an adaptive query denoising method that employs bipartite matching to selectively eliminate noised queries that detract from model improvement. When compared to models adopting a ResNet-50 backbone our proposed model yields remarkable improvements achieving +4.18 AP50 +4.59 AP50 and +4.99 AP50 on DOTA-v2.0 DOTA-v1.5 and DIOR-R respectively.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Hausdorff_Distance_Matching_with_Adaptive_Query_Denoising_for_Rotated_Detection_WACV_2025_paper.html	Hakjin Lee, MinKi Song, Jamyoung Koo, Junghoon Seo
HeightLane: BEV Heightmap Guided 3D Lane Detection	Accurate 3D lane detection from monocular images presents significant challenges due to depth ambiguity and imperfect ground modeling. Previous attempts to model the ground have often used a planar ground assumption with limited degrees of freedom making them unsuitable for complex road environments with varying slopes. Our study introduces HeightLane an innovative method that predicts a height map from monocular images by creating anchors based on a multi-slope assumption. This approach provides a detailed and accurate representation of the ground. HeightLane employs the predicted heightmap along with a deformable attention-based spatial feature transform framework to efficiently convert 2D image features into 3D bird's eye view (BEV) features enhancing spatial understanding and lane structure recognition. Additionally the heightmap is used for the positional encoding of BEV features further improving their spatial accuracy. This explicit view transformation bridges the gap between front-view perceptions and spatially accurate BEV representations significantly improving detection performance. To address the lack of the necessary ground truth height map in the original OpenLane dataset we leverage the Waymo dataset and accumulate its LiDAR data to generate a height map for the drivable area of each scene. The GT heightmaps are used to train the heightmap extraction module from monocular images. Extensive experiments on the OpenLane validation set show that HeightLane achieves state-of-the-art performance in terms of F-score highlighting its potential in real-world applications.	https://openaccess.thecvf.com//content/WACV2025/html/Park_HeightLane_BEV_Heightmap_Guided_3D_Lane_Detection_WACV_2025_paper.html	Chaesong Park, Eunbin Seo, Jongwoo Lim
HeightMapNet: Explicit Height Modeling for End-to-End HD Map Learning	Recent advances in high-definition (HD) map construction from surround-view images have highlighted their cost-effectiveness in deployment. However prevailing techniques often fall short in accurately extracting and utilizing road features as well as in the implementation of view transformation. In response we introduce HeightMapNet a novel framework that establishes a dynamic relationship between image features and road surface height distributions. By integrating height priors our approach refines the accuracy of Bird's-Eye-View (BEV) features beyond conventional methods. HeightMapNet also introduces a foreground-background separation network that sharply distinguishes between critical road elements and extraneous background components enabling precise focus on detailed road micro-features. Additionally our method leverages multi-scale features within the BEV space optimally utilizing spatial geometric information to boost model performance. HeightMapNet has shown exceptional results on the challenging nuScenes and Argoverse 2 datasets outperforming several widely recognized approaches. The code will be available at https://github.com/adasfag/HeightMapNet/.	https://openaccess.thecvf.com//content/WACV2025/html/Qiu_HeightMapNet_Explicit_Height_Modeling_for_End-to-End_HD_Map_Learning_WACV_2025_paper.html	Wenzhao Qiu, Shanmin Pang, Hao Zhang, Jianwu Fang, Jianru Xue
Heterogeneous Datasets for Unsupervised Image Anomaly Detection	Unsupervised anomaly detection (AD) is a critical task in various domains from manufacturing to infrastructure monitoring. To advance this field we introduce two novel datasets: CARS-AD and ROADS-AD designed to challenge existing unsupervised AD methods with their diverse and heterogeneous image content. CARS-AD comprises real images of cars with various defects while ROADS-AD contains images of roads from multiple countries each presenting unique challenges in anomaly detection. These datasets provide ground truth pixel-wise masks and image-level ground truth labels enabling detailed evaluation and benchmarking of AD algorithms. We evaluate state-of-the-art unsupervised AD methods on both datasets using the AUROC metric to assess detection and localization performance. Our results reveal significant room for improvement underscoring the complexity of the datasets and the need for robust AD techniques. Notably Csflow and U-Flow demonstrate superior performance on the CARS-AD Dataset leveraging their ability to process multi-scale features effectively. Conversely Reverse Distillation excels in anomaly localization on the ROADS-AD Dataset showcasing the importance of nuanced approaches for diverse anomaly types. Our findings underscore the importance of addressing the challenges posed by heterogeneous datasets in unsupervised AD. We hope that the introduction of CARS-AD and ROADS-AD will inspire further research in this field driving the development of innovative AD methods capable of handling real-world anomalies with greater accuracy and reliability. CARS-AD Dataset and ROADS-AD Dataset are publicly available at https://github.com/juanb09111/heterogeneousAD.	https://openaccess.thecvf.com//content/WACV2025/html/Lagos_Heterogeneous_Datasets_for_Unsupervised_Image_Anomaly_Detection_WACV_2025_paper.html	Juan Lagos, Haider Ali, Adnan Faroque, Esa Rahtu
HexaGen3D: StableDiffusion is One Step Away from Fast and Diverse Text-to-3D Generation	Despite the latest remarkable advances in generative modeling efficient generation of high-quality 3D objects from textual prompts remains a difficult task. A key challenge lies in data scarcity: the most extensive 3D datasets encompass merely millions of samples while their 2D counterparts contain billions of text-image pairs. To address this we propose a novel approach which harnesses the power of large pretrained 2D diffusion models. More specifically our approach HexaGen3D fine-tunes a pretrained text-to-image model to jointly predict 6 orthographic projections and the corresponding 3D latent. We then decode these latents to generate a textured mesh. HexaGen3D does not require per-sample optimization and can infer high-quality and diverse objects from textual prompts in 7 seconds offering significantly better quality-to-latency trade-offs than existing approaches. Furthermore HexaGen3D demonstrates strong generalization to new objects or compositions.	https://openaccess.thecvf.com//content/WACV2025/html/Mercier_HexaGen3D_StableDiffusion_is_One_Step_Away_from_Fast_and_Diverse_WACV_2025_paper.html	Antoine Mercier, Ramin Nakhli, Mahesh Reddy, Rajeev Yasarla, Hong Cai, Fatih Porikli, Guillaume Berger
Hierarchical Light Transformer Ensembles for Multimodal Trajectory Forecasting	Accurate trajectory forecasting is crucial for the performance of various systems such as advanced driver-assistance systems and self-driving vehicles. These forecasts allow us to anticipate events that lead to collisions and therefore to mitigate them. Deep Neural Networks have excelled in motion forecasting but overconfidence and weak uncertainty quantification persist. Deep Ensembles address these concerns yet applying them to multimodal distributions remains challenging. In this paper we propose a novel approach named Hierarchical Light Transformer Ensembles (HLT-Ens) aimed at efficiently training an ensemble of Transformer architectures using a novel hierarchical loss function. HLT-Ens leverages grouped fully connected layers inspired by grouped convolution techniques to capture multimodal distributions effectively. We demonstrate that HLT-Ens achieves state-of-the-art performance levels through extensive experimentation offering a promising avenue for improving trajectory forecasting techniques. We make our code available at github.com/alafage/hlt-ens.	https://openaccess.thecvf.com//content/WACV2025/html/Lafage_Hierarchical_Light_Transformer_Ensembles_for_Multimodal_Trajectory_Forecasting_WACV_2025_paper.html	Adrien Lafage, Mathieu Barbier, Gianni Franchi, David Filliat
High-Fidelity Document Stain Removal via A Large-Scale Real-World Dataset and A Memory-Augmented Transformer	Document images are often degraded by various stains significantly impacting their readability and hindering downstream applications such as document digitization and analysis. The absence of a comprehensive stained document dataset has limited the effectiveness of existing document enhancement methods in removing stains while preserving fine-grained details. To address this challenge we construct StainDoc the first large-scale high-resolution (2145x2245) dataset specifically designed for document stain removal. StainDoc comprises over 5000 pairs of stained and clean document images across multiple scenes. This dataset encompasses a diverse range of stain types severities and document backgrounds facilitating robust training and evaluation of document stain removal algorithms. Furthermore we propose StainRestorer a Transformer-based document stain removal approach. StainRestorer employs a memory-augmented Transformer architecture that captures hierarchical stain representations at part instance and semantic levels via the DocMemory module. The Stain Removal Transformer (SRTransformer) leverages these feature representations through a dual attention mechanism: an enhanced spatial attention with an expanded receptive field and a channel attention captures channel-wise feature importance. This combination enables precise stain removal while preserving document content integrity. Extensive experiments demonstrate StainRestorer's superior performance over state-of-the-art methods on the StainDoc dataset and its variants StainDoc_Mark and StainDoc_Seal establishing a new benchmark for document stain removal. Our work highlights the potential of memory-augmented Transformers for this task and contributes a valuable dataset to advance future research.	https://openaccess.thecvf.com//content/WACV2025/html/Li_High-Fidelity_Document_Stain_Removal_via_A_Large-Scale_Real-World_Dataset_and_WACV_2025_paper.html	Mingxian Li, Hao Sun, Yingtie Lei, Xiaofeng Zhang, Yihang Dong, Yilin Zhou, Zimeng Li, Xuhang Chen
High-Pass Kernel Prediction for Efficient Video Deblurring	State-of-the-art video deblurring methods use deep network architectures to recover sharpened video frames. Blurring especially degrades high-frequency (HF) information yet this aspect is often overlooked by recent models that focus more on enhancing architectural design. Recovering these fine details is challenging partly due to the spectral bias of neural networks which are inclined towards learning low-frequency functions. To address this we enforce explicit network structures to capture the fine details and edges. We dynamically predict adaptive high-pass kernels from a linear combination of high-pass basis kernels to extract high-frequency features. This strategy is highly efficient resulting in low-memory footprints for training and fast run times for inference all while achieving state-of-the-art when compared to low-budget models. The code is available at https://github.com/jibo27/AHFNet.	https://openaccess.thecvf.com//content/WACV2025/html/Ji_High-Pass_Kernel_Prediction_for_Efficient_Video_Deblurring_WACV_2025_paper.html	Bo Ji, Angela Yao
Hijacking Vision-and-Language Navigation Agents with Adversarial Environmental Attacks	Assistive embodied agents that can be instructed in natural language to perform tasks in open-world environments have the potential to significantly impact labor tasks like manufacturing or in-home care - benefiting the lives of those who come to depend on them. In this work we consider how this benefit might be hijacked by local modifications in the appearance of the agent's operating environment. Specifically we take the popular Vision-andLanguage Navigation (VLN) task as a representative setting and develop a whitebox adversarial attack that optimizes a 3D attack object's appearance to induce desired behaviors in pretrained VLN agents that observe it in the environment. We demonstrate that the proposed attack can cause VLN agents to ignore their instructions and execute alternative actions after encountering the attack object - even for instructions and agent paths not considered when optimizing the attack. For these novel settings we find our attacks can induce early-termination behaviors or divert an agent along an attacker-defined multi-step trajectory. Under both conditions environmental attacks significantly reduce agent capabilities to successfully follow user instructions.	https://openaccess.thecvf.com//content/WACV2025/html/Yang_Hijacking_Vision-and-Language_Navigation_Agents_with_Adversarial_Environmental_Attacks_WACV_2025_paper.html	Zijiao Yang, Xiangxi Shi, Eric Slyman, Stefan Lee
HybridDepth: Robust Metric Depth Fusion by Leveraging Depth from Focus and Single-Image Priors	We propose HYBRIDDEPTH a robust depth estimation pipeline that addresses key challenges in depth estimation including scale ambiguity hardware heterogeneity and generalizability. HYBRIDDEPTH leverages focal stack data conveniently accessible in common mobile devices to produce accurate metric depth maps. By incorporating depth priors afforded by recent advances in single-image depth estimation our model achieves a higher level of structural detail compared to existing methods. We test our pipeline as an end-to-end system with a newly developed mobile client to capture focal stacks which are then sent to a GPU-powered server for depth estimation. Comprehensive quantitative and qualitative analyses demonstrate that HYBRIDDEPTH outperforms state-of-the-art (SOTA) models on common datasets such as DDFF12 and NYU Depth V2. HYBRIDDEPTH also shows strong zero-shot generalization. When trained on NYU Depth V2 HYBRIDDEPTH surpasses SOTA models in zero-shot performance on ARKitScenes and delivers more structurally accurate depth maps on Mobile Depth. The code is available at https://github.com/cake-lab/HybridDepth/.	https://openaccess.thecvf.com//content/WACV2025/html/Ganj_HybridDepth_Robust_Metric_Depth_Fusion_by_Leveraging_Depth_from_Focus_WACV_2025_paper.html	Ashkan Ganj, Hang Su, Tian Guo
Hyperdimensional Representation for Adaptive Information Association and Memorization	Many computer vision applications rely on interpretable machine learning algorithms to analyze the data collected from various sources. We leverage Hyperdimensional Computing (HDC) as an innovative computational model that mimics key brain functionalities to achieve efficient and robust cognitive learning. We propose HDlm a novel HDC-based cognitive representation capable of adaptive information association and memorization. HDlm first theoretically expands HDC mathematics to support selective information association and adaptive memorization. Then it exploits the proposed operations to support cognitive operations including set membership information retrieval and item comparison. We evaluated our solution for a selection of applications related to visual data representation and sequence matching analysis. Our evaluation shows that HDlm provides more adaptive similarity metrics between objects that lead to better task performance.	https://openaccess.thecvf.com//content/WACV2025/html/Zou_Hyperdimensional_Representation_for_Adaptive_Information_Association_and_Memorization_WACV_2025_paper.html	Zhuowen Zou, Prathyush Poduval, Narayan Srinivasa, Mohsen Imani
I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt Generation for Text-Guided Multi-Mask Inpainting	Inpainting focuses on filling missing or corrupted regions of an image to blend seamlessly with its surrounding content and style. While conditional diffusion models have proven effective for text-guided inpainting we introduce the novel task of multi-mask inpainting where multiple regions are simultaneously inpainted using distinct prompts. Furthermore we design a fine-tuning procedure for multimodal LLMs such as LLaVA to generate multi-mask prompts automatically using corrupted images as inputs. These models can generate helpful and detailed prompt suggestions for filling the masked regions. The generated prompts are then fed to Stable Diffusion which is fine-tuned for the multi-mask inpainting problem using rectified cross-attention enforcing prompts onto their designated regions for filling. Experiments on digitized paintings from WikiArt and the Densely Captioned Images dataset demonstrate that our pipeline delivers creative and accurate inpainting results. Our code data and trained models are available at https://cilabuniba.github.io/i-dream-my-painting.	https://openaccess.thecvf.com//content/WACV2025/html/Fanelli_I_Dream_My_Painting_Connecting_MLLMs_and_Diffusion_Models_via_WACV_2025_paper.html	Nicola Fanelli, Gennaro Vessio, Giovanna Castellano
I Spy with My Little Eye A Minimum Cost Multicut Investigation of Dataset Frames	Visual framing analysis is a key method in social sciences for determining common themes and concepts in a given discourse. To reduce manual effort image clustering can significantly speed up the annotation process. In this work we phrase the clustering task as a Minimum Cost Multicut Problem [MP]. Solutions to the MP have been shown to provide clusterings that maximize the posterior probability solely from provided local pairwise probabilities of two images belonging to the same cluster. We discuss the efficacy of numerous embedding spaces to detect visual frames and show its superiority over other clustering methods. To this end we employ the climate change dataset ClimateTV which contains images commonly used for visual frame analysis. For broad visual frames DINOv2 is a suitable embedding space while ConvNeXt V2 returns a larger number of clusters which contain fine-grain differences i.e. speech and protest. Our insights into embedding space differences in combination with the optimal clustering - by definition - advances automated visual frame detection. Our code can be found at https://github.com/KathPra/MP4VisualFrameDetection.	https://openaccess.thecvf.com//content/WACV2025/html/Prasse_I_Spy_with_My_Little_Eye_A_Minimum_Cost_Multicut_WACV_2025_paper.html	Katharina Prasse, Isaac Bravo, Stefanie Walter, Margret Keuper
I3D-AE-LSTM: A 2-Stream Autoencoder for Action Quality Assessment using a Newly Created Cricket Batsman Video Dataset	In this study we introduce UJ-AQA-CricketVision a dataset comprising 8540 video clips of cricket strokes each annotated with detailed phase breakdowns. We develop a novel multi-variate approach for Action Quality Assessment (AQA) at a body level that leverages an Autoencoder for extracting sophisticated feature representations from video frames and pose estimated keypoints. These features are subsequently utilised by a multi-layer perceptron regression-based model to accurately predict the quality of cricket actions in terms of their head shoulder hands hips and feet. Our approach is benchmarked against contemporary state-of-the-art AQA methods and achieves a Spearman Rank Correlation score of 0.84. The performance highlights the significance of integrating pose keypoint and frame data for the nuanced analysis of short and complex action sequences in sports such as cricket. This work aims to foster the development of accurate Action Quality Assessment methods on Cricket Video data. The dataset can be found here: https://github.com/dvanderhaar/uj-aqa-cricketvision	https://openaccess.thecvf.com//content/WACV2025/html/Moodley_I3D-AE-LSTM_A_2-Stream_Autoencoder_for_Action_Quality_Assessment_using_a_WACV_2025_paper.html	Tevin Moodley, Dustin Terence van der Haar
IRIS-VIS: A New Dataset for Visibility Estimation in an Industrial Environment	Point cloud visibility estimation is fundamental as it is useful for many computer vision applications including surface reconstruction 3D segmentation from paired images and point densification. Previous works showed outstanding results on simple object and outdoor datasets. However unlike the previously studied scenes the most challenging environments are those providing a high amount of object points in the same direction typically in complex indoor scenes. In this kind of environments due to the lack of real data ground truth quantitative analysis are either missing or based on simulated data. In this work we present IRIS-VIS (Industrial Room In Saclay - VISibility) a new dataset for point visibility estimation in an indoor environment. It is a high complexity scene due to the large variety in the shape size and orientation of the objects. To our knowledge this is the first dataset on real indoor data providing a dense LiDAR station-based point cloud along with a well-fitted CAD model. The latter is useful to compute automatically quickly and accurately the visibility from any given viewpoint enabling evaluations under infinite conditions. We propose new metrics for the visibility estimation task and evaluate state-of-the-art methods in both sparse and dense conditions with the proposed dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Armangeon_IRIS-VIS_A_New_Dataset_for_Visibility_Estimation_in_an_Industrial_WACV_2025_paper.html	Flavien Armangeon, Thibaud Ehret, Enric Meinhardt-Llopis, Rafael Grompone von Gioi, Guillaume Thibault, Marc Petit, Gabriele Facciolo
Identify Backdoored Model in Federated Learning via Individual Unlearning	Backdoor attacks present a significant threat to the robustness of Federated Learning (FL) due to their stealth and effectiveness. They maintain both the main task of the FL system and the backdoor task simultaneously causing malicious models to appear statistically similar to benign ones which enables them to evade detection by existing defense methods. We find that malicious parameters in backdoored models are inactive on the main task resulting in a significantly large empirical loss during the machine unlearning process on clean inputs. Inspired by this we propose MASA a method that utilizes individual unlearning on local models to identify malicious models in FL. To improve the performance of MASA in challenging non-independent and identically distributed (non-IID) settings we design pre-unlearning model fusion that integrates local models with knowledge learned from other datasets to mitigate the divergence in their unlearning behaviors caused by the non-IID data distributions of clients. Additionally we propose a new anomaly detection metric with minimal hyperparameters to filter out malicious models efficiently. Extensive experiments on IID and non-IID datasets across six different attacks validate the effectiveness of MASA. To the best of our knowledge this is the first work to leverage machine unlearning to identify malicious models in FL. Code is available at https://github.com/JiiahaoXU/MASA.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Identify_Backdoored_Model_in_Federated_Learning_via_Individual_Unlearning_WACV_2025_paper.html	Jiahao Xu, Zikai Zhang, Rui Hu
Identity Curvature Laplace Approximation for Improved Out-of-Distribution Detection	Uncertainty estimation is crucial in safety-critical applications where robust out-of-distribution (OOD) detection is essential. Traditional Bayesian methods though effective are often hindered by high computational demands. As an alternative Laplace approximation offers a more practical and efficient approach to uncertainty estimation. In this paper we introduce the Identity Curvature Laplace Approximation (ICLA) a novel method that challenges the conventional posterior covariance formulation by using identity curvature and optimizing prior precision. This innovative design significantly enhances OOD detection performance on well-known datasets such as CIFAR-10 CIFAR-100 and ImageNet while maintaining calibration scores. We attribute this improvement to the alignment issues between typical feature embeddings and curvature as measured by the Fisher information matrix. Our findings are further supported by demonstrating that incorporating Fisher penalty or sharpness-aware minimization techniques can greatly enhance the uncertainty estimation capabilities of standard Laplace approximation.	https://openaccess.thecvf.com//content/WACV2025/html/Zhdanov_Identity_Curvature_Laplace_Approximation_for_Improved_Out-of-Distribution_Detection_WACV_2025_paper.html	Maksim Zhdanov, Stanislav Dereka, Sergey Kolesnikov
Image Adaptation for Colour Vision Deficient Viewers using Vision Transformers	Colour Vision Deficiency (CVD) occurs when anomalous retinal cone spectral responses impact the ability to distinguish between certain colours. To enhance image quality and viewing experience recolouring algorithms seek to modify pixel values so that this does not lead to a loss of detail or image quality. Recent approaches to recolouring for CVD viewers employ neural models which exploit higher order features to direct colour adaptation. In this work we build upon the idea that visual neural models exhibit emergent behaviour which mimics the human visual system. We make use of these learned behaviours to guide the colour adaptation process by considering regions of the image that are the most semantically meaningful for a non-CVD viewer and compensate for them appropriately if they are absent or distorted in a CVD-simulated version of the image. We find that a minimal algorithm built atop a pre-trained model produces results that substantially boost contrast and salience for viewers affected by CVD. We also investigate a few cases where modifications are absent indicating that a neurally guided salience-based model may also provide a means of determining when recolouring is not necessary. Additionally we introduce a novel metric that quantifies the contrast increase or decrease under changes in image colour.	https://openaccess.thecvf.com//content/WACV2025/html/Gillooly_Image_Adaptation_for_Colour_Vision_Deficient_Viewers_using_Vision_Transformers_WACV_2025_paper.html	Thomas Gillooly, Jean-Baptiste Thomas, Jon Y. Hardeberg, Giuseppe Claudio Guarnera
Image-Caption Encoding for Improving Zero-Shot Generalization	Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD datapoint is misclassified the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes we propose the Image-Caption Encoding (ICE) method a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on average and up to 3% on challenging datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Yu_Image-Caption_Encoding_for_Improving_Zero-Shot_Generalization_WACV_2025_paper.html	Eric Yu, Christopher Liao, Sathvik Ravi, Theodoros Tsiligkaridis, Brian Kulis
Image-Level Regression for Uncertainty-Aware Retinal Image Segmentation	Accurate retinal vessel (RV) segmentation is a crucial step in the quantitative assessment of retinal vasculature which is needed for the early detection of retinal diseases and other conditions. Numerous studies have been conducted to tackle the problem of segmenting vessels automatically using a pixel-wise classification approach. The common practice of creating ground truth labels is to categorize pixels as foreground and background. This approach is however biased and it ignores the uncertainty of a human annotator when it comes to annotating e.g. thin vessels. In this work we propose a simple and effective method that casts the RV segmentation task as an image-level regression. For this purpose we first introduce a novel Segmentation Annotation Uncertainty-Aware (SAUNA) transform which adds pixel uncertainty to the ground truth using the pixel's closeness to the annotation boundary and vessel thickness. To train our model with soft labels we generalize the earlier proposed Jaccard metric loss to arbitrary hypercubes for soft Jaccard index (Intersection-over-Union) optimization. Additionally we employ a stable version of the Focal-L1 loss for pixel-wise regression. We conduct thorough experiments and compare our method to a diverse set of baselines across 5 retinal image datasets. Our empirical results indicate that the integration of the SAUNA transform and these segmentation losses led to significant performance boosts for different segmentation models. Particularly our methodology enables UNet-like architectures to substantially outperform computational-intensive baselines. Our implementation is available at https://github.com/Oulu-IMEDS/SAUNA.	https://openaccess.thecvf.com//content/WACV2025/html/Dang_Image-Level_Regression_for_Uncertainty-Aware_Retinal_Image_Segmentation_WACV_2025_paper.html	Trung D. Q. Dang, Huy Hoang Nguyen, Aleksei Tiulpin
Importance-Guided Interpretability and Pruning for Video Transformers in Driver Action Recognition	Recently transformers have gained prominence in video action recognition due to their ability to capture spatio-temporal dependencies. Despite their effectiveness the interpretability of their self-attention mechanisms remains limited posing obstacles in understanding model decisions impacting transparency and bias identification. Additionally the computational demands of transformer architectures particularly the self-attention mechanism present practical difficulties. To tackle both challenges we adapt existing interpretability techniques and introduce a layer pruning method guided by importance metrics. In the context of driver action recognition our findings highlight the efficacy of the applied head importance metrics in pinpointing crucial attention heads and identifying key visual cues essential for recognizing driver behavior. Experimental results conducted on three mainstream video transformers demonstrate the effectiveness of the proposed pruning technique with significantly reduced computational costs and only slight performance degradation by removing low-relevance layers. Specifically on our DriverActionInsight (DAI) dataset we achieve a 23.5% FLOPs saving in compressing Video Swin with less than a 1% decrease in Top-1 accuracy.	https://openaccess.thecvf.com//content/WACV2025/html/Palenzuela_Importance-Guided_Interpretability_and_Pruning_for_Video_Transformers_in_Driver_Action_WACV_2025_paper.html	Raquel Panadero Palenzuela, Dominik SchÃ¶rkhuber, Margrit Gelautz
Improving Accuracy and Generalization for Efficient Visual Tracking	Efficient visual trackers overfit to their training distributions and lack generalization abilities resulting in them performing well on their respective in-distribution (ID) test sets and not as well on out-of-distribution (OOD) sequences imposing limitations to their deployment in-the-wild under constrained resources. We introduce SiamABC a highly efficient Siamese tracker that significantly improves tracking performance even on OOD sequences. SiamABC takes advantage of new architectural designs in the way it bridges the dynamic variability of the target and of new losses for training. Also it directly addresses OOD tracking generalization by including a fast backward-free dynamic test-time adaptation method that continuously adapts the model according to the dynamic visual changes of the target. Our extensive experiments suggest that SiamABC shows remarkable performance gains in OOD sets while maintaining accurate performance on the ID benchmarks. SiamABC outperforms MixFormerV2-S by 7.6% on the OOD AVisT benchmark while being 3x faster (100 FPS) on a CPU. Our code and models are available at https://wvuvl.github.io/SiamABC/.	https://openaccess.thecvf.com//content/WACV2025/html/Zaveri_Improving_Accuracy_and_Generalization_for_Efficient_Visual_Tracking_WACV_2025_paper.html	Ram Zaveri, Shivang Patel, Yu Gu, Gianfranco Doretto
Improving Conditional Diffusion Models through Re-Noising from Unconditional Diffusion Priors	Conditional diffusion probabilistic models can model the distribution of natural images and can generate diverse and realistic samples based on given conditions. However oftentimes their results can be unrealistic with observable color shifts and textures. We believe that this issue results from the divergence between the probabilistic distribution learned by the model and the distribution of natural images. The delicate conditions gradually enlarge the divergence during each sampling timestep. To address this issue we introduce a new method that brings the predicted samples to the training data manifold using a pretrained unconditional diffusion model. The unconditional model acts as a regularizer and reduces the divergence introduced by the conditional model at each sampling step. We perform comprehensive experiments to demonstrate the effectiveness of our approach on super-resolution colorization turbulence removal and image-deraining tasks. The improvements obtained by our method suggest that the priors can be incorporated as a general plugin for improving conditional diffusion models.	https://openaccess.thecvf.com//content/WACV2025/html/Mei_Improving_Conditional_Diffusion_Models_through_Re-Noising_from_Unconditional_Diffusion_Priors_WACV_2025_paper.html	Kangfu Mei, Nithin Gopalakrishnan Nair, Vishal Patel
Improving Deep Detector Robustness via Detection-Related Discriminant Maximization and Reorganization	Deep visual detectors are known to be vulnerable to adversarial attacks raising concerns about their real-world applications (e.g. self-driving perception). We argue that this vulnerability arises from the spurious dependency of final detections on irrelevant/loophole latent dimensions. The greater the number of such dimensions the higher the likelihood of the detector being compromised by adversarial attacks making it more susceptible to input perturbations. To enhance detection robustness we propose Detection-related Discriminant Maximization and Reorganization (DDMR) condensing the detection utility to a compressed number of relevant dimensions while deactivating the influence of irrelevant ones. This approach also alleviates the misalignment issue between the two task domains in visual detection and consequently their gradients. This enables the generation of more potent adversarial attacks and defenses for visual detectors within the adversarial training framework. Extensive experiments conducted with four cutting-edge visual detectors on the KITTI and COCO datasets showcase the efficacy of the proposed approach in improving the adversarial robustness of deep visual detectors against both white-box and black-box attacks. For example on the KITTI dataset our method demonstrates an increase in robustness of up to 12.4% and 28.0% without and with adversarial training respectively.	https://openaccess.thecvf.com//content/WACV2025/html/Choi_Improving_Deep_Detector_Robustness_via_Detection-Related_Discriminant_Maximization_and_Reorganization_WACV_2025_paper.html	Jung Im Choi, Qizhen Lan, Qing Tian
Improving Detail in Pluralistic Image Inpainting with Feature Dequantization	Pluralistic Image Inpainting (PII) offers multiple plausible solutions for restoring missing parts of images and has been successfully applied to various applications including image editing and object removal. Recently VQGAN-based methods have been proposed and have shown that they significantly improve the structural integrity in the generated images. Nevertheless the state-of-the-art VQGAN-based model PUT faces a critical challenge: degradation of detail quality in output images due to feature quantization. Feature quantization restricts the latent space and causes information loss which negatively affects the detail quality essential for image inpainting. To tackle the problem we propose the FDM (Feature Dequantization Module) specifically designed to restore the detail quality of images by compensating for the information loss. Furthermore we develop an efficient training method for FDM which drastically reduces training costs. We empirically demonstrate that our method significantly enhances the detail quality of the generated images with negligible training and inference overheads. The code is available at https://github.com/hyudsl/FDM	https://openaccess.thecvf.com//content/WACV2025/html/Park_Improving_Detail_in_Pluralistic_Image_Inpainting_with_Feature_Dequantization_WACV_2025_paper.html	Kyungri Park, Woohwan Jung
Improving Faithfulness of Text-to-Image Diffusion Models through Inference Intervention	Text-to-Image diffusion models have shown remarkable capabilities in generating high-quality images. However current models often struggle to adhere to the complete set of conditions specified in the input text and return unfaithful generations. Existing works address this problem by either fine-tuning the base model or modifying the latent representations during the inference stage with gradient-based updates. Not only are these approaches computationally expensive but also they usually only improve limited kinds of errors (e.g. the count of objects). In this work we propose an intervention-based mechanism to enhance the faithfulness of diffusion models by controlling the denoising process. Starting with layout-conditional diffusion models our approach first detects incorrectly-generated/missing objects during denoising steps. Next a layout is constructed from the erroneous objects (feedback). Finally we return to an earlier denoising step. The new layout is fed to the diffusion model to obtain its latent representation. Correction is applied by composing the new latents with the original ones and continuing the generation process thereby driving the generation away from erroneous directions. As additional feedback and correction strategy we also explore retrieval-augmented generation to help the model recover missing objects. We conduct experiments on VPEval and HRS-Bench datasets and measure faithfulness across four dimensions; presence of objects object counts scale of objects and spatial relations between objects. Compared to GLIGEN the state-of-the-art model on the VPEval dataset our approach significantly improves on all metrics (+6.7% average accuracy increase). On HRS-Bench dataset it also outperforms existing models in count and scale metrics.	https://openaccess.thecvf.com//content/WACV2025/html/Guo_Improving_Faithfulness_of_Text-to-Image_Diffusion_Models_through_Inference_Intervention_WACV_2025_paper.html	Danfeng Guo, Sanchit Agarwal, Yu-Hsiang Lin, Jiun-Yu Kao, Tagyoung Chung, Nanyun Peng, Mohit Bansal
Improving Pelvic MR-CT Image Alignment with Self-Supervised Reference-Augmented Pseudo-CT Generation Framework	RegistFormer our novel reference-augmented image synthesis framework generates aligned pseudo-CT images (with respect to MR) from misaligned MR and CT pairs. RegistFormer addresses the limitations of intensity-based registration methods which often fail due to dissimilar image features and complex deformation fields. Unlike conventional image-to-image (I2I) translation methods our method uses a misaligned CT scan as an auxiliary input to guide the synthesis task through the Deformation-Aware Cross-Attention (DACA) mechanism. DACA integrates the deformation field from a registration method to aggregate spatially matched features from the misaligned CT into MR spatial coordinates. Additionally we propose a novel combination of loss functions for training with datasets of misaligned MR-CT pairs in a self-supervised manner eliminating the need for pre-aligned training data. Experiments were conducted with the synthRAD2023 MR-CT pelvis pair dataset. RegistFormer outperforms past state-of-the-art methods including I2I registration and hybrid (registration + I2I) across metrics evaluating both structure alignment and distribution similarity. Moreover RegistFormer demonstrates superior performance in zero-shot segmentation downstream tasks highlighting its clinical value. Source code: https://github.com/danny4159/RegistFormer	https://openaccess.thecvf.com//content/WACV2025/html/Kim_Improving_Pelvic_MR-CT_Image_Alignment_with_Self-Supervised_Reference-Augmented_Pseudo-CT_Generation_WACV_2025_paper.html	Daniel Kim, Mohammed A. Al-masni, Jaehun Lee, Dong-Hyun Kim, Kanghyun Ryu
Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling	Convolutional neural networks (CNNs) widely deployed in several applications contain downsampling operators in their pooling layers which have been observed to be sensitive to pixel-level shift affecting the robustness of CNNs. We study shift invariance through the lens of maximum sampling bias (MSB) and find MSB to be negatively correlated with shift invariance. Based on this insight we propose a learnable pooling operator called Translation Invariant Polyphase Sampling (TIPS) to reduce MSB and learn translation-invariant representations. TIPS results in consistent performance gains on multiple benchmarks for image classification object detection and semantic segmentation in terms of accuracy shift consistency shift fidelity as well as improvements in adversarial and distributional robustness. TIPS results in the lowest MSB compared to all previous methods thus explaining the strong empirical results. TIPS can be integrated into any CNN and can be trained end-to-end with marginal computational overhead. Code: https://github.com/sourajitcs/tips/	https://openaccess.thecvf.com//content/WACV2025/html/Saha_Improving_Shift_Invariance_in_Convolutional_Neural_Networks_with_Translation_Invariant_WACV_2025_paper.html	Sourajit Saha, Tejas Gokhale
Improving Uncertainty Estimation with Confidence-Aware Training Data	AI-driven second-opinion systems play a crucial role in decision-making especially in medicine where accurate predictions guide clinicians. However quantifying uncertainty in deep learning is challenging as current methods often rely on hard class labels which do not reflect true prediction confidence. This often results in overconfident predictions and slow convergence to true probabilities. To address this we suggest a new method that separates uncertainty into two types: epistemic and aleatoric. We estimate these uncertainties using hard and soft confidence labels with experts providing confidence levels that indicate the likelihood of misclassification. We release an updated blood typing dataset consisting of 3139 images with soft labels of uncertainty annotations from six experts and hard labels collected from medical records. Proposed approach improves SotA uncertainty estimation quality by two times for blood typing (classification) and by 62% for histology (segmentation). The code is available at: https://github.com/createcolor/confidence-aware-uncertainty.	https://openaccess.thecvf.com//content/WACV2025/html/Korchagin_Improving_Uncertainty_Estimation_with_Confidence-Aware_Training_Data_WACV_2025_paper.html	Sergey Korchagin, Ekaterina Zaychenkova, Aleksei Khalin, Aleksandr Yugay, Alexey Zaytsev, Egor Ershov
Improving Zero-Shot Object-Level Change Detection by Incorporating Visual Correspondence	Detecting object-level changes between two images across possibly different views (Fig. 1) is a core task in many applications that involve visual inspection or camera surveillance. Existing change-detection approaches suffer from three major limitations: (1) lack of evaluation on image pairs that contain no changes leading to unreported false positive rates; (2) lack of correspondences (i.e. localizing the regions before and after a change); and (3) poor zero-shot generalization across different domains. To address these issues we introduce a novel method that leverages change correspondences (a) during training to improve change detection accuracy and (b) at test time to minimize false positives. That is we harness the supervision labels of where an object is added or removed to supervise change detectors improving their accuracy over previous work [21] by a large margin. Our work is also the first to predict correspondences between pairs of detected changes using estimated homography and the Hungarian algorithm. Our model demonstrates superior performance over existing methods achieving state-of-the-art results in change detection and change correspondence accuracy across both in-distribution and zero-shot benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Nguyen_Improving_Zero-Shot_Object-Level_Change_Detection_by_Incorporating_Visual_Correspondence_WACV_2025_paper.html	Hung Huy Nguyen, Pooyan Rahmanzadehgervi, Long Mai, Anh Totti Nguyen
InDistill: Information Flow-Preserving Knowledge Distillation for Model Compression	In this paper we introduce InDistill a method that serves as a warmup stage for enhancing Knowledge Distillation (KD) effectiveness. InDistill focuses on transferring critical information flow paths from a heavyweight teacher to a lightweight student. This is achieved via a training scheme based on curriculum learning that considers the distillation difficulty of each layer and the critical learning periods when the information flow paths are established. This procedure can lead to a student model that is better prepared to learn from the teacher. To ensure the applicability of InDistill across a wide range of teacher-student pairs we also incorporate a pruning operation when there is a discrepancy in the width of the teacher and student layers. This pruning operation reduces the width of the teacher's intermediate layers to match those of the student allowing direct distillation without the need for an encoding stage. The proposed method is extensively evaluated using various pairs of teacher-student architectures on CIFAR-10 CIFAR-100 and ImageNet datasets demonstrating that preserving the information flow paths consistently increases the performance of the baseline KD approaches on both classification and retrieval settings. The code is available at https://github.com/gsarridis/InDistill.	https://openaccess.thecvf.com//content/WACV2025/html/Sarridis_InDistill_Information_Flow-Preserving_Knowledge_Distillation_for_Model_Compression_WACV_2025_paper.html	Ioannis Sarridis, Christos Koutlis, Giorgos Kordopatis-Zilos, Yiannis Kompatsiaris, Symeon Papadopoulos
Incorporating Task Progress Knowledge for Subgoal Generation in Robotic Manipulation through Image Edits	Understanding the progress of a task allows humans to not only track what has been done but also to better plan for future goals. We demonstrate TaKSIE a novel framework that incorporates task progress knowledge into visual subgoal generation for robotic manipulation tasks. We jointly train a recurrent network with a latent diffusion model to generate the next visual subgoal based on the robot's current observation and the input language command. At execution time the robot leverages a visual progress representation to monitor the task progress and adaptively samples the next visual subgoal from the model to guide the manipulation policy. We train and validate our model in simulated and real-world robotic tasks achieving state-of-the-art performance on the CALVIN manipulation benchmark. We find that the inclusion of task progress knowledge can improve the robustness of trained policy for different initial robot poses or various movement speeds during demonstrations. The project page is available at https://live-robotics-uva.github.io/TaKSIE/.	https://openaccess.thecvf.com//content/WACV2025/html/Kang_Incorporating_Task_Progress_Knowledge_for_Subgoal_Generation_in_Robotic_Manipulation_WACV_2025_paper.html	Xuhui Kang, Yen-Ling Kuo
Infant Action Generative Modeling	Despite advancements in human motion generation models their performance drops in infant motion generation due to limited data available and lack of 3D skeleton ground truth. To address this we introduce the infant action generation and classification (InfAGenC) pipeline which combines a transformer-based variational autoencoder (VAE) with a spatial-temporal graph convolutional network (STGCN) to create synthetic infant action samples. By iterative refinement of the generative model with diverse and accurate data we improve the realism of synthetic data leading to more precise infant action recognition models. Our results show significant improvements in action recognition performance on real-world data demonstrating that synthetic data can enhance small training datasets and advance infant action recognition. Our pipeline increases action recognition accuracy up to 88.58% on the infant action dataset and up to 98% on an adult action dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_Infant_Action_Generative_Modeling_WACV_2025_paper.html	Xiaofei Huang, Elaheh Hatamimajoumerd, Amal Mathew, Sarah Ostadabbas
Inferring Past Human Actions in Homes with Abductive Reasoning	"Abductive reasoning aims to make the most likely inference for a given set of incomplete observations. In this paper we introduce ""Abductive Past Action Inference"" a novel research task aimed at identifying the past actions performed by individuals within homes to reach specific states captured in a single image using abductive inference. The research explores three key abductive inference problems: past action set prediction past action sequence prediction and abductive past action verification. We introduce several models tailored for abductive past action inference including a relational graph neural network a relational bilinear pooling model and a relational transformer model. Notably the newly proposed object-relational bilinear graph encoder-decoder (BiGED) model emerges as the most effective among all methods evaluated demonstrating good proficiency in handling the intricacies of the Action Genome dataset. The contributions of this research significantly advance the ability of deep learning models to reason about current scene evidence and make highly plausible inferences about past human actions. This advancement enables a deeper understanding of events and behaviors which can enhance decision-making and improve system capabilities across various real-world applications such as Human-Robot Interaction and Elderly Care and Health Monitoring. Code and data available at https://github.com/LUNAProject22/AAR"	https://openaccess.thecvf.com//content/WACV2025/html/Tan_Inferring_Past_Human_Actions_in_Homes_with_Abductive_Reasoning_WACV_2025_paper.html	Clement Tan, Chai Kiat Yeo, Cheston Tan, Basura Fernando
Information Extraction from Heterogeneous Documents without Ground Truth Labels using Synthetic Label Generation and Knowledge Distillation	Invoices and receipts submitted by employees are visually rich documents (VRDs) with textual visual and layout information. To protect against the risk of fraud and abuse it is crucial for organizations to efficiently extract desired information from submitted receipts. This helps in the assessment of key factors such as appropriateness of the expense claim adherence to spending and transaction policies the validity of the receipt as well as downstream anomaly detection at various levels. These documents are heterogeneous with multiple formats and languages uploaded with different image qualities and often do not contain ground truth labels for the efficient training of models. In this paper we propose Task Aware Instruction-based Labelling (TAIL) a method for synthetic label generation in VRD corpuses without labels and fine-tune a multimodal Visually Rich Document Understanding Model (VRDU) on TAIL labels using response-based knowledge distillation without using the teacher model's weights or training dataset to conditionally generate annotations in the appropriate format. Using a benchmark external dataset where ground truth labels are available we demonstrate conditions under which our approach performs at par with Claude 3 Sonnet through empirical studies. We then show that the resulting model performs at par or better on the internal expense documents of a large multinational organization than state-of-the-art LMM (large multimodal model) Claude 3 Sonnet while being 85% less costly and 5X faster and outperforms layout-aware baselines by more than 10% in Average Normalized Levenshtein Similarity (ANLS) scores due to its ability to reason and extract information from rare formats. Finally we illustrate the usage of our approach in overpayment prevention.	https://openaccess.thecvf.com//content/WACV2025/html/Bhattacharyya_Information_Extraction_from_Heterogeneous_Documents_without_Ground_Truth_Labels_using_WACV_2025_paper.html	Aniket Bhattacharyya, Anurag Tripathi
Information Theoretic Pruning of Coupled Channels in Deep Neural Networks	Variational channel pruning approaches have obtained impressive results thanks to their stochastic nature well established foundation in information theory and the practically appealing structured sparsity pattern they offer. Despite their success in pruning Plain Networks (PlainNets) their application has faced certain limitations in networks with structurally coupled channels such as ResNets. In such scenarios not only is it required to prune structurally coupled channels together but it is also necessary to ensure that the whole coupled group is irrelevant before pruning is applied. This is an under-investigated problem as most existing methods are designed without taking these couplings into account. In this paper we propose a novel approach based on Information Theoretic Pruning of structurally Coupled Channels (ITPCC) in neural networks. ITPCC allows for learning the probabilistic distribution of coupled channel set importance and prunes the ones withthe least relevant information to the task at hand. Experimental results for image classification on CIFAR10 CIFAR100 and ImageNet datasets show that the proposed method outperforms the state-of-the-art more significantly at high compression rates.	https://openaccess.thecvf.com//content/WACV2025/html/Rostami_Information_Theoretic_Pruning_of_Coupled_Channels_in_Deep_Neural_Networks_WACV_2025_paper.html	Peyman Rostami, Nilotpal Sinha, Nidhaleddine Chenni, Anis Kacem, Abd El Rahman Shabayek, Carl Shneider, Djamila Aouada
Instance-Warp: Saliency Guided Image Warping for Unsupervised Domain Adaptation	Driving is challenging in conditions like night rain and snow. Lack of good labeled datasets has hampered progress in scene understanding under such conditions. Unsupervised Domain Adaptation (UDA) using large labeled clear-day datasets is a promising research direction in such cases. However many UDA methods are trained with dominant scene backgrounds (e.g. roads sky sidewalks) that appear dramatically different across domains. As a result they struggle to learn effective features of smaller and often sparse foreground objects (e.g. people vehicles signs). In this work we improve UDA training by applying in-place image warping to focus on salient objects. We design instance-level saliency guidance to adaptively oversample object regions and undersample background areas which reduces adverse effects from background context and enhances backbone feature learning. Our approach improves adaptation across geographies lighting and weather conditions and is agnostic to the task (segmentation detection) domain adaptation algorithm saliency guidance and underlying model architecture. Result highlights include +6.1 mAP50 for BDD100K Clear to DENSE Foggy +3.7 mAP50 for BDD100K Day to Night +3.0 mAP50 for BDD100K Clear to Rainy and +6.3 mIoU for Cityscapes to ACDC. Besides Our method adds minimal training memory and no additional inference latency. Code is available at https://github.com/ShenZheng2000/Instance-Warp.	https://openaccess.thecvf.com//content/WACV2025/html/Zheng_Instance-Warp_Saliency_Guided_Image_Warping_for_Unsupervised_Domain_Adaptation_WACV_2025_paper.html	Shen Zheng, Anurag Ghosh, Srinivasa Narasimhan
Instructive3D: Editing Large Reconstruction Models with Text Instructions	Transformer based methods have enabled users to create modify and comprehend text and image data. Recently proposed Large Reconstruction Models (LRMs) further extend this by providing the ability to generate high-quality 3D models with the help of a single object image. These models however lack the ability to manipulate or edit the finer details such as adding standard design patterns or changing the color and reflectance of the generated objects thus lacking fine-grained control that may be very helpful in domains such as augmented reality animation and gaming. Naively training LRMs for this purpose would require generating precisely edited images and 3D object pairs which is computationally expensive. In this paper we propose Instructive3D a novel LRM based model that integrates generation and fine-grained editing through user text prompts of 3D objects into a single model. We accomplish this by adding an adapter that performs a diffusion process conditioned on a text prompt specifying edits in the triplane latent space representation of 3D object models. Our method does not require the generation of edited 3D objects. Additionally Instructive3D allows us to perform geometrically consistent modifications as the edits done through user-defined text prompts are applied to the triplane latent representation thus enhancing the versatility and precision of 3D objects generated. We compare the objects generated by Instructive3D and a baseline that first generates the 3D object meshes using a standard LRM model and then edits these 3D objects using text prompts when images are provided from the Objaverse LVIS dataset. We find that Instructive3D produces qualitatively superior 3D objects with the properties specified by the edit prompts.	https://openaccess.thecvf.com//content/WACV2025/html/Kathare_Instructive3D_Editing_Large_Reconstruction_Models_with_Text_Instructions_WACV_2025_paper.html	Kunal Kathare, Ankit Dhiman, K Vikas Gowda, Siddharth Aravindan, Shubham Monga, Basavaraja Shanthappa Vandrotti, Lokesh R Boregowda
Interactive Object Detection for Tiny Objects in Large Remotely Sensed Images	This paper highlights the potential of a Human-In-the-Loop (HIL) in interactive object detection methods. Although automation in computer vision is advancing rapidly certain critical tasks such as detecting UneXploded Ordnance (UXO) space/marine debris or the generation of new datasets require 100% recall and near-perfect precision. These tasks are often performed manually since automatic methods do not achieve the necessary accuracy. However interactive object detection frameworks can potentially enhance annotation speed while maintaining the recall and accuracy of manual annotation. We propose IRTDETR an interactive and real-time object detection method for very large imagery to address this. Using either point or bounding box annotations provided by a HIL it globally relates the full image with the annotator inputs via a cross-attention-like mechanism employs an attention loss to maximize the classification score based on similarity and reuses portions of the network outputs during iterative refinements to conserve resources. We conduct experiments on five different datasets (Tiny-DOTA CHAI AITOD SarDET and COCO) to verify the efficacy of our approach. Our method surpasses existing interactive annotation approaches achieving a higher mean Average Precision (mAP) with the same number of clicks. Additionally we validate the annotation efficiency of our method in a user study demonstrating it is 2.46x quicker and asks for only 72% of the task load (NASA-TLX) compared to fully manual annotation. The code will is available under https://github.com/mburges-cvl/WACV_IAODF.	https://openaccess.thecvf.com//content/WACV2025/html/Burges_Interactive_Object_Detection_for_Tiny_Objects_in_Large_Remotely_Sensed_WACV_2025_paper.html	Marvin Burges, Sebastian Zambanini, Robert Sablatnig
Invariant Shape Representation Learning for Image Classification	Geometric shape features have been widely used as strong predictors for image classification. Nevertheless most existing classifiers such as deep neural networks (DNNs) directly leverage the statistical correlations between these shape features and target variables. However these correlations can often be spurious and unstable across different environments (e.g. in different age groups certain types of brain changes have unstable relations with neurodegenerative disease); hence leading to biased or inaccurate predictions. In this paper we introduce a novel framework that for the first time develops invariant shape representation learning (ISRL) to further strengthen the robustness of image classifiers. In contrast to existing approaches that mainly derive features in the image space our model ISRL is designed to jointly capture invariant features in latent shape spaces parameterized by deformable transformations. To achieve this goal we develop a new learning paradigm based on invariant risk minimization (IRM) to learn invariant representations of image and shape features across multiple training distributions/environments. By embedding the features that are invariant with regard to target variables in different environments our model consistently offers more accurate predictions. We validate our method by performing classification tasks on both simulated 2D images real 3D brain and cine cardiovascular magnetic resonance images (MRIs). Our code is publicly available at https://github.com/tonmoy-hossain/ISRL.	https://openaccess.thecvf.com//content/WACV2025/html/Hossain_Invariant_Shape_Representation_Learning_for_Image_Classification_WACV_2025_paper.html	Tonmoy Hossain, Jing Ma, Jundong Li, Miaomiao Zhang
Inverse Problems with Diffusion Models: A MAP Estimation Perspective	Inverse problems have many applications in science and engineering. In Computer vision several image restoration tasks such as inpainting deblurring and super-resolution can be formally modeled as inverse problems. Recently methods have been developed for solving inverse problems that only leverage a pre-trained unconditional diffusion model and do not require additional task-specific training. In such methods however the inherent intractability of determining the conditional score function during the reverse diffusion process poses a real challenge leaving the methods to settle with an approximation instead which affects their performance in practice. Here we propose a MAP estimation framework to model the reverse conditional generation process of a continuous time diffusion model as an optimization process of the underlying MAP objective whose gradient term is tractable. In theory the proposed framework can be applied to solve general inverse problems using gradient-based optimization methods. However given the highly non-convex nature of the loss objective finding a perfect gradient-based optimization algorithm can be quite challenging nevertheless our framework offers several potential research directions. We use our proposed formulation to develop empirically effective algorithms for image restoration. We validate our proposed algorithms with extensive experiments over multiple datasets across several restoration tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Gutha_Inverse_Problems_with_Diffusion_Models_A_MAP_Estimation_Perspective_WACV_2025_paper.html	Sai Bharath Chandra Gutha, Ricardo Vinuesa, Hossein Azizpour
Inverting the Generation Process of Denoising Diffusion Implicit Models: Empirical Evaluation and a Novel Method	This paper studies the problem of inverting the DDIM image generation process to recover latent variables particularly the initial noise map from a generated image. Existing methods often struggle with accuracy in this task. We propose a novel hybrid approach that combines direct inversion via gradient descent for the first step followed by a fixed-point method for subsequent steps. Empirical evaluations across three datasets demonstrate that our method significantly improves the prediction of initial latent variables while achieving superior reconstruction accuracy. Additionally we introduce a new evaluation called the self-interpolation test which assesses the quality of images generated from interpolated points between the true and predicted latent maps offering deeper insights into performance. Our results reveal that while existing methods perform reasonably well in reconstruction they consistently fail to accurately predict the initial latent variables resulting in poor performance on the self-interpolation test. In contrast our method outperforms all others across all metrics providing valuable insights into diffusion models and enhancing their applications in image generation and editing.	https://openaccess.thecvf.com//content/WACV2025/html/Zeng_Inverting_the_Generation_Process_of_Denoising_Diffusion_Implicit_Models_Empirical_WACV_2025_paper.html	Yan Zeng, Masanori Suganuma, Takayuki Okatani
Investigating Imaging Annotation and Self-Supervision for the Classification of Continuously Developing Cells in Histological Whole Slide Images	The analysis of individual cells is increasingly automated through deep learning techniques. This is particularly relevant for high-resolution whole slide images (WSIs) which can contain thousands of cells making manual evaluation impractical. This increase in automation however requires higher levels of standardisation (with respect to the scanning hardware settings and staining) and is further aggravated by the dynamics of the underlying cellular processes rendering unique cell classifications difficult. To address these difficulties we investigated the entire processing pipeline (from imaging over annotation to model training) and study its underlying trade-offs. In particular we created a new dataset comprising of more than 6300 labelled and 500000 unlabelled cells scanned using two different scan settings resulting in fully registered image pairs with varying level of detail and quality. Using these alternative dataset versions we analysed the impact of inter- and intra-variability between three different annotators and addressed the challenge of limited labelled data by comparing the impact of different self-supervised pretraining strategies. Overall our analyses provide new insights into the dependencies between imaging annotation self-supervision and deep learning-based classification especially in the context of continuously developing cells and demonstrate the beneficial impact of these considerations on the overall classification accuracy. Code is available at https://zivgitlab.uni-muenster.de/cvmls/icdc and the data will be shared upon qualified request due to data privacy laws.	https://openaccess.thecvf.com//content/WACV2025/html/Thiele_Investigating_Imaging_Annotation_and_Self-Supervision_for_the_Classification_of_Continuously_WACV_2025_paper.html	Sebastian Thiele, Jacqueline Kockwelp, Joachim Wistuba, Sabine Kliesch, JÃ¶rg Gromoll, Benjamin Risse
InvisMark: Invisible and Robust Watermarking for AI-Generated Image Provenance	The proliferation of AI-generated images has intensified the need for robust content authentication methods. We present InvisMark a novel watermarking technique designed for high-resolution AI-generated images. Our approach leverages advanced neural network architectures and training strategies to embed imperceptible yet highly robust watermarks. InvisMark achieves state-of-the-art performance in imperceptibility (PSNR 51 SSIM 0.998) while maintaining over 97% bit accuracy across various image manipulations. Notably we demonstrate the successful encoding of 256-bit watermarks significantly expanding payload capacity while preserving image quality. This enables the embedding of UUIDs with error correction codes achieving near-perfect decoding success rates even under challenging image distortions. We also address potential vulnerabilities against advanced attacks and propose mitigation strategies. By combining high imperceptibility extended payload capacity and resilience to manipulations InvisMark provides a robust foundation for ensuring media provenance in an era of increasingly sophisticated AI-generated content.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_InvisMark_Invisible_and_Robust_Watermarking_for_AI-Generated_Image_Provenance_WACV_2025_paper.html	Rui Xu, Mengya Hu, Deren Lei, Yaxi Li, David Lowe, Alex Gorevski, Mingyu Wang, Emily Ching, Alex Deng
J-Invariant Volume Shuffle for Self-Supervised Cryo-Electron Tomogram Denoising on Single Noisy Volume	Cryo-Electron Tomography (Cryo-ET) enables detailed 3D visualization of cellular structures in near-native states but suffers from low signal-to-noise ratio due to imaging constraints. Traditional denoising methods and supervised learning approaches often struggle with complex noise patterns and the lack of paired datasets. Self-supervised methods which utilize noisy input itself as a target have been studied; however existing Cryo-ET self-supervised denoising methods face significant challenges due to losing information during training and the learned incomplete noise patterns. In this paper we propose a novel self-supervised learning model that denoises Cryo-ET volumetric images using a single noisy volume. Our method features a U-shape J-invariant blind spot network with sparse centrally masked convolutions dilated channel attention blocks and volume-unshuffle/shuffle technique. The volume-unshuffle/shuffle technique expands receptive fields and utilizes multi-scale representations significantly improving noise reduction and structural preservation. Experimental results demonstrate that our approach achieves superior performance compared to existing methods advancing Cryo-ET data processing for structural biology research.	https://openaccess.thecvf.com//content/WACV2025/html/Liu_J-Invariant_Volume_Shuffle_for_Self-Supervised_Cryo-Electron_Tomogram_Denoising_on_Single_WACV_2025_paper.html	Xiwei Liu, Mohamad Kassab, Min Xu, Qirong Ho
Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters	Recent advances in co-speech gesture and talking head generation have been impressive yet most methods focus on only one of the two tasks. Those that attempt to generate both often rely on separate models or network modules increasing training complexity and ignoring the inherent relationship between face and body movements. To address the challenges in this paper we propose a novel model architecture that jointly generates face and body motions within a single network. This approach leverages shared weights between modalities facilitated by adapters that enable adaptation to a common latent space. Our experiments demonstrate that the proposed framework not only maintains state-of-the-art co-speech gesture and talking head generation performance but also significantly reduces the number of parameters required.	https://openaccess.thecvf.com//content/WACV2025/html/Hogue_Joint_Co-Speech_Gesture_and_Expressive_Talking_Face_Generation_using_Diffusion_WACV_2025_paper.html	Steven Hogue, Chenxu Zhang, Yapeng Tian, Xiaohu Guo
Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models	Advancements in vision-language models (VLMs) have propelled the field of computer vision particularly in the zero-shot learning setting. Despite their promise the effectiveness of these models often diminishes due to domain shifts in test environments. To address this we introduce the Test-Time Prototype Shifting (TPS) framework a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time TPS dynamically learns shift vectors for each prototype based solely on the given test sample effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 image classification datasets involving natural distribution shifts and cross-dataset generalization as well as in context-dependent visual reasoning demonstrate TPS's superior performance achieving state-of-the-art results while reducing resource requirements. Code is available at https://github.com/elaine-sui/TPS.	https://openaccess.thecvf.com//content/WACV2025/html/Sui_Just_Shift_It_Test-Time_Prototype_Shifting_for_Zero-Shot_Generalization_with_WACV_2025_paper.html	Elaine Sui, Xiaohan Wang, Serena Yeung-Levy
KDC-MAE: Knowledge Distilled Contrastive Mask Auto-Encoder	In this work we attempted to extend the thought and showcase a way forward for the Self-supervised Learning (SSL) learning paradigm by combining contrastive learning self-distillation (knowledge distillation) and masked data modelling the three major SSL frameworks to learn a joint and coordinated representation. The proposed technique of SSL learns by the collaborative power of different learning objectives of SSL. Hence to jointly learn the different SSL objectives we proposed a new SSL architecture KDC-MAE a complementary masking strategy to learn the modular correspondence and a weighted way to combine them coordinately. Experimental results conclude that the contrastive masking correspondence along with the KD learning objective has lent a hand to performing better learning for multiple modalities over multiple tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Bora_KDC-MAE_Knowledge_Distilled_Contrastive_Mask_Auto-Encoder_WACV_2025_paper.html	Maheswar Bora, Saurabh Atreya, Aritra Mukherjee, Abhijit Das
Knockoff Branch: Model Stealing Attack via Adding Neurons in the Pre-Trained Model	We introduce Knockoff Branch: adding few neurons as a knockoff container for learning stolen features. Model stealing attacks extract the functionality from the victim model by querying APIs. Prior work substantially enhanced transferability and improved query efficiency between the adversary model and the victim model. However there is still a limited understanding of the knockoff itself. For knockoff the model is either compared to the same type but with different structures or different types and capacities. For this reason we propose a framework to analyze the knockoff quality for a single model specifically reinvestigating transformer-based extraction. We observed that 1) when the adversary can access the public pretrained model full fine-tuning is not necessary. This allows a knockoff to require only about 0.5% of trainable parameters and 20 epochs. 2) Although querying by out-of-distribution datasets leads to a sub-optimal knockoff this issue can be mitigated by scaling branch features even without using complicated sampling strategies. Our proposed method is lightweight and achieves high accuracy at most similar to white-box knowledge distillation (higher performance than the victim model). https://github.com/onlyin-hung/knockoff-branch.	https://openaccess.thecvf.com//content/WACV2025/html/Hung_Knockoff_Branch_Model_Stealing_Attack_via_Adding_Neurons_in_the_WACV_2025_paper.html	Li-Ying Hung, Cooper Cheng-Yuan Ku
LIME: Localized Image Editing via Attention Regularization in Diffusion Models	Diffusion models (DMs) have gained prominence due to their ability to generate high-quality varied images with recent advancements in text-to-image generation. The research focus is now shifting towards the controllability of DMs. A significant challenge within this domain is localized editing where specific areas of an image are modified without affecting the rest of the content. This paper introduces LIME for localized image editing in diffusion models. LIME does not require user-specified regions of interest (RoI) or additional text input but rather employs features from pre-trained methods and a straightforward clustering method to obtain precise editing mask. Then by leveraging cross-attention maps it refines these segments for finding regions to obtain localized edits. Finally we propose a novel cross-attention regularization technique that penalizes unrelated cross-attention scores in the RoI during the denoising steps ensuring localized edits. Our approach without re-training fine-tuning and additional user inputs consistently improves the performance of existing methods in various editing benchmarks. The project page can be found at https://enisimsar.github.io/LIME/.	https://openaccess.thecvf.com//content/WACV2025/html/Simsar_LIME_Localized_Image_Editing_via_Attention_Regularization_in_Diffusion_Models_WACV_2025_paper.html	Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari
LIPIDS: Learning-Based Illumination Planning in Discretized (Light) Space for Photometric Stereo	Photometric stereo is a powerful technique for estimating per-pixel surface normals from images under varied illumination. Although several methods address photometric stereo with different image (or light) counts ranging from one to two to a hundred very few focus on learning optimal lighting configuration. Finding an optimal configuration is challenging due to the large number of possible lighting directions. Moreover exhaustive sampling of all possibilities is impractical due to time and resource constraints. Photometric stereo methods have demonstrated promising performance on existing datasets which feature limited light directions sparsely sampled from the light space. Therefore can we optimally utilize these datasets for illumination planning? In this work we introduce LIPIDS - Learning-based Illumination Planning In Discretized light Space to achieve minimal and optimal lighting configurations for photometric stereo under arbitrary light distribution. We propose a Light Sampling Network (LSNet) that optimizes the lighting direction for a fixed number of lights by minimizing the normal loss through a normal regression network. The learned light configurations can directly estimate surface normals during inference even using an off-the-shelf photometric stereo method. Extensive qualitative and quantitative analysis on synthetic and real-world datasets show that photometric stereo under learned lighting configurations through LIPIDS either surpasses or is nearly comparable to existing illumination planning methods across different photometric stereo backbones.	https://openaccess.thecvf.com//content/WACV2025/html/Tiwari_LIPIDS_Learning-Based_Illumination_Planning_in_Discretized_Light_Space_for_Photometric_WACV_2025_paper.html	Ashish Tiwari, Mihirkumar Sutariya, Shanmuganathan Raman
LLM-Generated Rewrite and Context Modulation for Enhanced Vision Language Models in Digital Pathology	Recent advancements in vision-language models (VLMs) have found important applications in medical imaging particularly in digital pathology. VLMs demand large-scale datasets of image-caption pairs which is often hard to obtain in medical domains. State-of-the-art VLMs in digital pathology have been pre-trained on datasets that are significantly smaller than their computer vision counterparts. Furthermore the caption of a pathology slide often refers to a small sub-set of features in the image--an important point that is ignored in existing VLM pre-training schemes. Another important issue that is under-appericated is that the performance of state-of-the-art VLMs in zero-shot classification tasks can be sensitive to the choice of the prompts. In this paper we first employ language rewrites using a large language model (LLM) to enrich a public pathology image-caption dataset and make it publicly available. Our extensive experiments demonstrate that by training with language rewrites we can boost the performance of a state-of-the-art digital pathology VLM on downstream tasks such as zero-shot classification and text-to-image and image-to-text retrieval. We further leverage LLMs to demonstrate the sensitivity of zero-shot classification results to the choice of prompts and propose a scalable approach to characterize this when comparing models. Finally we present a novel context modulation layer that adjusts the image embeddings for better aligning with the paired text and use context-specific language rewrites for training this layer. In our results we show that the proposed context modulation framework can further yield substantial performance gains.	https://openaccess.thecvf.com//content/WACV2025/html/Bahadir_LLM-Generated_Rewrite_and_Context_Modulation_for_Enhanced_Vision_Language_Models_WACV_2025_paper.html	Cagla Deniz Bahadir, Gozde B. Akar, Mert R. Sabuncu
LLM-RSPF: Large Language Model-Based Robotic System Planning Framework for Domain Specific Use-Cases	The employment of large language models (LLMs) for task planning and reasoning has emerged as a focal point of interest within the robotics research community. However directly applying LLMs even with large token-sized prompts does not achieve the task planning performance required for an industrial-grade domain-specific use-case (DSU). This work aims to overcome the obstacles of a robotic task planner for DSUs by introducing a novel planning framework LLM-RSPF (Large Language Model-based Robotic System Planning Framework). Central to the LLM-RSPF is a novel robotic system ontology that organizes the components of the robotic system in a coherent and a systematic manner. The ontology empowers the LLM-RSPF to efficiently capture a contextual representation of the DSU using the LLMs. Subsequently the research introduces a LLM-tuning regimen referred as chain of hierarchical thought (CoHT) specifically crafted to complement the proposed system ontology. Integrating these two components the LLM-RSPF aims to enhance the accuracy robustness and throughput of a robotic system in a cost-effective manner. In addition the research presents an empirical methodology to generate the LLM-tuning dataset size for a guaranteed performance. The LLM-RSPF is validated on a retail order-fulfillment use-case thereby illustrating the efficacy of the framework. Through rigorous evaluation the LLM-RSPF demonstrates exceptional performance on the generated dataset effectively meeting the DSU objectives.	https://openaccess.thecvf.com//content/WACV2025/html/Singh_LLM-RSPF_Large_Language_Model-Based_Robotic_System_Planning_Framework_for_Domain_WACV_2025_paper.html	Chandan Kumar Singh, Devesh Kumar, Vipul Sanap, Rajesh Sinha
LLS: Local Learning Rule for Deep Neural Networks Inspired by Neural Activity Synchronization	Training deep neural networks (DNNs) using traditional backpropagation (BP) presents challenges in terms of computational complexity and energy consumption particularly for on-device learning where computational resources are limited. Various alternatives to BP including random feedback alignment forward-forward and local classifiers have been explored to address these challenges. These methods have their advantages but they can encounter difficulties when dealing with intricate visual tasks or demand considerable computational resources. In this paper we propose a novel Local Learning rule inspired by neural activity Synchronization phenomena (LLS) observed in the brain. LLS utilizes fixed periodic basis vectors to synchronize neuron activity within each layer enabling efficient training without the need for additional trainable parameters. We demonstrate the effectiveness of LLS and its variations LLS-M and LLS-MxM on multiple image classification datasets achieving accuracy comparable to BP with reduced computational complexity and minimal additional parameters. Specifically LLS achieves comparable performance with up to 300x fewer multiply-accumulate (MAC) operations and half the memory requirements of BP. Furthermore the performance of LLS on the Visual Wake Word (VWW) dataset highlights its suitability for on-device learning tasks making it a promising candidate for edge hardware implementations.	https://openaccess.thecvf.com//content/WACV2025/html/Apolinario_LLS_Local_Learning_Rule_for_Deep_Neural_Networks_Inspired_by_WACV_2025_paper.html	Marco P. E. Apolinario, Arani Roy, Kaushik Roy
LLaVA-SpaceSGG: Visual Instruct Tuning for Open-Vocabulary Scene Graph Generation with Enhanced Spatial Relations	Scene Graph Generation (SGG) converts visual scenes into structured graph representations providing deeper scene understanding for complex vision tasks. However existing SGG models often overlook essential spatial relationships and struggle with generalization in open-vocabulary contexts. To address these limitations we propose LLaVASpaceSGG a multimodal large language model (MLLM) designed for open-vocabulary SGG with enhanced spatial relation modeling. To train it we collect the SGG instruction-tuning dataset named SpaceSGG. This dataset is constructed by combining publicly available datasets and synthesizing data using open-source models within our data construction pipeline. It combines object locations object relations and depth information resulting in three data formats: spatial SGG description question-answering and conversation. To enhance the transfer of MLLMs' inherent capabilities to the SGG task we introduce a two-stage training paradigm. Experiments show that LLaVASpaceSGG outperforms other open-vocabulary SGG methods boosting recall by 8.6% and mean recall by 28.4% compared to the baseline. Our codebase dataset and trained models are publicly accessible on GitHub at the following URL: https://github.com/Endlinc/LLaVA-SpaceSGG.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_LLaVA-SpaceSGG_Visual_Instruct_Tuning_for_Open-Vocabulary_Scene_Graph_Generation_with_WACV_2025_paper.html	Mingjie Xu, Mengyang Wu, Yuzhi Zhao, Jason Chun Lok Li, Weifeng Ou
LORD: Large Models Based Opposite Reward Design for Autonomous Driving	"Reinforcement learning (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches. However crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios. Recently large pretrained models have gained significant attention as zero-shot reward models for tasks specified with desired linguistic goals. However the desired linguistic goals for autonomous driving such as ""drive safely"" are ambiguous and incomprehensible by pretrained models. On the other hand undesired linguistic goals like ""collision"" are more concrete and tractable. In this work we introduce LORD a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as zero-shot reward models. Through extensive experiments our proposed framework shows its efficiency in leveraging the power of large pretrained models for achieving safe and enhanced autonomous driving. Moreover the proposed approach shows improved generalization capabilities as it outperforms counterpart methods across diverse and challenging driving scenarios."	https://openaccess.thecvf.com//content/WACV2025/html/Ye_LORD_Large_Models_Based_Opposite_Reward_Design_for_Autonomous_Driving_WACV_2025_paper.html	Xin Ye, Feng Tao, Abhirup Mallik, Burhaneddin Yaman, Liu Ren
LQ-Adapter: ViT-Adapter with Learnable Queries for Gallbladder Cancer Detection from Ultrasound Images	We focus on the problem of Gallbladder Cancer (GBC) detection from Ultrasound (US) images. The problem presents unique challenges to modern Deep Neural Network (DNN) techniques due to low image quality arising from noise textures and viewpoint variations. Tackling such challenges would necessitate precise localization performance by the DNN to identify the discerning features for the downstream malignancy prediction. While several techniques have been proposed in the recent years for the problem all of these methods employ complex custom architectures. Inspired by the success of foundational models for natural image tasks along with the use of adapters to fine-tune such models for the custom tasks we investigate the merit of one such design ViT-Adapter for the GBC detection problem. We observe that ViT-Adapter relies predominantly on a primitive CNN-based spatial prior module to inject the localization information via cross-attention which is inefficient for our problem due to the small pathology sizes and variability in their appearances due to non-regular structure of the malignancy. In response we propose LQ-Adapter a modified Adapter design for ViT which improves localization information by leveraging learnable content queries over the basic spatial prior module. Our method surpasses existing approaches enhancing the mean IoU (mIoU) scores by 5.4% 5.8% and 2.7% over ViT-Adapters DINO and FocalNet-DINO respectively on the US image-based GBC detection dataset and establishing a new state-of-the-art (SOTA). Additionally we validate the applicability and effectiveness of LQ-Adapter on the Kvasir-Seg dataset for polyp detection from colonoscopy images. Superior performance of our design on this problem as well showcases its capability to handle diverse medical imaging tasks across different datasets. Source code and trained models are publicly released.	https://openaccess.thecvf.com//content/WACV2025/html/Madan_LQ-Adapter_ViT-Adapter_with_Learnable_Queries_for_Gallbladder_Cancer_Detection_from_WACV_2025_paper.html	Chetan Madan, Mayuna Gupta, Soumen Basu, Pankaj Gupta, Chetan Arora
Label Augmented Dataset Distillation	Traditional dataset distillation primarily focuses on image representation while often overlooking the important role of labels. In this study we introduce Label-Augmented Dataset Distillation (LADD) a new dataset distillation framework enhancing dataset distillation with label augmentations. LADD sub-samples each synthetic image generating additional dense labels to capture rich semantics. These dense labels require only a 2.5% increase in storage (ImageNet subsets) with significant performance benefits providing strong learning signals. Our label-generation strategy can complement existing dataset distillation methods and significantly enhance their training efficiency and performance. Experimental results demonstrate that LADD outperforms existing methods in terms of computational overhead and accuracy. With three high-performance dataset distillation algorithms LADD achieves remarkable gains by an average of 14.9% in accuracy. Furthermore the effectiveness of our method is proven across various datasets distillation hyperparameters and algorithms. Finally our method improves the cross-architecture robustness of the distilled dataset which is important in the application scenario.	https://openaccess.thecvf.com//content/WACV2025/html/Kang_Label_Augmented_Dataset_Distillation_WACV_2025_paper.html	Seoungyoon Kang, Youngsun Lim, Hyunjung Shim
Label Calibration in Source Free Domain Adaptation	Source-free domain adaptation (SFDA) utilizes a pre-trained source model with unlabeled target data. Self-supervised SFDA techniques generate pseudolabels from the pre-trained source model but these pseudolabels often contain noise due to domain discrepancies between the source and target domains. Traditional self-supervised SFDA techniques rely on deterministic model predictions using the softmax function leading to unreliable pseudolabels. In this work we propose to introduce predictive uncertainty and softmax calibration for pseudolabel refinement using evidential deep learning. The Dirichlet prior is placed over the output of the target network to capture uncertainty using evidence with a single forward pass. Furthermore softmax calibration solves the translation invariance problem to assist in learning with noisy labels. We incorporate a combination of evidential deep learning loss and information maximization loss with calibrated softmax in both prior and non-prior target knowledge SFDA settings. Extensive experimental analysis shows that our method outperforms other state-of-the-art methods on benchmark datasets. The code is available at https://visdomlab.github.io/EKS/.	https://openaccess.thecvf.com//content/WACV2025/html/Rai_Label_Calibration_in_Source_Free_Domain_Adaptation_WACV_2025_paper.html	Shivangi Rai, Rini Smita Thakur, Kunal Jangid, Vinod K Kurmi
Label Convergence: Defining an Upper Performance Bound in Object Recognition through Contradictory Annotations	"Annotation errors are a challenge not only during training of machine learning models but also during their evaluation. Label variations and inaccuracies in datasets often manifest as contradictory examples that deviate from established labeling conventions. Such inconsistencies when significant prevent models from achieving optimal performance on metrics such as mean Average Precision (mAP). We introduce the notion of ""label convergence"" to describe the highest achievable performance under the constraint of contradictory test annotations essentially defining an upper bound on model accuracy. Recognizing that noise is an inherent characteristic of all data our study analyzes five real-world datasets including the LVIS dataset to investigate the phenomenon of label convergence. We approximate that label convergence is between 62.63-67.52 mAP@[0.5:0.95:0.05] for LVIS with 95% confidence attributing these bounds to the presence of real annotation errors. With current state-of-the-art (SOTA) models at the upper end of the label convergence interval for the well-studied LVIS dataset we conclude that model capacity is sufficient to solve current object detection problems. Therefore future efforts should focus on three key aspects: (1) updating the problem specification and adjusting evaluation practices to account for unavoidable label noise (2) creating cleaner data especially test data and (3) including multi-annotated data to investigate annotation variation and make these issues visible from the outset."	https://openaccess.thecvf.com//content/WACV2025/html/Tschirschwitz_Label_Convergence_Defining_an_Upper_Performance_Bound_in_Object_Recognition_WACV_2025_paper.html	David Eike Tschirschwitz, Volker Rodehorst
Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation	The increasing relevance of panoptic segmentation is tied to the advancements in autonomous driving and AR/VR applications. However the deployment of such models has been limited due to the expensive nature of dense data annotation giving rise to unsupervised domain adaptation (UDA). A key challenge in panoptic UDA is reducing the domain gap between a labeled source and an unlabeled target domain while harmonizing the subtasks of semantic and instance segmentation to limit catastrophic interference. While considerable progress has been achieved existing approaches mainly focus on the adaptation of semantic segmentation. In this work we focus on incorporating instance-level adaptation via a novel instance-aware cross-domain mixing strategy IMix. IMix significantly enhances the panoptic quality by improving instance segmentation performance. Specifically we propose inserting high-confidence predicted instances from the target domain onto source images retaining the exhaustiveness of the resulting pseudo-labels while reducing the injected confirmation bias. Nevertheless such an enhancement comes at the cost of degraded semantic performance attributed to catastrophic forgetting. To mitigate this issue we regularize our semantic branch by employing CLIP-based domain alignment (CDA) exploiting the domain-robustness of natural language prompts. Finally we present an end-to-end model incorporating these two mechanisms called LIDAPS achieving state-of-the-art results on all popular panoptic UDA benchmarks. https://github.com/elhamAm/LIDAPS	https://openaccess.thecvf.com//content/WACV2025/html/Mansour_Language-Guided_Instance-Aware_Domain-Adaptive_Panoptic_Segmentation_WACV_2025_paper.html	Elham Amin Mansour, Ozan Unal, Suman Saha, Benjamin Bejar, Luc Van Gool
Latency Robust Cooperative Perception using Asynchronous Feature Fusion	Recent advancements in cooperative perception have showcased substantial improvements compared to single-agent perception. Nonetheless the inherent latency present in such systems can dramatically impair their effectiveness. In this paper we propose a Latency Robust Cooperative Perception framework named LRCP to compensate for the effect of temporal asynchrony. The intuition of LRCP is to directly fuse asynchronous bird's-eye view (BEV) features instead of estimating aligned features. To achieve this we first propose a novel flow prediction module that uses cached past BEV features to predict the flow with a non-discrete time delay at the BEV feature level. Then the predicted flow is employed to guide the spatial sampling location of interests. Our approach substantially enhances the robustness of temporal asynchronous cooperative perception. Specifically we achieved robust performance across a range of latencies up to 500 ms with a performance degradation of only 1 percent point for AP@0.5 metric and 4 percent points for AP@0.7 metric at 500 ms on two public datasets (V2X-Sim and Dair-V2X). Code to reproduce our results is available at https://github. com/JesseWong333/LRCP.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_Latency_Robust_Cooperative_Perception_using_Asynchronous_Feature_Fusion_WACV_2025_paper.html	Junjie Wang, Tomas NordstrÃ¶m
LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts	Large-scale vision-language pre-trained (VLP) models (e.g. CLIP) are renowned for their versatility as they can be applied to diverse applications in a zero-shot setup. However when these models are used in specific domains their performance often falls short due to domain gaps or the under-representation of these domains in the training data. While fine-tuning VLP models on custom datasets with human-annotated labels can address this issue annotating even a small-scale dataset (e.g. 100k samples) can be an expensive endeavor often requiring expert annotators if the task is complex. To address these challenges we propose LatteCLIP an unsupervised method for fine-tuning CLIP models on classification with known class names in custom domains without relying on human annotations. Our method leverages Large Multimodal Models (LMMs) to generate expressive textual descriptions for both individual images and groups of images. These provide additional contextual information to guide the fine-tuning process in the custom domains. Since LMM-generated descriptions are prone to hallucination or missing details we introduce a novel strategy to distill only the useful information and stabilize the training. Specifically we learn rich per-class prototype representations from noisy generated texts and dual pseudo-labels. Our experiments on 10 domain-specific datasets show that LatteCLIP outperforms pre-trained zero-shot methods by an average improvement of +4.74 points in top-1 accuracy and other state-of-the-art unsupervised methods by +3.45 points.	https://openaccess.thecvf.com//content/WACV2025/html/Cao_LatteCLIP_Unsupervised_CLIP_Fine-Tuning_via_LMM-Synthetic_Texts_WACV_2025_paper.html	Anh-Quan Cao, Maximilian Jaritz, Matthieu Guillaumin, Raoul de Charette, Loris Bazzani
Learning Anatomy-Disease Entangled Representation	Human experts demonstrate proficiency not only in disentangling anatomical structures from disease conditions but also in intertwining anatomical and disease information to accurately diagnose a variety of disorders. However deep learning models despite their prowess in acquiring intricate representation often struggle to learn representation where distinct semantic aspects of the data (both anatomy and pathology) are entangled particularly in medical images which present a rich array of anatomical structures and potential pathological conditions. We envision that a deep model when trained to comprehend medical images akin to human perception would offer powerful representation with higher generalizability robustness and interpretability. To realize this vision we have developed LeADER a framework for learning anatomy-disease entangled representation from medical images. As a proof of concept we have trained LeADER on 1M chest radiographs gathered from 10 public datasets. Experimental results across 11 medical tasks compared to 8 baselines in zero-shot linear probing limited data regimes and full fine-tuning settings demonstrate LeADER's superior performance over the Google CXR Foundation Model large-scale medical models and fully/self-supervised baselines across diverse downstream tasks. This enhanced performance is attributed to the significance of entangling anatomy-specific and disease-specific representations via our framework which enables the simultaneous acquisition of both anatomical and disease knowledge yet overlooked in existing supervised/self-supervised learning methods. All code and models are available at GitHub.com/JLiangLab/LeADER.	https://openaccess.thecvf.com//content/WACV2025/html/Haghighi_Learning_Anatomy-Disease_Entangled_Representation_WACV_2025_paper.html	Fatemeh Haghighi, Michael B. Gotway, Jianming Liang
Learning Deep Illumination-Robust Features from Multispectral Filter Array Images	Multispectral (MS) snapshot cameras equipped with a MS filter array (MSFA) capture multiple spectral bands in a single shot resulting in a raw mosaic image where each pixel holds only one channel value. The fully-defined MS image is estimated from the raw one through demosaicing which inevitably introduces spatio-spectral artifacts. Moreover training on fully-defined MS images can be computationally intensive particularly with deep neural networks (DNNs) and may result in features lacking discrimination power due to suboptimal learning of spatio-spectral interactions. Furthermore outdoor MS image acquisition occurs under varying lighting conditions leading to illumination-dependent features. This paper presents an original approach to learn discriminant and illumination-robust features directly from raw images. It involves: raw spectral constancy to mitigate the impact of illumination MSFA-preserving transformations suited for raw image augmentation to train DNNs on diverse raw textures and raw-mixing to capture discriminant spatio-spectral interactions in raw images. Experiments on MS image classification show that our approach outperforms both handcrafted and recent deep learning-based methods while also requiring significantly less computational effort. The source code is available at https://github.com/AnisAmziane/RawTexture.	https://openaccess.thecvf.com//content/WACV2025/html/Amziane_Learning_Deep_Illumination-Robust_Features_from_Multispectral_Filter_Array_Images_WACV_2025_paper.html	Anis Amziane
Learning Instance-Specific Parameters of Black-Box Models using Differentiable Surrogates	Tuning parameters of a non-differentiable or black-box compute is challenging. Existing methods rely mostly on random sampling or grid sampling from the parameter space. Further with all the current methods it is not possible to supply any input specific parameters to the black-box. To the best of our knowledge for the first time we are able to learn input-specific parameters for a black box in this work. As a test application we choose a popular image denoising method BM3D as our black-box compute. Then we use a differentiable surrogate model (a neural network) to approximate the black-box behaviour. Next another neural network is used in an end-to-end fashion to learn input instance-specific parameters for the black-box. Motivated by prior advances in surrogate-based optimization we applied our method to the Smartphone Image Denoising Dataset (SIDD) and the Color Berkeley Segmentation Dataset (CBSD68) for image denoising. The results are compelling demonstrating a significant increase in PSNR and a notable improvement in SSIM nearing 0.93. Experimental results underscore the effectiveness of our approach in achieving substantial improvements in both model performance and optimization efficiency. For code and implementation details please refer to our GitHub repository: https://github.com/arnisha-k/instance-specific-param.	https://openaccess.thecvf.com//content/WACV2025/html/Khondaker_Learning_Instance-Specific_Parameters_of_Black-Box_Models_using_Differentiable_Surrogates_WACV_2025_paper.html	Arnisha Khondaker, Nilanjan Ray
Learning Keypoints for Multi-Agent Behavior Analysis using Self-Supervision	The study of social interactions and collective behaviors through multi-agent video analysis is crucial in biology. While self-supervised keypoint discovery has emerged as a promising solution to reduce the need for manual keypoint annotations existing methods often struggle with videos containing multiple interacting agents especially those of the same species and color. To address this we introduce B-KinD-multi a novel approach that leverages pre-trained video segmentation models to guide keypoint discovery in multi-agent scenarios. This eliminates the need for time-consuming manual annotations on new experimental settings and organisms. Extensive evaluations demonstrate improved keypoint regression and downstream behavioral classification in videos of flies mice and rats. Furthermore our method generalizes well to other species including ants bees and humans highlighting its potential for broad applications in automated keypoint annotation for multi-agent behavior analysis. Code available under: B-KinD-Multi	https://openaccess.thecvf.com//content/WACV2025/html/Khalil_Learning_Keypoints_for_Multi-Agent_Behavior_Analysis_using_Self-Supervision_WACV_2025_paper.html	Daniel Khalil, Christina Liu, Pietro Perona, Jennifer Sun, Markus Marks
Learning Multiple Object States from Actions via Large Language Models	Recognizing the states of objects in a video is crucial in understanding the scene beyond actions and objects. For instance an egg can be raw cracked and whisked while cooking an omelet and these states can coexist simultaneously (an egg can be both raw and whisked). However most existing research assumes a single object state change (e.g. uncracked - cracked) overlooking the coexisting nature of multiple object states and the influence of past states on the current state. We formulate object state recognition as a multi-label classification task that explicitly handles multiple states. We then propose to learn multiple object states from narrated videos by leveraging large language models (LLMs) to generate pseudo-labels from the transcribed narrations capturing the influence of past states. The challenge is that narrations mostly describe human actions in the video but rarely explain object states. Therefore we use the LLM's knowledge of the relationship between actions and states to derive the missing object states. We further accumulate the derived object states to consider past state contexts to infer current object state pseudo-labels. We newly collect a dataset called the Multiple Object States Transition (MOST) dataset which includes manual multi-label annotation for evaluation purposes covering 60 object states across six object categories. Experimental results show that our model trained on LLM-generated pseudo-labels significantly outperforms strong vision-language models demonstrating the effectiveness of our pseudo-labeling framework that considers past context via LLMs.	https://openaccess.thecvf.com//content/WACV2025/html/Tateno_Learning_Multiple_Object_States_from_Actions_via_Large_Language_Models_WACV_2025_paper.html	Masatoshi Tateno, Takuma Yagi, Ryosuke Furuta, Yoichi Sato
Learning Semantic Part-Based Graph Structure for 3D Point Cloud Domain Generalization	In 3D data analysis point clouds provide detailed geometric insights for applications like computer vision and geospatial analysis. However their irregularity and diversity make classification challenging especially in domain generalization where models must generalize to new data distributions. Our research introduces a novel 3D Domain Generalization (3DDG) method using Unsupervised Part Decomposition (UPD) and Graph Structure Induction (GSI). The UPD module employs spectral clustering and a modified Shannon entropy method to segment point clouds into meaningful parts. The GSI module constructs a graph of these parts' spatial relationships processed by a Graph Neural Network (GNN) to understand complex geometries. Our approach enhances part-based analysis improving classification accuracy on the PointDA-10 and GraspNetPC-10 datasets by 1.25% and 2.6% respectively. These results highlight our advancements in 3D domain generalization enabling more robust classification models for diverse point cloud data.	https://openaccess.thecvf.com//content/WACV2025/html/Sai_Learning_Semantic_Part-Based_Graph_Structure_for_3D_Point_Cloud_Domain_WACV_2025_paper.html	G Ujwal Sai, Arkadipta De, Vartika Sengar, Anuj Rathore, Daksh Thapar, Manohar Kaul
Learning Semi-Supervised Medical Image Segmentation from Spatial Registration	Semi-supervised medical image segmentation has shown promise in training models with limited labeled data and abundant unlabeled data. However state-of-the-art methods ignore a potentially valuable source of unsupervised semantic information--spatial registration transforms between image volumes. To address this we propose CCT-R a contrastive cross-teaching framework incorporating registration information. To leverage the semantic information available in registrations between volume pairs CCT-R incorporates two proposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced Positive Sampling (REPS). The RSL leverages segmentation knowledge derived from transforms between labeled and unlabeled volume pairs providing an additional source of pseudo-labels. REPS enhances contrastive learning by identifying anatomically-corresponding positives across volumes using registration transforms. Experimental results on two challenging medical segmentation benchmarks demonstrate the effectiveness and superiority of CCT-R across various semi-supervised settings with as few as one labeled case. Our code is available at https://github.com/kathyliu579/ContrastiveCrossteachingWithRegistration.	https://openaccess.thecvf.com//content/WACV2025/html/Liu_Learning_Semi-Supervised_Medical_Image_Segmentation_from_Spatial_Registration_WACV_2025_paper.html	Qianying Liu, Paul Henderson, Xiao Gu, Hang Dai, Fani Deligianni
Learning Unified Distance Metric Across Diverse Data Distributions with Parameter-Efficient Transfer Learning	A common practice in metric learning is to train and test an embedding model for each dataset. This dataset-specific approach fails to simulate real-world scenarios that involve multiple heterogeneous distributions of data. In this regard we explore a new metric learning paradigm called Unified Metric Learning (UML) which learns a unified distance metric capable of capturing relations across multiple data distributions. UML presents new challenges such as imbalanced data distribution and bias towards dominant distributions. These issues cause standard metric learning methods to fail in learning a unified metric. To address these challenges we propose Parameter-efficient Unified Metric leArning (PUMA) which consists of a pre-trained frozen model and two additional modules stochastic adapter and prompt pool. These modules enable to capture dataset-specific knowledge while avoiding bias towards dominant distributions. Additionally we compile a new unified metric learning benchmark with a total of 8 different datasets. PUMA outperforms the state-of-the-art dataset-specific models while using about 69 times fewer trainable parameters.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_Learning_Unified_Distance_Metric_Across_Diverse_Data_Distributions_with_Parameter-Efficient_WACV_2025_paper.html	Sungyeon Kim, Donghyun Kim, Suha Kwak
Learning Visual Grounding from Generative Vision and Language Model	Visual grounding tasks aim to localize image regions based on natural language references. In this work we explore whether generative VLMs predominantly trained on image-text data could be leveraged to scale up the text annotation of visual grounding data. We find that grounding knowledge already exists in generative VLM and can be elicited by proper prompting. We thus prompt a VLM to generate object-level descriptions by feeding it object regions from existing object detection datasets. We further propose attribute modeling to explicitly capture the important object attributes and spatial relation modeling to capture inter-object relationship both of which are common linguistic pattern in referring expression. Our constructed dataset (500K images 1M objects 16M referring expressions) is one of the largest grounding datasets to date and the first grounding dataset with purely model-generated queries and human-annotated objects. To verify the quality of this data we conduct zero-shot transfer experiments to the popular RefCOCO benchmarks for both referring expression comprehension (REC) and segmentation (RES) tasks. On both tasks our model significantly outperform the state-of-the-art approaches without using human annotated visual grounding data. Our results demonstrate the promise of generative VLM to scale up visual grounding in the real world.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_Learning_Visual_Grounding_from_Generative_Vision_and_Language_Model_WACV_2025_paper.html	Shijie Wang, Dahun Kim, Ali Taalimi, Chen Sun, Weicheng Kuo
Learning Visual-Semantic Hierarchical Attribute Space for Interpretable Open-Set Recognition	In the field of open-set recognition conventional models often focus on addressing challenges within a single hierarchical category and these methods frequently lack interpretability. In this paper we propose a novel solution that utilizes attributes and hierarchical relationships to achieve interpretable open-set recognition. Our method is centered around the visual-semantic attribute space. By leveraging hierarchy division we can decompose the attributes into more granular components thereby yielding additional performance improvements. When confronted with an unfamiliar object our method not only classifies it as an unknown category but also provides insights into the broader category and its associated attributes. This capability enhances interpretability by offering valuable information regarding the potential category and characteristics of the object. Experimental results demonstrate great performance improvements compared to existing methods.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Learning_Visual-Semantic_Hierarchical_Attribute_Space_for_Interpretable_Open-Set_Recognition_WACV_2025_paper.html	Zhuo Xu, Xiang Xiang
"Learning the Power of ""No"": Foundation Models with Negations"	Negation is a fundamental aspect of natural language reasoning yet foundational vision-language models (VLMs) like CLIP face significant challenges in accurately interpreting it. These models often process text prompts holistically making it difficult to isolate and understand the role of negated terms. To overcome this limitation we present CC-Neg: a novel dataset consisting of 228246 images each paired with both true captions and their corresponding negated versions. CC-Neg provides a critical benchmark to assess and improve foundational VLMs' ability to process negations focusing specifically on how the presence of terms like 'not' alters the semantic relationship between images and their textual descriptions. To illustrate the effectiveness of the CC-Neg dataset in enhancing negation understanding we introduce the CoN-CLIP framework which incorporates targeted modifications to CLIP's contrastive loss function. When trained with CC-Neg CoN-CLIP achieves a 3.85% average improvement in top-1 accuracy for zero-shot image classification across eight datasets and a 4.4% performance boost on challenging compositionality benchmarks such as SugarCREPE. These results highlight CoN-CLIP's enhanced understanding of the nuanced semantic relationships involving negation. Our code and the CC-Neg benchmark are available at: https://github.com/jaisidhsingh/CoN-CLIP.	https://openaccess.thecvf.com//content/WACV2025/html/Singh_Learning_the_Power_of_No_Foundation_Models_with_Negations_WACV_2025_paper.html	Jaisidh Singh, Ishaan Shrivastava, Mayank Vatsa, Richa Singh, Aparna Bharati
Learning to Count from Pseudo-Labeled Segmentation	Class-agnostic counting (CAC) has numerous potential applications across various domains. The goal is to count objects of an arbitrary category during testing based on only a few annotated exemplars. However existing methods often count all objects in the image including those from different categories than the exemplars. To address this issue we propose localizing the area containing the objects of interest via an exemplar-based segmentation model before counting them. To train this model we propose a novel method to obtain pseudo-labeled segmentation masks. Specifically we use an unsupervised image clustering method to generate a set of candidate pseudo object masks from which we select the optimal one using a pre-trained CAC model. We show that the trained segmentation model can effectively localize objects of interest based on the exemplars and prevent the model from counting everything. To properly evaluate the performance of CAC methods in real-world scenarios we introduce two new benchmarks: a synthetic test set and a new test set of real images containing countable objects from multiple classes. Our proposed method shows a significant advantage over previous CAC methods on these two benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_Learning_to_Count_from_Pseudo-Labeled_Segmentation_WACV_2025_paper.html	Jingyi Xu, Hieu Le, Dimitris Samaras
Learning to Identify Seen Unseen and Unknown in the Open World: A Practical Setting for Zero-Shot Learning	As vision-language models advance addressing the Zero-Shot Learning (ZSL) problem in the open world becomes increasingly crucial. Specifically a robust model must handle three types of samples during inference: seen classes with visual and semantic information provided in training unseen classes with only the semantic information in training and unknown samples with no prior information from training. Existing methods either handle seen and unseen classes together (ZSL) or seen and unknown classes (known as Open-Set Recognition OSR). However none addresses the simultaneous handling of all three which we term Open-Set Zero-Shot Learning (OZSL). To address this problem we propose a two-stage approach for OZSL that recognizes seen unseen and unknown samples. The first stage classifies samples as either seen or not while the second stage distinguishes unseen from unknown. Furthermore we introduce a cross-stage knowledge transfer mechanism that leverages semantic relationships between seen and unseen classes to enhance learning in the second stage. Extensive experiments demonstrate the efficacy of the proposed approach compared to naively combining existing ZSL and OSR methods. The code is available at https://github.com/smufang/OZSL.	https://openaccess.thecvf.com//content/WACV2025/html/Parameswaran_Learning_to_Identify_Seen_Unseen_and_Unknown_in_the_Open_WACV_2025_paper.html	Sethupathy Parameswaran, Yuan Fang, Chandan Gautam, Savitha Ramasamy, Xiaoli Li
Learning to Visually Connect Actions and their Effects	We introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We identify and explore two different aspects of the concept of CATE: Action Selection (AS) and Effect-Affinity Assessment (EAA) where video understanding models connect actions and effects at semantic and fine-grained levels respectively. We design various baseline models for AS and EAA. Despite the intuitive nature of the task we observe that models struggle and humans outperform them by a large margin. Our experiments show that in solving AS and EAA models learn intuitive properties like object tracking and encoding pose-related features without explicit supervision. We demonstrate that CATE can be an effective self-supervised task for learning video representations from unlabeled videos. The study aims to showcase the fundamental nature and versatility of CATE with the hope of inspiring advanced formulations and models.	https://openaccess.thecvf.com//content/WACV2025/html/Parmar_Learning_to_Visually_Connect_Actions_and_their_Effects_WACV_2025_paper.html	Paritosh Parmar, Eric Peh, Basura Fernando
Learning under Noisy Labels Spurious Points and Diverse Structures: TS40K a 3D Point Cloud Dataset of Rural Terrain and Electrical Transmission Systems	Research in 3D scene understanding particularly in autonomous driving and indoor segmentation has made significant strides. However most available datasets focus on urban settings. We introduce TS40K a 3D point cloud dataset spanning 40000 km of electrical transmission systems in rural terrain addressing power-grid inspections to prevent outages damages and fires. TS40K offers high point density and no occlusion presenting challenges like noisy labels diverse structures and sensor noise causing spurious points. We evaluate state-of-the-art methods on 3D semantic segmentation and object detection revealing limitations in power grid inspection. TS40K invites further research to tackle these challenges. Resources available in: https://github.com/dlavado/TS40K	https://openaccess.thecvf.com//content/WACV2025/html/Lavado_Learning_under_Noisy_Labels_Spurious_Points_and_Diverse_Structures_TS40K_WACV_2025_paper.html	Diogo Lavado, Ricardo Santos, AndrÃ© Coelho, JoÃ£o Santos, Alessandra Micheletti, ClÃ¡udia Soares
Leveraging CLIP Encoder for Multimodal Emotion Recognition	Multimodal emotion recognition (MER) aims to identify human emotions by combining data from various modalities such as language audio and vision. Despite the recent advances of MER approaches the limitations in obtaining extensive datasets impede the improvement of performance. To mitigate this issue we leverage a Contrastive Language-Image Pre-training (CLIP)-based architecture and its semantic knowledge from massive datasets that aims to enhance the discriminative multimodal representation. We propose a label encoder-guided MER framework based on CLIP (MER-CLIP) to learn emotion-related representations across modalities. Our approach introduces a label encoder that treats labels as text embeddings to incorporate their semantic information leading to the learning of more representative emotional features. To further exploit label semantics we devise a cross-modal decoder that aligns each modality to a shared embedding space by sequentially fusing modality features based on emotion-related input from the label encoder. Finally the label encoder-guided prediction enables generalization across diverse labels by embedding their semantic information as well as word labels. Experimental results show that our method outperforms the state-of-the-art MER methods on the benchmark datasets CMU-MOSI and CMU-MOSEI.	https://openaccess.thecvf.com//content/WACV2025/html/Song_Leveraging_CLIP_Encoder_for_Multimodal_Emotion_Recognition_WACV_2025_paper.html	Yehun Song, Sunyoung Cho
Leveraging Vision Language Models for Specialized Agricultural Tasks	As Vision Language Models (VLMs) become increasingly accessible to farmers and agricultural experts there is a growing need to evaluate their potential in specialized tasks. We present AgEval a comprehensive benchmark for assessing VLMs' capabilities in plant stress phenotyping offering a solution to the challenge of limited annotated data in agriculture. Our study explores how general-purpose VLMs can be leveraged for domain-specific tasks with only a few annotated examples providing insights into their behavior and adaptability. AgEval encompasses 12 diverse plant stress phenotyping tasks evaluating zero-shot and few-shot in-context learning performance of state-of-the-art models including Claude GPT Gemini and LLaVA. Our results demonstrate VLMs' rapid adaptability to specialized tasks with the best-performing model showing an increase in F1 scores from 46.24% to 73.37% in 8-shot identification. To quantify performance disparities across classes we introduce metrics such as the coefficient of variation (CV) revealing that VLMs' training impacts classes differently with CV ranging from 26.02% to 58.03%. We also find that strategic example selection enhances model reliability with exact category examples improving F1 scores by 15.38% on average. AgEval establishes a framework for assessing VLMs in agricultural applications offering valuable benchmarks for future evaluations. Our findings suggest that VLMs with minimal few-shot examples show promise as a viable alternative to traditional specialized models in plant stress phenotyping while also highlighting areas for further refinement. Results and benchmark details are available at: https://github.com/arbab-ml/AgEval	https://openaccess.thecvf.com//content/WACV2025/html/Arshad_Leveraging_Vision_Language_Models_for_Specialized_Agricultural_Tasks_WACV_2025_paper.html	Muhammad Arbab Arshad, Talukder Zaki Jubery, Tirtho Roy, Rim Nassiri, Asheesh K. Singh, Arti Singh, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar
LiCamPose: Combining Multi-View LiDAR and RGB Cameras for Robust Single-Timestamp 3D Human Pose Estimation	Several methods have been proposed to estimate 3D human pose from multi-view images achieving satisfactory performance on public datasets collected under relatively simple conditions. However there are limited approaches studying extracting 3D human skeletons from multimodal inputs such as RGB and point cloud data. To address this gap we introduce LiCamPose a pipeline that integrates multi-view RGB and sparse point cloud information to estimate robust 3D human poses via single timestamp. We demonstrate the effectiveness of the volumetric architecture in combining these modalities. Furthermore to circumvent the need for manually labeled 3D human pose annotations we develop a synthetic dataset generator for pretraining and design an unsupervised domain adaptation strategy to train a 3D human pose estimator without manual annotations. To validate the generalization capability of our method LiCamPose is evaluated on four datasets including two public datasets one synthetic dataset and one challenging self-collected dataset named BasketBall covering diverse scenarios. The results demonstrate that LiCamPose exhibits great generalization performance and significant application potential. The code generator and datasets are available at https://github.com/Yu-Yy/LiCamPose.	https://openaccess.thecvf.com//content/WACV2025/html/Pan_LiCamPose_Combining_Multi-View_LiDAR_and_RGB_Cameras_for_Robust_Single-Timestamp_WACV_2025_paper.html	Zhiyu Pan, Zhicheng Zhong, Wenxuan Guo, Yifan Chen, Jianjiang Feng, Jie Zhou
LiGAR: LiDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity Recognition	Group Activity Recognition (GAR) remains challenging in computer vision due to the complex nature of multi-agent interactions. This paper introduces LiGAR a LIDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity Recognition. LiGAR leverages LiDAR data as a structural backbone to guide the processing of visual and textual information enabling robust handling of occlusions and complex spatial arrangements. Our framework incorporates a Multi-Scale LIDAR Transformer Cross-Modal Guided Attention and an Adaptive Fusion Module to integrate multi-modal data at different semantic levels effectively. LiGAR's hierarchical architecture captures group activities at various granularities from individual actions to scene-level dynamics. Extensive experiments on the JRDB-PAR Volleyball and NBA datasets demonstrate LiGAR's superior performance achieving state-of-the-art results with improvements of up to 10.6% in F1-score on JRDB-PAR and 5.9% in Mean Per Class Accuracy on the NBA dataset. Notably LiGAR maintains high performance even when LiDAR data is unavailable during inference showcasing its adaptability. Our ablation studies highlight the significant contributions of each component and the effectiveness of our multi-modal multi-scale approach in advancing the field of group activity recognition.	https://openaccess.thecvf.com//content/WACV2025/html/Chappa_LiGAR_LiDAR-Guided_Hierarchical_Transformer_for_Multi-Modal_Group_Activity_Recognition_WACV_2025_paper.html	Naga Venkata Sai Raviteja Chappa, Khoa Luu
LiLMaps: Learnable Implicit Language Maps	One of the current trends in robotics is to employ large language models (LLMs) to provide non-predefined commands execution and natural human-robot interaction. It is useful to have an environment map together with its language representation which can be further utilized by LLMs. Such a comprehensive scene representation enables numerous ways of interaction with the map for autonomously operating robots. In this work we present an approach that enhances incremental implicit mapping through the integration of visual-language features. Specifically we (i) propose a decoder optimization technique for implicit language maps which can be used when new objects appear on the scene and (ii) address the problem of inconsistent visual-language predictions between different viewing positions. Our experiments demonstrate the effectiveness of LiLMaps and solid improvements in performance.	https://openaccess.thecvf.com//content/WACV2025/html/Kruzhkov_LiLMaps_Learnable_Implicit_Language_Maps_WACV_2025_paper.html	Evgenii Kruzhkov, Sven Behnke
Lifting by Gaussians: A Simple Fast and Flexible Method for 3D Instance Segmentation	We introduce Lifting By Gaussians (LBG) a novel approach for open-world instance segmentation of 3D Gaussian Splatted Radiance Fields (3DGS). Recently 3DGS Fields have emerged as a highly efficient and explicit alternative to Neural Field-based methods for high-quality Novel View Synthesis. Our 3D instance segmentation method directly lifts 2D segmentation masks from SAM (alternately FastSAM etc.) together with CLIP and DINOv2 features directly fusing them onto 3DGS (or similar Gaussian radiance fields such as 2DGS). Unlike previous approaches LBG requires no per-scene training allowing it to operate seamlessly on any existing 3DGS reconstruction. Our approach is not only an order of magnitude faster and simpler than existing approaches; it is also highly modular enabling 3D semantic segmentation of existing 3DGS fields without requiring a specific parametrization of the 3D Gaussians. Furthermore our technique achieves superior semantic segmentation for 2D semantic novel view synthesis and 3D asset extraction results while maintaining flexibility and efficiency. We further introduce a novel approach to evaluate individually segmented 3D assets from 3D radiance field segmentation methods.	https://openaccess.thecvf.com//content/WACV2025/html/Chacko_Lifting_by_Gaussians_A_Simple_Fast_and_Flexible_Method_for_WACV_2025_paper.html	Rohan Chacko, Nicolai HÃ¤eni, Eldar Khaliullin, Lin Sun, Douglas Lee
LoSA: Long-Short-Range Adapter for Scaling End-to-End Temporal Action Localization	Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation we introduce LoSA the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Gated Fusion that strategically combines the output of these adapters from the video backbone layers to enhance the video features provided to the TAL head. Experiments show that LoSA significantly outperforms all existing methods on standard TAL benchmarks THUMOS-14 and ActivityNet-v1.3 by scaling end-to-end backbone adaptation to billion-parameter-plus models like VideoMAEv2 (ViT-g) and leveraging them beyond head-only transfer learning.	https://openaccess.thecvf.com//content/WACV2025/html/Gupta_LoSA_Long-Short-Range_Adapter_for_Scaling_End-to-End_Temporal_Action_Localization_WACV_2025_paper.html	Akshita Gupta, Gaurav Mittal, Ahmed Magooda, Ye Yu, Graham Taylor, Mei Chen
Local Masked Reconstruction for Efficient Self-Supervised Learning on High-Resolution Images	Self-supervised learning for computer vision has progressed tremendously and improved many downstream vision tasks such as image classification semantic segmentation and object detection. Among these generative self-supervised vision learning approaches such as MAE and BEiT show promising performance. However their global reconstruction mechanism is computationally demanding especially for high-resolution images. The computational cost increases extensively when scaled to a large-scale dataset. To address this issue we propose local masked reconstruction (LoMaR) a simple yet effective approach that reconstructs image patches from small neighboring regions. The strategy can be easily integrated into any generative self-supervised learning techniques and improves the trade-off between efficiency and accuracy compared to reconstruction over the entire image. LoMaR is 2.5x faster than MAE and 5.0x faster than BEiT on 384x384 ImageNet pretraining and surpasses them by 0.2% and 0.8% in accuracy respectively. It is 2.1x faster than MAE on iNaturalist pretraining and gains 0.2% in accuracy. On MS COCO LoMaR outperforms MAE by 0.5 APbox on object detection and 0.5 APmask on instance segmentation. It also outperforms MAE by 0.2% on semantic segmentation. Our code and pretrained models are available at: https://github.com/junchen14/LoMaR.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_Local_Masked_Reconstruction_for_Efficient_Self-Supervised_Learning_on_High-Resolution_Images_WACV_2025_paper.html	Jun Chen, Faizan Farooq Khan, Ming Hu, Ammar Sherif, Zongyuan Ge, Boyang Li, Mohamed Elhoseiny
Localized Gaussian Splatting Editing with Contextual Awareness	Recent advancements in text-guided 3D object generation using diffusion priors struggle with illumination inconsistencies when applied to scene editing tasks like object replacement or insertion. To address this we propose an illumination-aware 3D scene editing pipeline for 3D Gaussian Splatting (3DGS). Our method leverages state-of-the-art 2D diffusion inpainting to handle global illumination context effectively. Specifically we identify representative anchor views that capture scene-wide illumination inpaint them using 2D diffusion models and integrate the results into a coarse-to-fine 3DGS optimization process. In the fine step we introduce Depth-guided Inpainting Score Distillation Sampling (DI-SDS) to refine geometry and texture details capitalizing on the diversity of 2D priors. Our approach achieves locally precise edits with globally consistent illumination demonstrating robustness in real scenes with highlights and shadows. Comparisons show superior results over state-of-the-art text-to-3D editing methods. Project page: https://corneliushsiao.github.io/GSLE.html.	https://openaccess.thecvf.com//content/WACV2025/html/Xiao_Localized_Gaussian_Splatting_Editing_with_Contextual_Awareness_WACV_2025_paper.html	Hanyuan Xiao, Yingshu Chen, Huajian Huang, Haolin Xiong, Jing Yang, Pratusha Prasad, Yajie Zhao
LogicNet: A Logical Consistency Embedded Face Attribute Learning Network	Ensuring logical consistency in predictions is a crucial yet overlooked aspect in face attribute classification. We explore the potential reasons for this oversight and introduce two pressing challenges to the field: 1) How can we ensure that a model when trained with data checked for logical consistency yields predictions that are logically consistent? 2) How can we achieve the same with training data that hasn't undergone logical consistency checks? Minimizing manual effort is also essential for enhancing automation. To address these challenges we introduce two datasets FH41K and CelebA-logic and propose LogicNet which combines adversarial learning and label poisoning to learn the logical relationship between attributes without the need for post-processing steps. The accuracy of LogicNet surpasses that of the next-best approach by 13.36% 9.96% and 1.01% on FH37K FH41K and CelebA-logic respectively. In real-world case analysis our approach can achieve a reduction of more than 50% in the average number of failed cases (logically inconsistent attributes) compared to other methods. Code link: https://github.com/HaiyuWu/LogicNet.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_LogicNet_A_Logical_Consistency_Embedded_Face_Attribute_Learning_Network_WACV_2025_paper.html	Haiyu Wu, Sicong Tian, Huayu Li, Kevin W. Bowyer
Long-Term Ad Memorability: Understanding & Generating Memorable Ads	Despite the importance of long-term memory in marketing and brand building until now there has been no large-scale study on the memorability of ads. All previous memorability studies have been conducted on short-term recall on specific content types like action videos. On the other hand long-term memorability is crucial for advertising industry and ads are almost always highly multimodal. Therefore we release the first memorability dataset LAMBDA consisting of 1749 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad types we find many interesting insights into what makes an ad memorable e.g. fast-moving ads are more memorable than those with slower scenes; people who use ad-blockers remember a lower number of ads than those who don't. Next we present a model Henry to predict the memorability of a content. Henry achieves state-of-the-art performance across all prominent literature memorability datasets. It shows strong generalization performance with better results in 0-shot on unseen datasets. Finally with the intent of memorable ad generation we present a scalable method to build a high-quality memorable ad generation model by leveraging automatically annotated data. Our approach SEED (Self rEwarding mEmorability Modeling) starts with a language model trained on LAMBDA as seed data and progressively trains an LLM to generate more memorable ads. We show that the generated advertisements have 44% higher memorability scores than the original ads. We release this large-scale ad dataset UltraLAMBDA consisting of 5 million ads. Our code and the datasets LAMBDA and UltraLAMBDA are open-sourced at https://behavior-in-the-wild.github.io/memorability.	https://openaccess.thecvf.com//content/WACV2025/html/Si_Long-Term_Ad_Memorability_Understanding__Generating_Memorable_Ads_WACV_2025_paper.html	Harini Si, Somesh Singh, Yaman Kumar Singla, Aanisha Bhattacharyya, Veeky Baths, Changyou Chen, Rajiv Ratn Shah, Balaji Krishnamurthy
Looking at Model Debiasing through the Lens of Anomaly Detection	Deep neural networks are likely to learn unintended spurious correlations between training data and labels when dealing with biased data potentially limiting the generalization to unseen samples not presenting the same bias. In this context model debiasing approaches can be devised aiming at reducing the model's dependency on such unwanted correlations either leveraging the knowledge of bias information or not. In this work we focus on the latter and more realistic scenario showing the importance of accurately predicting the bias-conflicting and bias-aligned samples to obtain compelling performance in bias mitigation. On this ground we propose to conceive the problem of model bias from an out-of-distribution perspective introducing a new bias identification method based on anomaly detection. We claim that when data is mostly biased bias-conflicting samples can be regarded as outliers with respect to the bias-aligned distribution in the feature space of a biased model thus allowing for precisely detecting them with an anomaly detection method. Coupling the proposed bias identification approach with bias-conflicting data upsampling and augmentation in a two-step strategy we reach state-of-the-art performance on synthetic and real benchmark datasets. Ultimately our proposed approach shows that the data bias issue does not necessarily require complex debiasing methods given that an accurate bias identification procedure is defined. Source code is available at https://github.com/Malga-Vision/MoDAD	https://openaccess.thecvf.com//content/WACV2025/html/Pastore_Looking_at_Model_Debiasing_through_the_Lens_of_Anomaly_Detection_WACV_2025_paper.html	Vito Paolo Pastore, Massimiliano Ciranni, Davide Marinelli, Francesca Odone, Vittorio Murino
Loose Social-Interaction Recognition in Real-World Therapy Scenarios	The computer vision community has explored dyadic interactions for atomic actions such as pushing carrying-object etc. However with the advancement in deep learning models there is a need to explore more complex dyadic situations such as loose interactions. These are interactions where two people perform certain atomic activities to complete a global action irrespective of temporal synchronisation and physical engagement like cooking-together for example. Analysing these types of dyadic-interactions has several useful applications in the medical domain for social-skills development and mental health diagnosis. To achieve this we propose a novel dual-path architecture to capture the loose interaction between two individuals. Our model learns global abstract features from each stream via a CNNs backbone and fuses them using a new Global-Layer-Attention module based on a cross-attention strategy. We evaluate our model on real-world autism diagnoses such as our Loose-Interaction dataset and the publicly available Autism dataset for loose interactions. Our network achieves baseline results on the Loose-Interaction and SOTA results on the Autism datasets. Moreover we study different social interactions by experimenting on a publicly available dataset i.e. NTU-RGB+D (interactive classes from both NTU-60 and NTU-120). We have found that different interactions require different network designs. We also compare a slightly different version of our method (details in Section 3.6) by incorporating time information to address tight interactions achieving SOTA results.	https://openaccess.thecvf.com//content/WACV2025/html/Ali_Loose_Social-Interaction_Recognition_in_Real-World_Therapy_Scenarios_WACV_2025_paper.html	Abid Ali, Rui Dai, Ashish Marisetty, Guillaume Astruc, Monique Thonnat, Jean-Marc Odobez, Susanne Thummler, Francois Bremond
Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm	Convolutional Neural Networks (CNNs) that have excelled in diverse computer vision tasks are vulnerable to backdoor attacks enabling attacker-controlled predictions via specific triggers. Restricted to spatial domains recent research exploits perceptual traits by embedding triggers in the frequency domain yielding pixel-level indistinguishable perturbations. In black-box settings restricted access to model and training process necessitates advanced trigger designs. Current frequency-based attacks manipulate magnitude spectra introducing discrepancies between clean and poisoned data though vulnerable to common image processing operations like compression and filtering. In this paper we propose a robust low-frequency backdoor attack (LFBA) in black-box setup that minimally perturbs spectrum components and maintains the perceptual similarity in spatial space simultaneously. Our methodology capitalizes on the insight that optimal triggers can be located in low-frequency regions to maximize attack effectiveness robustness against image transformation operations and stealthiness in dual space. To effectively explore the discrete frequency space we utilize simulated annealing (SA) a form of evolutionary algorithm to optimize the properties of trigger including the frequency bands to be manipulated and the perturbation of each band under restricted attack scenario. Extensive experiments on both CNNs and Vision Transformers (ViT) confirm the effectiveness and robustness of LFBA against image processing operations and state-of-the-art backdoor defenses. Furthermore LFBA exhibits inherent stealthiness in both spatial and frequency spaces making it resistant to human and frequency inspection.	https://openaccess.thecvf.com//content/WACV2025/html/Qiao_Low-Frequency_Black-Box_Backdoor_Attack_via_Evolutionary_Algorithm_WACV_2025_paper.html	Yanqi Qiao, Dazhuang Liu, Rui Wang, Kaitai Liang
LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones	Research in efficient vision backbones is evolving into models that are a mixture of convolutions and transformer blocks. A smart combination of both architecture-wise and component-wise is mandatory to excel in the speed-accuracy trade-off. Most publications focus on maximizing accuracy and utilize MACs (multiply accumulate operations) as an efficiency metric. The latter however often do not measure accurately how fast a model actually is due to factors like memory access cost and degree of parallelism. We analyzed common modules and architectural design choices for backbones not in terms of MACs but rather in actual throughput and latency as the combination of the latter two is a better representation of the efficiency of models in real applications. We applied the conclusions taken from that analysis to create a recipe for increasing hardware-efficiency in macro design. Additionally we introduce a simple slimmed-down version of Multi-Head Self-Attention that aligns with our analysis. We combine both macro and micro design to create a new family of hardware-efficient backbone networks called LowFormer. LowFormer achieves a remarkable speedup in terms of throughput and latency while achieving similar or better accuracy than current state-of-the-art efficient backbones. In order to prove the generalizability of our hardware-efficient design we evaluate our method on GPU mobile GPU and ARM CPU. We further show that the downstream tasks object detection and semantic segmentation profit from our hardware-efficient architecture. Code and models are available at https://github.com/altair199797/LowFormer.	https://openaccess.thecvf.com//content/WACV2025/html/Nottebaum_LowFormer_Hardware_Efficient_Design_for_Convolutional_Transformer_Backbones_WACV_2025_paper.html	Moritz Nottebaum, Matteo Dunnhofer, Christian Micheloni
LumiGauss: Relightable Gaussian Splatting in the Wild	Decoupling lighting from geometry using unconstrained photo collections is notoriously challenging. Solving it would benefit many users as creating complex 3D assets takes days of manual labor. Many previous works have attempted to address this issue often at the expense of output fidelity which questions the practicality of such methods. We introduce LumiGauss - a technique that tackles 3D reconstruction of scenes and environmental lighting through 2D Gaussian Splatting. Our approach yields high-quality scene reconstructions and enables realistic lighting synthesis under novel environment maps. We also propose a method for enhancing the quality of shadows common in outdoor scenes by exploiting spherical harmonics properties. Our approach facilitates seamless integration with game engines and enables the use of fast precomputed radiance transfer. We validate our method on the NeRF-OSR dataset demonstrating superior performance over baseline methods. Moreover LumiGauss can synthesize realistic images for unseen environment maps. Our code: https: //github.com/joaxkal/lumigauss.	https://openaccess.thecvf.com//content/WACV2025/html/Kaleta_LumiGauss_Relightable_Gaussian_Splatting_in_the_Wild_WACV_2025_paper.html	Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, Marek Kowalski
MAGMA: Manifold Regularization for MAEs	Masked Autoencoders (MAEs) are an important divide in self-supervised learning (SSL) due to their independence from augmentation techniques for generating positive (and/or negative) pairs as in contrastive frameworks. Their masking and reconstruction strategy also nicely aligns with SSL approaches in natural language processing. Most MAEs are built upon Transformer-based architectures where visual features are not regularized as opposed to their convolutional neural network (CNN) based counterparts which can potentially hinder their performance. To address this we introduce MAGMA a novel batch-wide layer-wise regularization loss applied to representations of different Transformer layers. We demonstrate that by plugging in the proposed regularization loss one can significantly improve the performance of MAE-based models. We further demonstrate the impact of the proposed loss on optimizing other generic SSL approaches (such as VICReg and SimCLR) broadening the impact of the proposed approach. Our code base can be found at https://github.com/adondera/magma.	https://openaccess.thecvf.com//content/WACV2025/html/Dondera_MAGMA_Manifold_Regularization_for_MAEs_WACV_2025_paper.html	Alin-Eugen Dondera, Anuj R Singh, Hadi Jamali-Rad
MAISI: Medical AI for Synthetic Imaging	Medical imaging analysis faces challenges such as data scarcity high annotation costs and privacy concerns. This paper introduces the Medical AI for Synthetic Imaging (MAISI) an innovative approach using the diffusion model to generate synthetic 3D computed tomography (CT) images to address those challenges. MAISI leverages the foundation volume compression network and the latent diffusion model to produce high-resolution CT images (up to a landmark volume dimension of 512 x 512 x 768 ) with flexible volume dimensions and voxel spacing. By incorporating ControlNet MAISI can process organ segmentation including 127 anatomical structures as additional conditions and enables the generation of accurately annotated synthetic images that can be used for various downstream tasks. Our experiment results show that MAISI's capabilities in generating realistic anatomically accurate images for diverse regions and conditions reveal its promising potential to mitigate challenges using synthetic data.	https://openaccess.thecvf.com//content/WACV2025/html/Guo_MAISI_Medical_AI_for_Synthetic_Imaging_WACV_2025_paper.html	Pengfei Guo, Can Zhao, Dong Yang, Ziyue Xu, Vishwesh Nath, Yucheng Tang, Benjamin Simon, Mason Belue, Stephanie Harmon, Baris Turkbey, Daguang Xu
MDCN-PS: Monocular-Depth-Guided Coarse Normal Attention for Robust Photometric Stereo	Photometric Stereo (PS) is a technique for estimating surface normals from images illuminated by multiple light sources. However when the target object has a complex shape or the light sources are not appropriately arranged certain regions may experience severe shadows leading to insufficient information for accurate estimation. In this paper we propose a Monocular-Depth-guided Coarse Normal attention for Photometric Stereo (MDCN-PS). The MDCN-PS can effectively combine monocular depth from a single image with PS with multiple light sources by a Photometric Stereo network Adaptor (PS Adaptor) with Coarse Normal Attention. The key is to use the coarse normals obtained from Monocular Depth Estimation as supplementary information which can improve accuracy in regions where the light source is limited due to severe shadows or inhomogeneous light source distribution. Comprehensive experiments on real-world and synthetic datasets show that the proposed method achieved an accuracy improvement of 1.2 points in real-world datasets when limited to two input images and of 3.1 points in synthetic datasets in mean angular error compared to existing methods. Qualitative results also demonstrated that our method improves accuracy in areas with insufficient lighting patterns due to shadows.	https://openaccess.thecvf.com//content/WACV2025/html/Yamaguchi_MDCN-PS_Monocular-Depth-Guided_Coarse_Normal_Attention_for_Robust_Photometric_Stereo_WACV_2025_paper.html	Masahiro Yamaguchi, Takashi Shibata, Shoji Yachida, Keiko Yokoyama, Toshinori Hosoi
MENTOR: Human Perception-Guided Pretraining for Increased Generalization	Leveraging human perception into training of convolutional neural networks (CNN) has boosted generalization capabilities of such models in open-set recognition tasks. One of the active research questions is where (in the model architecture or training pipeline) and how to efficiently incorporate always-limited human perceptual data into training strategies of models. In this paper we introduce MENTOR (huMan pErceptioN-guided preTraining fOr increased geneRalization) which addresses this question through two unique rounds of training CNNs tasked with open-set anomaly detection. First we train an autoencoder to learn human saliency maps given an input image without any class labels. The autoencoder is thus tasked with discovering domain-specific salient features which mimic human perception. Second we remove the decoder part add a classification layer on top of the encoder and train this new model conventionally now using class labels. We show that MENTOR successfully raises the generalization performance across three different CNN backbones in a variety of anomaly detection tasks (demonstrated for detection of unknown iris presentation attacks synthetically-generated faces and anomalies in chest X-ray images) compared to traditional pretraining methods (e.g. sourcing the weights from ImageNet) and as well as state-of-the-art methods that incorporate human perception guidance into training. In addition we demonstrate that MENTOR can be flexibly applied to existing human perception-guided methods and subsequently increasing their generalization with no architectural modifications.	https://openaccess.thecvf.com//content/WACV2025/html/Crum_MENTOR_Human_Perception-Guided_Pretraining_for_Increased_Generalization_WACV_2025_paper.html	Colton R. Crum, Adam Czajka
MFNeRF: Memory Efficient NeRF with Mixed-Feature Hash Table	Recently neural radiance fields (NeRFs) have shown remarkable performance in generating photorealistic novel views in 3D modeling. The traditional NeRF typically requires extensive training and long rendering times inspiring many recent works to utilize efficient data structures such as feature grids to ease the computational burden of multilayer perceptron networks. However Those approaches require storing features in dense grids that demands a substantial amount of memory space resulting in a notable memory bottleneck. Consequently it leads to a significant increase in training time. To address this issue in this work we propose MFNeRF a memory-efficient NeRF framework that employs a mixed-feature hash table to improve memory efficiency and reduce training time while maintaining reconstruction quality. Specifically we first design a mixed-feature hash encoding method to adaptively mix parts of multi-level feature grids and map them to a single hash table. Following that in order to obtain the correct index of a grid point we further develop an index transformation method that transforms indices of an arbitrary-level grid to those of a canonical grid. Extensive benchmarking against the state-of-the-art methods including InstantNGP TensoRF and DVGO indicates that our MFNeRF achieves faster training and rendering times on the same GPU hardware with a significantly smaller memory while maintaining similar or even higher reconstruction quality. Compared to the InstantNGP-Big model our method could achieve 89% improvement in the figure of merit defined in terms of PSNR*FPS/MB.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_MFNeRF_Memory_Efficient_NeRF_with_Mixed-Feature_Hash_Table_WACV_2025_paper.html	Yongjae Lee, Li Yang, Deliang Fan
MFTIQ: Multi-Flow Tracker with Independent Matching Quality Estimation	"In this work we present MFTIQ a novel dense long-term tracking model that advances the Multi-Flow Tracker (MFT) framework to address challenges in point-level visual tracking in video sequences. MFTIQ builds upon the flow-chaining concepts of MFT integrating an Independent Quality (IQ) module that separates correspondence quality estimation from optical flow computations. This decoupling significantly enhances the accuracy and flexibility of the tracking process allowing MFTIQ to maintain reliable trajectory predictions even in scenarios of prolonged occlusions and complex dynamics. Designed to be ""plug-and-play"" MFTIQ can be employed with any off-the-shelf optical flow method without the need for fine-tuning or architectural modifications. Experimental validations on the TAP-Vid Davis dataset show that MFTIQ with RoMa optical flow not only surpasses MFT but also performs comparably to state-of-the-art trackers while having substantially faster processing speed. Code and models available at https://github.com/serycjon/MFTIQ."	https://openaccess.thecvf.com//content/WACV2025/html/Serych_MFTIQ_Multi-Flow_Tracker_with_Independent_Matching_Quality_Estimation_WACV_2025_paper.html	Jonas Serych, Michal Neoral, Jiri Matas
MFTrans: A Multi-Resolution Fusion Transformer for Robust Tumor Segmentation in Whole Slide Images	Accurate tumor segmentation in whole slide image (WSI) is essential for histopathological diagnosis and research but the traditional manual analysis is labor-intensive and prone to variability. Furthermore many artificial models focus on specific magnification images limiting the detailed information available for segmentation. To address these challenges we propose MFTrans a novel multi-resolution fusion transformer with a CNN-based architecture designed for efficient tumor segmentation in WSI. Inspired by the diagnostic procedures of expert pathologists MFTrans integrates both high- and low-magnification images capturing detailed local features and broader contextual relationships through a dual-branch architecture. The model employs a global token transformer and cross-attention mechanism to fuse hierarchical features from dual branches to improve segmentation performance. We evaluate MFTrans on three real-world WSI datasets: Camelyon16 PAIP2019 and Catholic Uijeongbu St. Mary's hospital dataset demonstrating its superior segmentation performance over state-of-the-art methods in balanced and imbalanced setups. These results highlight MFTrans's effectiveness in medical image analysis and its generalizability across different datasets making it a robust tool for automated cancer diagnostics. Our code is available at https://github.com/aimed-gist/MFTrans.	https://openaccess.thecvf.com//content/WACV2025/html/Yang_MFTrans_A_Multi-Resolution_Fusion_Transformer_for_Robust_Tumor_Segmentation_in_WACV_2025_paper.html	Sungkyu Yang, Woohyun Park, Kwangil Yim, Mansu Kim
MIP-GAF: A MLLM-Annotated Benchmark for Most Important Person Localization and Group Context Understanding	Estimating the Most Important Person (MIP) in any social event setup is a challenging problem mainly due to contextual complexity and scarcity of labeled data. Moreover the causality aspects of MIP estimation are quite subjective and diverse. To this end we aim to address the problem by annotating a large-scale 'in-the-wild' dataset for identifying human perceptions about the 'Most Important Person (MIP)' in an image. The paper provides a thorough description of our proposed Multimodal Large Language Model (MLLM) based data annotation strategy and a thorough data quality analysis. Further we perform a comprehensive benchmarking of the proposed dataset utilizing state-of-the-art MIP localization methods indicating a significant drop in performance compared to existing datasets. The performance drop shows that the existing MIP localization algorithms must be more robust with respect to 'in-the-wild' situations. We believe the proposed dataset will play a vital role in building the next-generation social situation understanding methods. The dataset and associated code will be made available for research purposes.	https://openaccess.thecvf.com//content/WACV2025/html/Madan_MIP-GAF_A_MLLM-Annotated_Benchmark_for_Most_Important_Person_Localization_and_WACV_2025_paper.html	S. Madan, S. Ghosh, L. R. Sookha, M.A. Ganaie, R. Subramanian, A. Dhall, T. Gedeon
MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated Learning	Previous studies on federated learning (FL) often encounter performance degradation due to data heterogeneity among different clients. In light of the recent advances in multimodal large language models (MLLMs) such as GPT-4v and LLaVA which demonstrate their exceptional proficiency in multimodal tasks such as image captioning and multimodal question answering. We introduce a novel federated learning framework named Multimodal Large Language Model Assisted Federated Learning (MLLM-LLaVA-FL) which employs powerful MLLMs at the server end to address the heterogeneous and long-tailed challenges. Owing to the advanced cross-modality representation capabilities and the extensive open-vocabulary prior knowledge of MLLMs our framework is adept at harnessing the extensive yet previously underexploited open-source data accessible from websites and powerful server-side computational resources. Hence the MLLM-LLaVA-FL not only enhances the performance but also avoids increasing the risk of privacy leakage and the computational burden on local devices distinguishing it from prior methodologies. Our framework has three key stages. Initially we conduct global visual-text pretraining of the model. This pretraining is facilitated by utilizing the extensive open-source data available online with the assistance of MLLMs. Subsequently the pretrained model is distributed among various clients for local training. Finally once the locally trained models are transmitted back to the server a global alignment is carried out under the supervision of MLLMs to further enhance the performance. Experimental evaluations on established benchmarks show that our framework delivers promising performance in the typical scenarios with data heterogeneity and long-tail distribution across different clients in FL.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_MLLM-LLaVA-FL_Multimodal_Large_Language_Model_Assisted_Federated_Learning_WACV_2025_paper.html	Jianyi Zhang, Hao Yang, Ang Li, Xin Guo, Pu Wang, Haiming Wang, Yiran Chen, Hai Li
MLLM-Tool: A Multimodal Large Language Model for Tool Agent Learning	Recently the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However the current LLMs' ability to perceive tool use is limited to a single text query which may result in ambiguity in understanding the users' real intentions. LLMs are expected to eliminate that by perceiving the information in the visual- or auditory-grounded instructions. Therefore in this paper we propose MLLM-Tool a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the model's capability we collect a dataset featuring multi-modal input tools from HuggingFace. Another essential feature of our dataset is that it also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions which provides more potential solutions for the same query. The experiments reveal that our MLLM-Tool is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at github.com/MLLMTool/MLLM-Tool.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_MLLM-Tool_A_Multimodal_Large_Language_Model_for_Tool_Agent_Learning_WACV_2025_paper.html	Chenyu Wang, Weixin Luo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, Shenghua Gao
MONAS-ESNN: Multi-Objective Neural Architecture Search for Efficient Spiking Neural Networks	Spiking Neural Networks (SNNs) have emerged as a compelling alternative to traditional Artificial Neural Networks (ANNs) due to their energy efficiency and biological plausibility. However current SNN models often rely on ANN architectures that may not fully exploit the unique properties of SNNs. Neural Architecture Search (NAS) approaches have been shown to automate the identification of suitable architectures for various applications. Nevertheless very few works have been presented on NAS for SNNs and particularly for identifying architectures that achieve high accuracy while capitalizing on the energy efficiency property of SNNs. In this paper we present a Multi-Objective Neural Architecture Search for Efficient SNNs (MONAS-ESNN) approach that utilizes a training-free NAS to discover SNN architectures that optimize both accuracy and energy efficiency. The proposed MONAS-ESNN uses the NSGA-II evolutionary algorithm to optimize both objective functions while leveraging the unique temporal dynamics of SNNs. We also introduce a new Adjusted Sparsity-Aware Hamming Distance (ASAHD) that enhances the evaluation of potential architectures by representing diverse spike activation patterns for different types of spiking neurons. Experimental results on CIFAR-10 CIFAR-100 and Tiny-ImageNet-200 datasets demonstrate that MONAS-ESNN identifies SNN architectures that have higher accuracy and are more efficient as measured by the number of generated spikes than existing methods. Therefore the proposed MONAS-ESNN can automate the search and discovery of SNN architectures with higher accuracy fewer generated spikes and faster convergence paving the way for more energy-efficient neural networks.	https://openaccess.thecvf.com//content/WACV2025/html/Saghand_MONAS-ESNN_Multi-Objective_Neural_Architecture_Search_for_Efficient_Spiking_Neural_Networks_WACV_2025_paper.html	Esmat Ghasemi Saghand, Susana K. Lai-Yuen
MOOSS: Mask-Enhanced Temporal Contrastive Learning for Smooth State Evolution in Visual Reinforcement Learning	In visual Reinforcement Learning (RL) learning from pixel-based observations poses significant challenges on sample efficiency primarily due to the complexity of extracting informative state representations from high-dimensional data. Previous methods such as contrastive-based approaches have made strides in improving sample efficiency but fall short in modeling the nuanced evolution of states. To address this we introduce MOOSS a novel framework that leverages a temporal contrastive objective with the help of graph-based spatial-temporal masking to explicitly model state evolution in visual RL. Specifically we propose a self-supervised dual-component strategy that integrates (1) a graph construction of pixel-based observations for spatial-temporal masking coupled with (2) a multi-level contrastive learning mechanism that enriches state representations by emphasizing temporal continuity and change of states. MOOSS advances the understanding of state dynamics by disrupting and learning from spatial-temporal correlations which facilitates policy learning. Our comprehensive evaluation on multiple continuous and discrete control benchmarks shows that MOOSS outperforms previous state-of-the-art visual RL methods in terms of sample efficiency demonstrating the effectiveness of our method.	https://openaccess.thecvf.com//content/WACV2025/html/Sun_MOOSS_Mask-Enhanced_Temporal_Contrastive_Learning_for_Smooth_State_Evolution_in_WACV_2025_paper.html	Jiarui Sun, M. Ugur Akcal, Girish Chowdhary, Wei Zhang
MRI Reconstruction with Regularized 3D Diffusion Model (R3DM)	Magnetic Resonance Imaging (MRI) is a powerful imaging technique widely used for visualizing structures within the human body and in other fields such as plant sciences. However there is a demand to develop fast 3D-MRI reconstruction algorithms to show the fine structure of objects from under-sampled acquisition data i.e. k-space data. This emphasizes the need for efficient solutions that can handle limited input while maintaining high-quality imaging. In contrast to previous methods only using 2D we propose a 3D MRI reconstruction method that leverages a regularized 3D diffusion model combined with optimization method. By incorporating diffusion-based priors our method improves image quality reduces noise and enhances the overall fidelity of 3D MRI reconstructions. We conduct comprehensive experiments analysis on clinical and plant science MRI datasets. To evaluate the algorithm effectiveness for under-sampled k-space data we also demonstrate its reconstruction performance with several undersampling patterns as well as with in- and out-of-distribution pre-trained data. In experiments we show that our method improves upon tested competitors	https://openaccess.thecvf.com//content/WACV2025/html/Bangun_MRI_Reconstruction_with_Regularized_3D_Diffusion_Model_R3DM_WACV_2025_paper.html	Arya Bangun, Zhuo Cao, Alessio Quercia, Hanno Scharr, Elisabeth Pfaehler
MS-Glance: Bio-Inspired Non-Semantic Context Vectors and their Applications in Supervising Image Reconstruction	Non-semantic context information is crucial for visual recognition as the human visual perception system first uses global statistics to process scenes rapidly before identifying specific objects. However while semantic information is increasingly incorporated into computer vision tasks such as image reconstruction non-semantic information such as global spatial structures is often overlooked. To bridge the gap we propose a biologically informed non-semantic context descriptor MS-Glance along with the Glance Index Measure for comparing two images. A Global Glance vector is formulated by randomly retrieving pixels based on a perception-driven rule from an image to form a vector representing non-semantic global context while a local Glance vector is a flattened local image window mimicking a zoom-in observation. The Glance Index is defined as the inner product of two standardized sets of Glance vectors. We evaluate the effectiveness of incorporating Glance supervision in two reconstruction tasks: image fitting with implicit neural representation (INR) and undersampled MRI reconstruction. Extensive experimental results show that MS-Glance outperforms existing image restoration losses across both natural and medical images. The code is available at https://github.com/Z7Gao/MSGlance.	https://openaccess.thecvf.com//content/WACV2025/html/Gao_MS-Glance_Bio-Inspired_Non-Semantic_Context_Vectors_and_their_Applications_in_Supervising_WACV_2025_paper.html	Ziqi Gao, Wendi Yang, Yujia Li, Lei Xing, S. Kevin Zhou
MSI-NeRF: Linking Omni-Depth with View Synthesis through Multi-Sphere Image Aided Generalizable Neural Radiance Field	Panoramic observation using fisheye cameras is significant in virtual reality (VR) and robot perception. However panoramic images synthesized by traditional methods lack depth information and can only provide three degrees-of-freedom (3DoF) rotation rendering in VR applications. To fully preserve and exploit the parallax information within the original fisheye cameras we introduce MSI-NeRF which combines deep learning omnidirectional depth estimation and novel view synthesis. We construct a multi-sphere image as a cost volume through feature extraction and warping of the input images. We further build an implicit radiance field using spatial points and interpolated 3D feature vectors as input which can simultaneously realize omnidirectional depth estimation and 6DoF view synthesis. Leveraging the knowledge from depth estimation task our method can learn scene appearance by source view supervision only. It does not require novel target views and can be trained conveniently on existing panorama depth estimation datasets. Our network has the generalization ability to reconstruct unknown scenes efficiently using only four images. Experimental results show that our method outperforms existing methods in both depth estimation and novel view synthesis tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Yan_MSI-NeRF_Linking_Omni-Depth_with_View_Synthesis_through_Multi-Sphere_Image_Aided_WACV_2025_paper.html	Dongyu Yan, Guanyu Huang, Fengyu Quan, Haoyao Chen
MVAD: A Multiple Visual Artifact Detector for Video Streaming	Visual artifacts are often introduced into streamed video content due to prevailing conditions during content production and delivery. Since these can degrade the quality of the user's experience it is important to automatically and accurately detect them in order to enable effective quality measurement and enhancement. Existing detection methods often focus on a single type of artifact and/or determine the presence of an artifact through thresholding objective quality indices. Such approaches have been reported to offer inconsistent prediction performance and are also impractical for real-world applications where multiple artifacts co-exist and interact. In this paper we propose a Multiple Visual Artifact Detector MVAD for video streaming which for the first time is able to detect multiple artifacts using a single framework that is not reliant on video quality assessment models. Our approach employs a new Artifact-aware Dynamic Feature Extractor (ADFE) to obtain artifact-relevant spatial features within each frame for multiple artifact types. The extracted features are further processed by a Recurrent Memory Vision Transformer (RMViT) module which captures both short-term and long-term temporal information within the input video. The proposed network architecture is optimized in an end-to-end manner based on a new large and diverse training database that is generated by simulating the video streaming pipeline and based on Adversarial Data Augmentation. This model has been evaluated on two video artifact databases Maxwell and BVI-Artifact and achieves consistent and improved prediction results for ten target visual artifacts when compared to seven existing single and multiple artifact detectors. The source code and training database will be available at https://chenfeng-bristol.github.io/MVAD/.	https://openaccess.thecvf.com//content/WACV2025/html/Feng_MVAD_A_Multiple_Visual_Artifact_Detector_for_Video_Streaming_WACV_2025_paper.html	Chen Feng, Duolikun Danier, Fan Zhang, Alex Mackin, Andrew Collins, David Bull
MVFNet: Multipurpose Video Forensics Network using Multiple Forms of Forensic Evidence	While videos can be falsified in many different ways most existing forensic networks are specialized to detect only a single manipulation type (e.g. deepfake inpainting). This poses a significant issue as the manipulation used to falsify a video is not known a priori. To address this problem we propose MVFNet - a multipurpose video forensics network capable of detecting multiple types of manipulations including inpainting deepfakes splicing and editing. Our network does this by extracting and jointly analyzing a broad set of forensic feature modalities that capture both spatial and temporal anomalies in falsified videos. To reliably detect and localize fake content of all shapes and sizes our network employs a novel Multi-Scale Hierarchical Transformer module to identify forensic inconsistencies across multiple spatial scales. Experimental results show that our network obtains state-of-the-art performance in general scenarios where multiple different manipulations are possible and rivals specialized detectors in targeted scenarios.	https://openaccess.thecvf.com//content/WACV2025/html/Nguyen_MVFNet_Multipurpose_Video_Forensics_Network_using_Multiple_Forms_of_Forensic_WACV_2025_paper.html	Tai D Nguyen, Matthew C Stamm
MVMD: A Multi-View Approach for Enhanced Mirror Detection	In 3D reconstruction mirrors introduce significant challenges by creating distorted and fragmented spaces resulting in inaccurate and unreliable 3D models. As 3D reconstruction typically relies on multi-view images to capture different perspectives of a scene detecting and labeling mirrors in multi-view images before reconstruction can effectively address this issue. However existing methods focus solely on single-image detection overlooking the rich information provided by multi-view setups. To overcome this limitation we propose MVMD a novel Multi-View Mirror Detection method along with the first database specifically designed for mirror detection in multi-view scenes. The design of MVMD is grounded in the inherent associations between objects seen from different views and those reflected inside and outside of mirrors. These relationships are learned through cross- and self-attention mechanisms. MVMD consists of three key blocks: the Inter-Views Block tracks the shifts of objects within mirrors caused by changes in viewpoint; the Intra-View Block detects object reflections inside mirrors; and the Refinement Block sharpens mirror boundaries and enhances detected details. Experimental results show that our method improves accuracy by up to 2.6% and IoU by up to 11.1% compared to single-image mirror detection techniques. This substantial improvement makes MVMD particularly effective for computer vision tasks especially in enhancing the accuracy of 3D reconstruction in mirror-dense environments. Code and data are available at: https://github.com/mvmdwacv25.	https://openaccess.thecvf.com//content/WACV2025/html/Shen_MVMD_A_Multi-View_Approach_for_Enhanced_Mirror_Detection_WACV_2025_paper.html	Yidan Shen, Yu Wen, Chen Zhang, Xin Fu, Renjie Hu
MagicStick: Controllable Video Editing via Control Handle Transformations	Text-based video editing has recently attracted considerable interest in changing the style or replacing the objects with a similar structure. Beyond this we demonstrate that properties such as shape size location motion etc. can also be edited in videos. Our key insight is that the keyframe's transformations of the specific internal feature (e.g. edge maps of objects or human pose) can easily propagate to other frames to provide generation guidance. We thus propose MagicStick a controllable video editing method that edits the video properties by utilizing the transformation on the extracted internal control signals. In detail to keep the appearance we inflate both the pre-trained image diffusion model and ControlNet to the temporal dimension and train low-rank adaptions (LoRA) layers to fit the specific scenes. Then in editing we perform an inversion and editing framework. Differently finetuned ControlNet is introduced in both inversion and generation for attention guidance with the proposed attention remix between the spatial attention maps of inversion and editing. Yet succinct our method is the first method to show the ability of video property editing from the pre-trained text-to-image model. We present experiments on numerous examples within our unified framework. We also compare with shape-aware text-based editing and handcrafted motion video generation demonstrating our superior temporal consistency and editing capability than previous works.	https://openaccess.thecvf.com//content/WACV2025/html/Ma_MagicStick_Controllable_Video_Editing_via_Control_Handle_Transformations_WACV_2025_paper.html	Yue Ma, Xiaodong Cun, Sen Liang, Jinbo Xing, Yingqing He, Chenyang Qi, Siran Chen, Qifeng Chen
Make VLM Recognize Visual Hallucination on Cartoon Character Image with Pose Information	Leveraging large-scale Text-to-Image (TTI) models have become a common technique to generate training or reference data in the field of image synthesis video editing 3D reconstruction. However semantic structural visual hallucinations which contain perceptually critical defects remain a concern especially in non-photorealistic rendering domains such as cartoon pixelization-style character. We propose a novel semantic structural hallucination detection system in cartoon-style images generated by TTI models collecting a new cartoon-hallucination dataset. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with public Vision-Language Models (VLMs) utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. Within selected two VLMs GPT-4v Gemini pro vision our proposed PA-ICVL improves the hallucination detection with 50% to 78% 57% to 80% respectively. This research advances a capability of TTI models toward real-world applications by mitigating visual hallucinations via in-context visual learning expanding their potential in non-photorealistic domains. Besides when VLM confront ambiguous tasks this results showcase thought-provoking insights how users boost the domain-adaptive capability of VLM by harnessing additional conditions. The dataset and demo VLMs are provided in the corresponding Git repository: https://github.com/gh-BumsooKim/Cartoon-Hallucinations-Detection.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_Make_VLM_Recognize_Visual_Hallucination_on_Cartoon_Character_Image_with_WACV_2025_paper.html	Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Yonghoon Jung, Sanghyun Seo
Make-A-Texture: Fast Shape-Aware 3D Texture Generation in 3 Seconds	We present Make-A-Texture a new framework that efficiently synthesizes high-resolution texture maps from textual prompts for given 3D geometries. Our approach progressively generates textures that are consistent across multiple viewpoints with a depth-aware inpainting diffusion model in an optimized sequence of viewpoints determined by an automatic view selection algorithm. A significant feature of our method is its remarkable efficiency achieving a full texture generation within an end-to-end runtime of just 3.07 seconds on a single NVIDIA H100 GPU significantly outperforming existing methods. Such an acceleration is achieved by optimizations in the diffusion model and a specialized backprojection method. Moreover our method reduces the artifacts in the backprojection phase by selectively masking out non-frontal faces and internal faces of open-surfaced objects. Experimental results demonstrate that Make-A-Texture matches or exceeds the quality of other state-of-the-art methods. Our work significantly improves the applicability and practicality of texture generation models for real-world 3D content creation including interactive creation and text-guided texture editing.	https://openaccess.thecvf.com//content/WACV2025/html/Gorelik_Make-A-Texture_Fast_Shape-Aware_3D_Texture_Generation_in_3_Seconds_WACV_2025_paper.html	Liat Sless Gorelik, Yuchen Fan, Omri Armstrong, Forrest N Iandola, Yilei Li, Ita Lifshitz, Rakesh Ranjan
Mamba-ST: State Space Model for Efficient Style Transfer	The goal of style transfer is given a content image and a style source generating a new image preserving the content but with the artistic representation of the style source. Most of the state-of-the-art architectures use transformers or diffusion-based models to perform this task despite the heavy computational burden that they require. In particular transformers use self- and cross-attention layers which have large memory footprint while diffusion models require high inference time. To overcome the above this paper explores a novel design of Mamba an emergent State-Space Model (SSM) called Mamba-ST to perform style transfer. To do so we adapt Mamba linear equation to simulate the behavior of cross-attention layers which are able to combine two separate embeddings into a single output but drastically reducing memory usage and time complexity. We modified the Mamba's inner equations so to accept inputs from and combine two separate data streams. To the best of our knowledge this is the first attempt to adapt the equations of SSMs to a vision task like style transfer without requiring any other module like cross-attention or custom normalization layers. An extensive set of experiments demonstrates the superiority and efficiency of our method in performing style transfer compared to transformers and diffusion models. Results show improved quality in terms of both ArtFID and FID metrics. Code is available at https://github.com/FilippoBotti/MambaST.	https://openaccess.thecvf.com//content/WACV2025/html/Botti_Mamba-ST_State_Space_Model_for_Efficient_Style_Transfer_WACV_2025_paper.html	Filippo Botti, Alex Ergasti, Leonardo Rossi, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati
MambaRecon: MRI Reconstruction with Structured State Space Models	Magnetic Resonance Imaging (MRI) is one of the most important medical imaging modalities as it provides superior resolution of soft tissues albeit with a notable limitation in scanning speed. The advent of deep learning has catalyzed the development of cutting-edge methods for the expedited reconstruction of MRI scans utilizing convolutional neural networks and more recently vision transformers. Recently proposed structured state space models (e.g. Mamba) have gained some traction due to their efficiency and low computational requirements compared to transformer models. We propose an innovative MRI reconstruction framework that employs structured state space models at its core aimed at amplifying both long-range contextual sensitivity and reconstruction efficacy. Comprehensive experiments on public brain MRI datasets show that our model sets new benchmarks beating state-of-the-art reconstruction baselines. Code is available at https://github.com/yilmazkorkmaz1/MambaRecon.	https://openaccess.thecvf.com//content/WACV2025/html/Korkmaz_MambaRecon_MRI_Reconstruction_with_Structured_State_Space_Models_WACV_2025_paper.html	Yilmaz Korkmaz, Vishal M. Patel
MaskVD: Region Masking for Efficient Video Object Detection	Video tasks are compute-heavy and thus pose a challenge when deploying in real-time applications particularly for tasks that require state-of-the-art Vision Transformers (ViTs). Several research efforts have tried to address this challenge by leveraging the fact that large portions of the video undergo very little change across frames leading to redundant computations in frame-based video processing. In particular some works leverage pixel or semantic differences across frames however this yields limited latency benefits with significantly increased memory overhead. This paper in contrast presents a strategy for masking regions in video frames that leverages the semantic information in images and the temporal correlation between frames to significantly reduce FLOPs and latency with little to no penalty in performance over baseline models. In particular we demonstrate that by leveraging extracted features from previous frames ViT backbones directly benefit from region masking skipping up to 80% of input regions improving FLOPs and latency by 3.14x and 1.5x. We improve memory and latency over the state-of-the-art (SOTA) by 2.3x and 1.14x while maintaining similar detection performance. Additionally our approach demonstrates promising results on convolutional neural networks (CNNs) and provides latency improvements over the SOTA up to 1.3x using specialized computational kernels. Code is available at: https://github.com/sreetamasarkar/MaskVD	https://openaccess.thecvf.com//content/WACV2025/html/Sarkar_MaskVD_Region_Masking_for_Efficient_Video_Object_Detection_WACV_2025_paper.html	Sreetama Sarkar, Gourav Datta, Souvik Kundu, Kai Zheng, Chirayata Bhattacharyya, Peter A. Beerel
MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction	Achieving accurate material segmentation for 3-channel RGB images is challenging due to the considerable variation in material appearance. Hyperspectral images which are sets of spectral measurements sampled at multiple wavelengths theoretically offer distinct information for material identification as variations in the intensity of electromagnetic radiation reflected by a surface depend on the material composition of a scene. However existing hyperspectral datasets are impoverished in terms of the number of images and material categories for the dense material segmentation task and collecting and annotating hyperspectral images with a spectral camera is prohibitively expensive. To address this we propose a novel model the MatSpectNet to segment materials with recovered hyperspectral images from RGB images. The network leverages the principles of colour perception in modern cameras to constrain the reconstructed hyperspectral images and employs the domain adaptation method to generalise the hyperspectral reconstruction capability from a spectral recovery dataset to material segmentation datasets. The reconstructed hyperspectral images are further filtered using learnt response curves and enhanced with human perception. The performance of MatSpectNet is evaluated on the LMD dataset as well as the OpenSurfaces dataset. Our experiments demonstrate that MatSpectNet attains a 1.60% increase in average pixel accuracy and a 3.42% improvement in mean class accuracy compared with the most recent publication. Additional experiments and the project code are published on https://github.com/heng-yuwen/MatSpectNet.	https://openaccess.thecvf.com//content/WACV2025/html/Heng_MatSpectNet_Material_Segmentation_Network_with_Domain-Aware_and_Physically-Constrained_Hyperspectral_Reconstruction_WACV_2025_paper.html	Yuwen Heng, Yihong Wu, Srinandan Dasmahapatra, Hansung Kim
McCaD: Multi-Contrast MRI Conditioned Adaptive Adversarial Diffusion Model for High-Fidelity MRI Synthesis	Magnetic Resonance Imaging (MRI) is instrumental in clinical diagnosis offering diverse contrasts that provide comprehensive diagnostic information. However acquiring multiple MRI contrasts is often constrained by high costs long scanning durations and patient discomfort. Current synthesis methods typically focused on single-image contrasts fall short in capturing the collective nuances across various contrasts. Moreover existing methods for multi-contrast MRI synthesis often fail to accurately map feature level information across multiple imaging contrasts. We introduce McCaD (Multi-Contrast MRI Conditioned Adaptive Adversarial Diffusion) a novel framework leveraging an adversarial diffusion model conditioned on multiple contrasts for high-fidelity MRI synthesis. McCaD significantly enhances synthesis accuracy by employing a multiscale feature-guided mechanism incorporating denoising and semantic encoders. An adaptive feature maximization strategy and a spatial feature-attentive loss have been introduced to capture more intrinsic features across multiple contrasts. This facilitates a precise and comprehensive feature-guided denoising process. Extensive experiments on tumor and healthy multi-contrast MRI datasets demonstrated that the McCaD outperforms state-of-the-art baselines quantitively and qualitatively. The code is available at https://github.com/sanuwanihewa/McCaD.	https://openaccess.thecvf.com//content/WACV2025/html/Dayarathna_McCaD_Multi-Contrast_MRI_Conditioned_Adaptive_Adversarial_Diffusion_Model_for_High-Fidelity_WACV_2025_paper.html	Sanuwani Dayarathna, Kh Tohidul Islam, Bohan Zhuang, Guang Yang, Jianfei Cai, Meng Law, Zhaolin Chen
MegaFusion: Extend Diffusion Models towards Higher-Resolution Image Generation without Further Tuning	Diffusion models have emerged as frontrunners in text-to-image generation but their fixed image resolution during training often leads to challenges in high-resolution image generation such as semantic deviations and object replication. This paper introduces MegaFusion a novel approach that extends existing diffusion-based text-to-image models towards efficient higher-resolution generation without additional fine-tuning or adaptation. Specifically we employ an innovative truncate and relay strategy to bridge the denoising processes across different resolutions allowing for high-resolution image generation in a coarse-to-fine manner. Moreover by integrating dilated convolutions and noise re-scheduling we further adapt the model's priors for higher resolution. The versatility and efficacy of MegaFusion make it universally applicable to both latent-space and pixel-space diffusion models along with other derivative models. Extensive experiments confirm that MegaFusion significantly boosts the capability of existing models to produce images of megapixels and various aspect ratios while only requiring about 40% of the original computational cost. Code is available at https://haoningwu3639.github.io/MegaFusion/.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_MegaFusion_Extend_Diffusion_Models_towards_Higher-Resolution_Image_Generation_without_Further_WACV_2025_paper.html	Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang
MemControl: Mitigating Memorization in Diffusion Models via Automated Parameter Selection	Diffusion models excel in generating images that closely resemble their training data but are also susceptible to data memorization raising privacy ethical and legal concerns particularly in sensitive domains such as medical imaging. We hypothesize that this memorization stems from the overparameterization of deep models and propose that regularizing model capacity during fine-tuning can mitigate this issue. Firstly we empirically show that regulating the model capacity via Parameter-efficient fine-tuning (PEFT) mitigates memorization to some extent however it further requires the identification of the exact parameter subsets to be fine-tuned for high-quality generation. To identify these subsets we introduce a bi-level optimization framework MemControl that automates parameter selection using memorization and generation quality metrics as rewards during fine-tuning. The parameter subsets discovered through MemControl achieve a superior tradeoff between generation quality and memorization. For the task of medical image generation our approach outperforms existing state-of-the-art memorization mitigation strategies by fine-tuning as few as 0.019% of model parameters. Moreover we demonstrate that the discovered parameter subsets are transferable to non-medical domains. Our framework is scalable to large datasets agnostic to reward functions and can be integrated with existing approaches for further memorization mitigation. To the best of our knowledge this is the first study to empirically evaluate memorization in medical images and propose a targeted yet universal mitigation strategy. The code is available at https://github.com/Raman1121/Diffusion_Memorization_HPO	https://openaccess.thecvf.com//content/WACV2025/html/Dutt_MemControl_Mitigating_Memorization_in_Diffusion_Models_via_Automated_Parameter_Selection_WACV_2025_paper.html	Raman Dutt, Ondrej Bohdal, Pedro Sanchez, Sotirios Tsaftaris, Timothy Hospedales
MemFusionMap: Working Memory Fusion for Online Vectorized HD Map Construction	High-definition (HD) maps provide environmental information for autonomous driving systems and are essential for safe planning. While existing methods with single-frame input achieve impressive performance for online vectorized HD map construction they still struggle with complex scenarios and occlusions. We propose MemFusionMap a novel temporal fusion model with enhanced temporal reasoning capabilities for online HD map construction. Specifically we contribute a working memory fusion module that improves the model's memory capacity to reason across a history of frames. We also design a novel temporal overlap heatmap to explicitly inform the model about the temporal overlap information and vehicle trajectory in the Bird's Eye View space. By integrating these two designs MemFusionMap significantly outperforms existing methods while also maintaining a versatile design for scalability. We conduct extensive evaluation on open-source benchmarks and demonstrate a maximum improvement of 5.4% in mAP over state-of-the-art methods. The project page for MemFusionMap is https://song-jingyu.github.io/MemFusionMap.	https://openaccess.thecvf.com//content/WACV2025/html/Song_MemFusionMap_Working_Memory_Fusion_for_Online_Vectorized_HD_Map_Construction_WACV_2025_paper.html	Jingyu Song, Xudong Chen, Liupei Lu, Jie Li, Katherine A. Skinner
Memory-Efficient Continual Learning with Neural Collapse Contrastive	"Contrastive learning has significantly improved representation quality enhancing knowledge transfer across tasks in continual learning (CL). However catastrophic forgetting remains a key challenge as contrastive based methods primarily focus on ""soft relationships"" or ""softness"" between samples which shift with changing data distributions and lead to representation overlap across tasks. Recently the newly identified Neural Collapse phenomenon has shown promise in CL by focusing on ""hard relationships"" or ""hardness"" between samples and fixed prototypes. However this approach overlooks ""softness"" crucial for capturing intra-class variability and this rigid focus can also pull old class representations toward current ones increasing forgetting. Building on these insights we propose Focal Neural Collapse Contrastive (FNC^2) a novel representation learning loss that effectively balances both soft and hard relationships. Additionally we introduce the Hardness-Softness Distillation (HSD) loss to progressively preserve the knowledge gained from these relationships across tasks. Our method outperforms state-of-the-art approaches particularly in minimizing memory reliance. Remarkably even without the use of memory our approach rivals rehearsal-based methods offering a compelling solution for data privacy concerns."	https://openaccess.thecvf.com//content/WACV2025/html/Dang_Memory-Efficient_Continual_Learning_with_Neural_Collapse_Contrastive_WACV_2025_paper.html	Trung-Anh Dang, Vincent Nguyen, Ngoc-Son Vu, Christel Vrain
Memory-Efficient Pseudo-Labeling for Online Source-Free Universal Domain Adaptation using a Gaussian Mixture Model	In practice domain shifts are likely to occur between training and test data necessitating domain adaptation (DA) to adjust the pre-trained source model to the target domain. Recently universal domain adaptation (UniDA) has gained attention for addressing the possibility of an additional category (label) shift between the source and target domain. This means new classes can appear in the target data some source classes may no longer be present or both at the same time. For practical applicability UniDA methods must handle both source-free and online scenarios enabling adaptation without access to the source data and performing batch-wise updates in parallel with prediction. In an online setting preserving knowledge across batches is crucial. However existing methods often require substantial memory which is impractical because memory is limited and valuable in particular on embedded systems. Therefore we consider memory-efficiency as an additional constraint. To achieve memory-efficient online source-free universal domain adaptation (SF-UniDA) we propose a novel method that continuously captures the distribution of known classes in the feature space using a Gaussian mixture model (GMM). This approach combined with entropy-based out-of-distribution detection allows for the generation of reliable pseudo-labels. Finally we combine a contrastive loss with a KL divergence loss to perform the adaptation. Our approach not only achieves state-of-the-art results in all experiments on the DomainNet and Office-Home datasets but also significantly outperforms the existing methods on the challenging VisDA-C dataset setting a new benchmark for online SF-UniDA. Our code is available at https://github.com/pascalschlachter/GMM.	https://openaccess.thecvf.com//content/WACV2025/html/Schlachter_Memory-Efficient_Pseudo-Labeling_for_Online_Source-Free_Universal_Domain_Adaptation_using_a_WACV_2025_paper.html	Pascal Schlachter, Simon Wagner, Bin Yang
Meta-Learning for Color-to-Infrared Cross-Modal Style Transfer	Recent object detection models for infrared (IR) imagery are based upon deep neural networks (DNNs) and require large amounts of labeled training imagery. However publicly available datasets that can be used for such training are limited in their size and diversity. To address this problem we explore cross-modal style transfer (CMST) to leverage large and diverse color imagery datasets so that they can be used to train DNN-based IR image-based object detectors. We evaluate six contemporary stylization methods on four publicly-available IR datasets - the first comparison of its kind - and find that CMST is highly effective for DNN-based detectors. Surprisingly we find that existing data-driven methods are outperformed by a simple grayscale stylization (an average of the color channels). Our analysis reveals that existing data-driven methods are either too simplistic or introduce significant artifacts into the imagery. To overcome these limitations we propose meta-learning style transfer (MLST) which learns a stylization by composing and tuning well-behaved analytic functions. We find that MLST leads to more complex stylizations without introducing significant image artifacts and achieves the best overall detector performance on our benchmark datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Stump_Meta-Learning_for_Color-to-Infrared_Cross-Modal_Style_Transfer_WACV_2025_paper.html	Evelyn A. Stump, Francesco Luzi, Leslie M. Collins, Jordan M. Malof
MetaVIn: Meteorological and Visual Integration for Atmospheric Turbulence Strength Estimation	Long-range image understanding is a challenging task for computer vision due to the presence of atmospheric turbulence. Turbulence can degrade image quality (blur and geometric distortion) due to the medium's spatio-temporal varying index of refraction bending light rays. The strength of atmospheric turbulence is quantified by the refractive index structure parameter Cn2 and estimating it is important both as an indicator of image degradation and is useful for downstream tasks including video restoration and estimating true shape and range/depth. However traditional methods for estimating Cn2 involve expensive and complex optical equipment limiting their practicality. In this paper we propose MetaVIn: a Meteorological and Visual Integration system to predict atmospheric turbulence strength. Our method leverages image quality metrics to capture sharpness and blur combined with meteorological information within a Kolmogorov Arnold Network (KAN). We demonstrate that this approach provides a more accurate and generalizable estimation of Cn2 outperforming previous state-of-the-art methods in both blind image quality assessment and passive video-based turbulence strength estimation on a large dataset of 35364 image samples with accompanying ground truth scintillometer measurements for Cn2. Our method enables better prediction and mitigation of atmospheric image degradation while being useful in applications such as shape and range estimation enhancing the practical utility of our approach.	https://openaccess.thecvf.com//content/WACV2025/html/Saha_MetaVIn_Meteorological_and_Visual_Integration_for_Atmospheric_Turbulence_Strength_Estimation_WACV_2025_paper.html	Ripon Kumar Saha, Scott McCloskey, Suren Jayasuriya
Metric Compatible Training for Online Backfilling in Large-Scale Retrieval	Backfilling is the process of re-extracting all gallery embeddings from upgraded models in image retrieval systems. It inevitably spends a prohibitively large amount of computational cost and even entails the downtime of the service. Although backward-compatible learning sidesteps this challenge by tackling query-side representations this leads to suboptimal solutions in principle because gallery embeddings cannot benefit from model upgrades. We address this dilemma by introducing an online backfilling algorithm which enables us to achieve a progressive performance improvement during the backfilling process without sacrificing the full performance of the new model after the completion of backfilling. To this end we first show that a simple distance rank merge is a reasonable option for online backfilling. Then we incorporate a reverse transformation module for more effective and efficient merging which is further enhanced by adopting metric-compatible contrastive learning. These two components help to make the distances of old and new models compatible resulting in desirable merge results during backfilling with no extra computational overhead. Extensive experiments show the benefit of our framework on four standard benchmarks in various settings.	https://openaccess.thecvf.com//content/WACV2025/html/Seo_Metric_Compatible_Training_for_Online_Backfilling_in_Large-Scale_Retrieval_WACV_2025_paper.html	Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim
MimicGait: A Model Agnostic Approach for Occluded Gait Recognition using Correlational Knowledge Distillation	Gait recognition is an important biometric technique over large distances. State-of-the-art gait recognition systems perform very well in controlled environments at close range. Recently there has been an increased interest in gait recognition in the wild prompted by the collection of outdoor more challenging datasets containing variations in terms of illumination pitch angles and distances. An important problem in these environments is that of occlusion where the subject is partially blocked from camera view. While important this problem has received little attention. Thus we propose MimicGait a model-agnostic approach for gait recognition in the presence of occlusions. We train the network using a multi-instance correlational distillation loss to capture both inter-sequence and intra-sequence correlations in the occluded gait patterns of a subject utilizing an auxiliary Visibility Estimation Network to guide the training of the proposed mimic network. We demonstrate the effectiveness of our approach on challenging real-world datasets like GREW Gait3D and BRIAR. The code is available at https://github.com/Ayush-00/mimicgait.	https://openaccess.thecvf.com//content/WACV2025/html/Gupta_MimicGait_A_Model_Agnostic_Approach_for_Occluded_Gait_Recognition_using_WACV_2025_paper.html	Ayush Gupta, Rama Chellappa
Mind the Map! Accounting for Existing Maps When Estimating Online HDMaps from Sensors	While HDMaps are a crucial component of autonomous driving they are expensive to acquire and maintain. Estimating these maps from sensors therefore promises to significantly lighten costs. These estimations however overlook existing HDMaps with current methods at most geolocalizing low quality maps or considering a general database of known maps. In this paper we propose to account for existing maps of the precise situation studied when estimating HDMaps. To prove this we identify 3 reasonable types of useful existing maps (minimalist noisy and outdated). We then introduce MapEX a novel online HDMap estimation framework that accounts for existing maps. MapEX achieves this by encoding map elements into query tokens and by refining the matching algorithm used to train classic query based map estimation models. We demonstrate that MapEX brings significant improvements on the nuScenes dataset. For instance MapEX - given noisy maps - improves by 38% over the MapTRv2 detector it is based on and by 8% over the current SOTA.	https://openaccess.thecvf.com//content/WACV2025/html/Sun_Mind_the_Map_Accounting_for_Existing_Maps_When_Estimating_Online_WACV_2025_paper.html	RÃ©my Sun, Li Yang, Diane Lingrand, Frederic Precioso
Mind the Prompt: A Novel Benchmark for Prompt-Based Class-Agnostic Counting	Recently object counting has shifted towards class-agnostic counting (CAC) which counts instances of arbitrary object classes never seen during model training. With advancements in robust vision-and-language foundation models there is a growing interest in prompt-based CAC where object categories are specified using natural language. However we identify significant limitations in current benchmarks for evaluating this task which hinder both accurate assessment and the development of more effective solutions. Specifically we argue that the current evaluation protocols do not measure the ability of the model to understand which object has to be counted. This is due to two main factors: (i) the shortcomings of CAC datasets which primarily consist of images containing objects from a single class and (ii) the limitations of current counting performance evaluators which are based on traditional class-specific counting and focus solely on counting errors. To fill this gap we introduce the Prompt-Aware Counting (PrACo) benchmark. It comprises two targeted tests coupled with evaluation metrics specifically designed to quantitatively measure the robustness and trustworthiness of existing prompt-based CAC models. We evaluate state-of-the-art methods and demonstrate that although some achieve impressive results on standard class-specific counting metrics they exhibit a significant deficiency in understanding the input prompt indicating the need for more careful training procedures or revised designs. The code for reproducing our results is available at https://github.com/ciampluca/PrACo.	https://openaccess.thecvf.com//content/WACV2025/html/Ciampi_Mind_the_Prompt_A_Novel_Benchmark_for_Prompt-Based_Class-Agnostic_Counting_WACV_2025_paper.html	Luca Ciampi, Nicola Messina, Matteo Pierucci, Giuseppe Amato, Marco Avvenuti, Fabrizio Falchi
MissionGNN: Hierarchical Multimodal GNN-Based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation	In the context of escalating safety concerns across various domains the tasks of Video Anomaly Detection (VAD) and Video Anomaly Recognition (VAR) have emerged as critically important for applications in intelligent surveillance evidence investigation violence alerting etc. These tasks aimed at identifying and classifying deviations from normal behavior in video data face significant challenges due to the rarity of anomalies which leads to extremely imbalanced data and the impracticality of extensive frame-level data annotation for supervised learning. This paper introduces a novel hierarchical graph neural network (GNN) based model MissionGNN that addresses these challenges by leveraging a state-of-the-art large language model and a comprehensive knowledge graph for efficient weakly supervised learning in VAR. Our approach circumvents the limitations of previous methods by avoiding heavy gradient computations on large multimodal models and enabling fully frame-level training without fixed video segmentation. Utilizing automated mission-specific knowledge graph generation our model provides a practical and efficient solution for real-time video analysis without the constraints of previous segmentation-based or multimodal approaches. Experimental validation on benchmark datasets demonstrates our model's performance in VAD and VAR highlighting its potential to redefine the landscape of anomaly detection and recognition in video surveillance systems.	https://openaccess.thecvf.com//content/WACV2025/html/Yun_MissionGNN_Hierarchical_Multimodal_GNN-Based_Weakly_Supervised_Video_Anomaly_Recognition_with_WACV_2025_paper.html	Sanggeon Yun, Ryozo Masukawa, Minhyoung Na, Mohsen Imani
MixDiff: Mixing Natural and Synthetic Images for Robust Self-Supervised Representations	This paper introduces MixDiff a new self-supervised learning (SSL) pre-training framework that combines real and synthetic images. Unlike traditional SSL methods that predominantly use real images MixDiff uses a variant of Stable Diffusion to replace an augmented instance of a real image facilitating the learning of cross real-synthetic image representations. Our key insight is that while models trained solely on synthetic images underperform combining real and synthetic data leads to more robust and adaptable representations. Experiments show MixDiff enhances SimCLR BarlowTwins and DINO across various robustness datasets and domain transfer tasks boosting SimCLR's ImageNet-1K accuracy by 4.56%. Our framework also demonstrates comparable performance without needing any augmentations a surprising finding in SSL where augmentations are typically crucial. Furthermore MixDiff achieves similar results to SimCLR while requiring less real data highlighting its efficiency in representation learning	https://openaccess.thecvf.com//content/WACV2025/html/Bafghi_MixDiff_Mixing_Natural_and_Synthetic_Images_for_Robust_Self-Supervised_Representations_WACV_2025_paper.html	Reza Akbarian Bafghi, Nidhin Harilal, Maziar Raissi, Claire Monteleoni
Mixed Patch Visible-Infrared Modality Agnostic Object Detection	In real-world scenarios using multiple modalities like visible (RGB) and infrared (IR) can greatly improve the performance of a predictive task such as object detection (OD). Multimodal learning is a common way to leverage these modalities where multiple modality-specific encoders and a fusion module are used to improve performance. In this paper we tackle a different way to employ RGB and IR modalities where only one modality or the other is observed by a single shared vision encoder. This realistic setting requires a lower memory footprint and is more suitable for applications such as autonomous driving and surveillance which commonly rely on RGB and IR data. However when learning a single encoder on multiple modalities one modality can dominate the other producing uneven recognition results. This work investigates how to efficiently leverage RGB and IR modalities to train a common transformer-based OD vision encoder while countering the effects of modality imbalance. For this we introduce a novel training technique to Mix Patches (MiPa) from the two modalities in conjunction with a patch-wise modality agnostic module for learning a common representation of both modalities. Our experiments show that MiPa can learn a representation to reach competitive results on traditional RGB/IR benchmarks while only requiring a single modality during inference. Our code is available at: https://github.com/heitorrapela/MiPa.	https://openaccess.thecvf.com//content/WACV2025/html/Medeiros_Mixed_Patch_Visible-Infrared_Modality_Agnostic_Object_Detection_WACV_2025_paper.html	Heitor R. Medeiros, David Latortue, Eric Granger, Marco Pedersoli
MoRAG - Multi-Fusion Retrieval Augmented Generation for Human Motion	We introduce MoRAG a novel multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation. The method enhances motion diffusion models by leveraging additional knowledge obtained through an improved motion retrieval process. By effectively prompting large language models (LLMs) we address spelling errors and rephrasing issues in motion retrieval. Our approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space. We create diverse samples through the spatial composition of the retrieved motions. Furthermore by utilizing low-level part-specific motion information we can construct motion samples for unseen text descriptions. Our experiments demonstrate that our framework can serve as a plug-and-play module improving the performance of motion diffusion models. Code pre-trained models and sample videos are available at https://motion-rag.github.io.	https://openaccess.thecvf.com//content/WACV2025/html/Kalakonda_MoRAG_-_Multi-Fusion_Retrieval_Augmented_Generation_for_Human_Motion_WACV_2025_paper.html	Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
Modality-Incremental Learning with Disjoint Relevance Mapping Networks for Image-Based Semantic Segmentation	In autonomous driving environment perception has significantly advanced with the utilization of deep learning techniques for diverse sensors such as cameras depth sensors or infrared sensors. The diversity in the sensor stack increases the safety and contributes to robustness against adverse weather and lighting conditions. However the variance in data acquired from different sensors poses challenges. In the context of continual learning (CL) incremental learning is especially challenging for considerably large domain shifts e.g. different sensor modalities. This amplifies the problem of catastrophic forgetting. To address this issue we formulate the concept of modality-incremental learning and examine its necessity by contrasting it with existing incremental learning paradigms. We propose the use of a modified Relevance Mapping Network (RMN) to incrementally learn new modalities while preserving performance on previously learned modalities in which relevance maps are disjoint. Experimental results demonstrate that the prevention of shared connections in this approach helps alleviate the problem of forgetting within the constraints of a strict continual learning framework.	https://openaccess.thecvf.com//content/WACV2025/html/Hegde_Modality-Incremental_Learning_with_Disjoint_Relevance_Mapping_Networks_for_Image-Based_Semantic_WACV_2025_paper.html	Niharika Hegde, Shishir Muralidhara, RenÃ© Schuster, Didier Stricker
Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval	Video Moment Retrieval is a common task to evaluate the performance of visual-language models - it involves localising start and end times of moments in videos from query sentences. The current task formulation assumes that the queried moment is present in the video resulting in false positive moment predictions when irrelevant query sentences are provided. In this paper we propose the task of Negative-Aware Video Moment Retrieval (NA-VMR) which considers both moment retrieval accuracy and negative query rejection accuracy. We make the distinction between In-Domain and Out-of-Domain negative queries and provide new evaluation benchmarks for two popular video moment retrieval datasets: QVHighlights and Charades-STA. We analyse the ability of current SOTA video moment retrieval approaches to adapt to Negative-Aware Video Moment Retrieval and propose UniVTG-NA an adaptation of UniVTG designed to tackle NA-VMR. UniVTG-NA achieves high negative rejection accuracy (avg. 98.4%) scores while retaining moment retrieval scores to within 3.87% Recall@1. Dataset splits are available at https://github.com/keflanagan/MomentofUntruth	https://openaccess.thecvf.com//content/WACV2025/html/Flanagan_Moment_of_Untruth_Dealing_with_Negative_Queries_in_Video_Moment_WACV_2025_paper.html	Kevin Flanagan, Dima Damen, Michael Wray
MonoPP: Metric-Scaled Self-Supervised Monocular Depth Estimation by Planar-Parallax Geometry in Automotive Applications	Self-supervised monocular depth estimation (MDE) has gained popularity for obtaining depth predictions directly from videos. However these methods often produce scale-invariant results unless additional training signals are provided. Addressing this challenge we introduce a novel self-supervised metric-scaled MDE model that requires only monocular video data and the camera's mounting position both of which are readily available in modern vehicles. Our approach leverages planar-parallax geometry to reconstruct scene structure. The full pipeline consists of three main networks a multi-frame network a single-frame network and a pose network. The multi-frame network processes sequential frames to estimate the structure of the static scene using planar-parallax geometry and the camera mounting position. Based on this reconstruction it acts as a teacher distilling knowledge such as scale information masked drivable area metric-scale depth for the static scene and dynamic object mask to the single-frame network. It also aids the pose network in predicting a metric-scaled relative pose between two subsequent images. Our method achieved state-of-the-art results for the driving benchmark KITTI for metric-scaled depth prediction. Notably it is one of the first methods to produce self-supervised metric-scaled depth prediction for the challenging Cityscapes dataset demonstrating its effectiveness and versatility. Project page: https://mono-pp.github.io/	https://openaccess.thecvf.com//content/WACV2025/html/Elazab_MonoPP_Metric-Scaled_Self-Supervised_Monocular_Depth_Estimation_by_Planar-Parallax_Geometry_in_WACV_2025_paper.html	Gasser Elazab, Torben GrÃ¤ber, Michael Unterreiner, Olaf Hellwich
MulModSeg: Enhancing Unpaired Multi-Modal Medical Image Segmentation with Modality-Conditioned Text Embedding and Alternating Training	In the diverse field of medical imaging automatic segmentation has numerous applications and must handle a wide variety of input domains such as different types of Computed Tomography (CT) scans and Magnetic Resonance (MR) images. This heterogeneity challenges automatic segmentation algorithms to maintain consistent performance across different modalities due to the requirement for spatially aligned and paired images. Typically segmentation models are trained using a single modality which limits their ability to generalize to other types of input data without employing transfer learning techniques. Additionally leveraging complementary information from different modalities to enhance segmentation precision often necessitates substantial modifications to popular encoder-decoder designs such as introducing multiple branched encoding or decoding paths for each modality. In this work we propose a simple Multi-Modal Segmentation (MulModSeg) strategy to enhance medical image segmentation across multiple modalities specifically CT and MR. It incorporates two key designs: a modality-conditioned text embedding framework via a frozen text encoder that adds modality awareness to existing segmentation frameworks without significant structural modifications or computational overhead and an alternating training procedure that facilitates the integration of essential features from unpaired CT and MR inputs. Through extensive experiments with both Fully Convolutional Network and Transformer-based backbones MulModSeg consistently outperforms previous methods in segmenting abdominal multi-organ and cardiac substructures for both CT and MR modalities. The code is available in this link.	https://openaccess.thecvf.com//content/WACV2025/html/Li_MulModSeg_Enhancing_Unpaired_Multi-Modal_Medical_Image_Segmentation_with_Modality-Conditioned_Text_WACV_2025_paper.html	Chengyin Li, Hui Zhu, Rafi Ibn Sultan, Hassan Bagher Ebadian, Prashant Khanduri, Chetty Indrin, Kundan Thind, Dongxiao Zhu
Multi-Aperture Transformers for 3D (MAT3D) Segmentation of Clinical and Microscopic Images	3D segmentation of biological structures is critical in biomedical imaging offering significant insights into structures and functions. This paper introduces a novel segmentation of biological images that couples Multi-Aperture representation with Transformers for 3D (MAT3D) segmentation. Our method integrates the global context-awareness of Transformer networks with the local feature extraction capabilities of Convolutional Neural Networks (CNNs) providing a comprehensive solution for accurately delineating complex biological structures. First we evaluated the performance of the proposed technique on two public clinical datasets of ACDC and Synapse multi-organ segmentation rendering superior Dice scores of 93.34+-0.05 and 89.73+-0.04 respectively with fewer parameters compared to the published literature. Next we assessed the performance of our technique on an organoid dataset comprising four breast cancer subtypes. The proposed method achieved a Dice 95.12+-0.02 and a PQ score of 97.01+-0.01 respectively. MAT3D also significantly reduces the parameters to 40 million. The code is available on https://github.com/sohaibcs1/MAT3D.	https://openaccess.thecvf.com//content/WACV2025/html/Sohaib_Multi-Aperture_Transformers_for_3D_MAT3D_Segmentation_of_Clinical_and_Microscopic_WACV_2025_paper.html	Muhammad Sohaib, Siyavash Shabani, Sahar A. Mohammed, Garrett Winkelmaier, Bahram Parvin
Multi-Class Textual-Inversion Secretly Yields a Semantic-Agnostic Classifier	With the advent of large pre-trained vision-language models such as CLIP prompt learning methods aim to enhance the transferability of the CLIP model. They learn the prompt given few samples from the downstream task given the specific class names as prior knowledge which we term as semantic-aware classification. However in many realistic scenarios we only have access to few samples and knowledge of the class names (e.g. when considering instances of classes). This challenging scenario represents the semantic-agnostic discriminative case. Text-to-Image (T2I) personalization methods aim to adapt T2I models to unseen concepts by learning new tokens and endowing these tokens with the capability of generating the learned concepts. These methods do not require knowledge of class names as a semantic-aware prior. Therefore in this paper we first explore Textual Inversion and reveal that the new concept tokens possess both generation and classification capabilities by regarding each category as a single concept. However learning classifiers from single-concept textual inversion is limited since the learned tokens are suboptimal for the discriminative tasks. To mitigate this issue we propose Multi-Class textual inversion which includes a discriminative regularization term for the token updating process. Using this technique our method MC-TI achieves stronger Semantic-Agnostic Classification while preserving the generation capability of these modifier tokens given only few samples per category. In the experiments we extensively evaluate MC-TI on 12 datasets covering various scenarios which demonstrates that MC-TI achieves superior results in terms of both classification and generation outcomes.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_Multi-Class_Textual-Inversion_Secretly_Yields_a_Semantic-Agnostic_Classifier_WACV_2025_paper.html	Kai Wang, Fei Yang, Bogdan Raducanu, Joost van de Weijer
Multi-HexPlanes: A Lightweight Map Representation for Rendering and 3D Reconstruction	Creating maps of the world around us is paramount to many applications including those related to robotics such as navigation and inspection. Given the computational resource limitations typical of robotic platforms there is a pressing need for lightweight 3D representations that capture detailed texture and geometric information with minimal storage. Traditional voxel-based approaches require substantial memory resources. On the other hand neural implicit and 3D Gaussian splatting representations require significant computational power (GPUs) and can hardly run in real-time. In this paper we introduce a novel scene representation Multi-HexPlanes that divides 3D environments into large boxes and utilizes the faces of the boxes to encapsulate texture and geometric information. This representation reduces the memory requirement to store the map making our approach especially suitable for systems with limited memory. Through extensive evaluations on large-scale datasets we find that our method achieves better performance on rendering and more complete 3D reconstruction. We also demonstrate that our map representation can output dense feature points with rich geometric information for downstream tasks such as training 3D Gaussian splats. The proposed technique promises substantial improvements in real-time 3D mapping applications particularly for devices constrained by processing power and storage.	https://openaccess.thecvf.com//content/WACV2025/html/Zheng_Multi-HexPlanes_A_Lightweight_Map_Representation_for_Rendering_and_3D_Reconstruction_WACV_2025_paper.html	Jianhao Zheng, GÃ¡bor Valasek, Daniel Barath, Iro Armeni
Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark	Despite the critical importance of the medical domain in Deep Learning most of the research in this area solely focuses on training models in static environments. It is only in recent years that research has begun to address dynamic environments and tackle the Catastrophic Forgetting problem through Continual Learning (CL) techniques. Previous studies have primarily focused on scenarios such as Domain Incremental Learning and Class Incremental Learning which do not fully capture the complexity of real-world applications. Therefore in this work we propose a novel benchmark combining the challenges of new class arrivals and domain shifts in a single framework by considering the New Instances and New Classes (NIC) scenario. This benchmark aims to model a realistic CL setting for the multi-label classification problem in medical imaging. Additionally it encompasses a greater number of tasks compared to previously tested scenarios. Specifically our benchmark consists of two datasets (NIH and CXP) nineteen classes and seven tasks a stream longer than the previously tested ones. To solve common challenges (e.g. the task inference problem) found in the CIL and NIC scenarios we propose a novel approach called Replay Consolidation with Label Propagation (RCLP). Our method surpasses existing approaches exhibiting superior performance with minimal forgetting.	https://openaccess.thecvf.com//content/WACV2025/html/Ceccon_Multi-Label_Continual_Learning_for_the_Medical_Domain_A_Novel_Benchmark_WACV_2025_paper.html	Marina Ceccon, Davide Dalle Pezze, Alessandro Fabris, Gian Antonio Susto
Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets	We propose a novel teacher-student framework to distill knowledge from multiple teachers trained on distinct datasets. Each teacher is first trained from scratch on its own dataset. Then the teachers are combined into a joint architecture which fuses the features of all teachers at multiple representation levels. The joint teacher architecture is fine-tuned on samples from all datasets thus gathering useful generic information from all data samples. Finally we employ a multi-level feature distillation procedure to transfer the knowledge to a student model for each of the considered datasets. We conduct image classification experiments on seven benchmarks and action recognition experiments on three benchmarks. To illustrate the power of our feature distillation procedure the student architectures are chosen to be identical to those of the individual teachers. To demonstrate the flexibility of our approach we combine teachers with distinct architectures. We show that our novel Multi-Level Feature Distillation (MLFD) can significantly surpass equivalent architectures that are either trained on individual datasets or jointly trained on all datasets at once. Furthermore we confirm that each step of the proposed training procedure is well motivated by a comprehensive ablation study. We publicly release our code at https://github.com/AdrianIordache/MLFD.	https://openaccess.thecvf.com//content/WACV2025/html/Iordache_Multi-Level_Feature_Distillation_of_Joint_Teachers_Trained_on_Distinct_Image_WACV_2025_paper.html	Adrian Iordache, Bogdan Alexe, Radu Tudor Ionescu
Multi-Modal Large Language Model with RAG Strategies in Soccer Commentary Generation	As a globally celebrated sport soccer has seen its appeal greatly amplified by engaging and vivid commentary. Recently Multi-Modal Large Language Models (MLLMs) have attracted attention in generating soccer commentaries due to their remarkable capacities of understanding different modalities of the input videos. Most of these methods have shown that the use of multiple modalities can enhance the commentary quality which includes video audio and structured meta-data. However delivering precise and rich commentary requires the ability to accurately discern subtle differences in similar backgrounds events and players. This presents a significant challenge for existing MLLMs. So we propose SoccerComment a framework for generating soccer commentary that integrates MLLMs with Retrieval-Augmented Generation (RAG) strategies. This framework enhances inference efficiency and reduces the need for continuous training through a multi-modal clustering memory unit and retrieval-augmented in-context learning mechanisms ultimately improving the accuracy and diversity of the commentary. Based on similar retrieved scenarios SoccerComment demonstrates outstanding zero-shot performance offering a new direction and scalable solution for future research in soccer commentary generation.	https://openaccess.thecvf.com//content/WACV2025/html/Li_Multi-Modal_Large_Language_Model_with_RAG_Strategies_in_Soccer_Commentary_WACV_2025_paper.html	Xiang Li, Yangfan He, Shuaishuai Zu, Zhengyang Li, Tianyu Shi, Yiting Xie, Kevin Zhang
Multi-Modal Large Language Models are Effective Vision Learners	Large language models (LLMs) pre-trained on vast amounts of text have shown remarkable abilities in understanding general knowledge and commonsense. Therefore it's desirable to leverage pre-trained LLM to help solve computer vision tasks. Previous works on multi-modal LLM mainly focus on the generation capability. In this work we propose LLM-augmented visual representation learning (LMVR). Our approach involves initially using a vision encoder to extract features which are then projected into the word embedding space of the LLM. The LLM then generates responses based on the visual representation and a text prompt. Finally we aggregate sequence-level features from the hidden layers of the LLM to obtain image-level representations. We conduct extensive experiments on multiple datasets and have the following findings: (a) LMVR outperforms traditional vision encoder on various downstream tasks and effectively learns the correspondence between words and image regions; (b) LMVR improves the generalizability compared to using a vision encoder alone as evidenced by its superior resistance to domain shift; (c) LMVR improves the robustness of models to corrupted and perturbed visual data. Our findings demonstrate LLM-augmented visual representation learning is effective as it learns object-level concepts and commonsense knowledge.	https://openaccess.thecvf.com//content/WACV2025/html/Sun_Multi-Modal_Large_Language_Models_are_Effective_Vision_Learners_WACV_2025_paper.html	Li Sun, Chaitanya Ahuja, Peng Chen, Matt D'Zmura, Kayhan Batmanghelich, Philip Bontrager
Multi-Resolution Guided 3D GANs for Medical Image Translation	Medical image translation is the process of converting from one imaging modality to another in order to reduce the need for multiple image acquisitions from the same patient. This can enhance the efficiency of treatment by reducing the time equipment and labor needed. In this paper we introduce a multi-resolution guided Generative Adversarial Network (GAN)-based framework for 3D medical image translation. Our framework uses a 3D multi-resolution Dense-Attention UNet (3D-mDAUNet) as the generator and a 3D multi-resolution UNet as the discriminator optimized with a unique combination of loss functions including voxel-wise GAN loss and 2.5D perception loss. Our approach yields promising results in volumetric image quality assessment (IQA) across a variety of imaging modalities body regions and age groups demonstrating its robustness. Furthermore we propose a synthetic-to-real applicability assessment as an additional evaluation to assess the effectiveness of synthetic data in downstream applications such as segmentation. This comprehensive evaluation shows that our method produces synthetic medical images not only of high-quality but also potentially useful in clinical applications. Our code is available at github.com/juhha/3D-mADUNet.	https://openaccess.thecvf.com//content/WACV2025/html/Ha_Multi-Resolution_Guided_3D_GANs_for_Medical_Image_Translation_WACV_2025_paper.html	Juhyung Ha, Jong Sung Park, David Crandall, Eleftherios Garyfallidis, Xuhong Zhang
Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation	Prototypical part learning is emerging as a promising approach for making semantic segmentation interpretable. The model selects real patches seen during training as prototypes and constructs the dense prediction map based on the similarity between parts of the test image and the prototypes. This improves interpretability since the user can inspect the link between the predicted output and the patterns learned by the model in terms of prototypical information. In this paper we propose a method for interpretable semantic segmentation that leverages multi-scale image representation for prototypical part learning. First we introduce a prototype layer that explicitly learns diverse prototypical parts at several scales leading to multi-scale representations in the prototype activation output. Then we propose a sparse grouping mechanism that produces multi-scale sparse groups of these scale-specific prototypical parts. This provides a deeper understanding of the interactions between multi-scale object representations while enhancing the interpretability of the segmentation model. The experiments conducted on Pascal VOC Cityscapes and ADE20K demonstrate that the proposed method increases model sparsity improves interpretability over existing prototype-based methods and narrows the performance gap with the non-interpretable counterpart models. Code is available at github.com/eceo-epfl/ScaleProtoSeg.	https://openaccess.thecvf.com//content/WACV2025/html/Porta_Multi-Scale_Grouped_Prototypes_for_Interpretable_Semantic_Segmentation_WACV_2025_paper.html	Hugo Porta, Emanuele Dalsasso, Diego Marcos, Devis Tuia
Multi-Spectral Image Color Reproduction	From camera to screen researchers have developed a well-established system for capturing and reproducing the color experience of human eyes. In this study we aim to upgrade this process by transiting from conventional RGB to multi-spectral image (MSI) color reproduction. While MSI offers evident advantages in color matching we find out it is not trivial to make good use of more spectral information for color constancy. Therefore we present a regularized color reproduction system that incorporates a spectral prior-guided optimization strategy to establish a sensor-optimized RGB projection for color matching along with a learning-based chromatic adaptation model for color constancy. Specifically we define the RGB projection through an end-to-end optimization under the guidance of sensor spectral sensitivities. Subsequently we devise a chromatic adaptation neural network that estimates the scene illuminance and an illuminance-adaptive matrix for auto white balancing and dynamic color correction respectively. Comprehensive experiments show the superiority of our system compared to alternative solutions.	https://openaccess.thecvf.com//content/WACV2025/html/Li_Multi-Spectral_Image_Color_Reproduction_WACV_2025_paper.html	Jiacheng Li, Chang Chen, Xue Hu, Fenglong Song, Youliang Yan, Zhiwei Xiong
Multi-Surrogate-Teacher Assistance for Representation Alignment in Fingerprint-Based Indoor Localization	Despite remarkable progress in knowledge transfer across visual and textual domains extending these achievements to indoor localization particularly for learning transferable representations among Received Signal Strength (RSS) fingerprint datasets remains a challenge. This is due to inherent discrepancies among these RSS datasets largely including variations in building structure the input number and disposition of WiFi anchors. Accordingly specialized networks which were deprived of the ability to discern transferable representations readily incorporate environment-sensitive clues into the learning process hence limiting their potential when applied to specific RSS datasets. Initially we design an Expert Training phase which features multiple surrogate generative teachers all serving as a global adapter that homogenizes the input disparities among independent source RSS datasets while preserving their unique characteristics. In a subsequent Expert Distilling phase we continue introducing a triplet of underlying constraints that requires minimizing the differences in essential knowledge between the specialized network and surrogate teachers through refining its representation learning on the target dataset. This process implicitly fosters a representational alignment in such a way that is less sensitive to specific environmental dynamics. Extensive experiments conducted on three benchmark WiFi RSS fingerprint datasets underscore the effectiveness of the framework that significantly exerts the full potential of specialized networks in localization	https://openaccess.thecvf.com//content/WACV2025/html/Nguyen_Multi-Surrogate-Teacher_Assistance_for_Representation_Alignment_in_Fingerprint-Based_Indoor_Localization_WACV_2025_paper.html	Son Minh Nguyen, Linh Duy Tran, Duc Le, Paul Havinga
Multi-Task Learning of Classification and Generation for Set-Structured Data	In this study we propose a multi-task learning model of classification and generation for set-structured data. The proposed model learns data generation and classification in a single neural network by integrating a classification layer into a variational autoencoder while maintaining permutation invariance and equivariance nature which are characteristics of set-structured data. The proposed model allows for semi-supervised learning in set-structured data classification and can also be applied to confidence calibration using the input data distribution estimated by the generative model. In the experiments we evaluated the performance of the proposed model in a semi-supervised classification task on set-structured datasets and compared it with a baseline model consisting only of a classifier. The results demonstrated that simultaneous learning of the classification and generation effectively improves the classification accuracy and confidence reliability for set-structured data even with a limited number of labeled data.	https://openaccess.thecvf.com//content/WACV2025/html/Sato_Multi-Task_Learning_of_Classification_and_Generation_for_Set-Structured_Data_WACV_2025_paper.html	Fumioki Sato, Hideaki Hayashi, Hajime Nagahara
Multi-View Factorizing and Disentangling: A Novel Framework for Incomplete Multi-View Multi-Label Classification	Multi-view multi-label classification (MvMLC) has recently garnered significant research attention due to its wide range of real-world applications. However incompleteness in views and labels is a common challenge often resulting from data collection oversights and uncertainties in manual annotation. Furthermore the task of learning robust multi-view representations that are both view-consistent and view-specific from diverse views still a challenge problem in MvMLC. To address these issues we propose a novel framework for incomplete multi-view multi-label classification (iMvMLC). Our method factorizes multi-view representations into two independent sets of factors: view-consistent and view-specific and we correspondingly design a graph disentangling loss to fully reduce redundancy between these representations. Additionally our framework innovatively decomposes consistent representation learning into three key sub-objectives: (i) how to extract view-shared information across different views (ii) how to eliminate intra-view redundancy in consistent representations and (iii) how to preserve task-relevant information. To this end we design a robust task-relevant consistency learning module that collaboratively learns high-quality consistent representations leveraging a masked cross-view prediction (MCP) strategy and information theory. Notably all modules in our framework are developed to function effectively under conditions of incomplete views and labels making our method adaptable to various multi-view and multi-label datasets. Extensive experiments on five datasets demonstrate that our method outperforms other leading approaches.	https://openaccess.thecvf.com//content/WACV2025/html/Xie_Multi-View_Factorizing_and_Disentangling_A_Novel_Framework_for_Incomplete_Multi-View_WACV_2025_paper.html	Wulin Xie, Lian Zhao, Jiang Long, Xiaohuan Lu, Bingyan Nie
Multi-View Image Diffusion via Coordinate Noise and Fourier Attention	Recently text-to-image generation with diffusion models has made significant advancements in both higher fidelity and generalization capabilities compared to previous baselines. However generating holistic multi-view consistent images from prompts still remains an important and challenging task. To address this challenge we propose a diffusion process that attends to time-dependent spatial frequencies of features with a novel attention mechanism as well as novel noise initialization technique and cross-attention loss. This Fourier-based attention block focuses on features from non-overlapping regions of the generated scene in order to better align the global appearance. Our noise initialization technique incorporates shared noise and low spatial frequency information derived from pixel coordinates and depth maps to induce noise correlations across views. The cross-attention loss further aligns features sharing the same prompt across the scene. Our technique improves SOTA on several quantitative metrics with qualitatively better results when compared to other state-of-the-art approaches for multi-view consistency.	https://openaccess.thecvf.com//content/WACV2025/html/Theiss_Multi-View_Image_Diffusion_via_Coordinate_Noise_and_Fourier_Attention_WACV_2025_paper.html	Justin Theiss, Norman MÃ¼ller, Daeil Kim, Aayush Prakash
Multimodal Fusion Learning with Dual Attention for Medical Imaging	Multimodal fusion learning has shown significant promise in classifying various diseases such as skin cancer and brain tumors. However existing methods face three key limitations. First they often lack generalizability to other diagnosis tasks due to their focus on a particular disease. Second they do not fully leverage multiple health records from diverse modalities to learn robust complementary information. And finally they typically rely on a single attention mechanism missing the benefits of multiple attention strategies within and across various modalities. To address these issues this paper proposes a dual robust information fusion attention mechanism (DRIFA) that leverages two attention modules - i.e. multi-branch fusion attention module and the multimodal information fusion attention module. DRIFA can be integrated with any deep neural network forming a multi modal fusion learning framework denoted as DRIFA-Net. We show that the multi-branch fusion attention of DRIFA learns enhanced representations for each modality such as dermoscopy pap smear MRI and CT-scan whereas multimodal information fusion attention module learns more refined multimodal shared representations - improving the network's generalization across multiple tasks and enhancing overall performance. Additionally to estimate the uncertainty of DRIFA-Net predictions we have employed an ensemble Monte Carlo dropout strategy. Extensive experiments on five publicly available datasets with diverse modalities demonstrate that our approach consistently outperforms state-of-the-art methods. The code is available at https://github.com/misti1203/DRIFA-Net.	https://openaccess.thecvf.com//content/WACV2025/html/Dhar_Multimodal_Fusion_Learning_with_Dual_Attention_for_Medical_Imaging_WACV_2025_paper.html	Joy Dhar, Nayyar Zaidi, Maryam Haghighat, Sudipta Roy, Puneet Goyal, Azadeh Alavi, Vikas Kumar
Multimodal Interpretable Depression Analysis using Visual Physiological Audio and Textual Data	Motivated by depression's significant impact on global health this work proposes MultiDepNet a novel multimodal interpretable depression detection system integrating visual physiological audio and textual data. Through dedicated feature extraction methods (MTCNN for video TS-CAN for physiological ResNet-18 for audio and RoBERTa for text modalities) and a strategic fusion of modality-specific networks including CNN-RNN Transformer MLP and ResNet-18 it achieves significant advancements in depression detection. Its performance evaluated across four benchmark datasets (AVEC 2013 AVEC 2014 DAIC and E-DAIC) demonstrates average MAE of 5.64 RMSE of 7.15 accuracy of 74.19 precision of 0.7373 recall of 0.7378 and F1 of 0.7376. It also implements a MultiViz-based interpretability mechanism that computes each modality's contribution to the model's performance. The results reveal the visual modality to be the most significant contributing 37.88% towards depression detection.	https://openaccess.thecvf.com//content/WACV2025/html/Kumar_Multimodal_Interpretable_Depression_Analysis_using_Visual_Physiological_Audio_and_Textual_WACV_2025_paper.html	Puneet Kumar, Shreshtha Misra, Zhuhong Shao, Bin Zhu, Balasubramanian Raman, Xiaobai Li
Multispectral Object Detection Enhanced by Cross-Modal Information Complementary and Cosine Similarity Channel Resampling Modules	Images obtained from different modalities can effectively enhance the accuracy and reliability of the detection model by complementing specialized information from visible (RGB) and infrared (IR) images. However integrating information from multiple modalities faces the following challenges: 1) distinct characteristics of RGB and IR images lead to the problem of modality imbalance 2) fusing multimodal information can greatly affect the detection accuracy as some of the unique information provided by each modality is lost during the integration process and 3) RGB and IR images are fused while preserving the noise of each modality. To address these issues we propose a novel multispectral object detection network which contains two main components; 1) Cross-modal Information Complementary (CIC) module and 2) Cosine Similarity Channel Resampling (CSCR) module. The proposed method addresses the modality imbalance problem and efficiently fuses RGB and IR images in the feature level. Extensive experimental results on LLVIP FLIR M3FD VEDAI and KAIST benchmark datasets verify the effectiveness and generalization performance of the proposed multispectral object detection network compared with other state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Jang_Multispectral_Object_Detection_Enhanced_by_Cross-Modal_Information_Complementary_and_Cosine_WACV_2025_paper.html	Junbo Jang, Chanyeong Park, Heegwang Kim, Jiyoon Lee, Joonki Paik
My3DGen: A Scalable Personalized 3D Generative Model	In recent years generative 3D face models (e.g. EG3D) have been developed to tackle the problem of synthesizing photo-realistic faces. However these models are often unable to capture facial features unique to each individual highlighting the importance of personalization. Some prior works have shown promise in personalizing generative face models but these studies primarily focus on 2D settings. Also these methods require both fine-tuning and storing a large number of parameters for each user posing a hindrance to achieving scalable personalization. Another challenge of personalization is the limited number of training images available for each individual which often leads to overfitting when using full fine-tuning methods. Our proposed approach My3DGen generates a personalized 3D prior of an individual using as few as 50 training images. My3DGen allows for novel view synthesis semantic editing of a given face (e.g. adding a smile) and synthesizing novel appearances all while preserving the original person's identity. We decouple the 3D facial features into global features and personalized features by freezing the pre-trained EG3D and training additional personalized weights through low-rank decomposition. As a result My3DGen introduces only 240K personalized parameters per individual leading to a 127x reduction in trainable parameters compared to the 30.6M required for fine-tuning the entire parameter space. Despite this significant reduction in storage our model preserves identity features without compromising the quality of downstream applications both quantitatively and qualitatively.	https://openaccess.thecvf.com//content/WACV2025/html/Qi_My3DGen_A_Scalable_Personalized_3D_Generative_Model_WACV_2025_paper.html	Luchao Qi, Jiaye Wu, Annie N. Wang, Shengze Wang, Roni Sengupta
NAT: Learning to Attack Neurons for Enhanced Adversarial Transferability	The generation of transferable adversarial perturbations typically involves training a generator to maximize embedding separation between clean and adversarial images at a single mid-layer of a source model. In this work we build on this approach and introduce Neuron Attack for Transferability (NAT) a method designed to target specific neuron within the embedding. Our approach is motivated by the observation that previous layer-level optimizations often disproportionately focus on a few neurons representing similar concepts leaving other neurons within the attacked layer minimally affected. NAT shifts the focus from embedding-level separation to a more fundamental neuron-specific approach. We find that targeting individual neurons effectively disrupts the core units of the neural network providing a common basis for transferability across different models. Through extensive experiments on 41 diverse ImageNet models and 9 fine-grained models NAT achieves fooling rates that surpass existing baselines by over 14% in cross-model and 4% in cross-domain settings. Furthermore by leveraging the complementary attacking capabilities of the trained generators we achieve impressive fooling rates within just 10 queries. Our code is available at: https://krishnakanthnakka.github.io/NAT/	https://openaccess.thecvf.com//content/WACV2025/html/Nakka_NAT_Learning_to_Attack_Neurons_for_Enhanced_Adversarial_Transferability_WACV_2025_paper.html	Krishna Kanth Nakka, Alexandre Alahi
NCAP: Scene Text Image Super-Resolution with Non-CAtegorical Prior	Scene text image super-resolution (STISR) enhances the resolution and quality of low-resolution images. Unlike previous studies that treated scene text images as natural images recent methods using a text prior (TP) extracted from a pre-trained text recognizer have shown strong performance. However two major issues emerge: (1) Explicit categorical priors like TP can negatively impact STISR if incorrect. We reveal that these explicit priors are unstable and propose replacing them with Non-CAtegorical Prior (NCAP) using penultimate layer representations. (2) Pre-trained recognizers used to generate TP struggle with low-resolution images. To address this most studies jointly train the recognizer with the STISR network to bridge the domain gap between low- and high-resolution images but this can cause an overconfidence phenomenon in the prior modality. We highlight this issue and propose a method to mitigate it by mixing hard and soft labels. Experiments on the TextZoom dataset demonstrate an improvement by 3.5% while our method significantly enhances generalization performance by 14.8% across four text recognition datasets. Our method generalizes to all TP-guided STISR networks.	https://openaccess.thecvf.com//content/WACV2025/html/Park_NCAP_Scene_Text_Image_Super-Resolution_with_Non-CAtegorical_Prior_WACV_2025_paper.html	Dongwoo Park, Suk Pil Ko
NCAdapt: Dynamic Adaptation with Domain-Specific Neural Cellular Automata for Continual Hippocampus Segmentation	Continual learning (CL) in medical imaging presents a unique challenge requiring models to adapt to new domains while retaining previously acquired knowledge. We introduce NCAdapt a Neural Cellular Automata (NCA) based method designed to address this challenge. NCAdapt features a domain-specific multi-head structure integrating adaptable convolutional layers into the NCA backbone for each new domain encountered. After initial training the NCA backbone is frozen and only the newly added adaptable convolutional layers consisting of 384 parameters are trained along with domain-specific NCA convolutions. We evaluate NCAdapt on hippocampus segmentation tasks benchmarking its performance against Lifelong nnU-Net and U-Net models with state-of-the-art (SOTA) CL methods. Our lightweight approach achieves SOTA performance underscoring its effectiveness in addressing CL challenges in medical imaging. Upon acceptance we will make our code base publicly accessible to support reproducibility and foster further advancements in medical CL.	https://openaccess.thecvf.com//content/WACV2025/html/Ranem_NCAdapt_Dynamic_Adaptation_with_Domain-Specific_Neural_Cellular_Automata_for_Continual_WACV_2025_paper.html	Amin Ranem, John Orlando Kalkhof, Anirban Mukhopadhyay
NPL-MVPS: Neural Point-Light Multi-View Photometric Stereo	In this work we present a novel multi-view photometric stereo (MVPS) method. Like many works in 3D reconstruction we are leveraging neural shape representations and learnt renderers. However our work differs from the state-of-the-art multi-view PS methods such as PS-NeRF or Supernormal in that we explicitly leverage per-pixel intensity renderings rather than relying mainly on estimated normals. We model point light attenuation and explicitly raytrace cast shadows in order to best approximate the incoming radiance for each point. The estimated incoming radiance is used as input to a fully neural material renderer that uses minimal prior assumptions and it is jointly optimised with the surface. Estimated normals and segmentation maps are also incorporated in order to maximise the surface accuracy. Our method is among the first (along with Supernormal) to outperform the classical MVPS approach proposed by the DiLiGenT-MV benchmark and achieves average 0.2mm Chamfer distance for objects imaged at approx 1.5m distance away with approximate 400x400 resolution. Moreover our method shows high robustness to the sparse MVPS setup (6 views 6 lights) greatly outperforming the SOTA competitor (0.38mm vs 0.61mm) illustrating the importance of neural rendering in multi-view photometric stereo.	https://openaccess.thecvf.com//content/WACV2025/html/Logothetis_NPL-MVPS_Neural_Point-Light_Multi-View_Photometric_Stereo_WACV_2025_paper.html	Fotios Logothetis, Ignas Budvytis, Roberto Cipolla
NarrAD: Automatic Generation of Audio Descriptions for Movies with Rich Narrative Context	Audio Description (AD) is a narration designed to enhance accessibility for visually impaired individuals by conveying the key visual elements of a video. Thus automating AD generation for long-form videos such as movies and dramas provides high social value but is a challenging task. First AD must reflect the narrative context of the entire movie including the storyline names of characters and places and the cultural setting. Second to avoid disrupting the immersive experience of the movie AD must not overlap with the characters' dialogues requiring the delivery of numerous visual elements in concise sentences. This paper presents NarrAD a training-free AD generation framework that satisfies both of the requirements by leveraging rich narrative context in movie scripts and curating information across narration slots. Experiments on the MAD dataset demonstrate that our approach outperforms prior works in both captioning and LLM-based metrics. In the user study with 600 subjects NarrAD achieves the highest user experience and movie comprehension. NarrAD's AD samples are available at https://bit.ly/4aSwOTr.	https://openaccess.thecvf.com//content/WACV2025/html/Park_NarrAD_Automatic_Generation_of_Audio_Descriptions_for_Movies_with_Rich_WACV_2025_paper.html	Jaehyeong Park, Junchel Ye, Seungkook Lee, Hyun W. Ka, Dongsu Han
Navigating Heterogeneity and Privacy in One-Shot Federated Learning with Diffusion Models	Federated learning (FL) enables multiple clients to train models collectively while preserving data privacy. However FL faces challenges in terms of communication cost and data heterogeneity. One-shot federated learning has emerged as a solution by reducing communication rounds improving efficiency and providing better security against eavesdropping attacks. Nevertheless data heterogeneity remains a significant challenge impacting performance. This work explores the effectiveness of diffusion models in one-shot FL demonstrating their applicability in addressing data heterogeneity and improving FL performance. Additionally we investigate the utility of our diffusion model approach FedDiff compared to other one-shot FL methods under differential privacy (DP). Furthermore to improve generated sample quality under DP settings we propose a pragmatic Fourier Magnitude Filtering (FMF) method enhancing the effectiveness of the generated data for global model training. Code available at https://github.com/mmendiet/FedDiff.	https://openaccess.thecvf.com//content/WACV2025/html/Mendieta_Navigating_Heterogeneity_and_Privacy_in_One-Shot_Federated_Learning_with_Diffusion_WACV_2025_paper.html	Matias Mendieta, Guangyu Sun, Chen Chen
NeRFs are Mirror Detectors: using Structural Similarity for Multi-View Mirror Scene Reconstruction with 3D Surface Primitives	While neural radiance fields (NeRF) led to a breakthrough in photorealistic novel view synthesis handling mirroring surfaces still denotes a particular challenge as they introduce severe inconsistencies in the scene representation. Previous attempts either focus on reconstructing single reflective objects or rely on strong supervision guidance in terms of additional user-provided annotations of visible image regions of the mirrors thereby limiting the practical usability. In contrast in this paper we present NeRF-MD a method which shows that NeRFs can be considered as mirror detectors and which is capable of reconstructing neural radiance fields of scenes containing mirroring surfaces without the need for prior annotations. To this end we first compute an initial estimate of the scene geometry by training a standard NeRF using a depth reprojection loss. Our key insight lies in the fact that parts of the scene corresponding to a mirroring surface will still exhibit a significant photometric inconsistency whereas the remaining parts are already reconstructed in a plausible manner. This allows us to detect mirror surfaces by fitting geometric primitives to such inconsistent regions in this initial stage of the training. Using this information we then jointly optimize the radiance field and mirror geometry in a second training stage to refine their quality. We demonstrate the capability of our method to allow the faithful detection of mirrors in the scene as well as the reconstruction of a single consistent scene representation and demonstrate its potential in comparison to baseline and mirror-aware approaches.	https://openaccess.thecvf.com//content/WACV2025/html/Van_Holland_NeRFs_are_Mirror_Detectors_using_Structural_Similarity_for_Multi-View_Mirror_WACV_2025_paper.html	Leif Van Holland, Michael Weinmann, Jan U. MÃ¼ller, Patrick Stotko, Reinhard Klein
Needles & Haystacks: Dataset and Benchmark for Domain-Agnostic Image-Based Rigid Slice-to-Volume Registration	"We address domain-agnostic slice-to-volume (S2V) registration the alignment of 2D sliced/tomographic images into 3D volumes without prior knowledge of structure shape or orientation. While S2V registration is well-studied in medical imaging which often relies on auxiliary information (e.g. landmarks segmentation masks pre-defined orientations canonical/atlas volumes) applications such as micro-structure characterization in materials science lack such domain-specific aids. This leaves the task inherently ill-posed due to noise unstructured regions repetitive patterns rotational and translational symmetries. To address this challenge we present ""Needles & Haystacks"" a novel multi-domain algorithm development dataset with 158436 unique registration problems and ground-truth solutions based on diverse and openly licensed real-world volumetric data. Additionally we provide an online platform with 8461 test problems for reproducible evaluation of competing methods. We also propose strong baseline solutions with public implementations and highlight opportunities for further algorithmic advancements."	https://openaccess.thecvf.com//content/WACV2025/html/Frolov_Needles__Haystacks_Dataset_and_Benchmark_for_Domain-Agnostic_Image-Based_Rigid_WACV_2025_paper.html	Anton Frolov, Florian Kleiner, Christiane RÃ¶Ãler, Volker Rodehorst
Negative-Prompt Inversion: Fast Image Inversion for Editing with Text-Guided Diffusion Models	In image editing employing diffusion models it is crucial to preserve the reconstruction fidelity to the original image while changing its style. Although existing methods ensure reconstruction fidelity through optimization a drawback of these is the significant amount of time required for optimization. In this paper we propose negative-prompt inversion a method capable of achieving equivalent reconstruction solely through forward propagation without optimization thereby enabling ultrafast editing processes. We experimentally demonstrate that the reconstruction fidelity of our method is comparable to that of existing methods allowing for inversion at a resolution of 512 pixels and with 50 sampling steps within approximately 5 seconds which is more than 30 times faster than null-text inversion. Reduction of the computation time by the proposed method further allows us to use a larger number of sampling steps in diffusion models to improve the reconstruction fidelity with a moderate increase in computation time.	https://openaccess.thecvf.com//content/WACV2025/html/Miyake_Negative-Prompt_Inversion_Fast_Image_Inversion_for_Editing_with_Text-Guided_Diffusion_WACV_2025_paper.html	Daiki Miyake, Akihiro Iohara, Yu Saito, Toshiyuki Tanaka
NestedMorph: Enhancing Deformable Medical Image Registration with Nested Attention Mechanisms	Deformable image registration is crucial for aligning medical images in a nonlinear fashion across different modalities allowing for precise spatial correspondence between varying anatomical structures. This paper presents NestedMorph a novel network utilizing a Nested Attention Fusion approach to improve intra-subject deformable registration between T1-weighted (T1w) MRI and diffusion MRI (dMRI) data. NestedMorph integrates high-resolution spatial details from an encoder with semantic information from a decoder using a multi-scale framework enhancing both local and global feature extraction. Our model notably outperforms existing methods including CNN-based approaches like VoxelMorph MIDIR and CycleMorph as well as Transformer-based models such as TransMorph and ViT-V-Net and traditional techniques like NiftyReg and SyN. Evaluations using the HCP dataset demonstrate that NestedMorph achieves superior performance across key metrics including SSIM HD95 and SDlogJ with the highest SSIM of 0.89 the lowest HD95 of 2.5 and SDlogJ of 0.22. These results highlight NestedMorph's ability to capture both local and global image features effectively leading to superior registration performance. The promising outcomes of this study underscore NestedMorph's potential to significantly advance deformable medical image registration providing a robust framework for future research and clinical applications. The source code and our implementation are available at: https://github.com/AS-Lab/Marthi-et-al-2024-NestedMorph-Deformable-Medical-Image-Registration	https://openaccess.thecvf.com//content/WACV2025/html/Kumar_NestedMorph_Enhancing_Deformable_Medical_Image_Registration_with_Nested_Attention_Mechanisms_WACV_2025_paper.html	Gurucharan Marthi Krishna Kumar, Janine Mendola, Amir Shmuel
NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support	While existing volumetric rendering approaches provide photorealistic results extracting high-quality meshes from optimized neural field representations is challenging. Conversely existing differentiable rasterization-based methods are typically sensitive to initialization and suffer from poor mesh rendering quality. In this paper we introduce NeuManifold a novel method for reconstructing watertight manifold meshes with high-quality textures from multi-view input images. NeuManifold overcomes the limitations of existing approaches by first learning a neural volumetric field and then refining it through differentiable mesh extraction and surface rendering. To eliminate artifacts and preserve mesh properties during iso-surface extraction we introduce a novel differentiable marching cubes method. Instead of traditional textures we use neural textures to enhance rendering quality. To integrate with modern graphics rendering pipelines we also provide customized GLSL shader support for neural textures. Extensive experiments demonstrate that NeuManifold outperforms existing mesh-based reconstruction methods in both mesh quality and rendering metrics achieving comparable or superior rendering quality to prior volume-rendering-based methods. The generated results enable real-time high-quality rendering and seamlessly support numerous graphics pipelines and applications requiring high-quality meshes such as 3D printing and physical simulation. https://sarahweiii.github.io/neumanifold/.	https://openaccess.thecvf.com//content/WACV2025/html/Wei_NeuManifold_Neural_Watertight_Manifold_Reconstruction_with_Efficient_and_High-Quality_Rendering_WACV_2025_paper.html	Xinyue Wei, Fanbo Xiang, Sai Bi, Anpei Chen, Kalyan Sunkavalli, Zexiang Xu, Hao Su
Neural Graph Map: Dense Mapping with Efficient Loop Closure Integration	Neural field-based SLAM methods typically employ a single monolithic field as their scene representation. This prevents efficient incorporation of loop closure constraints and limits scalability. To address these shortcomings we propose a novel RGB-D neural mapping framework in which the scene is represented by a collection of lightweight neural fields which are dynamically anchored to the pose graph of a sparse visual SLAM system. Our approach shows the ability to integrate large-scale loop closures while requiring only minimal reintegration. Furthermore we verify the scalability of our approach by demonstrating successful building-scale mapping taking multiple loop closures into account during the optimization and show that our method outperforms existing state-of-the-art approaches on large scenes in terms of quality and runtime. Our code is available open-source at https://github.com/KTH-RPL/neural_graph_mapping.	https://openaccess.thecvf.com//content/WACV2025/html/Bruns_Neural_Graph_Map_Dense_Mapping_with_Efficient_Loop_Closure_Integration_WACV_2025_paper.html	Leonard Bruns, Jun Zhang, Patric Jensfelt
Neural SDF for Shadow-Aware Unsupervised Structured Light	Among various active 3D measurement techniques Structured Light (SL) is one of the most popular method for its robustness and high accuracy. Ordinary SL system consists of a camera and a projector and by projecting a pre-defined pattern we can obtain pixel-to-pixel correspondences between the camera and the projector for triangulation. However if we lack the knowledge of the projected pattern for some reason e.g. the projected pattern is not as expected due to lens distortion inaccurate calibration undesired optical phenomenon like inter-reflection and so on the accuracy of conventional SL is severally degraded. As a remedy we propose unsupervised structured light (USSL) which does not explicitly use prior knowledge on the pattern. Inspired by the fact that human can recognize the scene structure illuminated by unknown light source (e.g. rotating mirror ball) and some prior work have succeeded in novel-view-synthesis under unknown illumination conditions we implement USSL on Neural Signed Distance Fields (Neural SDF) pipeline with implicit reflection module powered by neural network. Additionally since every SL method causes occlusion (shadow) by pattern projection we must take it into account for accurate shape reconstruction. To this end we integrate shadow volume rendering into the proposed pipeline. Experiments with synthetic and real dataset are conducted to confirm the feasibility of the proposed method.	https://openaccess.thecvf.com//content/WACV2025/html/Ichimaru_Neural_SDF_for_Shadow-Aware_Unsupervised_Structured_Light_WACV_2025_paper.html	Kazuto Ichimaru, Diego Thomas, Takafumi Iwaguchi, Hiroshi Kawasaki
NeuroViG - Integrating Event Cameras for Resource-Efficient Video Grounding	Spatio-Temporal Video Grounding (STVG) - the task of identifying the target object in the field-of-view that the language instruction refers to - is a fundamental vision-language task. Current STVG approaches typically utilize feeds from an RGB camera that is assumed to be always-on and process the video frames using complex neural network pipelines. As a result they often impose prohibitive system overheads (energy latency) on pervasive devices. To address this we propose NeuroViG with two key innovations: (a) leveraging on event streams from a low-power neuromorphic event camera sensor to perform selective triggering of the more energy-hungry RGB camera for STVG and (b) augmenting the STVG model with a lightweight Adaptive Frame Selector (AFS) that bypasses complex transformer-based operations for a majority of video frames thereby enabling its execution on a pervasive Jetson AGX device. We have also introduced modifications to the neural network processing pipeline such that the system can offer tunable tradeoffs between accuracy and energy/latency. Our proposed NeuroViG system allows us to reduce the STVG energy overhead and latency by 4x and 3.8x respectively for less than 1% loss in accuracy.	https://openaccess.thecvf.com//content/WACV2025/html/Weerakoon_NeuroViG_-_Integrating_Event_Cameras_for_Resource-Efficient_Video_Grounding_WACV_2025_paper.html	Dulanga Weerakoon, Vigneshwaran Subbaraju, Joo Hwee Lim, Archan Misra
No Annotations for Object Detection in Art through Stable Diffusion	Object detection in art is a valuable tool for the digital humanities as it allows for faster identification of objects in artistic and historical images compared to humans. However annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art) a pipeline that leverages diffusion models' art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets ArtDL 2.0 and IconArt outperforming prior work in weakly-supervised detection while being the first work for zero-shot object detection in art. Code is available at https://github.com/patrick-john-ramos/nada	https://openaccess.thecvf.com//content/WACV2025/html/Ramos_No_Annotations_for_Object_Detection_in_Art_through_Stable_Diffusion_WACV_2025_paper.html	Patrick Ramos, Nicolas Gonthier, Selina Khan, Yuta Nakashima, Noa Garcia
Noise-Aware Evaluation of Object Detectors	Supervised object detection requires annotated datasets for training and evaluation purposes. However human annotation of large datasets is error-prone and frequent mistakes are erroneous labels missing objects and imprecise bounding boxes. The main goals of this work are to quantify the extent of annotation noise in terms of corner-wise discrepancies assess how it impacts evaluation metrics for object detection and propose noise-aware alternatives that serve as upper and lower bounds for a baseline metric. We focus our analysis on the Microsoft COCO dataset and re-evaluate several state-of-the-art object detectors using the proposed metrics. We show that the Average Precision (AP) metric might be considerably over or under-estimated particularly for small objects and restrictive IoU acceptance thresholds. Our code is available at https://github.com/Artcs1/Error-Aware.	https://openaccess.thecvf.com//content/WACV2025/html/Llerena_Noise-Aware_Evaluation_of_Object_Detectors_WACV_2025_paper.html	Jeffri Murrugarra Llerena, Claudio R. Jung
Non-Cross Diffusion for Semantic Consistency	In diffusion models deviations from a straight generative flow are a common issue resulting in semantic inconsistencies and suboptimal generations. To address this challenge we introduce `Non-Cross Diffusion' an innovative approach in generative modeling for learning ordinary differential equation (ODE) models. Our methodology strategically incorporates an ascending dimension of input to effectively connect points sampled from two distributions with uncrossed paths. This design is pivotal in ensuring enhanced semantic consistency throughout the inference process which is especially critical for applications reliant on consistent generative flows including various distillation methods and deterministic sampling which are fundamental in image editing and interpolation tasks. Our empirical results demonstrate the effectiveness of Non-Cross Diffusion showing a substantial improvements in semantic consistencies at various inference steps and enhancing the overall performance of diffusion models.	https://openaccess.thecvf.com//content/WACV2025/html/Zheng_Non-Cross_Diffusion_for_Semantic_Consistency_WACV_2025_paper.html	Ziyang Zheng, Ruiyuan Gao, Qiang Xu
Now You See Me: Context-Aware Automatic Audio Description	Audio Description (AD) plays a pivotal role as an application system aimed at guaranteeing accessibility in multimedia content which provides additional narrations at suitable intervals to describe visual elements catering specifically to the needs of visually impaired audiences. In this paper we introduce CA3D the pioneering unified Context-Aware Automatic Audio Description system that provides AD event scripts with precise locations in the long cinematic content. Specifically CA3D system consists of: 1) a Temporal Feature Enhancement Module to efficiently capture longer term dependencies 2) an anchor-based AD event detector with feature suppression module that localizes the AD events and extracts discriminative feature for AD generation and 3) a self-refinement module that leverages the generated output to tweak AD event boundaries from coarse to fine. Unlike conventional methods which rely on metadata and ground truth AD timestamp for AD detection and generation tasks the proposed CA3D is the first end-to-end trainable system that only uses visual cue. Extensive experiments demonstrate that the proposed CA3D improves existing architectures for both AD event detection and script generation metrics establishing the new state-of-the-art performances in the AD automation.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Now_You_See_Me_Context-Aware_Automatic_Audio_Description_WACV_2025_paper.html	Seon-Ho Lee, Jue Wang, David Fan, Zhikang Zhang, Linda Liu, Xiang Hao, Vimal Bhat, Xinyu Li
OPTIMUS: Observing Persistent Transformations in Multi-Temporal Unlabeled Satellite-Data	In the face of pressing environmental issues in the 21st century monitoring surface changes on Earth is more important than ever. Large-scale remote sensing such as satellite imagery is an important tool for this task. However using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images only a small fraction may exhibit persistent changes of interest. To address this challenge we introduce OPTIMUS a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/.	https://openaccess.thecvf.com//content/WACV2025/html/Yu_OPTIMUS_Observing_Persistent_Transformations_in_Multi-Temporal_Unlabeled_Satellite-Data_WACV_2025_paper.html	Raymond Yu, Paul Han, Piper Wolters, Favyen Bastani
ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark Detection	Although facial landmark detection (FLD) has gained significant progress existing FLD methods still suffer from performance drops on partially non-visible faces such as faces with occlusions or under extreme lighting conditions or poses. To address this issue we introduce ORFormer a novel transformer-based method that can detect non-visible regions and recover their missing features from visible parts. Specifically ORFormer associates each image patch token with one additional learnable token called the messenger token. The messenger token aggregates features from all but its patch. This way the consensus between a patch and other patches can be assessed by referring to the similarity between its regular and messenger embeddings enabling non-visible region identification. Our method then recovers occluded patches with features aggregated by the messenger tokens. Leveraging the recovered features ORFormer compiles high-quality heatmaps for the downstream FLD task. Extensive experiments show that our method generates heatmaps resilient to partial occlusions. By integrating the resultant heatmaps into existing FLD methods our method performs favorably against the state of the arts on challenging datasets such as WFLW and COFW.	https://openaccess.thecvf.com//content/WACV2025/html/Chiang_ORFormer_Occlusion-Robust_Transformer_for_Accurate_Facial_Landmark_Detection_WACV_2025_paper.html	Jui-Che Chiang, Hou-Ning Hu, Bo-Syuan Hou, Chia-Yu Tseng, Yu-Lun Liu, Min-Hung Chen, Yen-Yu Lin
ORID: Organ-Regional Information Driven Framework for Radiology Report Generation	The objective of Radiology Report Generation (RRG) is to automatically generate coherent textual analyses of diseases based on radiological images thereby alleviating the workload of radiologists. Current AI-based methods for RRG primarily focus on modifications to the encoder-decoder model architecture. To advance these approaches this paper introduces an Organ-Regional Information Driven (ORID) framework which can effectively integrate multi-modal information and reduce the influence of noise from unrelated organs. Specifically based on the LLaVA-Med we first construct an RRG-related instruction dataset to improve organ-regional diagnosis description ability and get the LLaVA-Med-RRG. After that we propose an organ-based cross-modal fusion module to effectively combine the information from the organ-regional diagnosis description and radiology image. To further reduce the influence of noise from unrelated organs on the radiology report generation we introduce an organ importance coefficient analysis module which leverages Graph Neural Network (GNN) to examine the interconnections of the cross-modal information of each organ region. Extensive experiments and comparisons with state-of-the-art methods across various evaluation metrics demonstrate the superior performance of our proposed method.	https://openaccess.thecvf.com//content/WACV2025/html/Gu_ORID_Organ-Regional_Information_Driven_Framework_for_Radiology_Report_Generation_WACV_2025_paper.html	Tiancheng Gu, Kaicheng Yang, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai
OT-VP: Optimal Transport-Guided Visual Prompting for Test-Time Adaptation	Vision Transformers (ViTs) have demonstrated remarkable capabilities in learning representations but their performance is compromised when applied to unseen domains. Previous methods either engage in prompt learning during the training phase or modify model parameters at test time through entropy minimization. The former often overlooks unlabeled target data while the latter doesn't fully address domain shifts. In this work our approach Optimal Transport-guided Test-Time Visual Prompting (OT-VP) handles these problems by leveraging prompt learning at test time to align the target and source domains without accessing the training process or altering pre-trained model parameters. This method involves learning a universal visual prompt for the target domain by optimizing the Optimal Transport distance. With only four learned prompt tokens OT-VP exceeds state-of-the-art performance across three stylistic datasets--PACS VLCS OfficeHome and one corrupted dataset ImageNet-C. Additionally OT-VP operates efficiently both in terms of memory and computation and is adaptable for extension to online settings. The code is available at https://github.com/zybeich/OT-VP.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_OT-VP_Optimal_Transport-Guided_Visual_Prompting_for_Test-Time_Adaptation_WACV_2025_paper.html	Yunbei Zhang, Akshay Mehra, Jihun Hamm
OTCXR: Rethinking Self-Supervised Alignment using Optimal Transport for Chest X-ray Analysis	Self-supervised learning (SSL) has emerged as a promising technique for analyzing medical modalities such as X-rays due to its ability to learn without annotations. However conventional SSL methods face challenges in achieving semantic alignment and capturing subtle details which limits their ability to accurately represent the underlying anatomical structures and pathological features. To address these limitations we propose OTCXR a novel SSL framework that leverages optimal transport (OT) to learn dense semantic invariance. By integrating OT with our innovative Cross-Viewpoint Semantics Infusion Module (CV-SIM) OTCXR enhances the model's ability to capture not only local spatial features but also global contextual dependencies across different viewpoints. This approach enriches the effectiveness of SSL in the context of chest radiographs. Furthermore OTCXR incorporates variance and covariance regularizations within the OT framework to prioritize clinically relevant information while suppressing less informative features. This ensures that the learned representations are comprehensive and discriminative particularly beneficial for tasks such as thoracic disease diagnosis. We validate OTCXR's efficacy through comprehensive experiments on three publicly available chest X-ray datasets. Our empirical results demonstrate the superiority of OTCXR over state-of-the-art methods across all evaluated tasks confirming its capability to learn semantically rich representations.	https://openaccess.thecvf.com//content/WACV2025/html/Gorade_OTCXR_Rethinking_Self-Supervised_Alignment_using_Optimal_Transport_for_Chest_X-ray_WACV_2025_paper.html	Vandan Gorade, Azad Singh, Deepak Mishra
OccFlowNet: Occupancy Estimation via Differentiable Rendering and Occupancy Flow	Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However most existing camera-based methods rely on large and costly datasets with fine-grained 3D voxel labels for training which limits their practicality and scalability. Furthermore approaches in this domain lack the modelling of scene dynamics. In this work we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using supervision in 2D based on 3D labels provided by LiDAR that offers a more natural way of supervision than voxel labels. In particular we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on supervision in 2D space only. To enhance geometric accuracy and increase the supervisory signal we introduce temporal rendering of adjacent time steps. Additionally we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that supervision in 2D with LiDAR can achieve state-of-the-art performance compared to methods using voxel labels and when combining it with voxel supervision in 3D temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation.	https://openaccess.thecvf.com//content/WACV2025/html/Boeder_OccFlowNet_Occupancy_Estimation_via_Differentiable_Rendering_and_Occupancy_Flow_WACV_2025_paper.html	Simon Boeder, Benjamin Risse
OccLoff: Learning Optimized Feature Fusion for 3D Occupancy Prediction	3D semantic occupancy prediction is crucial for finely representing the surrounding environment which is essential for ensuring the safety in autonomous driving. Existing fusion-based occupancy methods typically involve performing a 2D-to-3D view transformation on image features followed by computationally intensive 3D operations to fuse these with LiDAR features leading to high computational costs and reduced accuracy. Moreover current research on occupancy prediction predominantly focuses on designing specific network architectures often tailored to particular models with limited attention given to the more fundamental aspect of semantic feature learning. This gap hinders the development of more transferable methods that could enhance the performance of various occupancy models. To address these challenges we propose OccLoff a framework that Learns to Optimize Feature Fusion for 3D occupancy prediction. Specifically we introduce a sparse fusion encoder with entropy masks that directly fuses 3D and 2D features improving model accuracy while reducing computational overhead. Additionally we propose a transferable proxy-based loss function and an adaptive hard sample weighting algorithm which enhance the performance of several state-of-the-art methods. Extensive evaluations on the nuScenes and SemanticKITTI benchmarks demonstrate the superiority of our framework and ablation studies confirm the effectiveness of each proposed module.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_OccLoff_Learning_Optimized_Feature_Fusion_for_3D_Occupancy_Prediction_WACV_2025_paper.html	Ji Zhang, Yiran Ding, Zixin Liu
OmniDiffusion: Reformulating 360 Monocular Depth Estimation using Semantic and Surface Normal Conditioned Diffusion	Depth estimation is the fundamental computer vision task for scene analysis. With the emergence of the deep learning era supervised monocular image depth estimation (MDE) became a popular choice for the task. Predominantly MDE methods utilize 360 images as ideal input due to their comprehensive field of view scene content compared to perspective images but they suffer from distortions in polar regions making it a more challenging ill-posed problem to date. Over the years methods using CNNs and/or large transformers taking 360 and/or projected perspective patch inputs have been proposed to solve the 360 MDE problem by formulating it as a regression or a classification task. Nevertheless their performance still suffers from global discrepancy inaccuracy poor details and generalizability. Lately diffusion-generating models have shown state-of-the-art performance in image synthesis that captures exceptionally rich knowledge of the visual world. However their ability to perform omnidirectional perception tasks is still unexplored. In this paper we explore a new approach called OmniDiffusion that reformulates the 360 MDE task as a diffusion denoising process. We present a diffusion-based framework to learn an iterative denoising process that denoises random depth distribution into the required depths. The diffusion process is performed in the latent space and uses the guidance of encoded RGB image visual as a condition. Furthermore to advance the image latent in a geometrically meaningful direction we leverage semantic segmentation and surface normal information to provide a more detailed contextual assistance to the denoising process. The performed experiments on the multiple real-world datasets show that our diffusion-denoising approach with the proposed conditions more appropriately refines depths outperforming the existing MDE and diffusion-based methods with state-of-the-art generalization ability while generating more accurate high-quality and detailed 360 depths.	https://openaccess.thecvf.com//content/WACV2025/html/Mohadikar_OmniDiffusion_Reformulating_360_Monocular_Depth_Estimation_using_Semantic_and_Surface_WACV_2025_paper.html	Payal Mohadikar, Ye Duan
OmniGS: Fast Radiance Field Reconstruction using Omnidirectional Gaussian Splatting	Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in various domains. However the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper we present OmniGS a novel omnidirectional Gaussian splatting system to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. We realize differentiable optimization of the omnidirectional radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. The code will be publicly available at https://github.com/liquorleaf/OmniGS.	https://openaccess.thecvf.com//content/WACV2025/html/Li_OmniGS_Fast_Radiance_Field_Reconstruction_using_Omnidirectional_Gaussian_Splatting_WACV_2025_paper.html	Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng
On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process	Knowledge distillation (KD) remains challenging due to the opaque nature of the knowledge transfer process from a Teacher to a Student making it difficult to address certain issues related to KD. To address this we proposed UniCAM a novel gradient-based visual explanation method which effectively interprets the knowledge learned during KD. Our experimental results demonstrate that with the guidance of the Teacher's knowledge the Student model becomes more efficient learning more relevant features while discarding those that are not relevant. We refer to the features learned with the Teacher's guidance as distilled features and the features irrelevant to the task and ignored by the Student as residual features. Distilled features focus on key aspects of the input such as textures and parts of objects. In contrast residual features demonstrate more diffused attention often targeting irrelevant areas including the backgrounds of the target objects. In addition we proposed two novel metrics: the feature similarity score (FSS) and the relevance score (RS) which quantify the relevance of the distilled knowledge. Experiments on the CIFAR10 ASIRRA and Plant Disease datasets demonstrate that UniCAM and the two metrics offer valuable insights to explain the KD process.	https://openaccess.thecvf.com//content/WACV2025/html/Adhane_On_Explaining_Knowledge_Distillation_Measuring_and_Visualising_the_Knowledge_Transfer_WACV_2025_paper.html	Gereziher Adhane, Mohammad Mahdi Dehshibi, Dennis Vetter, David Masip, Gemma Roig
On Neural BRDFs: A Thorough Comparison of State-of-the-Art Approaches	The bidirectional reflectance distribution function (BRDF) is an essential tool to capture the complex interaction of light and matter. Recently several works have employed neural methods for BRDF modeling following various strategies ranging from utilizing existing parametric models to purely neural parametrizations. While all methods yield impressive results a comprehensive comparison of the different approaches is missing in the literature. In this work we present a thorough evaluation of several approaches including results for qualitative and quantitative reconstruction quality and an analysis of reciprocity and energy conservation. Moreover we propose two extensions that can be added to existing approaches: A novel additive combination strategy for neural BRDFs that split the reflectance into a diffuse and a specular part and an input mapping that ensures reciprocity exactly by construction while previous approaches only ensure it by soft constraints.	https://openaccess.thecvf.com//content/WACV2025/html/Hofherr_On_Neural_BRDFs_A_Thorough_Comparison_of_State-of-the-Art_Approaches_WACV_2025_paper.html	Florian Hofherr, Bjoern Haefner, Daniel Cremers
On Which Data Distribution (Synthetic or Real) We Should Rely for Soft Biometric Classification	"Identification of gender is critical not only for human-computer interaction but also for scrutinizing the search space in which an identity needs to be determined. Traditionally ""real"" facial images are employed for gender identification by computer vision algorithms. Due to the tremendous rise of privacy and advancement in generative networks synthetic face images are heavily developed and can be used for several face-related studies including gender classification. However their effectiveness compared to real images is still unexplored for gender classification. In response this study explores the effectiveness of gender classification networks trained on real and synthetic face images offering novel insights into the effectiveness of these two data distributions. For that we implemented several state-of-the-art gender classification architectures covering convolutional neural networks (CNNs) and vision transformers (ViT). Our research builds on the rigorous evaluation of 8 Deep Neural Networks (DNNs) across 4 diverse datasets and 6 types of image corruptions. To make the research interpretable we have also used several explainable mechanisms including Grad-CAM and t-SNE visualizations. In brief the impact of the proposed research is multifold: (i) understand the effectiveness of real vs. synthetic data distributions in network training and (ii) whether the synthetic models reflect the true physical world distribution to ensure that the models trained on them are resilient against image perturbations."	https://openaccess.thecvf.com//content/WACV2025/html/A_On_Which_Data_Distribution_Synthetic_or_Real_We_Should_Rely_WACV_2025_paper.html	Manju R. A, Atul Kumar, Akshay Agarwal
On the Importance of Dual-Space Augmentation for Domain Generalized Object Detection	The distribution gap between training data and real-world data often causes significant performance drops in networks trained via naive supervised learning. To address this domain generalization methods have been developed to gain robust performance in unseen domains. In this paper we propose a single-domain generalized object detection (S-DGOD) method. Unlike previous works we utilize both image-level and feature-level augmentations and experimentally demonstrate their synergistic effects. Image-level augmentations expand the source domain while feature-level augmentations leverage CLIP to incorporate potential domain descriptions. Our method achieves superior performance with 29.2% mAP on the Cityscapes-C and 37.1% mAP on the Diverse-Weather dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Park_On_the_Importance_of_Dual-Space_Augmentation_for_Domain_Generalized_Object_WACV_2025_paper.html	Hayoung Park, Choongsang Cho, Guisik Kim
On-the-Fly Object-aware Representative Point Selection in Point Cloud	Point clouds are essential for object modeling and play a critical role in assisting driving tasks for autonomous vehicles (AVs). However the significant volume of data generated by AVs creates challenges for storage bandwidth and processing cost. To tackle these challenges we propose a representative point selection framework for point cloud downsampling which preserves critical object-related information while effectively filtering out irrelevant background points. Our method involves two steps: (1) Object Presence Detection where we introduce an unsupervised density peak-based classifier and a supervised Naive Bayes classifier to handle diverse scenarios and (2) Sampling Budget Allocation where we propose a strategy that selects object-relevant points while maintaining a high retention rate of object information. Extensive experiments on the KITTI and nuScenes datasets demonstrate that our method consistently outperforms state-of-the-art baselines in both efficiency and effectiveness across varying sampling rates. As a model-agnostic solution our approach integrates seamlessly with diverse downstream models making it a valuable and scalable addition to the 3D point cloud downsampling toolkit for AV applications.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_On-the-Fly_Object-aware_Representative_Point_Selection_in_Point_Cloud_WACV_2025_paper.html	Xiaoyu Zhang, Ziwei Wang, Hai Dong, Zhifeng Bao, Jiajun Liu
One VLM to Keep it Learning: Generation and Balancing for Data-Free Continual Visual Question Answering	Vision-Language Models (VLMs) have shown significant promise in Visual Question Answering (VQA) tasks by leveraging web-scale multimodal datasets. However these models often struggle with continual learning due to catastrophic forgetting when adapting to new tasks. As an effective remedy to mitigate catastrophic forgetting rehearsal strategy uses the data of past tasks upon learning new task. However such strategy incurs the need of storing past data which might not be feasible due to hardware constraints or privacy concerns. In this work we propose the first data-free method that leverages the language generation capability of a VLM instead of relying on external models to produce pseudo-rehearsal data for addressing continual VQA. Our proposal named as GaB generates pseudo-rehearsal data by posing previous task questions on new task data. Yet despite being effective the distribution of generated questions skews towards the most frequently posed questions due to the limited and task-specific training data. To mitigate this issue we introduce a pseudo-rehearsal balancing module that aligns the generated data towards the ground-truth data distribution using either the question meta-statistics or an unsupervised clustering method. We evaluate our proposed method on two recent benchmarks i.e. VQACL-VQAv2 and CLOVE-function benchmarks. GaB outperforms all the data-free baselines with substantial improvement in maintaining VQA performance across evolving tasks while being on-par with methods with access to the past data. Code will be made public soon.	https://openaccess.thecvf.com//content/WACV2025/html/Das_One_VLM_to_Keep_it_Learning_Generation_and_Balancing_for_WACV_2025_paper.html	Deepayan Das, Davide Talon, Massimiliano Mancini, Yiming Wang, Elisa Ricci
Online-LoRA: Task-Free Online Continual Learning via Low Rank Adaptation	Catastrophic forgetting is a significant challenge in online continual learning (OCL) especially for non-stationary data streams that do not have well-defined task boundaries. This challenge is exacerbated by the memory constraints and privacy concerns inherent in rehearsal buffers. To tackle catastrophic forgetting in this paper we introduce Online-LoRA a novel framework for task-free OCL. Online-LoRA allows to finetune pre-trained Vision Transformer (ViT) models in real-time to address the limitations of rehearsal buffers and leverage pre-trained models' performance benefits. As the main contribution our approach features a novel online weight regularization strategy to identify and consolidate important model parameters. Moreover Online-LoRA leverages the training dynamics of loss values to enable the automatic recognition of the data distribution shifts. Extensive experiments across many task-free OCL scenarios and benchmark datasets (including CIFAR-100 ImageNet-R ImageNet-S CUB-200 and CORe50) demonstrate that Online-LoRA can be robustly adapted to various ViT architectures while achieving better performance compared to SOTA methods.	https://openaccess.thecvf.com//content/WACV2025/html/Wei_Online-LoRA_Task-Free_Online_Continual_Learning_via_Low_Rank_Adaptation_WACV_2025_paper.html	Xiwen Wei, Guihong Li, Radu Marculescu
OpenCapBench: A Benchmark to Bridge Pose Estimation and Biomechanics	Pose estimation has promised to impact healthcare by enabling more practical methods to quantify nuances of human movement and biomechanics. However despite the inherent connection between pose estimation and biomechanics these disciplines have largely remained disparate. For example most current pose estimation benchmarks use metrics such as Mean Per Joint Position Error Percentage of Correct Keypoints or mean Average Precision to assess performance without quantifying kinematic and physiological correctness - key aspects for biomechanics. To alleviate this challenge we develop OpenCapBench to offer an easy-to-use unified benchmark to assess common tasks in human pose estimation evaluated under physiological constraints. OpenCapBench computes consistent kinematic metrics through joints angles provided by an open-source musculoskeletal modeling software (OpenSim). Through OpenCapBench we demonstrate that current pose estimation models use keypoints that are too sparse for accurate biomechanics analysis. To mitigate this challenge we introduce SynthPose a new approach that enables finetuning of pre-trained 2D human pose models to predict an arbitrarily denser set of keypoints for accurate kinematic analysis through the use of synthetic data. Incorporating such finetuning on synthetic data of prior models leads to twofold reduced joint angle errors. Moreover OpenCapBench allows users to benchmark their own developed models on our clinically relevant cohort. Overall OpenCapBench bridges the computer vision and biomechanics communities aiming to drive simultaneous advances in both areas.	https://openaccess.thecvf.com//content/WACV2025/html/Gozlan_OpenCapBench_A_Benchmark_to_Bridge_Pose_Estimation_and_Biomechanics_WACV_2025_paper.html	Yoni Gozlan, Antoine Falisse, Scott Uhlrich, Anthony Gatti, Michael Black, Jennifer Hicks, Scott Delp, Akshay Chaudhari
OpenCity3D: What do Vision-Language Models Know About Urban Environments?	The rise of 2D vision-language models (VLMs) has enabled new possibilities for language-driven 3D scene understanding tasks. Existing works focus on indoor scenes or autonomous driving scenarios and typically validate against a pre-defined set of semantic object classes. In this work we analyze the capabilities of vision-language models for large-scale urban 3D scene understanding and propose new applications of VLMs that directly operate on aerial 3D reconstructions of cities. In particular we address higher-level 3D scene understanding tasks such as population density building age property prices crime rate and noise pollution. Our analysis reveals surprising zero-shot and few-shot performance of VLMs in urban environments.	https://openaccess.thecvf.com//content/WACV2025/html/Bieri_OpenCity3D_What_do_Vision-Language_Models_Know_About_Urban_Environments_WACV_2025_paper.html	Valentin Bieri, Marco Zamboni, Nicolas Samuel Blumer, Qingxuan Chen, Francis Engelmann
Optimizing Dense Visual Predictions Through Multi-Task Coherence and Prioritization	Multi-Task Learning (MTL) involves the concurrent training of multiple tasks offering notable advantages for dense prediction tasks in computer vision. MTL not only reduces training and inference time as opposed to having multiple single-task models but also enhances task accuracy through the interaction of multiple tasks. However existing methods face limitations. They often rely on suboptimal cross-task interactions resulting in task-specific predictions with poor geometric and predictive coherence. In addition many approaches use inadequate loss weighting strategies which do not address the inherent variability in task evolution during training. To overcome these challenges we propose an advanced MTL model specifically designed for dense vision tasks. Our model leverages state-of-the-art vision transformers with task-specific decoders. To enhance cross-task coherence we introduce a trace-back method that improves both cross-task geometric and predictive features. Furthermore we present a novel dynamic task balancing approach that projects task losses onto a common scale and prioritizes more challenging tasks during training. Extensive experiments demonstrate the superiority of our method establishing new state-of-the-art performance across two benchmark datasets. The code is available at: https://github.com/Klodivio355/MT-CP.	https://openaccess.thecvf.com//content/WACV2025/html/Fontana_Optimizing_Dense_Visual_Predictions_Through_Multi-Task_Coherence_and_Prioritization_WACV_2025_paper.html	Maxime Fontana, Michael Spratling, Miaojing Shi
Optimizing Neural Network Effectiveness via Non-Monotonicity Refinement	Activation functions play a crucial role in artificial neural networks by introducing non-linearities that enable networks to learn complex patterns in data. An appropriate choice of an activation function plays a crucial role in the training dynamics of a neural network which can boost network performance significantly. Rectified Linear Unit (ReLU) and its variants like leaky ReLU and parametric ReLU have emerged as the most popular activations due to their ability to enable faster training and generalization in deep neural networks despite having some significant issues like vanishing gradient problems. In this paper we have proposed smooth functions which we call the AMSU family which are smooth approximations of the maximum function. We derive three activations from the AMSU family namely AMSU-1 AMSU-2 & AMSU-3 and show their effectiveness in different deep learning problems. By simply replacing the ReLU function Top-1 accuracy improves by 5.88% 5.96% and 5.32% on the CIFAR100 dataset on the ShuffleNet V2 model. Also replacing ReLU with AMSU-1 AMSU-2 and AMSU-3 Top-1 accuracy improves by 8.50% 8.29% and 7.70% on the CIFAR100 dataset on the ShuffleNet V2 model with FGSM attack. Also Replacing ReLU with AMSU-1 AMSU-2 and AMSU-3 on ImageNet-1K data we got 3%-5% improvement on ShuffleNet and MobileNet models. The source code is publicly available at https://github.com/koushik313/AMSU.	https://openaccess.thecvf.com//content/WACV2025/html/Biswas_Optimizing_Neural_Network_Effectiveness_via_Non-Monotonicity_Refinement_WACV_2025_paper.html	Koushik Biswas, Amit Reza, Meghana Karri, Debesh Jha, Hongyi Pan, Nikhil Tomar, Aliza Subedi, Smriti Regmi, Ulas Bagci
Optimizing Vision-Language Model for Road Crossing Intention Estimation	Identifying a pedestrian's intention to cross the road is crucial for autonomous driving as it alerts the system to stop or slow down. However determining crossing intention from video is challenging due to the need for extracting complex high-level semantics. This paper introduces ClipCross a novel classification framework optimized to extract high-level semantic features using the vision-language model CLIP for determining crossing intention. Existing CLIP-based methods perform poorly in this task as CLIP's image and text encoders fail to capture the nuanced semantic distinctions between crossing and non-crossing intention images. ClipCross addresses this by optimizing a set of CLIP text embeddings to extract high-level semantic features which a multi-layer perceptron uses to distinguish between crossing and non-crossing intentions. ClipCross achieves state-of-the-art performance on crossing intention estimation benchmark datasets: PIE PSI and JAAD.	https://openaccess.thecvf.com//content/WACV2025/html/Uziel_Optimizing_Vision-Language_Model_for_Road_Crossing_Intention_Estimation_WACV_2025_paper.html	Roy Uziel, Oded Bialer
Ordinal Multiple-Instance Learning for Ulcerative Colitis Severity Estimation with Selective Aggregated Transformer	Patient-level diagnosis of severity in ulcerative colitis (UC) is common in clinical practice where the most severe score for a patient is typically recorded as the diagnosis result. However previous UC classification methods (i.e. image-level estimation) mainly assumed the input was a single image. Thus these methods can not utilize severity labels recorded in real clinical settings. In this paper we propose a patient-level severity estimation method by a transformer with selective aggregator tokens where a severity label is estimated from multiple images taken from a patient similar to a clinical setting. Our method can effectively aggregate features of severe parts from a set of images captured in each patient and it facilitates improving the discriminative ability between adjacent severity classes. Experiments demonstrate the effectiveness of the proposed method on two datasets compared with the state-of-the-art MIL methods. Moreover we evaluated our method using real clinical data and confirmed that our method outperformed the previous image-level methods. The code is publicly available at https://github.com/Shiku-Kaito/Ordinal-Multiple-instance-Learning-for-Ulcerative-Colitis-Severity-Estimation.	https://openaccess.thecvf.com//content/WACV2025/html/Shiku_Ordinal_Multiple-Instance_Learning_for_Ulcerative_Colitis_Severity_Estimation_with_Selective_WACV_2025_paper.html	Kaito Shiku, Kazuya Nishimura, Daiki Suehiro, Kiyohito Tanaka, Ryoma Bise
Oriented Cell Dataset: A Dataset and Benchmark for Oriented Cell Detection and Applications	This work presents a new public dataset for cell detection in bright-field microscopy images annotated with Oriented Bounding Boxes (OBBs) named Oriented Cell Dataset (OCD). Our dataset also contains a subset of images with five independent expert annotations which allows inter-annotation analysis to determine a suitable IoU acceptance threshold for evaluating cell detectors. We show that OBBs and a derived representation Oriented Ellipses (OEs) provide a more accurate shape representation than standard Horizontal Bounding Boxes (HBBs) with a slight overhead of one extra click in the annotation process. We benchmarked OCD using 14 state-of-the-art oriented object detectors and explored two main problems in cancer biology: cell confluence and polarity determination. Our code and dataset are available at https://github.com/LucasKirsten/Deep-Cell-Tracking-EBB.	https://openaccess.thecvf.com//content/WACV2025/html/Kirsten_Oriented_Cell_Dataset_A_Dataset_and_Benchmark_for_Oriented_Cell_WACV_2025_paper.html	Lucas Kirsten, Angelo Angonezi, Jose Marques, Fernanda Oliveira, Juliano Faccioni, Camila Cassel, DÃ©bora de Sousa, Samlai Vedovatto, Guido Lenz, Claudio Jung
PACA: Prespective-Aware Cross-Attention Representation for Zero-Shot Scene Rearrangement	Scene rearrangement like table tidying is a challenging task in robotic manipulation due to the complexity of predicting diverse object arrangements. Web-scale trained generative models such as Stable Diffusion can aid by generating natural scenes as goals. To facilitate robot execution object-level representations must be extracted to match the real scenes with the generated goals and to calculate object pose transformations. Current methods typically use a multi-step design that involves separate models for generation segmentation and feature encoding which can lead to a low success rate due to error accumulation. Furthermore they lack control over the viewing perspectives of the generated goals restricting the tasks to 3-DoF settings. In this paper we propose PACA a zero-shot pipeline for scene rearrangement that leverages perspective-aware cross-attention representation derived from Stable Diffusion. Specifically we develop an object-level representation that integrates generation segmentation and feature encoding into a single step. Additionally we introduce perspective control thus enabling the matching of 6-DoF camera views and extending past approaches that were limited to 3-DoF top-down settings. The efficacy of our method is demonstrated through its zero-shot performance in real robot experiments across various scenes achieving an average matching accuracy and execution success rate of 87% and 67% respectively.	https://openaccess.thecvf.com//content/WACV2025/html/Jin_PACA_Prespective-Aware_Cross-Attention_Representation_for_Zero-Shot_Scene_Rearrangement_WACV_2025_paper.html	Shutong Jin, Ruiyu Wang, Kuangyi Chen, Florian T. Pokorny
PALO: A Polyglot Large Multimodal Model for 5B People	In pursuit of more inclusive Vision-Language Models (VLMs) this study introduces a Large Multilingual Multimodal Model called PALO. PALO offers visual reasoning capabilities in 10 major languages including English Chinese Hindi Spanish French Arabic Bengali Russian Urdu and Japanese that span a total of 5B people (65% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi Arabic Bengali and Urdu. The resulting models are trained across three scales (1.7B 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages. Code: https://github.com/mbzuai-oryx/PALO.	https://openaccess.thecvf.com//content/WACV2025/html/Rasheed_PALO_A_Polyglot_Large_Multimodal_Model_for_5B_People_WACV_2025_paper.html	Hanoona Rasheed, Muhammad Maaz, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, Fahad S. Khan
PC-GZSL: Prior Correction for Generalized Zero Shot Learning	Generalized Zero Shot Learning (GZSL) aims at achieving a good accuracy on both seen and unseen classes by relying on the information acquired from auxiliary attributes. Existing approaches have devised many frameworks to make this knowledge transfer more efficient and informative. Despite their effectiveness on boosting the overall performance there has always been a strong bias in the model towards the seen classes which makes GZSL problem more challenging. The effect of this bias on the model performance has never been properly explored. We observe that GZSL algorithms in literature have an evident bias towards the seen classes. Further we also show that techniques like calibrated stacking fall short of resolving this conflict between the seen and unseen classes effectively. In this work we analyze and develop a logit-adjustment approach in GZSL setting and propose a simple yet effective method to remove the bias from trained models in a post-hoc manner. Moreover as a consequence of the post-hoc nature of the proposed approach there is no additional training cost. We exhaustively compare the proposed method on both embedding-based and generative-based GZSL frameworks surpassing the SOTA results by 3.1% 4.6% and 3.1% on CUB SUN and AwA2 datasets. We also present theoretical analysis showing effectiveness of proposed approach.	https://openaccess.thecvf.com//content/WACV2025/html/Bhat_PC-GZSL_Prior_Correction_for_Generalized_Zero_Shot_Learning_WACV_2025_paper.html	S Divakar Bhat, Amit More, Mudit Soni, Bhuvan Aggarwal
PETALface: Parameter Efficient Transfer Learning for Low-Resolution Face Recognition	Pre-training on large-scale datasets and utilizing margin-based loss functions have been highly successful in training models for high-resolution face recognition. However these models struggle with low-resolution face datasets in which the faces lack the facial attributes necessary for distinguishing different faces. Full fine-tuning on low-resolution datasets a naive method for adapting the model yields inferior performance due to catastrophic forgetting of pre-trained knowledge. Additionally the domain difference between high-resolution (HR) gallery images and low-resolution (LR) probe images in low resolution datasets leads to poor convergence for a single model to adapt to both gallery and probe after fine-tuning. To this end we propose PETALface a Parameter-Efficient Transfer Learning approach for low-resolution face recognition. Through PETALface we attempt to solve both the aforementioned problems. (1) We solve catastrophic forgetting by leveraging the power of parameter efficient fine-tuning(PEFT). (2) We introduce two low-rank adaptation modules to the backbone with weights adjusted based on the input image quality to account for the difference in quality for the gallery and probe images. To the best of our knowledge PETALface is the first work leveraging the powers of PEFT for low resolution face recognition. Extensive experiments demonstrate that the proposed method outperforms full fine-tuning on low-resolution datasets while preserving performance on high-resolution and mixed-quality datasets all while using only 0.48% of the parameters.	https://openaccess.thecvf.com//content/WACV2025/html/Narayan_PETALface_Parameter_Efficient_Transfer_Learning_for_Low-Resolution_Face_Recognition_WACV_2025_paper.html	Kartik Narayan, Nithin Gopalakrishnan Nair, Jennifer Xu, Rama Chellappa, Vishal M. Patel
PGRID: Power Grid Reconstruction in Informal Developments using High-Resolution Aerial Imagery	As of 2023 a record 117 million people have been displaced worldwide more than double the number from a decade ago [22]. Of these 32 million are refugees under the UNHCR mandate with 8.7 million residing in refugee camps. A critical issue faced by these populations is the lack of access to electricity with 80% of the 8.7 million refugees and displaced persons in camps globally relying on traditional biomass for cooking and lacking reliable power for essential tasks such as cooking and charging phones. Often the burden of collecting firewood falls on women and children who frequently travel up to 20 kilometers into dangerous areas increasing their vulnerability. [7] Electricity access could significantly alleviate these challenges but a major obstacle is the lack of accurate power grid infrastructure maps particularly in resource-constrained environments like refugee camps needed for energy access planning. Existing power grid maps are often outdated incomplete or dependent on costly complex technologies limiting their practicality. To address this issue PGRID is a novel application-based approach which utilizes high-resolution aerial imagery to detect electrical poles and segment electrical lines creating precise power grid maps. PGRID was tested in the Turkana region of Kenya specifically the Kakuma and Kalobeyei Camps covering 84 km2 and housing over 200000 residents. Our findings show that PGRID delivers high-fidelity power grid maps especially in unplanned settlements with F1-scores of 0.71 and 0.82 for pole detection and line segmentation respectively. This study highlights a practical application for leveraging open data and limited labels to improve power grid mapping in unplanned settlements where the growing number of displaced persons urgently need sustainable energy infrastructure solutions.	https://openaccess.thecvf.com//content/WACV2025/html/Nsutezo_PGRID_Power_Grid_Reconstruction_in_Informal_Developments_using_High-Resolution_Aerial_WACV_2025_paper.html	Simone Fobi Nsutezo, Amrita Gupta, Duncan Kebut, Seema Iyer, Luana Marotti, Rahul Dodhia, Juan M. Lavista Ferres, Anthony Ortiz
PICASSO: A Feed-Forward Framework for Parametric Inference of CAD Sketches via Rendering Self-Supervision	This work introduces PICASSO a framework for the parameterization of 2D CAD sketches from hand-drawn and precise sketch images. PICASSO converts a given CAD sketch image into parametric primitives that can be seamlessly integrated into CAD software. Our framework leverages rendering self-supervision to enable the pre-training of a CAD sketch parameterization network using sketch renderings only thereby eliminating the need for corresponding CAD parameterization. Thus we significantly reduce reliance on parameter-level annotations which are often unavailable particularly for hand-drawn sketches. The two primary components of PICASSO are (1) a Sketch Parameterization Network (SPN) that predicts a series of parametric primitives from CAD sketch images and (2) a Sketch Rendering Network (SRN) that renders parametric CAD sketches in a differentiable manner and facilitates the computation of a rendering (image-level) loss for self-supervision. We demonstrate that the proposed PICASSO can achieve reasonable performance even when finetuned with only a small number of parametric CAD sketches. Extensive evaluation on the widely used SketchGraphs and CAD as Language datasets validates the effectiveness of the proposed approach on zero- and few-shot learning scenarios.	https://openaccess.thecvf.com//content/WACV2025/html/Karadeniz_PICASSO_A_Feed-Forward_Framework_for_Parametric_Inference_of_CAD_Sketches_WACV_2025_paper.html	Ahmet Serdar Karadeniz, Dimitrios Mallis, Nesryne Mejri, Kseniya Cherenkova, Anis Kacem, Djamila Aouada
PK-YOLO: Pretrained Knowledge Guided YOLO for Brain Tumor Detection in Multiplanar MRI Slices	Brain tumor detection in multiplane Magnetic Resonance Imaging (MRI) slices is a challenging task due to the various appearances and relationships in the structure of the multiplane images. In this paper we propose a new You Only Look Once (YOLO)-based detection model that incorporates Pretrained Knowledge (PK) called PK-YOLO to improve the performance for brain tumor detection in multiplane MRI slices. To our best knowledge PK-YOLO is the first pretrained knowledge guided YOLO-based object detector. The main components of the new method are a pretrained pure lightweight convolutional neural network-based backbone via sparse masked modeling a YOLO architecture with the pretrained backbone and a regression loss function for improving small object detection. The pretrained backbone allows for feature transferability of object queries on individual plane MRI slices into the model encoders and the learned domain knowledge base can improve in-domain detection. The improved loss function can further boost detection performance on small-size brain tumors in multiplanar two-dimensional MRI slices. Experimental results show that the proposed PK-YOLO achieves competitive performance on the multiplanar MRI brain tumor detection datasets compared to state-of-the-art YOLO-like and DETR-like object detectors. The code is available at https://github.com/mkang315/PK-YOLO.	https://openaccess.thecvf.com//content/WACV2025/html/Kang_PK-YOLO_Pretrained_Knowledge_Guided_YOLO_for_Brain_Tumor_Detection_in_WACV_2025_paper.html	Ming Kang, Fung Fung Ting, Raphael C.-W. Phan, Chee-Ming Ting
PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning	Recently the usage of Contrastive Representation Learning (CRL) as a pre-training technique improves the performance of learning with noisy labels (LNL) methods. However instead of pre-training when trivially combining CRL loss with LNL methods as an end-to-end framework the empirical experiments show severe degeneration of the performance. We verify through experiments that this issue is caused by optimization conflicts of losses and propose an end-to-end PLReMix framework by introducing a Pseudo-Label Relaxed (PLR) contrastive loss. This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs alleviating the loss conflicts by trivially combining these losses. The proposed PLR loss is pluggable and we have integrated it into other LNL methods observing their improved performance. Furthermore a two-dimensional Gaussian Mixture Model is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously. Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method. Codes will be available.	https://openaccess.thecvf.com//content/WACV2025/html/Liu_PLReMix_Combating_Noisy_Labels_with_Pseudo-Label_Relaxed_Contrastive_Representation_Learning_WACV_2025_paper.html	Xiaoyu Liu, Beitong Zhou, Zuogong Yue, Cheng Cheng
PRoGS: Progressive Rendering of Gaussian Splats	Over the past year 3D Gaussian Splatting (3DGS) has received significant attention for its ability to represent 3D scenes in a perceptually accurate manner. However it can require a substantial amount of storage since each splat's individual data must be stored. While compression techniques offer a potential solution by reducing the memory footprint they still necessitate retrieving the entire scene before any part of it can be rendered. In this work we introduce a novel approach for progressively rendering such scenes aiming to display visible content that closely approximates the final scene as early as possible without loading the entire scene into memory. This approach benefits both on-device rendering applications limited by memory constraints and streaming applications where minimal bandwidth usage is preferred. To achieve this we approximate the contribution of each Gaussian to the final scene and construct an order of prioritization on their inclusion in the rendering process. Additionally we demonstrate that our approach can be combined with existing compression methods to progressively render (and stream) 3DGS scenes optimizing bandwidth usage by focusing on the most important splats within a scene. Overall our work establishes a foundation for making remotely hosted 3DGS content more quickly accessible to end-users in over-the-top consumption scenarios with our results showing significant improvements in quality across all metrics compared to existing methods.	https://openaccess.thecvf.com//content/WACV2025/html/Zoomers_PRoGS_Progressive_Rendering_of_Gaussian_Splats_WACV_2025_paper.html	Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels
PTQ4VM: Post-Training Quantization for Visual Mamba	Visual Mamba is an approach that extends the selective space state model Mamba to vision tasks. It processes image tokens sequentially in a fixed order accumulating information to generate outputs. Despite its growing popularity for delivering high-quality outputs at a low computational cost across various tasks Visual Mamba is highly susceptible to quantization which makes further performance improvements challenging. Our analysis reveals that the fixed token access order in Visual Mamba introduces unique quantization challenges which we categorize into three main issues: 1) token-wise variance 2) channel-wise outliers and 3) a long tail of activations. To address these challenges we propose Post-Training Quantization for Visual Mamba (PTQ4VM) which introduces two key strategies: Per-Token Static (PTS) quantization and Joint Learning of Smoothing Scale and Step Size (JLSS). To the our best knowledge this is the first quantization study on Visual Mamba. PTQ4VM can be applied to various Visual Mamba backbones converting the pretrained model to a quantized format in under 15 minutes without notable quality degradation. Extensive experiments on large-scale classification and regression tasks demonstrate its effectiveness achieving up to 1.83X speedup on GPUs with negligible accuracy loss compared to FP16. Our code is available at https://github.com/YoungHyun197/ptq4vm.	https://openaccess.thecvf.com//content/WACV2025/html/Cho_PTQ4VM_Post-Training_Quantization_for_Visual_Mamba_WACV_2025_paper.html	Younghyun Cho, Changhun Lee, Seonggon Kim, Eunhyeok Park
PULSE: Physiological Understanding with Liquid Signal Extraction	The non-contact estimation of vital signs particularly heart rate from video data is a promising method for remote health monitoring. 3D convolutional layers are widely used for this task due to their ability to capture both spatial and temporal features. However traditional 3D convolutions while effective in many cases lack the capacity to adjust dynamically to the temporal variability inherent in physiological signals such as remote photoplethysmography (rPPG) which are characterized by subtle frequency changes over time. To address this we propose PULSE (Physiological Understanding with Liquid Signal Extraction) a framework that employs Liquid Time-Constant (LTC) models with 3D convolutional layers to enhance temporal sensitivity and improve the extraction of these fine-grained rPPG signals. In PULSE traditional 3D-conv layers are deployed for initial feature extraction while LTC-based 3D-conv layers dynamically adapt and guide the temporal processing allowing the model to better track and interpret the subtle variations in heart rate signals under different conditions such as motion artifacts and lighting changes. We evaluated the effectiveness of PULSE in an unsupervised training setting demonstrating that our solution performs well even in the absence of labeled datasets a common challenge in rPPG signal extraction. Experimental evaluations on three public datasets confirm that PULSE achieves comparable or superior results to existing methods proving its robustness and efficacy for real-world non-contact health monitoring applications.	https://openaccess.thecvf.com//content/WACV2025/html/Ahmad_PULSE_Physiological_Understanding_with_Liquid_Signal_Extraction_WACV_2025_paper.html	Shahzad Ahmad, Sania Bano, Sachin Verma, Yogesh Singh Rawat, Sukalpa Chanda, Santosh Kumar Vipparthi, Subrahmanyam Murala
PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection and Natural Language Interpretation	Video crime detection is a significant application of computer vision and artificial intelligence. However existing datasets primarily focus on detecting severe crimes by analyzing entire video clips often neglecting the precursor activities (i.e. privacy violations) that could potentially prevent these crimes. To address this limitation we present PV-VTT (Privacy Violation Video To Text) a unique multimodal dataset aimed at identifying privacy violations. PV-VTT provides detailed annotations for both video and text in scenarios. To ensure the privacy of individuals in the videos we only provide video feature vectors avoiding the release of any raw video data. This privacy-focused approach allows researchers to use the dataset while protecting participant confidentiality. Recognizing that privacy violations are often ambiguous and context-dependent we propose a Graph Neural Network (GNN)-based video description model. Our model generates a GNN-based prompt with an image for a Large Language Model (LLM) which delivers cost-effective and high-quality video descriptions. By leveraging a single video frame along with relevant text our method reduces the number of input tokens required maintaining descriptive quality while optimizing LLM API usage. Extensive experiments validate the effectiveness and interpretability of our approach in video description tasks and the flexibility of our PV-VTT dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Masukawa_PV-VTT_A_Privacy-Centric_Dataset_for_Mission-Specific_Anomaly_Detection_and_Natural_WACV_2025_paper.html	Ryozo Masukawa, Sanggeon Yun, Yoshiki Yamaguchi, Mohsen Imani
PVP: Polar Representation Boost for 3D Semantic Occupancy Prediction	Recently representations based on polar coordinates have exhibited promising characteristics for 3D perceptual tasks. In addition to Cartesian-based methods representing surrounding spaces through polar grids offers a compelling alternative in these tasks. This approach is advantageous for its ability to represent larger areas while preserving greater detail of nearby spaces. However polar-based methods are inherently challenged by the issue of feature distortion due to the non-uniform division inherent to polar representation. To harness the advantages of polar representation while addressing its challenges we propose Polar Voxel Occupancy Predictor (PVP) a novel 3D multi-modal occupancy predictor operating in polar coordinates. PVP mitigates the issues of feature distortion and misalignment across different modalities with the following two design elements: 1) Global Represent Propagation (GRP) module which incorporates global spatial information into the intermediate 3D volume taking into account the prior spatial structure. It then employs Global Decomposed Attention to accurately propagate features to their correct locations. 2) Plane Decomposed Convolution (PD-Conv) which simplifies 3D distortions in polar coordinates by replacing 3D convolution with a series of 2D convolutions. With these straightforward yet impactful modifications our PVP surpasses state-of-the-art works by significant margins--improving by 1.9% mIoU and 2.9% IoU over LiDAR-only methods and by 7.9% mIoU and 6.8% IoU over multimodal methods on the OpenOccupancy dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Xue_PVP_Polar_Representation_Boost_for_3D_Semantic_Occupancy_Prediction_WACV_2025_paper.html	Yujing Xue, Jiaxiang Liu, Jiawei Du, Joey Tianyi Zhou
PVT: An Implicit Surface Reconstruction Framework via Point Voxel Geometric-Aware Transformer	3D surface reconstruction from unorganized point clouds is a fundamental task in visual computing with numerous applications in areas such as robotics virtual reality augmented reality and animation. To date many deep learning-based surface reconstruction methods have been proposed demonstrating great performance on many benchmark datasets. Among these neural implicit field learning-based methods have gained popularity for their capability of representing complex structures in a continuous implicit distance field. Existing neural implicit field learning methods either utilize voxelized point cloud then feed them to a deep network or directly take points as input. In this paper we propose an implicit surface reconstruction framework based on point voxel geometric-aware transformer PVT to seamlessly integrate point-based convolution with voxel-based convolution using bidirectional transformers. Experiments show that the proposed PVT framework can better encode local geometry details and provide a significant performance boost over existing state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Fan_PVT_An_Implicit_Surface_Reconstruction_Framework_via_Point_Voxel_Geometric-Aware_WACV_2025_paper.html	Chuanmao Fan, Chenxi Zhao, Ye Duan
Paladin: Understanding Video Intentions in Political Advertisement Videos	"In this paper we introduce a novel task for video understanding that focuses on detecting editing intentions in political advertisement videos. Political advertisement videos are edited with some intentions (e.g. ""associating some candidates with negative emotions"") of making people unthinkingly believe the messages in the videos potentially ending up with some irrational bias. Detecting such intentions is thus the primary step toward fairer decision-making based on the messages themselves. To this end we classify such editing intentions into 10 categories (referred to as communication techniques) in consultation with a professional editor as well as based on communication techniques presented in the natural language processing community and build a dataset of 12526 political advertisement videos each of which is annotated with several communication technique segments. We also explore the capability of existing video understanding models in detecting editing intentions over the dataset which identifies new dimensions of challenges to be addressed."	https://openaccess.thecvf.com//content/WACV2025/html/Liu_Paladin_Understanding_Video_Intentions_in_Political_Advertisement_Videos_WACV_2025_paper.html	Hong Liu, Yuta Nakashima, Noboru Babaguchi
Partial Filter-Sharing: Improved Parameter-Sharing Method for Single Image Super-Resolution Networks	Numerous deep learning techniques have been developed for Single Image Super-Resolution (SISR) leading to significant performance improvements. However these techniques have also resulted in a substantial increase in parameter size. As a result there is a growing interest in reducing network complexity for more practical usage while still maintaining high SR quality. One such method is parameter-sharing which includes recursive recurrent and multi-scale learning approaches. However sharing identical kernels across layers or up-scaling tasks can reduce the network's representational capacity. To address this we propose Partial filter-Sharing (PS) a new parameter-sharing method that preserves the network's representational power more effectively than previous approaches. Instead of sharing a single filter PS shares segments of filters called partial filters across layers. This approach enables parameter-sharing layers to use diverse filters for each layer or task striking a balance between parameter efficiency and the network's representational ability without imposing excessive computational or parameter overhead. Furthermore the PS framework provides precise control over the network's performance and complexity by adjusting the quantity of partial filters. Extensive experiments demonstrate that our PS framework outperforms traditional parameter-sharing super-resolution (SR) methods without incurring excessive additional parameters or computational cost.	https://openaccess.thecvf.com//content/WACV2025/html/Park_Partial_Filter-Sharing_Improved_Parameter-Sharing_Method_for_Single_Image_Super-Resolution_Networks_WACV_2025_paper.html	Karam Park, Nam Ik Cho
Partial Texture VAE: Color and Texture Encoder for Rock Particle Images	We propose Partial Texture VAE (PT-VAE) for rock particle image analysis a variant of the variational autoencoder (VAE) specialized in encoding color and texture properties from arbitrarily sized and shaped unresized images containing invalid background pixels into the vectors with the same size. PT-VAE integrates partial convolution and the mask aware feature aggregation layer in the encoder and is trained by minimizing a simplified style loss in an unsupervised manner. Input texture images do not require resizing or cropping thus preventing deformation and under-representation of the overall texture pattern. This consideration is often overlooked in data-driven texture analysis. Experiments using our textured rock particle images show that the encoded features efficiently capture color and texture information and are invariant to size and shape of the image as well as invalid pixels. Image retrieval tests show that the PT-VAE incorporating the mask aware Texture Encoding Layer (TEL) outperformed other configurations and existing methods.	https://openaccess.thecvf.com//content/WACV2025/html/Yamada_Partial_Texture_VAE_Color_and_Texture_Encoder_for_Rock_Particle_WACV_2025_paper.html	Tetsushi Yamada, Simone Di Santo
Patch Ranking: Token Pruning as Ranking Prediction for Efficient CLIP	"Contrastive image-text pre-trained models such as CLIP have shown remarkable adaptability to downstream tasks. However they face challenges due to the high computational requirements of the Vision Transformer (ViT) backbone. Current strategies to boost ViT efficiency focus on pruning patch tokens but fall short in addressing the multimodal nature of CLIP and identifying the optimal subset of tokens for maximum performance. To address this we propose greedy search methods to establish a ""Golden Ranking"" and introduce a lightweight predictor specifically trained to approximate this Ranking. To compensate for any performance degradation resulting from token pruning we incorporate learnable visual tokens that aid in restoring and potentially enhancing the model's performance. Our work presents a comprehensive and systematic investigation of pruning tokens within the ViT backbone of CLIP models. Through our framework we successfully reduced 40% of patch tokens in CLIP's ViT while only suffering a minimal average accuracy loss of 0.3% across seven datasets. Our study lays the groundwork for building more computationally efficient multimodal models without sacrificing their performance addressing a key challenge in the application of advanced vision-language models."	https://openaccess.thecvf.com//content/WACV2025/html/Wu_Patch_Ranking_Token_Pruning_as_Ranking_Prediction_for_Efficient_CLIP_WACV_2025_paper.html	Cheng-En Wu, Jinhong Lin, Yu Hen Hu, Pedro Morgado
PatchFinder: Leveraging Visual Language Models for Accurate Information Retrieval using Model Uncertainty	For decades corporations and governments have relied on scanned documents to record vast amounts of information. However extracting this information is a slow and tedious process due to the sheer volume and complexity of these records. The rise of Vision Language Models (VLMs) presents a way to efficiently and accurately extract the information out of these documents. The current automated workflow often requires a two-step approach involving the extraction of information using optical character recognition software and subsequent usage of large language models for processing this information. Unfortunately these methods encounter significant challenges when dealing with noisy scanned documents often requiring computationally expensive language models to handle high information density effectively. In this study we propose PatchFinder an algorithm that builds upon VLMs to improve information extraction. First we devise a confidence-based score called Patch Confidence based on the Maximum Softmax Probability of the VLMs' output to measure the model's confidence in its predictions. Using this metric PatchFinder determines a suitable patch size partitions the input document into overlapping patches and generates confidence-based predictions for the target information. Our experimental results show that PatchFinder leveraging Phi-3v a 4.2 billion parameter VLM achieves an accuracy of 94% on our dataset of 190 noisy scanned documents outperforming ChatGPT-4o by 18.5 percentage points.	https://openaccess.thecvf.com//content/WACV2025/html/Colman_PatchFinder_Leveraging_Visual_Language_Models_for_Accurate_Information_Retrieval_using_WACV_2025_paper.html	Roman Colman, Minh Vu, Manish Bhattarai, Martin Ma, Hari Viswanathan, Daniel O'Malley, Javier Santos
Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation	Despite the significant progress in deep learning for dense visual recognition problems such as semantic segmentation traditional methods are constrained by fixed class sets. Meanwhile vision-language foundation models such as CLIP have showcased remarkable effectiveness in numerous zero-shot image-level tasks owing to their robust generalizability. Recently a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However existing approaches often rely on impractical supervised pre-training or access to additional pre-trained networks. In this work we propose a strong baseline for training-free OVSS termed Neighbour-Aware CLIP (NACLIP) representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the self-attention of CLIP's vision transformer which despite being crucial for dense prediction tasks has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation our approach significantly improves performance without requiring additional data auxiliary pre-trained networks or extensive hyperparameter tuning making it highly practical for real-world applications. Experiments are performed on 8 popular semantic segmentation benchmarks yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP.	https://openaccess.thecvf.com//content/WACV2025/html/Hajimiri_Pay_Attention_to_Your_Neighbours_Training-Free_Open-Vocabulary_Semantic_Segmentation_WACV_2025_paper.html	Sina Hajimiri, Ismail Ben Ayed, Jose Dolz
Per-Pixel Solution of Multispectral Photometric Stereo	Photometric Stereo (PS) estimates surface normals by analyzing images lit from different angles. Enhancing PS with spectral imaging known as multispectral photometric stereo (MPS) uses varying light source colors for simultaneous image capture. As in traditional PS obtaining a unique solution is challenging in MPS when the reflectance properties of the object are unknown. This paper presents an approach utilizing the spatial arrangement and color of light sources to solve the MPS problem in the condition of spatially varying reflectance from a minimum of seven spectral images without spatial smoothness constraints. A robust optimization technique is introduced to manage real data. Experiments on synthetic and real scenes validate the method's effectiveness including for non-Lambertian surfaces. The method can contribute to advanced digital archiving that simultaneously records surface normal and spectral reflectance.	https://openaccess.thecvf.com//content/WACV2025/html/Ishihara_Per-Pixel_Solution_of_Multispectral_Photometric_Stereo_WACV_2025_paper.html	Shin Ishihara, Imari Sato
Perceive Query & Reason: Enhancing Video QA with Question-Guided Temporal Queries	Video Question Answering (Video QA) is a challenging video understanding task that requires models to comprehend entire videos identify the most relevant information based on contextual cues from a given question and reason accurately to provide answers. Recent advancements in Multimodal Large Language Models (MLLMs) have transformed video QA by leveraging their exceptional commonsense reasoning capabilities. This progress is largely driven by the effective alignment between visual data and the language space of MLLMs. However for video QA an additional space-time alignment poses a considerable challenge for extracting question-relevant information across frames. In this work we investigate diverse temporal modeling techniques to integrate with MLLMs aiming to achieve question-guided temporal modeling that leverages pre-trained visual and textual alignment in MLLMs. We propose T-Former a novel temporal modeling method that creates a question-guided temporal bridge between frame-wise visual perception and the reasoning capabilities of LLMs. Our evaluation across multiple video QA benchmarks demonstrates that T-Former competes favorably with existing temporal modeling approaches and aligns with recent advancements in video QA.	https://openaccess.thecvf.com//content/WACV2025/html/Amoroso_Perceive_Query__Reason_Enhancing_Video_QA_with_Question-Guided_Temporal_WACV_2025_paper.html	Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp
Personalized Mixture of Experts for Multi-Site Medical Image Segmentation	The sharing of sensitive medical data among institutions presents a significant challenge due to strict privacy regulations the need for robust de-identification processes and the ethical imperative to protect patient confidentiality. Federated Learning (FL) addresses these challenges by enabling institutions to collaboratively train AI models on decentralized data thereby enhancing privacy and security without directly sharing sensitive patient information. However FL requires complex synchronization implementations has costly communication overheads and may fail to capture data heterogeneity across institutions. In this work we propose Personalized Mixture of Local Experts (P-MoLE) a Personalized Federated Learning (PFL) approach that effectively combines predictions from multiple locally trained models in a sample-specific manner. Leveraging both the individuality of each local model and variation across the ensemble P-MoLE learns the profile of each institution's local model and strategically weighs their prediction's contributions to the final segmentation. This approach harnesses the heterogeneity of each institution's unique data to increase the generalization capabilities across all institutions. By each institution sharing only the final models trained locally on the sensitive data no private patient data is exposed and the need for expensive communication infrastructure is removed. Results across two popular multi-institutional medical imaging datasets show P-MoLE achieves state-of-the-art performance without the extensive cooperative effort requirement of previous works. Additionally ablation study results show that P-MoLE is flexible to the number of local models in the ensemble increasing performance over the local models alone in each case.	https://openaccess.thecvf.com//content/WACV2025/html/Rahman_Personalized_Mixture_of_Experts_for_Multi-Site_Medical_Image_Segmentation_WACV_2025_paper.html	Md Motiur Rahman, Mohamed Trabelsi, Huseyin Uzunalioglu, Aidan Boyd
Phaseformer: Phase-Based Attention Mechanism for Underwater Image Restoration and Beyond	Quality degradation is observed in underwater images due to the effects of light refraction and absorption by water leading to issues like color cast haziness and limited visibility. This degradation negatively affects the performance of autonomous underwater vehicles used in marine applications. To address these challenges we propose a lightweight phase-based transformer network with 1.77M parameters for underwater image restoration (UIR). Our approach focuses on effectively extracting non-contaminated features using a phase-based self-attention mechanism. We also introduce an optimized phase attention block to restore structural information by propagating prominent attentive features from the input. We evaluate our method on both synthetic (UIEB UFO-120) and real-world (UIEB U45 UCCS SQUID) underwater image datasets. Additionally we demonstrate its effectiveness for low-light image enhancement using the LOL dataset. Through extensive ablation studies and comparative analysis it is clear that the proposed approach outperforms existing state-of-the-art (SOTA) methods. The testing code is included in the supplementary material and will be publicly released upon acceptance.	https://openaccess.thecvf.com//content/WACV2025/html/Khan_Phaseformer_Phase-Based_Attention_Mechanism_for_Underwater_Image_Restoration_and_Beyond_WACV_2025_paper.html	Raqib Khan, Anshul Negi, Ashutosh Kulkarni, Shruti S. Phutke, Santosh Kumar Vipparthi, Subrahmanyam Murala
Physiology-Aware PolySnake for Coronary Vessel Segmentation	Coronary artery disease (CAD) is a significant health risk that requires early detection for effective treatment. While recent advances in deep learning have shown promise in automating CAD detection from coronary computed tomography angiography (CCTA) images the accurate segmentation of coronary vessels remains a challenge particularly due to the imbalanced presence of plaque in unhealthy vessels. This paper introduces a physiology-aware approach to coronary vessel segmentation that addresses these challenges. Our proposed pipeline consists of three main components. First a hybrid UNeXt architecture is designed to segment artery boundaries and predict initial boundary contours by leveraging 3D spatial relations among adjacent slices. Second we introduce multi-class circular convolution for iterative contour deformation which generates well-connected contour pairs of the artery wall's inner and outer boundaries through iterative refinement. Finally we propose a focal smooth L1 loss function to handle the implicit class imbalance caused by plaque in unhealthy vessels and to enhance the robustness of the physiology-aware polysnake network by explicitly limiting the accuracy of initial contours. Extensive evaluations demonstrate that our methods significantly improve model performance achieving state-of-the-art results in coronary vessel segmentation.	https://openaccess.thecvf.com//content/WACV2025/html/Ruan_Physiology-Aware_PolySnake_for_Coronary_Vessel_Segmentation_WACV_2025_paper.html	Yizhe Ruan, Lin Gu, Yusuke Kurose, Junichi Iho, Youji Tokunaga, Makoto Horie, Yusaku Hayashi, Keisuke Nishizawa, Yasushi Koyama, Tatsuya Harada
PivotAlign: Improve Semi-Supervised Learning by Learning Intra-Class Heterogeneity and Aligning with Pivots	Self-supervised learning plays an important role in current state-of-the-art semi-supervised learning (SSL) methods. These methods learn inter-class heterogeneity among data and generate pseudo-labels based on class level representations. However they often neglect intra-class heterogeneity resulting in the under-exploitation of finer-grained semantic relationships within classes. To address this limitation we introduce PivotAlign a novel SSL approach that aims to 1) learn hierarchical representations to detect both inter-class and intra-class semantic relationships and 2) refine pseudo-labels based on learned representations with a class-debiasing strategy. Specifically we first learn a set of pivots as sub-prototypes of classes. We then train representations so that features align with the assigned pivot and are hierarchically grouped based on both inter-class and intra-class heterogeneity. This allows us to capture both inter-class and intra-class semantic relationships among data and leverage them to better assign and refine pseudo-labels. Additionally since SSL methods are prone to bias toward classes that are easier to learn we further re-balance class predictions to alleviate this class bias. We demonstrate the effectiveness of PivotAlign on various SSL benchmarks where PivotAlign achieves state-of-the-art performances. The source code will be released upon publication of the work.	https://openaccess.thecvf.com//content/WACV2025/html/Yi_PivotAlign_Improve_Semi-Supervised_Learning_by_Learning_Intra-Class_Heterogeneity_and_Aligning_WACV_2025_paper.html	Lingjie Yi, Tao Sun, Yikai Zhang, Songzhu Zheng, Weimin Lyu, Haibin Ling, Chao Chen
Pix2Poly: A Sequence Prediction Method for End-to-End Polygonal Building Footprint Extraction from Remote Sensing Imagery	Extraction of building footprint polygons from remotely sensed data is essential for several urban understanding tasks such as reconstruction navigation & mapping. Despite significant progress in the area extracting accurate polygonal vector building footprints remains an open problem. In this paper we introduce Pix2Poly an attention-based end-to-end trainable & differentiable deep neural network capable of directly generating explicit high-quality building footprints in a ring graph format. Pix2Poly employs a generative encoder-decoder transformer to produce a sequence of graph vertex tokens whose connectivity information is learned by an optimal matching network. Compared to previous graph learning methods ours is a truly end-to-end trainable approach that extracts high-quality building footprints & road networks without requiring complicated computationally intensive raster loss functions & intricate training pipelines. Upon evaluating Pix2Poly on several complex & challenging datasets we report that Pix2Poly outperforms state-of-the-art methods in several vector shape quality metrics while being an entirely explicit method. Our code is available at https://github.com/yeshwanth95/Pix2Poly.	https://openaccess.thecvf.com//content/WACV2025/html/Adimoolam_Pix2Poly_A_Sequence_Prediction_Method_for_End-to-End_Polygonal_Building_Footprint_WACV_2025_paper.html	Yeshwanth Kumar Adimoolam, Charalambos Poullis, Melinos Averkiou
PixSwap: High-Resolution Face Swapping for Effective Reflection of Identity via Pixel-Level Supervision with Synthetic Paired Dataset	Face swapping is to interchange identity features such as eyes nose and lips between a source and a target face while preserving the target attributes like expression pose skin color and hair. Despite considerable advancements in quality over the years recent studies conducting high-resolution face swapping still encounter challenges in reflecting the source identity and ensuring robust performance. We identify two primary reasons for these challenges: (1) the absence of pixel-level supervision and (2) limitations in the model architecture and pipeline. To address the first problem we construct a pseudo-ground truth paired dataset and provide essential pixel-level supervision for effective attribute and identity handling during face swapping. Models trained on the paired dataset significantly enhance the source identity reflection compared to those without the paired dataset. On the other hand existing StyleGAN-based approaches often underutilize source latent vectors or heavily rely on pre-trained models during inference resulting in incomplete identity representation or reduced robustness. Therefore we introduce a novel face-swapping model that is adept at leveraging spatial information of the target attributes and fully utilizing features of the source identity. Thanks to the effective paired dataset and network our model achieves state-of-the-art performance in both image and video-level face swapping notably improving the source identity reflection while preserving the target attributes. Extensive experiments validate the superior performance of our model over existing baselines.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_PixSwap_High-Resolution_Face_Swapping_for_Effective_Reflection_of_Identity_via_WACV_2025_paper.html	Taewoo Kim, Geonsu Lee, Hyukgi Lee, Seongtae Kim, Younggun Lee
Pixel-Wise Shuffling with Collaborative Sparsity for Melanoma Hyperspectral Image Classification	Hyperspectral imaging has emerged as a promising technology for medical image classification particularly in skin cancer diagnosis. However current methods face significant challenges in accurately and robustly classifying non-cancerous skin lesions especially when melanoma lesions overlap with pigmented regions. Existing methods also lack sensitivity to spectral variations and accumulate excess redundant data leading to inefficiencies misclassifications and overfitting while struggling to integrate spatial and spectral information effectively. To overcome these challenges we propose a novel method featuring collaborative sparse unmixing and an advanced pixel-wise shuffling approach with inter-similarity hybrid attention aiming to improve the accuracy of skin cancer diagnosis in real-world scenarios. Experiments are conducted on a publicly available histology-verified dataset to evaluate the efficacy of the proposed method. The experimental results demonstrate that the proposed method can accurately classify melanoma lesions even in cases where the lesions overlap with pigmented regions. The findings indicate that the proposed method outperforms state-of-the-art methods by obtaining an overall accuracy of 73.34% even when limited to 20% of the training data. The proposed approach has the potential to be a valuable tool for improving the diagnostic accuracy of skin cancer in clinical practice.	https://openaccess.thecvf.com//content/WACV2025/html/Ekong_Pixel-Wise_Shuffling_with_Collaborative_Sparsity_for_Melanoma_Hyperspectral_Image_Classification_WACV_2025_paper.html	Favour Ekong, Jun Zhou, Kwabena Sarpong, Yongsheng Gao
Planar Gaussian Splatting	This paper presents Planar Gaussian Splatting (PGS) a novel neural rendering approach to learn the 3D geometry and parse the 3D planes of a scene directly from multiple RGB images. The PGS leverages Gaussian primitives to model the scene and employ a hierarchical Gaussian mixture approach to group them. Similar Gaussians are progressively merged probabilistically in the tree-structured Gaussian mixtures to identify distinct 3D plane instances and form the overall 3D scene geometry. In order to enable the grouping the Gaussian primitives contain additional parameters such as plane descriptors derived by lifting 2D masks from a general 2D segmentation model and surface normals. Experiments show that the proposed PGS achieves state-of-the-art performance in 3D planar reconstruction without requiring either 3D plane labels or depth supervision. In contrast to existing supervised methods that have limited generalizability and struggle under domain shift PGS maintains its performance across datasets thanks to its neural rendering and scene specific optimization mechanism while also being significantly faster than existing optimization-based approaches.	https://openaccess.thecvf.com//content/WACV2025/html/Zanjani_Planar_Gaussian_Splatting_WACV_2025_paper.html	Farhad G. Zanjani, Hong Cai, Hanno Ackermann, Leila Mirvakhabova, Fatih Porikli
PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing	Modeling a human avatar that can plausibly deform to articulations is an active area of research. We present PocoLoco - the first template-free point-based pose-conditioned generative model for 3D humans in loose clothing. We motivate our work by noting that most methods require a parametric model of the human body to ground pose-dependent deformations. Consequently they are restricted to modeling clothing that is topologically similar to the naked body and do not extend well to loose clothing. The few methods that attempt to model loose clothing typically require either canonicalization or a UV-parameterization and need to address the challenging problem of explicitly estimating correspondences for the deforming clothes. In this work we formulate avatar clothing deformation as a conditional point-cloud generation task within the denoising diffusion framework. Crucially our framework operates directly on unordered point clouds eliminating the need for a parametric model or a clothing template. This also enables a variety of practical applications such as point-cloud completion and pose-based editing - important features for virtual human animation. As current datasets for human avatars in loose clothing are far too small for training diffusion models we release a dataset of two subjects performing various poses in loose clothing with a total of 75K point clouds. By contributing towards tackling the challenging task of effectively modeling loose clothing and expanding the available data for training these models we aim to set the stage for further innovation in digital humans. The source code is available at https: //github.com/sidsunny/pocoloco.	https://openaccess.thecvf.com//content/WACV2025/html/Seth_PocoLoco_A_Point_Cloud_Diffusion_Model_of_Human_Shape_in_WACV_2025_paper.html	Siddharth Seth, Rishabh Dabral, Diogo C Luvizon, Marc Habermann, Ming-Hsuan Yang, Christian Theobalt, Adam Kortylewski
Point Cloud Color Upsampling with Attention-Based Coarse Colorization and Refinement	Point cloud color upsampling is an important and less explored research topic. State-of-the-art methods colorize points based on the colors of neighboring points and geometric distances. However these methods often suffer from blurring and noise at color boundaries since object textures can have large color variations even between geometrically neighboring positions. In this paper we propose a point cloud color upsampling method with attention weights for neighboring points. The proposed method first performs coarse colorization with the colors of low-resolution points neighboring the high-resolution points and predicted weights. Then it refines the colors by predicting offsets for high-resolution points with aggregate features obtained from the low-resolution points. Both quantitative and qualitative experimental results on datasets acquired in real-world environments demonstrate that the proposed method achieves significantly superior color upsampling performance compared to state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Matsuzaki_Point_Cloud_Color_Upsampling_with_Attention-Based_Coarse_Colorization_and_Refinement_WACV_2025_paper.html	Kohei Matsuzaki, Keisuke Nonaka
Point-GN: A Non-Parametric Network using Gaussian Positional Encoding for Point Cloud Classification	This paper introduces Point-GN a novel non-parametric network for efficient and accurate 3D point cloud classification. Unlike conventional deep learning models that rely on a large number of trainable parameters Point-GN leverages non-learnable components-specifically Farthest Point Sampling (FPS) k-Nearest Neighbors (k-NN) and Gaussian Positional Encoding (GPE)-to extract both local and global geometric features. This design eliminates the need for additional training while maintaining high performance making Point-GN particularly suited for real-time resource-constrained applications. We evaluate Point-GN on two benchmark datasets ModelNet40 and ScanObjectNN achieving classification accuracies of 85.29% and 85.89% respectively while significantly reducing computational complexity. Point-GN outperforms existing non-parametric methods and matches the performance of fully trained models all with zero learnable parameters. Our results demonstrate that Point-GN is a promising solution for 3D point cloud classification in practical real-time environments.	https://openaccess.thecvf.com//content/WACV2025/html/Mohammadi_Point-GN_A_Non-Parametric_Network_using_Gaussian_Positional_Encoding_for_Point_WACV_2025_paper.html	Marzieh Mohammadi, Amir Salarpour
Point-JEPA: A Joint Embedding Predictive Architecture for Self-Supervised Learning on Point Cloud	Recent advancements in self-supervised learning in the point cloud domain have demonstrated significant potential. However these methods often suffer from drawbacks such as lengthy pre-training time the necessity of reconstruction in the input space and the necessity of additional modalities. In order to address these issues we introduce Point-JEPA a joint embedding predictive architecture designed specifically for point cloud data. To this end we introduce a sequencer that orders point cloud patch embeddings to efficiently compute and utilize their proximity based on their indices during target and context selection. The sequencer also allows shared computations of the patch embeddings' proximity between context and target selection further improving the efficiency. Experimentally our method demonstrates state-of-the-art performance while avoiding the reconstruction in the input space or additional modality. In particular Point-JEPA attains a classification accuracy of 93.7% for linear SVM on ModelNet40 surpassing all other self-supervised models. Moreover Point-JEPA also establishes new state-of-the-art performance levels across all four few-shot learning evaluation frameworks. The code is available at https://github.com/Ayumu-JS/Point-JEPA	https://openaccess.thecvf.com//content/WACV2025/html/Saito_Point-JEPA_A_Joint_Embedding_Predictive_Architecture_for_Self-Supervised_Learning_on_WACV_2025_paper.html	Ayumu Saito, Prachi Kudeshia, Jiju Poovvancheri
Polarization as Texture: Microscale 3D Shape from Polarized Light Focus	Defocus is a crucial cue for image-based microscale depth estimation yet its measurement depends on spatial appearance changes such as texture. We show that passively observed polarization is responsive to small irregularities of the surface visible in the microscopic world and can be leveraged for focus measure as a strong texture. Our key idea is to leverage texture from polarization for blur analysis and accurately estimate the focus level of microscopic polarization images. We further utilize normal cues from polarization to create a prior distribution of the focus level between neighboring pixels. We then interpolatively propagate the focus level of discrete image slices at different focus depths while denoising. We implement our method with a single polarization camera with a microscope and recover the per-pixel depth from the multi-focus images. The reconstructed results demonstrate the effectiveness of our method for various microscale objects regardless of the surface texture.	https://openaccess.thecvf.com//content/WACV2025/html/Matsumoto_Polarization_as_Texture_Microscale_3D_Shape_from_Polarized_Light_Focus_WACV_2025_paper.html	Ren Matsumoto, Takahiro Okabe, Ryo Kawahara
PoolAtnRes: Towards Generalisable Differential Morphing Attack Detection	Morphing attacks can successfully deceive face recognition systems resulting in unreliable access control especially in the border control scenario. Consequently the development of Morphing Attack Detection (MAD) algorithms is crucial for detecting morphing attacks based on either a single facial image (S-MAD) or two facial images (Differential-MAD or D-MAD). In this work we proposed a novel D-MAD approach PoolAtnRes to reliably detect morphing attacks. The proposed PoolAtnRes architecture is constructed using three main functional blocks namely convolution pooling Hybrid Attention and Residual blocks which are serially connected to detect morphing attacks. Extensive experiments were performed on the newly constructed morphing dataset using nine morphing-generation techniques. The detection performance of the proposed PoolAtnRes model was compared with three state-of-the-art (SOTA) D-MAD techniques with different performance evaluation protocols to benchmark its generalizability to unseen morphing generation. The results obtained indicated the best performance of the proposed PoolAtnRes D-MAD.	https://openaccess.thecvf.com//content/WACV2025/html/Ramachandra_PoolAtnRes_Towards_Generalisable_Differential_Morphing_Attack_Detection_WACV_2025_paper.html	Raghavendra Ramachandra, Sushma Krupa Venkatesh, Guoqiang Li
PositiveCoOp: Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations	Vision-language models (VLMs) like CLIP have been adapted for Multi-Label Recognition (MLR) with partial annotations by leveraging prompt-learning where positive and negative prompts are learned for each class to associate their embeddings with class presence or absence in the shared vision-text feature space. While this approach improves MLR performance by relying on VLM priors we hypothesize that learning negative prompts may be suboptimal as the datasets used to train VLMs lack image-caption pairs explicitly focusing on class absence. To analyze the impact of positive and negative prompt learning on MLR we introduce PositiveCoOp and NegativeCoOp where only one prompt is learned with VLM guidance while the other is replaced by an embedding vector learned directly in the shared feature space without relying on the text encoder. Through empirical analysis we observe that negative prompts degrade MLR performance and learning only positive prompts combined with learned negative embeddings (PositiveCoOp) outperforms dual prompt learning approaches. Moreover we quantify the performance benefits that prompt-learning offers over a simple vision-features-only baseline observing that the baseline displays strong performance comparable to dual prompt learning approach (DualCoOp) when the proportion of missing labels is low while requiring half the training compute and 16 times fewer parameters.	https://openaccess.thecvf.com//content/WACV2025/html/Rawlekar_PositiveCoOp_Rethinking_Prompting_Strategies_for_Multi-Label_Recognition_with_Partial_Annotations_WACV_2025_paper.html	Samyak Rawlekar, Shubhang Bhatnagar, Narendra Ahuja
PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery	With the recent advancements in single-image-based 3D human pose and shape estimation (3DHPSE) there is a growing amount of works that can achieve good results on standard benchmarks but struggle to yield accurate human mesh in extreme scenarios like occlusion. Previous works propose to leverage 2D poses to help 3D HPSE model improve performance under occlusion but usually rely on manual design to integrate 2D poses and only aim for specific kinds of occlusion. In this paper we present PostoMETRO (POSe TOken enhanced MEsh TRansfOrmer) which integrates 2D pose prior knowledge as tokens into transformers to improve model's performance under occlusion. Using a VQ-VAE-based pose tokenizer we efficiently represent 2D poses as tokens and feed them to transformers together with image tokens. Subsequently these tokens are queried by vertex and joint tokens to decode 3D coordinates of mesh vertices and human joints. Our proposed 2D poses integration strategy is manual-design-free and suitable for various kinds of occlusion. Experiments on both standard and occlusion-specific benchmarks demonstrate the effectiveness of PostoMETRO.	https://openaccess.thecvf.com//content/WACV2025/html/Yang_PostoMETRO_Pose_Token_Enhanced_Mesh_Transformer_for_Robust_3D_Human_WACV_2025_paper.html	Wendi Yang, Zi-Hang Jiang, Shang Zhao, S. Kevin Zhou
Pre-Capture Privacy via Adaptive Single-Pixel Imaging	As cameras become ubiquitous in our living environment invasion of privacy is becoming a significant concern. A common approach to privacy preservation is to remove personally identifiable information from a captured image but there is a risk of the original image being leaked. In this paper we propose a pre-capture privacy-aware imaging method that captures images from which the details of a pre-specified anonymized target have been eliminated. The proposed method applies a single-pixel imaging framework in which we introduce a feedback mechanism called an aperture pattern generator (APG). The introduced APG adaptively outputs the next aperture pattern to avoid sampling the anonymized target by using already acquired data as a clue. Furthermore the anonymized target can be set to any object without changing hardware. Except for the removed detailed features of the anonymized target the captured images are of comparable quality to those captured by a general camera and can be used for various computer vision applications. We target faces and license plates and experimentally show that the proposed method can capture clear images in which detailed features of the anonymized target are eliminated achieving both privacy and utility.	https://openaccess.thecvf.com//content/WACV2025/html/Sogabe_Pre-Capture_Privacy_via_Adaptive_Single-Pixel_Imaging_WACV_2025_paper.html	Yoko Sogabe, Shiori Sugimoto, Ayumi Matsumoto, Masaki Kitahara
Pre-Trained Multiple Latent Variable Generative Models are Good Defenders Against Adversarial Attacks	Attackers can deliberately perturb classifiers' input with subtle noise altering final predictions. Among proposed countermeasures adversarial purification employs generative networks to preprocess input images filtering out adversarial noise. In this study we propose specific generators defined Multiple Latent Variable Generative Models (MLVGMs) for adversarial purification. These models possess multiple latent variables that naturally disentangle coarse from fine features. Taking advantage of these properties we autoencode images to maintain class-relevant information while discarding and re-sampling any detail including adversarial noise. The procedure is completely training-free exploring the generalization abilities of pre-trained MLVGMs on the adversarial purification downstream task. Despite the lack of large models trained on billions of samples we show that smaller MLVGMs are already competitive with traditional methods and can be used as foundation models. Official code released at https://github.com/SerezD/gen_adversarial.	https://openaccess.thecvf.com//content/WACV2025/html/Serez_Pre-Trained_Multiple_Latent_Variable_Generative_Models_are_Good_Defenders_Against_WACV_2025_paper.html	Dario Serez, Marco Cristani, Alessio Del Bue, Vittorio Murino, Pietro Morerio
Precise Integral in NeRFs: Overcoming the Approximation Errors of Numerical Quadrature	Neural Radiance Fields (NeRFs) use neural networks to translate spatial coordinates to corresponding volume density and directional radiance enabling realistic novel view synthesis through volume rendering. Rendering new viewpoints involves computing volume rendering integrals along rays usually approximated by numerical quadrature because of lacking closed-form solutions. In this paper utilizing Taylor expansion we demonstrate that numerical quadrature causes inevitable approximation error in NeRF integrals due to ignoring the parameter associated with the Lagrange remainder. To mitigate the approximation error we propose a novel neural field with segment representation as input to implicitly model the remainder parameter. In theory our proposed method is proven to possess the potential to achieve fully precise rendering integral as demonstrated by comprehensive experiments on several commonly used datasets with state-of-the-art results.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_Precise_Integral_in_NeRFs_Overcoming_the_Approximation_Errors_of_Numerical_WACV_2025_paper.html	Boyuan Zhang, Zhenliang He, Meina Kan, Shiguang Shan
Predicting Event Memorability using Personalized Federated Learning	Lifelog images are very useful as memory cues for recalling past events. Estimating the level of event memory recall induced by a given lifelog image (event memorability) is useful for selecting images for cognitive interventions. Previous works for predicting event memorability follow a centralized model training paradigm that requires several users to share their lifelog images. This risks violating the privacy of individual lifeloggers. Alternatively a personal model trained with a lifelogger's own data guarantees privacy. However it imposes significant effort on the lifelogger to provide a large enough sample of self-rated images to develop a well-performing model for event memorability. Therefore we propose a clustered personalized federated learning setup FedMEM that avoids sharing raw images but still enables collaborative learning via model sharing. For an enhanced learning performance in the presence of data heterogeneity FedMEM evaluates similarity among users to group them into clusters. We demonstrate that our approach furnishes high-performing personalized models compared to the state-of-the-art.	https://openaccess.thecvf.com//content/WACV2025/html/Banerjee_Predicting_Event_Memorability_using_Personalized_Federated_Learning_WACV_2025_paper.html	Sourasekhar Banerjee, Debaditya Roy, Vigneshwaran Subbaraju, Monowar Bhuyan
PrevPredMap: Exploring Temporal Modeling with Previous Predictions for Online Vectorized HD Map Construction	Temporal information is crucial for detecting occluded instances. Existing temporal representations have progressed from BEV or PV (perspective view) features to more compact query features. Compared to these aforementioned features predictions offer the highest level of abstraction providing explicit information. In the context of online vectorized HD map construction this unique characteristic of predictions is potentially advantageous for long-term temporal modeling and the integration of map priors. This paper introduces PrevPredMap a pioneering temporal modeling framework that leverages previous predictions for constructing online vectorized HD maps. We have meticulously crafted two essential modules for PrevPredMap: the previous-predictions-based query generator and the dynamic-position-query decoder. Specifically the previous-predictions-based query generator is designed to separately encode different types of information from previous predictions which are then effectively utilized by the dynamic-position-query decoder to generate current predictions. Furthermore we have developed a dual-mode strategy to ensure PrevPredMap's robust performance across both single-frame and temporal modes. Extensive experiments demonstrate that PrevPredMap achieves state-of-the-art performance on the nuScenes and Argoverse2 datasets. Code is available at https://github.com/pnnnnnnn/PrevPredMap.	https://openaccess.thecvf.com//content/WACV2025/html/Peng_PrevPredMap_Exploring_Temporal_Modeling_with_Previous_Predictions_for_Online_Vectorized_WACV_2025_paper.html	Nan Peng, Xun Zhou, Mingming Wang, Xiaojun Yang, Songming Chen, Guisong Chen
Prior2Posterior: Model Prior Correction for Long-Tailed Learning	Learning-based solutions for long-tailed recognition face difficulties in generalizing on balanced test datasets. Due to imbalanced data prior the learned a posteriori distribution is biased toward the most frequent (head) classes leading to an inferior performance on the least frequent (tail) classes. In general the performance can be improved by removing such a bias by eliminating the effect of imbalanced prior modeled using the number of class samples (frequencies). We first observe that the effective prior on the classes learned by the model at the end of the training can differ from the empirical prior obtained using class frequencies. Thus we propose a novel approach to accurately model the effective prior of a trained model using a posteriori probabilities. We propose to correct the imbalanced prior by adjusting the predicted a posteriori probabilities (Prior2Posterior: P2P) using the calculated prior in a post-hoc manner after the training and show that it can result in improved model performance. We present theoretical analysis showing the optimality of our approach for models trained with naive cross-entropy loss as well as logit adjusted loss. Our experiments show that the proposed approach achieves new state-of-the-art (SOTA) on several benchmark datasets from the long-tail literature in the category of logit adjustment methods. Further the proposed approach can be used to inspect any existing method to capture the effective prior and remove any residual bias to improve its performance post-hoc without model retraining. We also show that by using the proposed post-hoc approach the performance of many existing methods can be improved further.	https://openaccess.thecvf.com//content/WACV2025/html/Bhat_Prior2Posterior_Model_Prior_Correction_for_Long-Tailed_Learning_WACV_2025_paper.html	S Divakar Bhat, Amit More, Mudit Soni, Surbhi Agrawal
PrivateEye: In-Sensor Privacy Preservation Through Optical Feature Separation	We address privacy issues in applications where images captured by an edge device (camera) are sent to the cloud for inference on utility tasks such as classification. Sending raw images to the cloud exposes them to data sniffing attacks and misuse by untrusted third-party service providers beyond the user's intended tasks. We propose an encoding scheme that not only evades direct visual inspection to the images or image reconstruction but also prevents sensitive information from being ascertained. Unlike commonly used adversarial learning approaches the proposed method is two-fold: first it uses a diffractive optical neural network to spatially separate features corresponding to different tasks on the sensor plane in the optical domain. Then only the pixels corresponding to the utility task region are read. This encoding ensures that private features are never digitally stored on the edge device thereby preventing privacy leakage. The proposed method successfully reduces the privacy retrieval in binary tasks with minimal accuracy loss ( 2%) of the utility task while reducing private task accuracy by 35% and defending against reconstruction attacks with SSIM score of 0.43.	https://openaccess.thecvf.com//content/WACV2025/html/Boloor_PrivateEye_In-Sensor_Privacy_Preservation_Through_Optical_Feature_Separation_WACV_2025_paper.html	Adith Boloor, Weikai Lin, Tianrui Ma, Yu Feng, Yuhao Zhu, Xuan Zhang
Pruning One More Token is Enough: Leveraging Latency-Workload Non-Linearities for Vision Transformers on the Edge	This paper investigates how to efficiently deploy vision transformers on edge devices for small workloads. Recent methods reduce the latency of transformer neural networks by removing or merging tokens with small accuracy degradation. However these methods are not designed with edge device deployment in mind: they do not leverage information about the latency-workload trends to improve efficiency. We address this shortcoming in our work. First we identify factors that affect ViT latency-workload relationships. Second we determine token pruning schedule by leveraging non-linear latency-workload relationships. Third we demonstrate a training-free token pruning method utilizing this schedule. We show other methods may increase latency by 2-30% while we reduce latency by 9-26%. For similar latency (within 5.2% or 7ms) across devices we achieve 78.6%-84.5% ImageNet1K classification accuracy while the state-of-the-art Token Merging achieves 45.8%-85.4%.	https://openaccess.thecvf.com//content/WACV2025/html/Eliopoulos_Pruning_One_More_Token_is_Enough_Leveraging_Latency-Workload_Non-Linearities_for_WACV_2025_paper.html	Nicholas John Eliopoulos, Purvish Jajal, James C. Davis, Gaowen Liu, George K. Thiruvathukal, Yung-Hsiang Lu
Psych-Occlusion: using Visual Psychophysics for Aerial Detection of Occluded Persons during Search and Rescue	"The success of Emergency Response (ER) scenarios such as search and rescue is often dependent upon the prompt location of a lost or injured person. With the increasing use of small Unmanned Aerial Systems (sUAS) as ""eyes in the sky"" during ER scenarios efficient detection of persons from aerial views plays a crucial role in achieving a successful mission outcome. Fatigue of human operators during prolonged ER missions coupled with limited human resources highlights the need for sUAS equipped with Computer Vision (CV) capabilities to aid in finding the person from aerial views. However the performance of CV models onboard sUAS substantially degrades under real-life rigorous conditions of a typical ER scenario where person search is hampered by occlusion and low target resolution. To address these challenges we extracted images from the NOMAD dataset and performed a crowdsource experiment to collect behavioural measurements when humans were asked to ""find the person in the picture"". We exemplify the use of our behavioral dataset Psych-ER by using its human accuracy data to adapt the loss function of a detection model. We tested our loss adaptation on a RetinaNet model evaluated on NOMAD against increasing distance and occlusion with our psychophysical loss adaptation showing improvements over the baseline at higher distances across different levels of occlusion without degrading performance at closer distances. To the best of our knowledge our work is the first human-guided approach to address the location task of a detection model while addressing real-world challenges of aerial search and rescue. All datasets and code can be found at: https://github.com/ArtRuss/NOMAD."	https://openaccess.thecvf.com//content/WACV2025/html/Bernal_Psych-Occlusion_using_Visual_Psychophysics_for_Aerial_Detection_of_Occluded_Persons_WACV_2025_paper.html	Arturo Miguel Russell Bernal, Jane Cleland-Huang, Walter Scheirer
PureForest: A Large-Scale Aerial Lidar and Aerial Imagery Dataset for Tree Species Classification in Monospecific Forests	Sustainable forest management is a cornerstone of climate and environmental action. Responsible management relies on forest models such as biomass or fire vulnerability estimates which depend on mapping the spatial distribution of tree species. Because forest mapping relies heavily on visual identification it is a time-consuming endeavor that would benefit from more automation. To develop scalable automated methods researchers need suitable benchmarks but unfortunately tree species classification datasets are scarce. Most of them contain only imagery data which capture signal only at the canopy level and ignore the three-dimensional structure of the forest making it very difficult to distinguish between different species. Lidar which penetrates the canopy and captures the geometry of trees provides a rich signal for distinguishing tree species. As an increasingly available and cost-effective remote sensing modality it could become the new standard for large-scale tree species mapping. However current Lidar benchmarks for tree species classification are extremely limited and all insufficient in size and diversity; with less than 1500 annotated trees from at most a dozen different sites they fall short from the requirements of deep learning development. To fill this data gap we release PureForest: a large-scale open multimodal dataset designed for tree species classification from both Aerial Lidar Scanning point clouds and Very High Resolution aerial images. PureForest covers 339 km2 in 449 different monospecific forests with verified labels for 18 tree species grouped into 13 semantic classes. It is the largest and most comprehensive Lidar dataset for tree species identification. By making PureForest publicly available we hope to provide a challenging benchmark to support the development of deep learning approaches for tree species identification from Lidar and aerial imagery. In this data paper we describe the annotation workflow and the dataset and we establish a baseline performance from both 2D and 3D modalities.	https://openaccess.thecvf.com//content/WACV2025/html/Gaydon_PureForest_A_Large-Scale_Aerial_Lidar_and_Aerial_Imagery_Dataset_for_WACV_2025_paper.html	Charles Gaydon, Floryne Roche
Q-TempFusion: Quantization-Aware Temporal Multi-Sensor Fusion on Bird's-Eye View Representation	Recent advancements in bird's-eye view (BEV) perception models have highlighted the superior performance of LiDAR-camera fusion systems over single-modality approaches garnering considerable interest in the field. Despite the progress the integration of temporal information a technique that has considerably benefitted camera-only BEV models remains underexplored for LiDAR-camera fusion. This paper presents Q-TempFusion a novel approach for temporal multi-sensor fusion designed to enhance the BEV model's inference speed while keeping high predictive performance compared with the current state-of-the-art. Moreover we are the first to make the multi-modality BEV model profiling on hardware devices. To address the challenges of substantial memory demands and non-trivial latency that hinder deployment in on-vehicle systems particularly when temporal dynamics are incorporated into complex multi-sensor models we introduce an activation-aware quantization framework to generate the fully 8-bit quantized Q-TempFusion model based on the profiling result which can be directly deployed to target devices with negligible detection performance degradation. Our experiments show that our Q-TempFusion (8-bit) achieves 70.3% mAP and 72.7% NDS with 3x - 18x FPS improvement over leading multi-modality baselines and the Q-TempFusion (32-bit) achieves 72.1% mAP and 74.8% NDS comparable to SOTA multi-modality approaches. The results suggest that Q-TempFusion is a promising step toward real-time multi-sensor BEV applications setting a new benchmark for efficient and reliable perception.	https://openaccess.thecvf.com//content/WACV2025/html/Yu_Q-TempFusion_Quantization-Aware_Temporal_Multi-Sensor_Fusion_on_Birds-Eye_View_Representation_WACV_2025_paper.html	Pinrui Yu, Zhenglun Kong, Pu Zhao, Peiyan Dong, Hao Tang, Fei Sun, Xue Lin, Yanzhi Wang
QuantAttack: Exploiting Quantization Techniques to Attack Vision Transformers	In recent years there has been a significant trend in deep neural networks (DNNs) particularly transformer-based models of developing ever-larger and more capable models. While they demonstrate state-of-the-art performance their growing scale requires increased computational resources (e.g. GPUs with greater memory capacity). To address this problem quantization techniques (i.e. low-bit-precision representation and matrix multiplication) have been proposed. Most quantization techniques employ a static strategy in which the model parameters are quantized either during training or inference without considering the test-time sample. In contrast dynamic quantization techniques which have become increasingly popular adapt during inference based on the input provided while maintaining full-precision performance. However their dynamic behavior and average-case performance assumption makes them vulnerable to a novel threat vector - adversarial attacks that target the model's efficiency and availability. In this paper we present QuantAttack a novel attack that targets the availability of quantized vision transformers slowing down the inference and increasing memory usage and energy consumption. The source code is available online.	https://openaccess.thecvf.com//content/WACV2025/html/Baras_QuantAttack_Exploiting_Quantization_Techniques_to_Attack_Vision_Transformers_WACV_2025_paper.html	Amit Baras, Alon Zolfi, Yuval Elovici, Asaf Shabtai
RAW-Diffusion: RGB-Guided Diffusion Models for High-Fidelity RAW Image Generation	Current deep learning approaches in computer vision primarily focus on RGB data sacrificing information. In contrast RAW images offer richer representation which is crucial for precise recognition particularly in challenging conditions like low-light environments. The resultant demand for comprehensive RAW image datasets contrasts with the labor-intensive process of creating specific datasets for individual sensors. To address this we propose a novel diffusion-based method for generating RAW images guided by RGB images. Our approach integrates an RGB-guidance module for feature extraction from RGB inputs then incorporates these features into the reverse diffusion process with RGB-guided residual blocks across various resolutions. This approach yields high-fidelity RAW images enabling the creation of camera-specific RAW datasets. Our RGB2RAW experiments on four DSLR datasets demonstrate state-of-the-art performance. Moreover RAW-Diffusion demonstrates exceptional data efficiency achieving remarkable performance with as few as 25 training samples or even fewer. We extend our method to create BDD100K-RAW and Cityscapes-RAW datasets revealing its effectiveness for object detection in RAW imagery significantly reducing the amount of required RAW images. The code is available at https:// github.com/SonyResearch/RAW-Diffusion.	https://openaccess.thecvf.com//content/WACV2025/html/Reinders_RAW-Diffusion_RGB-Guided_Diffusion_Models_for_High-Fidelity_RAW_Image_Generation_WACV_2025_paper.html	Christoph Reinders, Radu Berdan, Beril Besbinar, Junji Otsuka, Daisuke Iso
RD-DPP: Rate-Distortion Theory Meets Determinantal Point Process to Diversify Learning Data Samples	Selecting representative samples plays an indispensable role in many machine learning and computer vision applications under limited resources (e.g. limited communication bandwidth and computational power). Determinantal Point Process (DPP) is a widely used method for selecting the most diverse representative samples that can summarize a dataset. However its adaptability to different tasks remains an open challenge as it is challenging for DPP to perform task-specific tuning. In contrast Rate-Distortion (RD) theory provides a way to measure task-specific diversity. However optimizing RD for a data selection problem remains challenging because the quantity that needs to be optimized is the index set of the selected samples. To tackle these challenges we first draw an inherent relationship between DPP and RD theory. Our theoretical derivation paves the way to take advantage of both RD and DPP for a task-specific data selection. To this end we propose a novel method for task-specific data selection for multi-level classification tasks named RD-DPP. Empirical studies on seven different datasets using five benchmark models demonstrate the effectiveness of the proposed RD-DPP method. Our method also outperforms recent strong competing methods while exhibiting high generalizability to a variety of learning tasks. The source code is available on https://github.com/xiwenc1/RD-DPP	https://openaccess.thecvf.com//content/WACV2025/html/Chen_RD-DPP_Rate-Distortion_Theory_Meets_Determinantal_Point_Process_to_Diversify_Learning_WACV_2025_paper.html	Xiwen Chen, Huayu Li, Peijie Qiu, Wenhui Zhu, Rahul Amin, Abolfazl Razi
RGB-D Video Mirror Detection	Mirror detection aims to identify mirror areas in a scene with recent methods either integrating depth information (RGB-D) or making use of temporal information (video). However utilizing both data is still under-explored due to the lack of a high-quality dataset and an effective method for the RGB-D Video Mirror Detection (DVMD) problem. To the best of our knowledge this is the first work to address the DVMD problem. To exploit depth and temporal information in mirror segmentation we first construct a large-scale RGB-D Video Mirror Detection Dataset (DVMD-D) which contains 17977 RGB-D images from 273 diverse videos. We further develop a novel model named DVMDNet which can first locate the mirrors based on triple consistencies: local consistency cross-modality consistency and global consistency and then refine the mirror boundaries through content discontinuity taking the temporal information within videos into account. We conduct a comparative study on the DVMD dataset evaluating 12 state-of-the art models (including single-image mirror detection single-image glass detection RGB-D mirror detection video shadow detection video glass detection and video mirror detection methods). Code is available from https://github.com/UpChen/2025_DVMDNet.	https://openaccess.thecvf.com//content/WACV2025/html/Xu_RGB-D_Video_Mirror_Detection_WACV_2025_paper.html	Mingchen Xu, Peter Herbert, Yu-Kun Lai, Ze Ji, Jing Wu
RGB2Point: 3D Point Cloud Generation from Single RGB Images	We introduce RGB2Point an unposed single-view RGB image to a 3D point cloud generation based on Transformer. RGB2Point takes an input image of an object and generates a dense 3D point cloud. Contrary to prior works based on CNN layers and diffusion-denoising approaches we use pre-trained Transformer layers that are fast and generate high-quality point clouds with consistent quality over available categories. Our generated point clouds demonstrate high quality on a real-world dataset as evidenced by improved Chamfer distance (51.15%) and Earth Mover's distance (36.17%) metrics compared to the current state-of-the-art. Additionally our approach shows a better quality on a synthetic dataset achieving better Chamfer distance (39.26%) Earth Mover's distance (26.95%) and F-score (47.16%). Moreover our method produces 63.1% more consistent high-quality results across various object categories compared to prior works. Furthermore RGB2Point is computationally efficient requiring only 2.3GB of VRAM to reconstruct a 3D point cloud from a single RGB image and our implementation generates the results 15133x faster than a SOTA diffusion-based model.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_RGB2Point_3D_Point_Cloud_Generation_from_Single_RGB_Images_WACV_2025_paper.html	Jae Joong Lee, Bedrich Benes
ROADS: Robust Prompt-Driven Multi-Class Anomaly Detection under Domain Shift	Recent advancements in anomaly detection have shifted focus towards Multi-class Unified Anomaly Detection (MUAD) offering more scalable and practical alternatives compared to traditional one-class-one-model approaches. However existing MUAD methods often suffer from inter-class interference and are highly susceptible to domain shifts leading to substantial performance degradation in real-world applications. In this paper we propose a novel robust prompt-driven MUAD framework called ROADS to address these challenges. ROADS employs a hierarchical class-aware prompt integration mechanism that dynamically encodes class-specific information into our anomaly detector to mitigate interference among anomaly classes. Additionally ROADS incorporates a domain adapter to enhance robustness against domain shifts by learning domain-invariant representations. Extensive experiments on MVTec-AD and VISA datasets demonstrate that ROADS surpasses state-of-the-art methods in both anomaly detection and localization with notable improvements in out-of-distribution settings.	https://openaccess.thecvf.com//content/WACV2025/html/Kashiani_ROADS_Robust_Prompt-Driven_Multi-Class_Anomaly_Detection_under_Domain_Shift_WACV_2025_paper.html	Hossein Kashiani, Niloufar Alipour Talemi, Fatemeh Afghah
ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive Detail Transfer	Reconstructing an object's shape and appearance in terms of a mesh textured by a spatially-varying bidirectional reflectance distribution function (SVBRDF) from a limited set of images captured under collocated light is an ill-posed problem. Previous state-of-the-art approaches either aim to reconstruct the appearance directly on the geometry or additionally use texture normals as part of the appearance features. However this requires detailed but inefficiently large meshes that would have to be simplified in a post-processing step or suffers from well-known limitations of normal maps such as missing shadows or incorrect silhouettes. Another limiting factor is the fixed and typically low resolution of the texture estimation resulting in loss of important surface details. To overcome these problems we present ROSA an inverse rendering method that directly optimizes mesh geometry with spatially adaptive mesh resolution solely based on the image data. In particular we refine the mesh and locally condition the surface smoothness based on the estimated normal texture and mesh curvature. In addition we enable the reconstruction of fine appearance details in high-resolution textures through a pioneering tile-based method that operates on a single pre-trained decoder network but is not limited by the network output resolution.	https://openaccess.thecvf.com//content/WACV2025/html/Kaltheuner_ROSA_Reconstructing_Object_Shape_and_Appearance_Textures_by_Adaptive_Detail_WACV_2025_paper.html	Julian Kaltheuner, Patrick Stotko, Reinhard Klein
RT-DETRv3: Real-Time End-to-End Object Detection with Hierarchical Dense Positive Supervision	RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However compared to dense supervision detectors like the YOLO series the Hungarian matching provides much sparser supervision leading to insufficient model training and difficult to achieve optimal results. To address these issues we proposed a hierarchical dense positive supervision method based on RT-DETR named RT-DETRv3. Firstly we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder's feature representation. Secondly to address insufficient decoder training we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups thereby enriching positive supervisions. Additionally we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors including the RT-DETR series and the YOLO series. For example RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18 while maintaining the same latency. Furthermore RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. The code will be released at https://github.com/clxia12/RT-DETRv3.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_RT-DETRv3_Real-Time_End-to-End_Object_Detection_with_Hierarchical_Dense_Positive_Supervision_WACV_2025_paper.html	Shuo Wang, Chunlong Xia, Feng Lv, Yifeng Shi
Radiance Field-Based Pose Estimation via Decoupled Optimization Under Challenging Initial Conditions	Estimating six-degree-of-freedom poses is essential but difficult especially under challenging initial conditions as existing methods relying on photometric loss often fail catastrophically. To address this we propose a novel radiance field-based pose estimation framework that combines Monte Carlo initialization and decoupled optimization. First we design an initialization stage based on Monte Carlo sampling which screens the current optimal initial pose by generating multiple pose hypotheses within the known scene space. Subsequently our decoupled optimization strategy independently refines the rotational and translational components significantly improving the motion stability of the camera during the convergence process. Comprehensive evaluations on synthetic and real-world datasets demonstrate that our method outperforms existing approaches in terms of pose regression accuracy and robustness even under challenging initial pose.	https://openaccess.thecvf.com//content/WACV2025/html/Lu_Radiance_Field-Based_Pose_Estimation_via_Decoupled_Optimization_Under_Challenging_Initial_WACV_2025_paper.html	Si-Yu Lu, Yung-Yao Chen, Yi-Tong Wu, Hsin-Chun Lin, Sin-Ye Jhong, Wen-Huang Cheng
RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone	Vision transformers (ViTs) have dominated computer vision in recent years. However ViTs are computationally expensive and not well suited for mobile devices; this led to the prevalence of convolutional neural network (CNN) and ViT-based hybrid models for mobile vision applications. Recently Vision GNN (ViG) and CNN hybrid models have also been proposed for mobile vision tasks. However all of these methods remain slower compared to pure CNN-based models. In this work we propose Multi-Level Dilated Convolutions to devise a purely CNN-based mobile backbone. Using Multi-Level Dilated Convolutions allows for a larger theoretical receptive field than standard convolutions. Different levels of dilation also allow for interactions between the short-range and long-range features in an image. Experiments show that our proposed model outperforms state-of-the-art (SOTA) mobile CNN ViT ViG and hybrid architectures in terms of accuracy and/or speed on image classification object detection instance segmentation and semantic segmentation. Our fastest model RapidNet-Ti achieves 76.3% top-1 accuracy on ImageNet-1K with 0.9 ms inference latency on an iPhone 13 mini NPU which is faster and more accurate than MobileNetV2x1.4 (74.7% top-1 with 1.0 ms latency). Our work shows that pure CNN architectures can beat SOTA hybrid and ViT models in terms of accuracy and speed when designed properly.	https://openaccess.thecvf.com//content/WACV2025/html/Munir_RapidNet_Multi-Level_Dilated_Convolution_Based_Mobile_Backbone_WACV_2025_paper.html	Mustafa Munir, Md Mostafijur Rahman, Radu Marculescu
RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel View Synthesis	Differentiable volumetric rendering-based methods made significant progress in novel view synthesis. On one hand innovative methods have replaced the Neural Radiance Fields (NeRF) network with locally parameterized structures enabling high-quality renderings in a reasonable time. On the other hand approaches have used differentiable splatting instead of NeRF's ray casting to optimize radiance fields rapidly using Gaussian kernels allowing for fine adaptation to the scene. However differentiable ray casting of irregularly spaced kernels has been scarcely explored while splatting despite enabling fast rendering times is susceptible to clearly visible artifacts. Our work closes this gap by providing a physically consistent formulation of the emitted radiance and density decomposed with Gaussian functions associated with Spherical Gaussians/Harmonics for all-frequency colorimetric representation. We also introduce a method enabling differentiable ray casting of irregularly distributed Gaussians using an algorithm that integrates radiance fields slab by slab and leverages a BVH structure. This allows our approach to finely adapt to the scene while avoiding splatting artifacts. As a result we achieve superior rendering quality compared to the state-of-the-art while maintaining reasonable training times and achieving inference speeds of 25 FPS on the Blender dataset. The associated code is available at: github.com/hugobl1/ray gauss.	https://openaccess.thecvf.com//content/WACV2025/html/Blanc_RayGauss_Volumetric_Gaussian-Based_Ray_Casting_for_Photorealistic_Novel_View_Synthesis_WACV_2025_paper.html	Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic
Re-Evaluating Group Robustness via Adaptive Class-Specific Scaling	Group distributionally robust optimization which aims to improve robust accuracies--worst-group and unbiased accuracies--is a prominent algorithm used to mitigate spurious correlations and address dataset bias. Although existing approaches have reported improvements in robust accuracies these gains often come at the cost of average accuracy due to inherent trade-offs. To control this trade-off flexibly and efficiently we propose a simple class-specific scaling strategy directly applicable to existing debiasing algorithms with no additional training. We further develop an instance-wise adaptive scaling technique to alleviate this trade-off even leading to improvements in both robust and average accuracies. Our approach reveals that a naive ERM baseline matches or even outperforms the recent debiasing methods by simply adopting the class-specific scaling technique. Additionally we introduce a novel unified metric that quantifies the trade-off between the two accuracies as a scalar value allowing for a comprehensive evaluation of existing algorithms. By tackling the inherent trade-off and offering a performance landscape our approach provides valuable insights into robust techniques beyond just robust accuracy. We validate the effectiveness of our framework through experiments across datasets in computer vision and natural language processing domains.	https://openaccess.thecvf.com//content/WACV2025/html/Seo_Re-Evaluating_Group_Robustness_via_Adaptive_Class-Specific_Scaling_WACV_2025_paper.html	Seonguk Seo, Bohyung Han
Re-Identifying People in Video via Learned Temporal Attention and Multi-Modal Foundation Models	Biometric recognition from security camera video is a challenging problem when the individuals change clothes or when they are partly occluded. Others have recently demonstrated that CLIP's visual encoder performs well in this domain but existing methods fail to make use of the model's text encoder or temporal information available in video. In this paper we present VCLIP a method for person identification in videos captured in challenging poses and with changes to a person's clothing. Harnessing the power of pre-trained vision-language models we jointly train a temporal fusion network while fine-tuning the visual encoder. To leverage the cross-modal embedding space we use learned biometric pedestrian attribute features to further enhance our model's person re-identification (Re-ID) ability. We demonstrate significant performance improvements via experiments with the MEVID and CCVID datasets particularly in the more challenging clothes-changing conditions. In support of this and future methods that use textual attributes for Re-ID with multimodal models we release a dataset of annotated pedestrian attributes for the popular MEVID dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Hill_Re-Identifying_People_in_Video_via_Learned_Temporal_Attention_and_Multi-Modal_WACV_2025_paper.html	Cole Hill, Florence Yellin, Krishna Regmi, Dawei Du, Scott McCloskey
ReBotNet: Fast Real-Time Video Enhancement	Most video restoration networks are slow have high computational load and can't be used for real-time video enhancement. In this work we design an efficient and fast framework to perform real-time video enhancement for practical use-cases like live video calls and video streams. Our proposed method called Recurrent Bottleneck Mixer Network (ReBotNet) employs a dual-branch framework. The first branch learns spatio-temporal features by tokenizing the input frames along the spatial and temporal dimensions using a ConvNext-based encoder and processing these abstract tokens using a bottleneck mixer. To further improve temporal consistency the second branch employs a mixer directly on tokens extracted from individual frames. A common decoder then merges the features form the two branches to predict the enhanced frame. In addition we use a recurrent training approach where the last frame's prediction is leveraged to efficiently enhance the current frame while improving temporal consistency. To evaluate our method we curate two new datasets that emulate real-world video call and streaming scenarios and show extensive results on multiple datasets where ReBotNet outperforms existing approaches with lower computations reduced memory requirements and faster inference time. Code: https://github.com/jeya-mariajose/rebot-net.	https://openaccess.thecvf.com//content/WACV2025/html/Valanarasu_ReBotNet_Fast_Real-Time_Video_Enhancement_WACV_2025_paper.html	Jeya Maria Jose Valanarasu, Rahul Garg, Andeep Toor, Xin Tong, Weijuan Xi, Andreas Lugmayr, Vishal M. Patel, Anne Menini
ReC-TTT: Contrastive Feature Reconstruction for Test-Time Training	The remarkable progress in deep learning (DL) showcases outstanding results in various computer vision tasks. However adaptation to real-time variations in data distributions remains an important challenge. Test-Time Training (TTT) was proposed as an effective solution to this issue which increases the generalization ability of trained models by adding an auxiliary task at train time and then using its loss at test time to adapt the model. Inspired by the recent achievements of contrastive representation learning in unsupervised tasks we propose ReC-TTT a test-time training technique that can adapt a DL model to new unseen domains by generating discriminative views of the input data. ReC-TTT uses cross-reconstruction as an auxiliary task between a frozen encoder and two trainable encoders taking advantage of a single shared decoder. This enables at test time to adapt the encoders to extract features that will be correctly reconstructed by the decoder that in this phase is frozen on the source domain. Experimental results show that ReC-TTT achieves better results than other state-of-the-art techniques in most domain shift classification challenges. The code is available at: https://github.com/warpcut/ReC-TTT	https://openaccess.thecvf.com//content/WACV2025/html/Colussi_ReC-TTT_Contrastive_Feature_Reconstruction_for_Test-Time_Training_WACV_2025_paper.html	Marco Colussi, Sergio Mascetti, Jose Dolz, Christian Desrosiers
ReEdit: Multimodal Exemplar-Based Image Editing	Modern Text-to-Image (T2I) Diffusion models have revolutionized image editing by enabling the generation of high-quality photorealistic images. While the de-facto method for performing edits with T2I models is through text instructions this approach is non-trivial due to the complex many-to-many mapping between natural language and images. In this work we address exemplar-based image editing - the task of transferring an edit from an exemplar pair to a content image(s). We propose ReEdit a modular and efficient end-to-end framework that captures edits in both text and image modalities while ensuring the fidelity of the edited image. We validate the effectiveness of ReEdit through extensive comparisons with state-of-the-art baselines and sensitivity analyses of key design choices. Our results demonstrate that ReEdit consistently outperforms contemporary approaches both qualitatively and quantitatively. Additionally ReEdit boasts high practical applicability as it does not require any task-specific optimization and is 4 times faster than the existing state-of-the-art. The code and data for our work is available at https://reedit-diffusion.github.io/.	https://openaccess.thecvf.com//content/WACV2025/html/Srivastava_ReEdit_Multimodal_Exemplar-Based_Image_Editing_WACV_2025_paper.html	Ashutosh Srivastava, Tarun Ram Menta, Abhinav Java, Avadhoot Gorakh Jadhav, Silky Singh, Surgan Jandial, Balaji Krishnamurthy
ReFu: Recursive Fusion for Exemplar-Free 3D Class-Incremental Learning	We introduce a novel Recursive Fusion model dubbed ReFu designed to integrate point clouds and meshes for exemplar-free 3D Class-Incremental Learning where the model learns new 3D classes while retaining knowledge of previously learned ones. Unlike existing methods that either rely on storing historical data to mitigate forgetting or focus on single data modalities ReFu eliminates the need for exemplar storage while utilizing the complementary strengths of both point clouds and meshes. To achieve this we introduce a recursive method which continuously accumulates knowledge by updating the regularized auto-correlation matrix. Furthermore we propose a fusion module featuring a Pointcloud-guided Mesh Attention Layer that learns correlations between the two modalities. This mechanism effectively integrates point cloud and mesh features leading to more robust and stable continual learning. Experiments across various datasets demonstrate that our proposed framework outperforms existing methods in 3D class-incremental learning. Project Page: https://arlo-yang.github.io/ReFu/	https://openaccess.thecvf.com//content/WACV2025/html/Yang_ReFu_Recursive_Fusion_for_Exemplar-Free_3D_Class-Incremental_Learning_WACV_2025_paper.html	Yi Yang, Lei Zhong, Huiping Zhuang
ReMP: Reusable Motion Prior for Multi-Domain 3D Human Pose Estimation and Motion Inbetweening	We present Reusable Motion prior (ReMP) an effective motion prior that can accurately track the temporal evolution of motion in various downstream tasks. Inspired by the success of foundation models we argue that a robust spatio-temporal motion prior can encapsulate underlying 3D dynamics applicable to various sensor modalities. We learn the rich motion prior from a sequence of complete parametric models of posed human body shape. Our prior can easily estimate poses in missing frames or noisy measurements despite significant occlusion by employing a temporal attention mechanism. More interestingly our prior can guide the system with incomplete and challenging input measurements to quickly extract critical information to estimate the sequence of poses significantly improving the training efficiency for mesh sequence recovery. ReMP consistently outperforms the baseline method on diverse and practical 3D motion data including depth point clouds LiDAR scans and IMU sensor data. Project page is available in https://hojunjang17.github.io/ReMP.	https://openaccess.thecvf.com//content/WACV2025/html/Jang_ReMP_Reusable_Motion_Prior_for_Multi-Domain_3D_Human_Pose_Estimation_WACV_2025_paper.html	Hojun Jang, Young Min Kim
ReMix: Training Generalized Person Re-Identification on a Mixture of Data	Modern person re-identification (Re-ID) methods have a weak generalization ability and experience a major accuracy drop when capturing environments change. This is because existing multi-camera Re-ID datasets are limited in size and diversity since such data is difficult to obtain. At the same time enormous volumes of unlabeled single-camera records are available. Such data can be easily collected and therefore it is more diverse. Currently single-camera data is used only for self-supervised pre-training of Re-ID methods. However the diversity of single-camera data is suppressed by fine-tuning on limited multi-camera data after pre-training. In this paper we propose ReMix a generalized Re-ID method jointly trained on a mixture of limited labeled multi-camera and large unlabeled single-camera data. Effective training of our method is achieved through a novel data sampling strategy and new loss functions that are adapted for joint use with both types of data. Experiments show that ReMix has a high generalization ability and outperforms state-of-the-art methods in generalizable person Re-ID. To the best of our knowledge this is the first work that explores joint training on a mixture of multi-camera and single-camera data in person Re-ID.	https://openaccess.thecvf.com//content/WACV2025/html/Mamedov_ReMix_Training_Generalized_Person_Re-Identification_on_a_Mixture_of_Data_WACV_2025_paper.html	Timur Mamedov, Anton Konushin, Vadim Konushin
Realistic and Efficient Face Swapping: A Unified Approach with Diffusion Models	Despite promising progress in face swapping task realistic swapped images remain elusive often marred by artifacts particularly in scenarios involving high pose variation color differences and occlusion. To address these issues we propose a novel approach that better harnesses diffusion models for face-swapping by making following core contributions. (a) We propose to re-frame the face-swapping task as a self-supervised train-time inpainting problem enhancing the identity transfer while blending with the target image. (b) We introduce a multi-step Denoising Diffusion Implicit Model (DDIM) sampling during training reinforcing identity and perceptual similarities. (c) Third we introduce CLIP feature disentanglement to extract pose expression and lighting information from the target image improving fidelity. (d) Further we introduce a mask shuffling technique during inpainting training which allows us to create a so-called universal model for swapping with an additional feature of head swapping. Ours can swap hair and even accessories beyond traditional face swapping. Unlike prior works reliant on multiple off-the-shelf models ours is a relatively unified approach and so it is resilient to errors in other off-the-shelf models. Extensive experiments on FFHQ and CelebA datasets validate the efficacy and robustness of our approach showcasing high-fidelity realistic face-swapping with minimal inference time. Our code is available at https://github.com/Sanoojan/REFace.	https://openaccess.thecvf.com//content/WACV2025/html/Baliah_Realistic_and_Efficient_Face_Swapping_A_Unified_Approach_with_Diffusion_WACV_2025_paper.html	Sanoojan Baliah, Qinliang Lin, Shengcai Liao, Xiaodan Liang, Muhammad Haris Khan
Recognizing Unseen States of Unknown Objects by Leveraging Knowledge Graphs	We investigate the problem of Object State Classification (OSC) in the context of zero-shot learning. Specifically we propose the first method for Zero-shot Object-agnostic State Classification (OaSC) that given an image infers the state of a single object without relying on the knowledge or the estimation of the object class. In that direction we capitalize on Knowledge Graphs (KGs) for structuring and organizing external knowledge which in combination with visual information enable effective inference of the states of objects that have not been encountered in the training set. Having this unique property a significant strength of our method is that it can handle an Open Set of object classes. We investigate the performance of OaSC in various datasets and settings against several hypotheses and in comparison with state-of-the-art approaches for object attribute classification. OaSC outperforms these methods significantly across all benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Gouidis_Recognizing_Unseen_States_of_Unknown_Objects_by_Leveraging_Knowledge_Graphs_WACV_2025_paper.html	Filippos Gouidis, Konstantinos Papoutsakis, Theodore Patkos, Antonis Argyros, Dimitris Plexousakis
Recoverable Anonymization for Pose Estimation: A Privacy-Enhancing Approach	Human pose estimation (HPE) is crucial for various applications. However deploying HPE algorithms in surveillance contexts raises significant privacy concerns due to the potential leakage of sensitive personal information (SPI) such as facial features and ethnicity. Existing privacy-enhancing methods often compromise either privacy or performance or they require costly additional modalities. We propose a novel privacy-enhancing system that generates privacy-enhanced portraits while maintaining high HPE performance. Our key innovations include the reversible recovery of SPI for authorized personnel and the preservation of contextual information. By jointly optimizing a privacy-enhancing module a privacy recovery module and a pose estimator our system ensures robust privacy protection efficient SPI recovery and high-performance HPE. Experimental results demonstrate the system's robust performance in privacy enhancement SPI recovery and HPE. The code associated with this study can be found at this URL.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_Recoverable_Anonymization_for_Pose_Estimation_A_Privacy-Enhancing_Approach_WACV_2025_paper.html	Wenjun Huang, Yang Ni, Arghavan Rezvani Dehaghani, SungHeon Evan Jeong, Hanning Chen, Yezi Liu, Fei Wen, Mohsen Imani
Recurrence-Based Vanishing Point Detection	Classical approaches to Vanishing Point Detection (VPD) rely solely on the presence of explicit straight lines in images. Recent supervised deep learning methods rely on learned filters from labeled datasets. We propose an alternative unsupervised approach: Recurrence-based Vanishing Point Detection (R-VPD) that uses implicit lines derived from discovered recurring correspondences in addition to explicit lines. Furthermore we contribute two new VPD datasets: 1) a Synthetic Image dataset with 3200 ground truth vanishing points and 2) a Real-World Image dataset with 1400 human annotated vanishing points. We compare our method with two classical methods and two state-of-the-art deep learning-based VPD methods. We demonstrate that our unsupervised approach outperforms all the methods on the synthetic images dataset outperforms the classical methods and is on par with the supervised learning approaches on real-world images. Code and data can be found here: http://vision.cse.psu.edu/data/data.shtml	https://openaccess.thecvf.com//content/WACV2025/html/Bharadwaj_Recurrence-Based_Vanishing_Point_Detection_WACV_2025_paper.html	Skanda Bharadwaj, Robert T. Collins, Yanxi Liu
Reducing the Content Bias for AI-Generated Image Detection	Identifying AI-generated content is critical for the safe and ethical use of generative AI. Recent research has focused on developing detectors that generalize to unknown generators with popular methods relying either on high-level features or low-level fingerprints. However these methods have clear limitations: biased towards unseen content or vulnerable to common image degradations such as JPEG compression. To address these issues we propose a novel approach SFLD which incorporates PatchShuffle to integrate high-level semantic and low-level textural information. SFLD applies PatchShuffle at multiple levels improving robustness and generalization across various generative models. Additionally current benchmarks face challenges such as low image quality insufficient content preservation and limited class diversity. In response we introduce TwinSynths a new benchmark generation methodology that constructs visually near-identical pairs of real and synthetic images to ensure high quality and content preservation. Our extensive experiments and analysis show that SFLD outperforms existing methods on detecting a wide variety of fake images sourced from GANs diffusion models and TwinSynths demonstrating the state-of-the-art performance and generalization capabilities to novel generative models.	https://openaccess.thecvf.com//content/WACV2025/html/Gye_Reducing_the_Content_Bias_for_AI-Generated_Image_Detection_WACV_2025_paper.html	Seoyeon Gye, Junwon Ko, Hyounguk Shon, Minchan Kwon, Junmo Kim
RefVSR++: Exploiting Reference Inputs for Reference-Based Video Super-Resolution	Smartphones with multi-camera systems featuring cameras with varying field-of-views (FoVs) are increasingly common. This variation in FoVs results in content differences across videos paving the way for an innovative approach to video super-resolution (VSR). This method enhances the VSR performance of lower resolution (LR) videos by leveraging higher resolution reference (Ref) videos. Previous works which operate on this principle generally expand on traditional VSR models by combining LR and Ref inputs over time into a unified stream. However we can expect that better results are obtained by independently aggregating these Ref image sequences temporally. Therefore we introduce an improved method RefVSR++ which performs the parallel aggregation of LR and Ref images in the temporal direction aiming to optimize the use of the available data. RefVSR++ also incorporates improved mechanisms for aligning image features over time crucial for effective VSR. Our experiments demonstrate that RefVSR++ outperforms previous works by over 1dB in PSNR setting a new benchmark in the field.	https://openaccess.thecvf.com//content/WACV2025/html/Zou_RefVSR_Exploiting_Reference_Inputs_for_Reference-Based_Video_Super-Resolution_WACV_2025_paper.html	Han Zou, Masanori Suganuma, Takayuki Okatani
Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation	Over the past few years Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention. However vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images. The capability to generate visual text is crucial offering both academic interest and a wide range of practical applications. To produce accurate visual text images state-of-the-art techniques adopt a glyph-controlled image generation approach consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout. Nevertheless our study reveals that these models still face three primary challenges prompting us to develop a testbed to facilitate future research. We introduce a benchmark LenCom-Eval specifically designed for testing models capability in generating images with Lengthy and Complex visual text. Subsequently we introduce a training-free framework to enhance the two-stage generation approaches. We examine the effectiveness of our approach on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable improvements across a range of evaluation metrics including CLIPScore OCR precision recall F1 score accuracy and edit distance scores. For instance our proposed framework improves the backbone model TextDiffuser by more than 23% and 13.5% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval respectively. Our work makes a unique contribution to the field by focusing on generating images with long and rare text sequences a niche previously unexplored by existing literature.	https://openaccess.thecvf.com//content/WACV2025/html/Lakhanpal_Refining_Text-to-Image_Generation_Towards_Accurate_Training-Free_Glyph-Enhanced_Image_Generation_WACV_2025_paper.html	Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, Man Luo
Reflective Teacher: Semi-Supervised Multimodal 3D Object Detection in Bird's-Eye-View via Uncertainty Measure	Applying pseudo labeling techniques has been found to be advantageous in semi-supervised 3D object detection (SSOD) in Bird's-Eye-View (BEV) for autonomous driving particularly where labeled data is limited. In the literature Exponential Moving Average (EMA) has been used for adjustments of the weights of teacher network by the student network. However the same induces catastrophic forgetting in the teacher network. In this work we address this issue by introducing a novel concept of Reflective Teacher where the student is trained by both labeled and pseudo labeled data while its knowledge is progressively passed to the teacher through a regularizer to ensure retention of previous knowledge. Additionally we propose Geometry Aware BEV Fusion (GA-BEVFusion) for efficient alignment of multi-modal BEV features thus reducing the disparity between the modalities - camera and LiDAR. This helps to map the precise geometric information embedded among LiDAR points reliably with the spatial priors for extraction of semantic information from camera images. Our experiments on the nuScenes and Waymo datasets demonstrate: 1) improved performance over state-of-the-art methods in both fully supervised and semi-supervised settings; 2) Reflective Teacher achieves equivalent performance with only 25% and 22% of labeled data for nuScenes and Waymo datasets respectively in contrast to other fully supervised methods that utilize the full labeled dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Hazra_Reflective_Teacher_Semi-Supervised_Multimodal_3D_Object_Detection_in_Birds-Eye-View_via_WACV_2025_paper.html	Saheli Hazra, Sudip Das, Rohit Choudhary, Arindam Das, Ganesh Sistu, CiarÃ¡n Eising, Ujjwal Bhattacharya
Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation	The rise of the generative models quality during the past years enabled the generation of edited variations of images at an important scale. To counter the harmful effects of such technology the Image Difference Captioning (IDC) task aims to describe the differences between two images. While this task is successfully handled for simple 3D rendered images it struggles on real-world images. The reason is twofold: the training data-scarcity and the difficulty to capture fine-grained differences between complex images. To address those issues we propose in this paper a simple yet effective framework to both adapt existing image captioning models to the IDC task and augment IDC datasets. We introduce BLIP2IDC an adaptation of BLIP2 to the IDC task at low computational cost and show it outperforms two-streams approaches by a significant margin on real-world IDC datasets. We also propose to use synthetic augmentation to improve the performance of IDC models in an agnostic fashion. We show that our synthetic augmentation strategy provides high quality data leading to a challenging new dataset well-suited for IDC named Syned. The code weights and dataset are available here https://github.com/gautierevn/BLIP2IDC.	https://openaccess.thecvf.com//content/WACV2025/html/Evennou_Reframing_Image_Difference_Captioning_with_BLIP2IDC_and_Synthetic_Augmentation_WACV_2025_paper.html	Gautier Evennou, Antoine Chaffin, Vivien Chappelier, Ewa Kijak
ReinDiffuse: Crafting Physically Plausible Motions with Reinforced Diffusion Model	Generating human motion from textual descriptions is a challenging task. Existing methods either struggle with physical credibility or are limited by the complexities of physics simulations. In this paper we present ReinDiffuse that combines reinforcement learning with motion diffusion model to generate physically credible human motions that align with textual descriptions. Our method adapts Motion Diffusion Model to output a parameterized distribution of actions making them compatible with reinforcement learning paradigms. We employ reinforcement learning with the objective of maximizing physically plausible rewards to optimize motion generation for physical fidelity. Our approach outperforms existing state-of-the-art models on two major datasets HumanML3D and KIT-ML achieving significant improvements in physical plausibility and motion quality. Project: https://reindiffuse.github.io/	https://openaccess.thecvf.com//content/WACV2025/html/Han_ReinDiffuse_Crafting_Physically_Plausible_Motions_with_Reinforced_Diffusion_Model_WACV_2025_paper.html	Gaoge Han, Mingjiang Liang, Jinglei Tang, Yongkang Cheng, Wei Liu, Shaoli Huang
Relational Self-Supervised Distillation with Compact Descriptors for Image Copy Detection	Image copy detection is the task of detecting edited copies of any image within a reference database. While previous approaches have shown remarkable progress the large size of their networks and descriptors remains a dis-advantage complicating their practical application. In this paper we propose a novel method that achieves competitive performance by using a lightweight network and compact descriptors. By utilizing relational self-supervised distillation to transfer knowledge from a large network to a small network we enable the training of lightweight networks with smaller descriptor sizes. We introduce relational self-supervised distillation for flexible representation in a smaller feature space and apply contrastive learning with a hard negative loss to prevent dimensional collapse. For the DISC2021 benchmark ResNet-50 and EfficientNet-B0 are used as the teacher and student models respectively with micro average precision improving by 5.0%/4.9%/5.9% for 64/128/256 descriptor sizes compared to the baseline method. The code is available at https://github.com/juntae9926/RDCD.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_Relational_Self-Supervised_Distillation_with_Compact_Descriptors_for_Image_Copy_Detection_WACV_2025_paper.html	Juntae Kim, Sungwon Woo, Jongho Nang
Relaxing Binary Constraints in Contrastive Vision-Language Medical Representation Learning	"By aligning paired image and caption embeddings as input contrastive vision-language representation learning has witnessed significant advances as illustrated by CLIP allowing visual encoders to learn from textual supervision and vice versa. Benefiting from millions of image-caption pairs collected from the Internet CLIP-like models show competitive performances against fully supervised baselines. However the learned visual representations are still undermined due to the binary constraint as most contrastive learning frameworks follow strict one-to-one correspondence for the input pairs of data and optimize the models using the InfoNCE loss function. The embeddings of the paired image-text are aligned while the unpaired image-text are pushed away from each other. In fact there are naturally many ""false negatives"" among these negative pairs since unpaired data can also have a high similarity. In this work we aim to overcome the impact of false negatives in vision-language representation learning by introducing soft targets for estimating the similarity between unpaired images and texts using external semantic knowledge structured in the form of graphs. The interest of such a method is demonstrated in the application context of medical imaging."	https://openaccess.thecvf.com//content/WACV2025/html/Wei_Relaxing_Binary_Constraints_in_Contrastive_Vision-Language_Medical_Representation_Learning_WACV_2025_paper.html	Xiaoyang Wei, Camille Kurtz, Florence Cloppet
Remote Blood Pressure Estimation from Facial Videos using Transfer Learning: Leveraging PPG to rPPG Conversion	Blood pressure (BP) monitoring is crucial for health assessment but existing contact-based methods face cost and comfort barriers. Remote photoplethysmography (rPPG) offers a promising contactless solution yet research is hampered by limited rPPG datasets with corresponding BP labels. This paper presents a transfer learning methodology for BP measurement. This approach involves utilizing a base dataset comprising signals produced by PPG (PPG-signals) to acquire knowledge that can be transferred to a target dataset containing signals generated by rPPG (rPPG-signals). In our study we trained diverse deep learning models using publicly available datasets containing PPG-signals. Subsequently these models were fine-tuned and evaluated using a public dataset that specifically consists of rPPG-signals. Additionally we explored the relationship between BP and heart rate and examined different loss functions and normalization approaches to optimize the performance of the deep learning models. The findings of our study demonstrate that our best model achieved a better performance than the state-of-the-art model with mean absolute error (MAE) of 8.721 (reduced by 4.879) mmHg and 8.653 (reduced by 1.647) mmHg for systolic blood pressure (SBP) and diastolic blood pressure (DBP) in a dataset with clinical settings showing promising potential for remote BP estimation.	https://openaccess.thecvf.com//content/WACV2025/html/Cheng_Remote_Blood_Pressure_Estimation_from_Facial_Videos_using_Transfer_Learning_WACV_2025_paper.html	Chun-Hong Cheng, Jing Wei Chin, Kwan Long Wong, Tsz Tai Chan, Hau Ching Lo, Kwan Lok Pang, Richard So, Bryan Yan
Removing Geometric Bias in One-Class Anomaly Detection with Adaptive Feature Perturbation	One-class anomaly detection aims to detect objects that do not belong to a predefined normal class. In practice training data lack those anomalous samples; hence state-of-the-art methods are trained to discriminate between normal and synthetically-generated pseudo-anomalous data. Most methods use data augmentation techniques on normal images to simulate anomalies. However the best-performing ones implicitly leverage a geometric bias present in the benchmarking datasets. This limits their usability in more general conditions. Others are relying on basic noising schemes that may be suboptimal in capturing the underlying structure of normal data. In addition most still favour the image domain to generate pseudo-anomalies training models end-to-end from only the normal class and overlooking richer representations of the information. To overcome these limitations we consider frozen yet rich feature spaces given by pretrained models and create pseudo-anomalous features with a novel adaptive linear feature perturbation technique. It adapts the noise distribution to each sample applies decaying linear perturbations to feature vectors and further guides the classification process using a contrastive learning objective. Experimental evaluation conducted on both standard and geometric bias-free datasets demonstrates the superiority of our approach with respect to comparable baselines. The codebase is accessible via our public repository.	https://openaccess.thecvf.com//content/WACV2025/html/Hermary_Removing_Geometric_Bias_in_One-Class_Anomaly_Detection_with_Adaptive_Feature_WACV_2025_paper.html	Romain Hermary, Vincent Gaudilliere, Abd El Rahman Shabayek, Djamila Aouada
RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation	Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention as a useful representation of the environment to tackle assisted and autonomous driving tasks. However most of the existing work focuses on the fully supervised setting training networks on large annotated datasets. In this work we present RendBEV a new method for the self-supervised training of BEV semantic segmentation networks leveraging differentiable volumetric rendering to receive supervision from semantic perspective views computed by a 2D semantic segmentation model. Our method enables zero-shot BEV semantic segmentation and already delivers competitive results in this challenging setting. When used as pretraining to then fine-tune on labeled BEV ground truth our method significantly boosts performance in low-annotation regimes and sets a new state of the art when fine-tuning on all available labels.	https://openaccess.thecvf.com//content/WACV2025/html/Monteagudo_RendBEV_Semantic_Novel_View_Synthesis_for_Self-Supervised_Birds_Eye_View_WACV_2025_paper.html	Henrique PiÃ±eiro Monteagudo, Leonardo Taccari, Aurel Pjetri, Francesco Sambo, Samuele Salti
Retaining and Enhancing Pre-Trained Knowledge in Vision-Language Models with Prompt Ensembling	The advancement of vision-language models particularly the Contrastive Language-Image Pre-training (CLIP) model has revolutionized the field of machine learning by enabling robust zero-shot learning capabilities. These capabilities allow models to understand and respond to previously unseen data without task-specific training. However adapting CLIP to integrate specialized knowledge from various domains while retaining its zero-shot capabilities remains a significant challenge. To address this we introduce a novel prompt ensemble learning approach called Group-wise Prompt Ensemble (GPE). This method aims to enhance CLIP's zero-shot capabilities by incorporating new domain knowledge while improving its adaptability and robustness against data distribution shifts. Our approach hinges on three main strategies: prompt grouping with masked attention to optimize CLIP's adaptability while safeguarding its zero-shot capabilities; the incorporation of auxiliary prompts for the seamless integration of new domain insights without disrupting the original model's representation; and an ensemble learning strategy that effectively merges original and new knowledge. Through rigorous experimentation including more challenging cross-dataset transfer evaluations our GPE method redefines the benchmarks for the adaptability and efficiency of vision-language models surpassing existing models across various scenarios.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_Retaining_and_Enhancing_Pre-Trained_Knowledge_in_Vision-Language_Models_with_Prompt_WACV_2025_paper.html	Donggeun Kim, Yujin Jo, Myungjoo Lee, Taesup Kim
Rethinking Cluster-Conditioned Diffusion Models for Label-Free Image Synthesis	Diffusion-based image generation models can enhance image quality when conditioned on ground truth labels. Here we conduct a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We investigate how individual clustering determinants such as the number of clusters and the clustering method impact image synthesis across three different datasets. Given the optimal number of clusters with respect to image synthesis we show that cluster-conditioning can achieve state-of-the-art performance with an FID of 1.67 for CIFAR10 and 2.17 for CIFAR100 along with a strong increase in training sample efficiency. We further propose a novel empirical method to estimate an upper bound for the optimal number of clusters. Unlike existing approaches we find no significant association between clustering performance and the corresponding cluster-conditional FID scores. Code is available at https://github.com/ HHU-MMBS/cedm-official-wavc2025	https://openaccess.thecvf.com//content/WACV2025/html/Adaloglou_Rethinking_Cluster-Conditioned_Diffusion_Models_for_Label-Free_Image_Synthesis_WACV_2025_paper.html	Nikolaos Adaloglou, Tim Kaiser, Felix Michels, Markus Kollmann
Rethinking Low-Rank Adaptation in Vision: Exploring Head-Level Responsiveness Across Diverse Tasks	Low-rank adaptation (LoRA) has shifted the paradigm of adapting pre-trained Vision Transformers (ViT) achieving great efficiency by updating only a subset of tailored parameters to approximate weight updates. However the multi-head design of the self-attention mechanism with the heads working in parallel in the computation flow exhibiting similar visual patterns and requiring update over all of them incurs unnecessary storage and computational overhead. In this paper we propose Head-level responsiveness tuning for low-rank adaptation (Heart-LoRA). The proposed method explores redundancy among the heads and selectively activates task-responsive heads thus enabling fine-grained head-level tuning. Additionally given the different responsiveness of heads to diverse visual tasks our proposed method dynamically activates a subset of the approximated heads that are tailored to the current task. Experimental results show that Heart-LoRA yields superior performance over state-of-the-art PETL approaches on visual adaptation benchmark datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Zhong_Rethinking_Low-Rank_Adaptation_in_Vision_Exploring_Head-Level_Responsiveness_Across_Diverse_WACV_2025_paper.html	Yibo Zhong, Yao Zhou
Retrieval Augmented Recipe Generation	The growing interest in generating recipes from food images has drawn substantial research attention in recent years. Existing works for recipe generation primarily utilize a two-stage training method--first predicting ingredients from a food image and then generating instructions from both the image and ingredients. Large Multi-modal Models (LMMs) which have achieved notable success across a variety of vision and language tasks shed light on generating both ingredients and instructions directly from images. Nevertheless LMMs still face the common issue of hallucinations during recipe generation leading to suboptimal performance. To tackle this issue we propose a retrieval augmented large multimodal model for recipe generation. We first introduce Stochastic Diversified Retrieval Augmentation (SDRA) to retrieve recipes semantically related to the image from an existing datastore as a supplement integrating them into the prompt to add diverse and rich context to the input image. Additionally Self-Consistency Ensemble Voting mechanism is proposed to determine the most confident prediction recipes as the final output. It calculates the consistency among generated recipe candidates which use different retrieval recipes as context for generation. Extensive experiments validate the effectiveness of our proposed method which demonstrates state-of-the-art (SOTA) performance in recipe generation on the Recipe1M dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Liu_Retrieval_Augmented_Recipe_Generation_WACV_2025_paper.html	Guoshan Liu, Hailong Yin, Bin Zhu, Jingjing Chen, Chong-Wah Ngo, Yu-Gang Jiang
Reversing the Damage: A QP-Aware Transformer-Diffusion Approach for 8K Video Restoration under Codec Compression	"In this paper we introduce DiQP; a novel Transformer-Diffusion model for restoring 8K video quality degraded by codec compression. To the best of our knowledge our model is the first to consider restoring the artifacts introduced by various codecs (AV1 HEVC) by Denoising Diffusion without considering additional noise. This approach allows us to model the complex non-Gaussian nature of compression artifacts effectively learning to reverse the degradation. Our architecture combines the power of Transformers to capture long-range dependencies with an enhanced windowed mechanism that preserves spatio-temporal context within groups of pixels across frames. To further enhance restoration the model incorporates auxiliary ""Look Ahead"" and ""Look Around"" modules providing both future and surrounding frame information to aid in reconstructing fine details and enhancing overall visual quality. Extensive experiments on different datasets demonstrate that our model outperforms state-of-the-art methods particularly for high-resolution videos such as 4K and 8K showcasing its effectiveness in restoring perceptually pleasing videos from highly compressed sources"	https://openaccess.thecvf.com//content/WACV2025/html/Dehaghi_Reversing_the_Damage_A_QP-Aware_Transformer-Diffusion_Approach_for_8K_Video_WACV_2025_paper.html	Ali Mollaahmadi Dehaghi, Reza Razavi, Mohammad Moshirpour
Revisiting Deep Archetypal Analysis for Phenotype Discovery in High Content Imaging	The discovery of unique treatment candidates for complex diseases is a challenging task for current drug discovery programs. Biopharma research has developed automated and scalable screening assays of cell culture models to screen thousands of drug candidates in parallel e.g. by considering bio-image based assays. However the large amount of data hinders a systematic review by human experts to distinguish between different disease and healthy phenotypes. A prevalent approach to uncover phenotypic endpoints in a dataset is based on the concept of archetypal analysis which seeks for extremal points in a dataset. State-of-the-art non-linear archetypal methods based on variational autoencoders require k - 1 latent dimensions to encode k archetypes. However in high content imaging we frequently require a significantly larger number of latent dimensions than archetypes to encode HCIs which results in weak latent representations and ambiguous archetypes. To overcome this limitation we propose to relax the simplex constraint in the latent space to a unit hypersphere and learn the respective archetypes based on online dictionary learning. Extensive experiments on two industry-relevant assays and a synthetic MNIST example demonstrate that our method outperforms state-of-the-art deep archetypal analysis approaches.	https://openaccess.thecvf.com//content/WACV2025/html/Wieser_Revisiting_Deep_Archetypal_Analysis_for_Phenotype_Discovery_in_High_Content_WACV_2025_paper.html	Mario Wieser, Daniel Siegismund, Stephan Steigele
Revisiting Disparity from Dual-Pixel Images: Physics-Informed Lightweight Depth Estimation	In this study we propose a high-performance disparity (depth) estimation method using dual-pixel (DP) images with few parameters. Conventional end-to-end deep-learning methods have many parameters but do not fully exploit disparity constraints which limits their performance. Therefore we propose a lightweight disparity estimation method based on a completion-based network that explicitly constrains disparity and learns the physical and systemic disparity properties of DP. By modeling the DP-specific disparity error parametrically and using it for sampling during training the network acquires the unique properties of DP and enhances robustness. This learning also allows us to use a common RGB-D dataset for training without a DP dataset which is labor-intensive to acquire. Furthermore we propose a non-learning-based refinement framework that efficiently handles inherent disparity expansion errors by appropriately refining the confidence map of the network output. As a result the proposed method achieved state-of-the-art results while reducing the overall system size to 1/5 of that of the conventional method even without using the DP dataset for training thereby demonstrating its effectiveness. The code and dataset are available on our project site.	https://openaccess.thecvf.com//content/WACV2025/html/Kurita_Revisiting_Disparity_from_Dual-Pixel_Images_Physics-Informed_Lightweight_Depth_Estimation_WACV_2025_paper.html	Teppei Kurita, Yuhi Kondo, Legong Sun, Takayuki Sasaki, Sho Nitta, Yasuhiro Hashimoto, Yoshinori Muramatsu, Yusuke Moriuchi
Revisiting Machine Unlearning with Dimensional Alignment	Machine unlearning an emerging research topic focusing on data privacy compliance enables trained models to erase information learned from specific data. While many existing methods indirectly address this issue by intentionally injecting incorrect supervision they often result in drastic and unpredictable changes to decision boundaries and feature spaces leading to training instability and undesired side effects. To address this challenge more fundamentally we first analyze the changes in latent feature spaces between the original and retrained models and observe that the feature representations of samples not included in training are closely aligned with the feature manifolds of previously seen samples. Building on this insight we introduce a novel evaluation metric for machine unlearning coined dimensional alignment which measures the alignment between the eigenspaces of the forget and retain sets. We incorporate this metric as a regularizer loss to develop a robust and stable unlearning framework which is further enhanced by a self-distillation loss and an alternating training scheme. Our framework effectively eliminates information from the forget set while preserving knowledge from the retain set. Finally we identify critical flaws in existing evaluation metrics for machine unlearning and propose new tools that more accurately capture its fundamental objectives.	https://openaccess.thecvf.com//content/WACV2025/html/Seo_Revisiting_Machine_Unlearning_with_Dimensional_Alignment_WACV_2025_paper.html	Seonguk Seo, Dongwan Kim, Bohyung Han
Reviving Poor Object Segmentations in OOD Medical Images using Variational-Deep-PCA Modeling on Segmentation Maps with Sampling-Free Learning	For object segmentation in medical images deep neural networks (DNNs) typically perform poorly on out-of-distribution (OOD) images stemming from the large variability in image-acquisition equipment and protocols across sites. However compared to such variability in the acquired medical images we observe that the variability in the underlying object-segmentation maps is far lower. Thus we propose a novel DNN framework to model this variability in segmentation maps and leverage it to revive poor segmentations produced by existing DNNs on OOD images. Our DNN framework (i) learns the principal modes of variation in a class of segmentation maps (ii) models each segmentation map using a low-dimensional mixture-of-modes latent representation on a simplex (iii) enables sampling-free variational learning and uncertainty estimation and (iv) trains using small in-distribution image sets. When OOD-image segmentations are extremely poor we propose a novel human-in-the-loop method needing minuscule human intervention. Results using 6 publicly-available datasets and 8 existing DNN segmenters show the benefits of our framework in OOD-image object segmentation.	https://openaccess.thecvf.com//content/WACV2025/html/Pal_Reviving_Poor_Object_Segmentations_in_OOD_Medical_Images_using_Variational-Deep-PCA_WACV_2025_paper.html	Jimut B. Pal, Shantanu Welling, Himali Saini, Suyash P. Awate
RiemStega: Covariance-Based Loss for Print-Proof Transmission of Data in Images	Covariance matrices outperform first-order features in many tasks attracting considerable attention from the computer vision research community. Covariance matrices encode second-order statistics between features at the same time it is robust to noise. Based on this we propose representing images by covariance matrices and defining a loss function that measures the distance between them through the Riemannian distance. Motivated by the robustness and invariance properties of the affine invariant Riemannian metric the proposed method was validated in printer-proof data transmission which is a challenging task due to the trade-off between image quality and message recovery capabilities after printing and digitization procedures. The effectiveness of this approach was systematically assessed using MS COCO and IMM Face datasets. The results demonstrated that the proposed approach outperforms conventional methods that use Euclidean distance generating encoded images with better quality and achieving higher recovery accuracy in printed images. Additionally a broader application of the proposed loss was successfully tested in image generation tasks using generative adversarial networks (GANs).	https://openaccess.thecvf.com//content/WACV2025/html/Cruz_RiemStega_Covariance-Based_Loss_for_Print-Proof_Transmission_of_Data_in_Images_WACV_2025_paper.html	Aniana Cruz, Guilherme Schardong, Luiz Schirmer, JoÃ£o Marcos, Farhad Shadmand, Nuno GonÃ§alves
Robot Instance Segmentation with Few Annotations for Grasping	The ability of robots to manipulate objects relies heavily on their aptitude for visual perception. In domains characterized by cluttered scenes and high object variability most methods call for vast labeled datasets laboriously hand-annotated with the aim of training capable models. Once deployed the challenge of generalizing to unfamiliar objects implies that the model must evolve alongside its domain. To address this we propose a novel framework that combines Semi-Supervised Learning (SSL) with Learning Through Interaction (LTI) allowing a model to learn by observing scene alterations and leverage visual consistency despite temporal gaps without requiring curated data of interaction sequences. As a result our approach exploits partially annotated data through self-supervision and incorporates temporal context using pseudo-sequences generated from unlabeled still images. We validate our method on two common benchmarks ARMBench mix-object-tote and OCID where it achieves state-of-the-art performance. Notably on ARMBench we attain an AP50 of 86.37 almost a 20% improvement over existing work and obtain remarkable results in scenarios with extremely low annotation achieving an AP50 score of 84.89 with just 1% of annotated data compared to previous state of the art of 82 which targeted the fully annotated dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Kimhi_Robot_Instance_Segmentation_with_Few_Annotations_for_Grasping_WACV_2025_paper.html	Moshe Kimhi, David Vainshtein, Chaim Baskin, Dotan Di Castro
Robust Long-Range Perception Against Sensor Misalignment in Autonomous Vehicles	Advances in machine learning algorithms for sensor fusion have significantly improved the detection and prediction of other road users thereby enhancing safety. However even a small angular displacement in the sensor's placement can cause significant degradation in output especially at long range. In this paper we demonstrate a simple yet generic and efficient multi-task learning approach that not only detects misalignment between different sensor modalities but is also robust against them for long-range perception. Along with the amount of misalignment our method also predicts calibrated uncertainty which can be useful for filtering and fusing predicted misalignment values over time. In addition we show that the predicted misalignment parameters can be used for self-correcting input sensor data further improving the perception performance under sensor misalignment.	https://openaccess.thecvf.com//content/WACV2025/html/Xia_Robust_Long-Range_Perception_Against_Sensor_Misalignment_in_Autonomous_Vehicles_WACV_2025_paper.html	Zi-Xiang Xia, Sudeep Fadadu, Yi Shi, Louis Foucard
Robust Novelty Detection through Style-Conscious Feature Ranking	Novelty detection seeks to identify samples deviating from a known distribution yet data shifts in a multitude of ways and only a few consist of relevant changes. Aligned with out-of-distribution generalization literature we advocate for a formal distinction between task-relevant semantic or content changes and irrelevant style changes. This distinction forms the basis for robust novelty detection emphasizing the identification of semantic changes resilient to style distributional shifts. To this end we introduce Stylist a method that utilizes pretrained large-scale model representations to selectively discard environment-biased features. By computing per-feature scores based on feature distribution distances between environments Stylist effectively eliminates features responsible for spurious correlations enhancing novelty detection performance. Evaluations on adapted domain generalization datasets and a synthetic dataset demonstrate Stylist's efficacy in improving novelty detection across diverse datasets with stylistic and content shifts. We make our code available at https://github.com/bit-ml/Stylist.	https://openaccess.thecvf.com//content/WACV2025/html/Smeu_Robust_Novelty_Detection_through_Style-Conscious_Feature_Ranking_WACV_2025_paper.html	Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu
Robust Portrait Image Matting and Depth-of-Field Synthesis via Multiplane Images	High-quality portrait photography has become an essential function in our daily lives. However due to the limited aperture and focal length of a smartphone camera images captured by a smartphone cannot match the same level of bokeh effect by a digital single-lens reflex camera. A typical solution on a smartphone is to simulate out-of-focus effects from an all-in-focus image where the key is robust depth estimation and portrait matting. To achieve this we propose a multi-stage multi-branch matting network to estimate a strand-level portrait alpha mask which is then used to refine the coarse depth map from the pre-trained model. Combining the input portrait image with the estimated depth map and alpha mask we propose a learning-free optimization mechanism to construct a multi-plane image (MPI) representation for depth-of-field synthesis. The MPI consists of multiple layers of disk-blurred images with kernel size proportional to the absolute depth distance to the focus layer. Then a depth-aware blurring process is applied to enforce the bokeh effect. Besides each MPI layer has an alpha channel controlling the visibility according to the corresponding depth. Finally an image with bokeh is rendered by compositing all MPI layers. We conduct comprehensive experiments to evaluate our method which demonstrates that our method can generate more accurate alpha masks and more realistic images with bokeh compared to prior work.	https://openaccess.thecvf.com//content/WACV2025/html/Rao_Robust_Portrait_Image_Matting_and_Depth-of-Field_Synthesis_via_Multiplane_Images_WACV_2025_paper.html	Zhefan Rao, Tianjia Zhang, Yuen Fui Lau, Qifeng Chen
RopeTP: Global Human Motion Recovery via Integrating Robust Pose Estimation with Diffusion Trajectory Prior	We present RopeTP a novel framework that combines Robust pose estimation with a diffusion Trajectory Prior to reconstruct global human motion from videos. At the heart of RopeTP is a hierarchical attention mechanism that significantly improves context awareness which is essential for accurately inferring the posture of occluded body parts. This is achieved by exploiting the relationships with visible anatomical structures enhancing the accuracy of local pose estimations. The improved robustness of these local estimations allows for the reconstruction of precise and stable global trajectories. Additionally RopeTP incorporates a diffusion trajectory model that predicts realistic human motion from local pose sequences. This model ensures that the generated trajectories are not only consistent with observed local actions but also unfold naturally over time thereby improving the realism and stability of 3D human motion reconstruction. Extensive experimental validation shows that RopeTP surpasses current methods on two benchmark datasets particularly excelling in scenarios with occlusions. It also outperforms methods that rely on SLAM for initial camera estimates and extensive optimization delivering more accurate and realistic trajectories.	https://openaccess.thecvf.com//content/WACV2025/html/Liang_RopeTP_Global_Human_Motion_Recovery_via_Integrating_Robust_Pose_Estimation_WACV_2025_paper.html	Mingjiang Liang, Yongkang Cheng, Hualin Liang, Shaoli Huang, Wei Liu
Rubric-Constrained Figure Skating Scoring	Figure skating automatic scoring is the task of estimating the competition score of a performance video. The technical element score (TES) aggregates the technical quality (grade of execution) and difficulty (base value) scores for each element. Most prior work adapted from short-term action quality assessment entangle difficulty and quality and compute TES for the entire video reducing interpretability for athletes. This is mainly due to a lack of element segmentation and difficulty annotations in existing datasets. Motivated by increasing interpretability we propose a novel method that implicitly segments a video to produce element-level representations and uses adherence with a natural language rubric to score each element without needing additional annotations. We compute element-level representations using learnable element queries in a transformer and propose implicit segmentation regularization to encourage element queries to attend to elements rather than background transitions between elements (most of video). Additionally we use the element list (sequence of elements) to isolate difficulty just like judges who receive the routine list in advance so we can focus on the more critical problem of how well elements are done. These components significantly improve interpretability scoring precision and ranking capability. Code is released at https://arushirai1.github.io/rcs-project.	https://openaccess.thecvf.com//content/WACV2025/html/Rai_Rubric-Constrained_Figure_Skating_Scoring_WACV_2025_paper.html	Arushi Rai, Adriana Kovashka
S3PT: Scene Semantics and Structure Guided Clustering to Boost Self-Supervised Pre-Training for Autonomous Driving	Recent self-supervised clustering-based pre-training techniques like DINO and CriBo have shown impressive results for downstream detection and segmentation tasks. However real-world applications such as autonomous driving face challenges with imbalanced object class and size distributions and complex scene geometries. In this paper we propose S3PT a novel scene semantics and structure guided clustering to provide more scene-consistent objectives for self-supervised training. Specifically our contributions are threefold: First we incorporate semantic distribution consistent clustering to encourage better representation of rare classes such as motorcycles or animals. Second we introduce object diversity consistent spatial clustering to handle imbalanced and diverse object sizes ranging from large background areas to small objects such as pedestrians and traffic signs. Third we propose a depth-guided spatial clustering to regularize learning based on geometric information of the scene thus further refining region separation on the feature level. Our learned representations significantly improve performance in downstream semantic segmentation and 3D object detection tasks on the nuScenes nuImages and Cityscapes datasets and show promising domain translation properties.	https://openaccess.thecvf.com//content/WACV2025/html/Wozniak_S3PT_Scene_Semantics_and_Structure_Guided_Clustering_to_Boost_Self-Supervised_WACV_2025_paper.html	Maciej K. Wozniak, Hariprasath Govindarajan, Marvin Klingner, Camille Maurice, B Ravi Kiran, Senthil Yogamani
SADA: Semantic Adversarial Unsupervised Domain Adaptation for Temporal Action Localization	Temporal Action Localization (TAL) is a complex task that poses relevant challenges particularly when attempting to generalize on new - unseen - domains in real-world applications. These scenarios despite realistic are often neglected in the literature exposing these solutions to important performance degradation. In this work we tackle this issue by introducing for the first time an approach for Unsupervised Domain Adaptation (UDA) in sparse TAL which we refer to as Semantic Adversarial unsupervised Domain Adaptation (SADA). Our contributions are threefold: (1) we pioneer the development of a domain adaptation model that operates on realistic sparse action detection benchmarks; (2) we tackle the limitations of global-distribution alignment techniques by introducing a novel adversarial loss that is sensitive to local class distributions ensuring finer-grained adaptation; and (3) we present a novel set of benchmarks based on EpicKitchens100 and CharadesEgo that evaluate multiple domain shifts in a comprehensive manner. Our experiments indicate that SADA improves the adaptation across domains when compared to fully supervised state-of-the-art and alternative UDA methods attaining a performance boost of up to 6.14% mAP. The code is publicly available at https://github.com/davidpujol/SADA.	https://openaccess.thecvf.com//content/WACV2025/html/Pujol-Perich_SADA_Semantic_Adversarial_Unsupervised_Domain_Adaptation_for_Temporal_Action_Localization_WACV_2025_paper.html	David Pujol-Perich, Albert ClapÃ©s, Sergio Escalera
SADDLe: Sharpness-Aware Decentralized Deep Learning with Heterogeneous Data	Decentralized training enables learning with distributed datasets generated at different locations without relying on a central server. In realistic scenarios the data distribution across these sparsely connected learning agents can be significantly heterogeneous leading to local model over-fitting and poor global model generalization. Another challenge is the high communication cost of training models in such a peer-to-peer fashion without any central coordination. In this paper we jointly tackle these two-fold practical challenges by proposing SADDLe a set of sharpness-aware decentralized deep learning algorithms. SADDLe leverages Sharpness-Aware Minimization (SAM) to seek a flatter loss landscape during training resulting in better model generalization as well as enhanced robustness to communication compression. We present two versions of our approach and demonstrate its effectiveness through extensive experiments on various Computer Vision datasets (CIFAR-10 CIFAR-100 Imagenette and ImageNet) model architectures and graph topologies. Our results show that SADDLe leads to 1-20% improvement in test accuracy as compared to existing techniques while incurring a minimal accuracy drop ( 1%) in the presence of up to 4x compression.	https://openaccess.thecvf.com//content/WACV2025/html/Choudhary_SADDLe_Sharpness-Aware_Decentralized_Deep_Learning_with_Heterogeneous_Data_WACV_2025_paper.html	Sakshi Choudhary, Sai Aparna Aketi, Kaushik Roy
SALVE: A 3D Reconstruction Benchmark of Wounds from Consumer-Grade Videos	Managing chronic wounds is a global challenge that can be alleviated by the adoption of automatic systems for clinical wound assessment from consumer-grade videos. While 2D image analysis approaches are insufficient for handling the 3D features of wounds existing approaches utilizing 3D reconstruction methods have not been thoroughly evaluated. To address this gap this paper presents a comprehensive study on 3D wound reconstruction from consumer-grade videos. Specifically we introduce the SALVE dataset comprising video recordings of realistic wound phantoms captured with different cameras. Using this dataset we assess the accuracy and precision of state-of-the-art methods for 3D reconstruction ranging from traditional photogrammetry pipelines to advanced neural rendering approaches. In our experiments we observe that photogrammetry approaches do not provide smooth surfaces suitable for precise clinical measurements of wounds. Neural rendering approaches show promise in addressing this issue advancing the use of this technology in wound care practices.	https://openaccess.thecvf.com//content/WACV2025/html/Chierchia_SALVE_A_3D_Reconstruction_Benchmark_of_Wounds_from_Consumer-Grade_Videos_WACV_2025_paper.html	Remi Chierchia, Leo Lebrat, David Ahmedt-Aristizabal, Olivier Salvado, Clinton Fookes, Rodrigo Santa Cruz
SAM-DA: Decoder Adapter for Efficient Medical Domain Adaptation	This paper addresses the domain adaptation challenge for semantic segmentation in medical imaging. Despite the impressive performance of recent foundational segmentation models like SAM on natural images they struggle with medical domain images. Beyond this recent approaches that perform end-to-end fine-tuning of models are simply not computationally tractable. To address this we propose a novel SAM adapter approach that minimizes the number of trainable parameters while achieving comparable performances to full fine-tuning. The proposed SAM adapter is strategically placed in the mask decoder offering excellent and broad generalization capabilities and improved segmentation across both fully supervised and test-time domain adaptation tasks. Extensive validation on four datasets showcases the adapter's efficacy outperforming existing methods while training less than 1% of SAM's total parameters.	https://openaccess.thecvf.com//content/WACV2025/html/Tejero_SAM-DA_Decoder_Adapter_for_Efficient_Medical_Domain_Adaptation_WACV_2025_paper.html	Javier Gamazo Tejero, Moritz J Schmid, Pablo MÃ¡rquez Neila, Martin Zinkernagel, Sebastian Wolf, Raphael Sznitman
SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp Segmentation	Polyp segmentation in colonoscopy images is crucial for detecting colorectal cancer but is challenging due to variations in the structure color and size of polyps as well as the lack of clear boundaries with surrounding tissues. Traditional segmentation models based on Convolutional Neural Networks (CNNs) struggle to capture detailed patterns and global context limiting their performance. Vision Transformer (ViT)-based models address some of these issues but have difficulties in capturing local context and lack strong zero-shot generalization. To this end we propose the Mamba-guided Segment Anything Model (SAM-Mamba) for efficient polyp segmentation. Our approach introduces a Mamba-Prior module in the encoder to bridge the gap between the general pre-trained representation of SAM and polyp-relevant trivial clues. It injects salient cues of polyp images into the SAM image encoder as a domain prior while capturing global dependencies at various scales leading to more accurate segmentation results. Extensive experiments on five benchmark datasets show that SAM-Mamba outperforms traditional CNN ViT and Adapter-based models in both quantitative and qualitative measures. Additionally SAM-Mamba demonstrates excellent adaptability to unseen datasets making it highly suitable for real-time clinical use.	https://openaccess.thecvf.com//content/WACV2025/html/Dutta_SAM-Mamba_Mamba_Guided_SAM_Architecture_for_Generalized_Zero-Shot_Polyp_Segmentation_WACV_2025_paper.html	Tapas Kumar Dutta, Snehashis Majhi, Deepak Ranjan Nayak, Debesh Jha
SAND: Enhancing Open-Set Neuron Descriptions through Spatial Awareness	We propose Spatially-Aware open-set Network Dissection (SAND) a technique to identify and label the learned representation of the neurons of deep vision networks. Contrary to earlier open-vocabulary neuron explanation methods we also leverage a neuron's spatial pattern of activation to guide our predictions towards more accurate and relevant concepts while avoiding being misled by confounding visual information. We highlight important regions for a neuron through image masking which has the advantage of being able to block out irrelevant concepts from an image handling irregularly shaped activation regions and revealing the visual concepts that a neuron learns in order to identify objects. We use CLIP to connect highly activating image regions with descriptive concepts and measure the quality of our results through human evaluation. Further since such manual evaluation can be highly time consuming costly and unscalable we also propose an automated approach which uses image generation to get quantitative feedback on the generated concepts. Finally as an application of our interpretability method we demonstrate how it can be tuned to the medical domain. Our code is available at https://github.com/Trustworthy-ML-Lab/SAND.	https://openaccess.thecvf.com//content/WACV2025/html/Srinivas_SAND_Enhancing_Open-Set_Neuron_Descriptions_through_Spatial_Awareness_WACV_2025_paper.html	Anvita Agarwal Srinivas, Tuomas Oikarinen, Divyansh Srivastava, Wei-Hung Weng, Tsui-Wei Weng
SANPO: A Scene Understanding Accessibility and Human Navigation Dataset	Vision is essential for human navigation. The World Health Organization (WHO) estimates that 43.3 million people were blind in 2020 and this number is projected to reach 61 million by 2050. Modern scene understanding models could empower these people by assisting them with navigation obstacle avoidance and visual recognition capabilities. The research community needs high quality datasets for both training and evaluation to build these systems. And while datasets for autonomous vehicles are abundant there is a critical gap in datasets tailored for outdoor human navigation. This gap poses a major obstacle to the development of computer vision based Assistive Technologies. To overcome this obstacle we present SANPO a large-scale egocentric video dataset designed for dense prediction in outdoor human navigation environments. SANPO contains 701 stereo videos of 30+ seconds captured in diverse real-world outdoor environments across four geographic locations in the USA. Every frame has a high resolution depth map and 112K frames were annotated with temporally consistent dense video panoptic segmentation labels. The dataset also includes 1961 high-quality synthetic videos with pixel accurate depth and panoptic segmentation annotations to balance the noisy real world annotations with the high precision synthetic annotations. SANPO is already publicly available and is being used by applications like Project Guideline to train mobile models that help low-vision users run independently. To preserve anonymization during peer review a link to the dataset will be provided upon acceptance.	https://openaccess.thecvf.com//content/WACV2025/html/Waghmare_SANPO_A_Scene_Understanding_Accessibility_and_Human_Navigation_Dataset_WACV_2025_paper.html	Sagar M. Waghmare, Kimberly Wilber, Dave Hawkey, Xuan Yang, Matthew Wilson, Stephanie Debats, Cattalyya Nuengsigkapian, Astuti Sharma, Lars Pandikow, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko
SCOT: Self-Supervised Contrastive Pretraining for Zero-Shot Compositional Retrieval	Compositional image retrieval (CIR) is a multimodal learning task where a model combines a query image with a user-provided text modification to retrieve a target image. CIR finds applications in a variety of domains including product retrieval (e-commerce) and web search. Existing methods primarily focus on fully-supervised learning wherein models are trained on datasets of labeled triplets such as FashionIQ and CIRR. This poses two significant challenges: (i) curating such triplet datasets is labor intensive; and (ii) models lack generalization to unseen objects and domains. In this work we propose SCOT (Self-supervised COmpositional Training) a novel zero-shot compositional pretraining strategy that combines existing large image-text pair datasets with the generative capabilities of large language models to contrastively train an embedding composition network. Specifically we show that the text embedding from a large-scale contrastively-pretrained vision-language model can be utilized as proxy target supervision during compositional pretraining replacing the target image embedding. In zero-shot settings this strategy surpasses SOTA zero-shot compositional retrieval methods as well as many fully-supervised methods on standard benchmarks such as FashionIQ and CIRR.	https://openaccess.thecvf.com//content/WACV2025/html/Jawade_SCOT_Self-Supervised_Contrastive_Pretraining_for_Zero-Shot_Compositional_Retrieval_WACV_2025_paper.html	Bhavin Jawade, JoÃ£o V. B. Soares, Kapil Thadani, Deen Dayal Mohan, Amir Erfan Eshratifar, Benjamin Culpepper, Paloma de Juan, Srirangaraj Setlur, Venu Govindaraju
SEED4D: A Synthetic Ego-Exo Dynamic 4D Data Generator Driving Dataset and Benchmark	Models for egocentric 3D and 4D reconstruction including few-shot interpolation and extrapolation settings can benefit from having images from exocentric viewpoints as supervision signals. No existing dataset provides the necessary mixture of complex dynamic and multi-view data. To facilitate the development of 3D and 4D reconstruction methods in the autonomous driving context we propose a Synthetic Ego-Exo Dynamic 4D (SEED4D) data generator and dataset. We present a customizable easy-to-use data generator for spatio-temporal multi-view data creation. Our open-source data generator allows the creation of synthetic data for camera setups commonly used in the NuScenes KITTI360 and Waymo datasets. Additionally SEED4D encompasses two large-scale multi-view synthetic urban scene datasets. Our static (3D) dataset encompasses 212k inward- and outward-facing vehicle images from 2k scenes while our dynamic (4D) dataset contains 16.8M images from 10k trajectories each sampled at 100 points in time with egocentric images exocentric images and LiDAR data. The datasets and the data generator can be found here: https://seed4d.github.io	https://openaccess.thecvf.com//content/WACV2025/html/Kastingschafer_SEED4D_A_Synthetic_Ego-Exo_Dynamic_4D_Data_Generator_Driving_Dataset_WACV_2025_paper.html	Marius KÃ¤stingschÃ¤fer, ThÃ©o Gieruc, Sebastian Bernhard, Dylan Campbell, Eldar Insafutdinov, Eyvaz Najafli, Thomas Brox
SEM-Net: Efficient Pixel Modelling for Image Inpainting with Spatially Enhanced SSM	Image inpainting aims to repair a partially damaged image based on the information from known regions of the images. Achieving semantically plausible inpainting results is particularly challenging because it requires the reconstructed regions to exhibit similar patterns to the semantically consistent regions. This requires a model with a strong capacity to capture long-range dependencies. Existing models struggle in this regard due to the slow growth of receptive field for Convolutional Neural Networks (CNNs) based methods and patch-level interactions in Transformer-based methods which are ineffective for capturing long-range dependencies. Motivated by this we propose SEM-Net a novel visual State Space model (SSM) vision network modelling corrupted images at the pixel level while capturing long-range dependencies (LRDs) in state space achieving a linear computational complexity. To address the inherent lack of spatial awareness in SSM we introduce the Snake Mamba Block (SMB) and Spatially-Enhanced Feedforward Network. These innovations enable SEM-Net to outperform state-of-the-art inpainting methods on two distinct datasets showing significant improvements in capturing LRDs and enhancement in spatial consistency. Additionally SEM-Net achieves state-of-the-art performance on motion deblurring demonstrating its generalizability. Our source code is available:https://github.com/ChrisChen1023/SEM-Net.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_SEM-Net_Efficient_Pixel_Modelling_for_Image_Inpainting_with_Spatially_Enhanced_WACV_2025_paper.html	Shuang Chen, Haozheng Zhang, Amir Atapour-Abarghouei, Hubert P. H. Shum
SEMU-Net: A Segmentation-Based Corrector for Fabrication Process Variations of Nanophotonics with Microscopic Images	Integrated silicon photonic devices which manipulate light to transmit and process information on a silicon-on-insulator chip are highly sensitive to structural variations. Minor deviations during nanofabrication--the precise process of building structures at the nanometer scale--such as over- or under-etching corner rounding and unintended defects can significantly impact performance. To address these challenges we introduce SEMU-Net a comprehensive set of methods that automatically segments scanning electron microscope (SEM) images and uses them to train two deep neural network models based on U-Net and its variants. The predictor model anticipates fabrication-induced variations while the corrector model adjusts the design to address these issues ensuring that the final fabricated structures closely align with the intended specifications. Experimental results show that the segmentation U-Net reaches an average IoU score of 99.30% while the corrector attention U-Net in a tandem architecture achieves an average IoU score of 98.67%.	https://openaccess.thecvf.com//content/WACV2025/html/Azimi_SEMU-Net_A_Segmentation-Based_Corrector_for_Fabrication_Process_Variations_of_Nanophotonics_WACV_2025_paper.html	Rambod Azimi, Yijian Kong, Dusan Gostimirovic, James J. Clark, Odile Liboiron-Ladouceur
SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior	Novel View Synthesis (NVS) for street scenes plays a critical role in the autonomous driving simulation. Current mainstream methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically we first fine-tune a Diffusion Model by adding images from adjacent frames as condition meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the fine-tuned Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models and demonstrate its advances in rendering images from broader views.	https://openaccess.thecvf.com//content/WACV2025/html/Yu_SGD_Street_View_Synthesis_with_Gaussian_Splatting_and_Diffusion_Prior_WACV_2025_paper.html	Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Jiale Cao, Zhong Ji, Mingming Sun
SHIP: Structural Hierarchies for Instance-Dependent Partial Labels	Partial label learning (PLL) aims to train classification models under conditions where each training sample is associated with a candidate set of labels. This set contains multiple labels among which only one is correct. This work addresses instance-dependent noise in PLL by leveraging hierarchical structures within the label space. We introduce a method to derive label hierarchies from instance-dependent partial labels. Subsequently we propose Structural Hierarchies for Instance-dependent Partial label (SHIP). SHIP is a modular component that integrates into deep learning architectures with applications that have intrinsic hierarchies. SHIP harnesses label hierarchy to enhance instance-dependent PLL performance across various deep-learning algorithms with hierarchy in the dataset. We conduct experiments on five publicly available benchmark datasets with four recent PLL algorithms. Experimental results show that incorporating SHIP into state-of-the-art architectures yields up to a 2.6% improvement in accuracy when hierarchies are present in the data. Moreover when the number of classes is high SHIP achieves up to a 2.5% reduction in mean mistake severity highlighting its effectiveness in mitigating error severity.	https://openaccess.thecvf.com//content/WACV2025/html/Kadam_SHIP_Structural_Hierarchies_for_Instance-Dependent_Partial_Labels_WACV_2025_paper.html	Tushar Kadam, Utkarsh Mishra, Aakarsh Malhotra
SIGNN - Star Identification using Graph Neural Networks	As a solution for the lost-in-space star identification problem we present Star Identification using Graph Neural Network (SIGNN) a novel approach using Graph Attention Networks. By representing the celestial sphere as a graph data structure created from the ESA's Hipparcos catalogue we are able to accurately capture the rich information and relationships within local star fields. Graph learning techniques allow our model to aggregate information and learn the relative importance of the nodes and structure within each stars local neighbourhood to it's identification. This approach combined with our parametric data-generation and noise simulation allows us to train a highly robust model capable of accurate star identification even under intensive noise outperforming existing methods. Code and generation techniques will be available on https://github.com/FloydHepburn/SIGNN.	https://openaccess.thecvf.com//content/WACV2025/html/Hepburn-Dickins_SIGNN_-_Star_Identification_using_Graph_Neural_Networks_WACV_2025_paper.html	Floyd Hepburn-Dickins, Mark W. Jones, Mike Edwards, Jay Paul Morgan, Steve Bell
SMDAF: A Scalable Sidewalk Material Data Acquisition Framework with Bidirectional Cross-Modal Knowledge Distillation	Ensuring safe and independent navigation poses considerable difficulties for individuals who are blind or have low vision (BLV) as it requires detailed knowledge of their immediate environment. Our research highlights the critical need for accessible data on sidewalk materials and objects which is currently lacking in existing map services. To bridge this gap we present the Sidewalk Material Data Acquisition Framework (SMDAF) designed for large-scale data collection. This framework includes (1) a lightweight data collection system embedded in a white cane which captures audio data through the interaction of the cane tip with the sidewalk surface and a mobile app that facilitates data storage and management resulting in a novel multimodal dataset comprising both image and audio data; and (2) a unique Cross-Modal Knowledge Distillation (CMKD) technique for an enhanced audio material classifier. Our CMKD approach employs an image-based model as the teacher to improve the audio model incorporating an Enhanced Bidirectional learning method with an intuitive filtering technique: Bidirectional Correct Sample Filtering (BCSF). BCSF filters correct samples to prevent the distillation of incorrect knowledge addressing the issue of inaccurate cross-modal learning. This novel approach has resulted in a 1.84% improvement in Macro Accuracy achieving an overall accuracy of 87.62% surpassing all state-of-the-art KD and CMKD methods. This study underscores the efficacy of SMDAF and provides a practical CMKD technique for future cross-modal learning tasks. Code and dataset are available https://github.com/FgSurewin/SMDAF-CMKD.	https://openaccess.thecvf.com//content/WACV2025/html/Liu_SMDAF_A_Scalable_Sidewalk_Material_Data_Acquisition_Framework_with_Bidirectional_WACV_2025_paper.html	Jiawei Liu, Wayne Lam, Zhigang Zhu, Hao Tang
SODA: Spectral Orthogonal Decomposition Adaptation for Diffusion Models	Adapting large-scale pre-trained generative models in a parameter-efficient manner is gaining traction. Traditional methods like low rank adaptation achieve parameter efficiency by imposing constraints but may not be optimal for tasks requiring high representation capacity. We propose a novel spectrum-aware adaptation framework for generative models. Our method adjusts both singular values and their basis vectors of pretrained weights. Using the Kronecker product and efficient Stiefel optimizers we achieve parameter-efficient adaptation of orthogonal matrices. Specifically we introduce Spectral Orthogonal Decomposition Adaptation (SODA) which balances computational efficiency and representation capacity. Extensive evaluations on text-to-image diffusion models demonstrate SODA's effectiveness offering a spectrum-aware alternative to existing fine-tuning methods.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_SODA_Spectral_Orthogonal_Decomposition_Adaptation_for_Diffusion_Models_WACV_2025_paper.html	Xinxi Zhang, Song Wen, Ligong Han, Felix Juefei-Xu, Akash Srivastava, Junzhou Huang, Vladimir Pavlovic, Hao Wang, Molei Tao, Dimitris Metaxas
SPACE: SPAtial-Aware Consistency rEgularization for Anomaly Detection in Industrial Applications	In this paper we propose SPACE a novel anomaly detection methodology that integrates a Feature Encoder (FE) into the structure of the Student-Teacher method. The proposed method has two key elements: Spatial Consistency regularization Loss (SCL) and Feature converter Module (FM). SCL prevents overfitting in student models by avoiding excessive imitation of the teacher model. Simultaneously it facilitates the expansion of normal data features by steering clear of abnormal areas generated through data augmentation. This dual functionality ensures a robust boundary between normal and abnormal data. The FM prevents the learning of ambiguous information from the FE. This protects the learned features and enables more effective detection of structural and logical anomalies. Through these elements SPACE is available to minimize the influence of the FE while integrating various data augmentations. In this study we evaluated the proposed method on the MVTec LOCO MVTec AD and VisA datasets. Experimental results through qualitative evaluation demonstrate the superiority of detection and efficiency of each module compared to state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_SPACE_SPAtial-Aware_Consistency_rEgularization_for_Anomaly_Detection_in_Industrial_Applications_WACV_2025_paper.html	Daehwan Kim, Hyungmin Kim, Daun Jeong, Sungho Suh, Hansang Cho
STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation	In layout-to-image (L2I) synthesis controlled complex scenes are generated from coarse information like bounding boxes. Such a task is exciting to many downstream applications because the input layouts offer strong guidance to the generation process while remaining easily reconfigurable by humans. In this paper we proposed STyled LAYout Diffusion (STAY Diffusion) a diffusion-based model that produces photo-realistic images and provides fine-grained control of stylized objects in scenes. Our approach learns a global condition for each layout and a self-supervised semantic map for weight modulation using a novel Edge-Aware Normalization (EA Norm). A new Styled-Mask Attention (SM Attention) is also introduced to cross-condition the global condition and image feature for capturing the objects' relationships. These measures provide consistent guidance through the model enabling more accurate and controllable image generation. Extensive benchmarking demonstrates that our STAY Diffusion presents high-quality images while surpassing previous state-of-the-art methods in generation diversity accuracy and controllability.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_STAY_Diffusion_Styled_Layout_Diffusion_Model_for_Diverse_Layout-to-Image_Generation_WACV_2025_paper.html	Ruyu Wang, Xuefeng Hou, Sabrina Schmedding, Marco Huber
STLight: A Fully Convolutional Approach for Efficient Predictive Learning by Spatio-Temporal Joint Processing	Spatio-Temporal predictive Learning is a self-supervised learning paradigm that enables models to identify spatial and temporal patterns by predicting future frames based on past frames. Traditional methods which use recurrent neural networks to capture temporal patterns have proven their effectiveness but come with high system complexity and computational demand. Convolutions could offer a more efficient alternative but are limited by their characteristic of treating all previous frames equally resulting in poor temporal characterization and by their local receptive field limiting the capacity to capture distant correlations among frames. In this paper we propose STLight a novel method for spatio-temporal learning that relies solely on channel-wise and depth-wise convolutions as learnable layers. STLight overcomes the limitations of traditional convolutional approaches by rearranging spatial and temporal dimensions together using a single convolution to mix both types of features into a comprehensive spatio-temporal patch representation. This representation is then processed in a purely convolutional framework capable of focusing simultaneously on the interaction among near and distant patches and subsequently allowing for efficient reconstruction of the predicted frames. Our architecture achieves state-of-the-art performance on STL benchmarks across different datasets and settings while significantly improving computational efficiency in terms of parameters and computational FLOPs. The code is publicly available	https://openaccess.thecvf.com//content/WACV2025/html/Alfarano_STLight_A_Fully_Convolutional_Approach_for_Efficient_Predictive_Learning_by_WACV_2025_paper.html	Andrea Alfarano, Alberto Alfarano, Linda Friso, Andrea Bacciu, Irene Amerini, Fabrizio Silvestri
STRIDE: Single-Video Based Temporally Continuous Occlusion-Robust 3D Pose Estimation	The capability to accurately estimate 3D human poses is crucial for diverse fields such as action recognition gait recognition and virtual/augmented reality. However a persistent and significant challenge within this field is the accurate prediction of human poses under conditions of severe occlusion. Traditional image-based estimators struggle with heavy occlusions due to a lack of temporal context resulting in inconsistent predictions. While video-based models benefit from processing temporal data they encounter limitations when faced with prolonged occlusions that extend over multiple frames. This challenge arises because these models struggle to generalize beyond their training datasets and the variety of occlusions is hard to capture in the training data. Addressing these challenges we propose STRIDE (Single-video based TempoRally contInuous occlusion Robust 3D Pose Estimation) a novel Test-Time Training (TTT) approach to fit a human motion prior for each video. This approach specifically handles occlusions that were not encountered during the model's training. By employing STRIDE we can refine a sequence of noisy initial pose estimates into accurate temporally coherent poses during test time effectively overcoming the limitations of prior methods. Our framework demonstrates flexibility by being model-agnostic allowing us to use any off-the-shelf 3D pose estimation method for improving robustness and temporal consistency. We validate STRIDE's efficacy through comprehensive experiments on challenging datasets like Occluded Human3.6M Human3.6M and OCMotion where it not only outperforms existing single-image and video-based pose estimation models but also showcases superior handling of substantial occlusions achieving fast robust accurate and temporally consistent 3D pose estimates. Code is made publicly available at https://github.com/take2rohit/stride	https://openaccess.thecvf.com//content/WACV2025/html/Lal_STRIDE_Single-Video_Based_Temporally_Continuous_Occlusion-Robust_3D_Pose_Estimation_WACV_2025_paper.html	Rohit Lal, Saketh Bachu, Yash Garg, Arindam Dutta, Calvin-Khang Ta, Hannah Dela Cruz, Dripta S. Raychaudhuri, M. Salman Asif, Amit Roy-Chowdhury
SUM: Saliency Unification through Mamba for Visual Attention Modeling	Visual attention modeling important for interpreting and prioritizing visual stimuli plays a significant role in applications such as marketing multimedia and robotics. Traditional saliency prediction models especially those based on Convolutional Neural Networks (CNNs) or Transformers achieve notable success by leveraging large-scale annotated datasets. However the current state-of-the-art (SOTA) models that use Transformers are computationally expensive. Additionally separate models are often required for each image type lacking a unified approach. In this paper we propose Saliency Unification through Mamba (SUM) a novel approach that integrates the efficient long-range dependency modeling of Mamba with U-Net to provide a unified model for diverse image types. Using a novel Conditional Visual State Space (C-VSS) block SUM dynamically adapts to various image types including natural scenes web pages and commercial imagery ensuring universal applicability across different data types. Our comprehensive evaluations across five benchmarks demonstrate that SUM seamlessly adapts to different visual characteristics and consistently outperforms existing models. These results position SUM as a versatile and powerful tool for advancing visual attention modeling offering a robust solution universally applicable across different types of visual content. Our codebase and pretrained models are publicly accessible on the https://arhosseini77.github.io/sum_page/.	https://openaccess.thecvf.com//content/WACV2025/html/Hosseini_SUM_Saliency_Unification_through_Mamba_for_Visual_Attention_Modeling_WACV_2025_paper.html	Alireza Hosseini, Amirhossein Kazerouni, Saeed Akhavan, Michael Brudno, Babak Taati
SV-data2vec: Guiding Video Representation Learning with Latent Skeleton Targets	Recent advancements in action recognition leverage both skeleton and video modalities to achieve state-of-the-art performance. However due to the challenges of early fusion which tends to underutilize the strengths of each modality existing methods often resort to late fusion consequently leading to more complex designs. Additionally self-supervised learning approaches utilizing both modalities remain underexplored. In this paper we introduce SV-data2vec a novel self-supervised framework for learning from skeleton and video data. Our approach employs a student-teacher architecture where the teacher network generates contextualized targets based on skeleton data. The student network performs a masked prediction task using both skeleton and visual data. Remarkably after pretraining with both modalities our method allows for fine-tuning with RGB data alone achieving results on par with multimodal approaches by effectively learning video representations through skeleton data guidance. Extensive experiments on benchmark datasets NTU RGB+D 60 NTU RGB+D 120 and Toyota Smarthome confirm that our method outperforms existing RGB based state-of-the-art techniques. The code is available at github.com/zoranadozdor/SVdata2vec.	https://openaccess.thecvf.com//content/WACV2025/html/Dozdor_SV-data2vec_Guiding_Video_Representation_Learning_with_Latent_Skeleton_Targets_WACV_2025_paper.html	Zorana DoÅ¾dor, Tomislav Hrkac, Zoran Kalafatic
Scene-LLM: Extending Language Model for 3D Visual Reasoning	This paper introduces Scene-LLM a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a unified 3D visual feature representation that incorporates dense spatial information and supports scene state updates. The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space enabling effective interpretation of 3D visual information. Unique to our approach is the integration of both scene-level and egocentric 3D information with a compact hybrid representation. This combination is pivotal for interactive planning where scene-level data supports global planning and egocentric data is important for localization. Notably we use egocentric 3D frame features for feature alignment an efficient technique that incorporates the model with fine-grained concepts. Our experiments with Scene-LLM demonstrate its strong capabilities in scene captioning question answering and interactive planning. We believe Scene-LLM advances the field of 3D visual understanding and reasoning offering new possibilities for sophisticated agent interactions in indoor settings.	https://openaccess.thecvf.com//content/WACV2025/html/Fu_Scene-LLM_Extending_Language_Model_for_3D_Visual_Reasoning_WACV_2025_paper.html	Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, Wenhan Xiong
SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution	Implicit Neural Representations (INRs) have recently advanced the field of deep learning due to their ability to learn continuous representations of signals without the need for large training datasets. Although INR methods have been studied for medical image super-resolution their adaptability to localized priors in medical images has not been extensively explored. Medical images contain rich anatomical divisions that could provide valuable local prior information to enhance the accuracy and robustness of INRs. In this work we propose a novel framework referred to as the Semantically Conditioned INR (SeCo-INR) that conditions an INR using local priors from a medical image enabling accurate model fitting and interpolation capabilities to achieve super-resolution. Our framework learns a continuous representation of the semantic segmentation features of a medical image and utilizes it to derive the optimal INR for each semantic region of the image. We tested our framework using several medical imaging modalities and achieved higher quantitative scores and more realistic super-resolution outputs compared to state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Ekanayake_SeCo-INR_Semantically_Conditioned_Implicit_Neural_Representations_for_Improved_Medical_Image_WACV_2025_paper.html	Mevan Ekanayake, Zhifeng Chen, Gary Egan, Mehrtash Harandi, Zhaolin Chen
Secrets of Edge-Informed Contrast Maximization for Event-Based Vision	Event cameras capture the motion of intensity gradients (edges) in the image plane in the form of rapid asynchronous events. When accumulated in 2D histograms these events depict overlays of the edges in motion consequently obscuring the spatial structure of the generating edges. Contrast maximization (CM) is an optimization framework that can reverse this effect and produce sharp spatial structures that resemble the moving intensity gradients by estimating the motion trajectories of the events. Nonetheless CM is still an underexplored area of research with avenues for improvement. In this paper we propose a novel hybrid approach that extends CM from uni-modal (events only) to bi-modal (events and edges). We leverage the underpinning concept that given a reference time optimally warped events produce sharp gradients consistent with the moving edge at that time. Specifically we formalize a correlation-based objective to aid CM and provide key insights into the incorporation of multiscale and multireference techniques. Moreover our edge-informed CM method yields superior sharpness scores and establishes new state-of-the-art event optical flow benchmarks on the MVSEC DSEC and ECD datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Karmokar_Secrets_of_Edge-Informed_Contrast_Maximization_for_Event-Based_Vision_WACV_2025_paper.html	Pritam P. Karmokar, Quan H. Nguyen, William J. Beksi
Seeing Eye to AI: Comparing Human Gaze and Model Attention in Video Memorability	Understanding what makes a video memorable has important applications in advertising and education technology. Towards this goal we investigate spatio-temporal attention mechanisms underlying video memorability. Different from previous works that fuse multiple features we adopt a simple CNN+Transformer architecture that enables analysis of spatio-temporal attention while matching state-of-the-art (SoTA) performance on video memorability prediction. We compare model attention against human gaze fixations collected through a small-scale eye-tracking study where humans perform the video memory task. We uncover the following insights: (i) Quantitative saliency metrics show that our model trained only to predict a memorability score exhibits similar spatial attention patterns to human gaze especially for more memorable videos. (ii) The model assigns greater importance to initial frames in a video mimicking human attention patterns. (iii) Panoptic segmentation reveals that both (model and humans) assign a greater share of attention to things and less attention to stuff as compared to their occurrence probability.	https://openaccess.thecvf.com//content/WACV2025/html/Kumar_Seeing_Eye_to_AI_Comparing_Human_Gaze_and_Model_Attention_WACV_2025_paper.html	Prajneya Kumar, Eshika Khandelwal, Makarand Tapaswi, Vishnu Sreekumar
SegBuilder: A Semi-Automatic Annotation Tool for Segmentation	This paper addresses the problem of image annotation for segmentation tasks. Semantic segmentation involves labeling each pixel in an image with predefined categories such as sky cars roads and humans. Deep learning models require numerous annotated images for effective training but manual annotation is slow and time-consuming. To mitigate this challenge we leverage the Segment Anything Model (SAM)- a vision foundation model. We introduce SegBuilder a framework that incorporates SAM to automatically generate segments which are then tagged by human annotators using a quick selection list. To demonstrate SegBuilder's effectiveness we introduced a novel dataset for image segmentation in underwater environments featuring animals such as sea lions beavers and jellyfish. Experiments on this dataset showed that SegBuilder significantly speeds up the annotation process compared to the publicly available tool Label Studio. SegBuilder also includes a free-form drawing tool allowing users to create correct segments missed by SAM. This feature is particularly useful for scenes with shadows camouflaged objects and part-based segmentation tasks where SAM falls short. Experimentally we demonstrated SegBuilder's efficacy in these scenarios showcasing its potential for generating pixel-wise annotations crucial for training robust deep learning models for semantic segmentation.	https://openaccess.thecvf.com//content/WACV2025/html/Reza_SegBuilder_A_Semi-Automatic_Annotation_Tool_for_Segmentation_WACV_2025_paper.html	Md Alimoor Reza, Eric Manley, Sean Chen, Sameer Chaudhary, Jacob Elafros
SegDesicNet: Lightweight Semantic Segmentation in Remote Sensing with Geo-Coordinate Embeddings for Domain Adaptation	Semantic segmentation is essential for analyzing high-definition remote sensing images (HRSIs) because it allows the precise classification of objects and regions at the pixel level. However remote sensing data present challenges owing to geographical location weather and environmental variations making it difficult for semantic segmentation models to generalize across diverse scenarios. Existing methods are often limited to specific data domains and require expert annotators and specialized equipment for semantic labeling. In this study we propose a novel unsupervised domain adaptation technique for remote sensing semantic segmentation by utilizing geographical coordinates that are readily accessible in remote sensing setups as metadata in a dataset. To bridge the domain gap we propose a novel approach that considers the combination of an image's location-encoding trait and the spherical nature of Earth's surface. Our proposed SegDesicNet module regresses the GRID positional encoding of the geocoordinates projected over the unit sphere to obtain the domain loss. Our experimental results demonstrate that the proposed SegDesicNet outperforms state-of-the-art domain adaptation methods in remote sensing image segmentation achieving an improvement of approximately 6% in the mean intersection over union (MIoU) with a 27% drop in parameter count on benchmarked subsets of the publicly available FLAIR #1 dataset. We also benchmarked our method performance on the custom split of the ISPRS Potsdam dataset. Our algorithm seeks to reduce the modeling disparity between artificial neural networks and human comprehension of the physical world making the technology more human-centric and scalable.	https://openaccess.thecvf.com//content/WACV2025/html/Verma_SegDesicNet_Lightweight_Semantic_Segmentation_in_Remote_Sensing_with_Geo-Coordinate_Embeddings_WACV_2025_paper.html	Sachin Verma, Frank Lindseth, Gabriel Kiss
Segment Anything Meets Point Tracking	Foundation models have marked a significant stride toward addressing generalization challenges in deep learning. While the Segment Anything Model (SAM) has established a strong foothold in image segmentation existing video segmentation methods still require extensive mask labeling for fine-tuning or face performance drops on unseen data domains otherwise. In this paper we show how foundation models for image segmentation make a step toward enhancing domain generalizability in video segmentation. We discover that combined with long-term point tracking image segmentation models yield state-of-the-art results in zero-shot video segmentation across multiple benchmarks. Surprisingly point trackers exhibit generalization to domains beyond their synthetic pre-training sequences which we attribute to the trackers' ability to harness the rich local information in the vicinity of each tracked point. Thus we introduce SAM-PT an innovative method for point-centric video segmentation leveraging the capabilities of SAM alongside long-term point tracking. SAM-PT extends SAM's capability to tracking and segmenting anything in dynamic videos. Unlike traditional video segmentation methods that focus on object-centric mask propagation our approach uniquely exploits point propagation to utilize local structure information independent of object semantics. The effectiveness of point-based tracking is underscored by direct evaluation on the zero-shot open-world UVO benchmark. Our experiments on popular video object segmentation and multi-object segmentation tracking benchmarks including DAVIS YouTube-VOS and BDD100K suggest that a point-based segmentation tracker yields better zero-shot performance and efficient interactions. We release our code at https://github.com/SysCV/sam-pt.	https://openaccess.thecvf.com//content/WACV2025/html/Rajic_Segment_Anything_Meets_Point_Tracking_WACV_2025_paper.html	Frano RajiÄ, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu
Self-Aligning Depth-Regularized Radiance Fields for Asynchronous RGB-D Sequences	It has been shown that learning radiance fields with depth rendering and depth supervision can effectively promote the quality and convergence of view synthesis. However this paradigm requires input RGB-D sequences to be synchronized. In the UAV city modeling scenario there exists asynchrony between RGB images and depth images due to the different frequencies of the solid-state LiDAR and RGB sensors. To synthesize high-quality views in such a scenario we propose a novel time-pose function which is an implicit network that maps timestamps to SE(3) elements. To train this function we also design a joint optimization scheme to jointly learn the large-scale depth-regularized radiance fields and the time-pose function. Furthermore we propose a large synthetic dataset with diverse controlled mismatches and ground truth to evaluate this new problem setting systematically. The proposed approach has been evaluated on both datasets and in a real drone. To evaluate the impact of view density each algorithm was test on three different trajectories with different view densities. Compared to state-of-the-art baseline methods the proposed approach reduces reconstruction error by 35.26% in city modeling scenarios. Our code is available at github.com/saythe17/AsyncNeRF.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_Self-Aligning_Depth-Regularized_Radiance_Fields_for_Asynchronous_RGB-D_Sequences_WACV_2025_paper.html	Yuxin Huang, Andong Yang, Yuantao Chen, Runyi Yang, Zhenxin Zhu, Chao Hou, Hao Zhao, Guyue Zhou
Self-Relaxed Joint Training: Sample Selection for Severity Estimation with Ordinal Noisy Labels	"Severity level estimation is a crucial task in medical image diagnosis. However accurately assigning severity class labels to individual images is very costly and challenging. Consequently the attached labels tend to be noisy. In this paper we propose a new framework for training with ""ordinal"" noisy labels. Since severity levels have an ordinal relationship we can leverage this to train a classifier while mitigating the negative effects of noisy labels. Our framework uses two techniques: clean sample selection and dual-network architecture. A technical highlight of our approach is the use of soft labels derived from noisy hard labels. By appropriately using the soft and hard labels in the two techniques we achieve more accurate sample selection and robust network training. The proposed method outperforms various state-of-the-art methods in experiments using two endoscopic ulcerative colitis (UC) datasets and a retinal Diabetic Retinopathy (DR) dataset. Our codes are available at https://github.com/shumpei-takezaki/Self-Relaxed-Joint-Training."	https://openaccess.thecvf.com//content/WACV2025/html/Takezaki_Self-Relaxed_Joint_Training_Sample_Selection_for_Severity_Estimation_with_Ordinal_WACV_2025_paper.html	Shumpei Takezaki, Kiyohito Tanaka, Seiichi Uchida
Self-Supervised Anomaly Segmentation via Diffusion Models with Dynamic Transformer UNet	A robust anomaly detection mechanism should possess the capability to effectively remediate anomalies restoring them to a healthy state while preserving essential healthy information. Despite the efficacy of existing generative models in learning the underlying distribution of healthy reference data they face primary challenges when it comes to efficiently repair larger anomalies or anomalies situated near high pixel-density regions. In this paper we introduce a self-supervised anomaly detection method based on a diffusion model that samples from multi-frequency four dimensional simplex noise and makes predictions using our proposed Dynamic Transformer UNet (DTUNet). This simplex-based noise function helps address primary problems to some extent and is scalable for three-dimensional and colored images. In the evolution of ViT our developed architecture serving as the backbone for the diffusion model is tailored to treat time and noise image patches as tokens. We incorporate long skip connections bridging the shallow and deep layers along with smaller skip connections within these layers. Furthermore we integrate a partial diffusion Markov process which reduces sampling time thus enhancing scalability. Our method surpasses existing generative-based anomaly detection methods across three diverse datasets which include BrainMRI Brats2021 and the MVtec dataset. It achieves an average improvement of +10.1% in Dice coefficient +10.4% in IOU and +9.6% in AUC. Our source code is made publicly available on Github.	https://openaccess.thecvf.com//content/WACV2025/html/Kumar_Self-Supervised_Anomaly_Segmentation_via_Diffusion_Models_with_Dynamic_Transformer_UNet_WACV_2025_paper.html	Komal Kumar, Snehashis Chakraborty, Dwarikanath Mahapatra, Behzad Bozorgtabar, Sudipta Roy
Self-Supervised Incremental Learning of Object Representations from Arbitrary Image Sets	Computing a comprehensive and robust visual representation of an arbitrary object or category of objects is a complex problem. The difficulty increases when one starts from a set of uncalibrated images obtained from different sources. We propose a self-supervised approach Multi-Image Latent Embedding (MILE) which computes a single representation from such an image set. MILE operates incrementally considering one image at a time while processing various depictions of the class through a shared gated cross-attention mechanism. The representations are progressively refined as more available images are incorporated without requiring additional training. Our experiments on Amazon Berkeley Objects (ABO) and iNaturalist demonstrate the effectiveness in two tasks: object or category-specific image retrieval and unsupervised context-conditioned object segmentation. Moreover the proposed multi-image input setup opens new frontiers for the task of object retrieval. Our studies indicate that our models can capture descriptive representations that better encapsulate the intrinsic characteristics of the objects. Our code is available at https://github.com/amazon-science/mile.	https://openaccess.thecvf.com//content/WACV2025/html/Leotescu_Self-Supervised_Incremental_Learning_of_Object_Representations_from_Arbitrary_Image_Sets_WACV_2025_paper.html	George Leotescu, Alin-Ionut Popa, Diana-Nicoleta N Grigore, Daniel Voinea, Pietro Perona
Self-Supervised Learning with Probabilistic Density Labeling for Rainfall Probability Estimation	Numerical weather prediction (NWP) models are fundamental in meteorology for simulating and forecasting the behavior of various atmospheric variables. The accuracy of precipitation forecasts and the acquisition of sufficient lead time are crucial for preventing hazardous weather events. However the performance of NWP models is limited by the nonlinear and unpredictable patterns of extreme weather phenomena driven by temporal dynamics. In this regard we propose a Self-Supervised Learning with Probabilistic Density Labeling (SSLPDL) for estimating rainfall probability by post-processing NWP forecasts. Our post-processing method uses self-supervised learning (SSL) with masked modeling for reconstructing atmospheric physics variables enabling the model to learn the dependency between variables. The pre-trained encoder is then utilized in transfer learning to a precipitation segmentation task. Furthermore we introduce a straightforward labeling approach based on probability density to address the class imbalance in extreme weather phenomena like heavy rain events. Experimental results show that SSLPDL surpasses other precipitation forecasting models in regional precipitation post-processing and demonstrates competitive performance in extending forecast lead times. Our code is available at https://github.com/joonha425/SSLPDL.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Self-Supervised_Learning_with_Probabilistic_Density_Labeling_for_Rainfall_Probability_Estimation_WACV_2025_paper.html	Junha Lee, Sojung An, Sujeong You, Namik Cho
Self-Supervised Learning with Spectral Low-Rank Prior for Hyperspectral Image Reconstruction	Hyperspectral image (HSI) reconstruction from coded measurement is significant for acquiring images with higher spectral resolution than traditional RGB images. Current advanced neural networks have already shown impressive performance in some datasets like CAVE and KAIST. However these networks rely on a large amount of simulated ground truth measurement pairs. Unfortunately in some scenarios it is hard to obtain a sufficient high-quality HSI training set resulting in low generalization ability. Although iterative algorithms show good generalization ability they are limited by slow speed and low reconstruction quality. To address this challenge in this paper we propose a self-supervised learning framework which can train and fine-tune networks using measurements without ground truth. Besides we propose the spectral low-rank loss function that enables networks to learn the signal model of HSI. Finally we train and fine-tune a representative deep unfolding network GAP-net using our proposed framework. Extensive simulation and real data results show that the proposed self-supervised framework is capable of achieving results competitive with those of supervised networks. Code is available at https://github.com/zjhe02/CASSI-SSL.	https://openaccess.thecvf.com//content/WACV2025/html/He_Self-Supervised_Learning_with_Spectral_Low-Rank_Prior_for_Hyperspectral_Image_Reconstruction_WACV_2025_paper.html	Zijun He, Lishun Wang, Ziyi Meng, Xin Yuan
Self-Supervised Pre-Training with Diffusion Model for Few-Shot Landmark Detection in X-Ray Images	Deep neural networks have been extensively applied in the medical domain for various tasks including image classification segmentation and landmark detection. However their application is often hindered by data scarcity both in terms of available annotations and images. This study introduces a novel application of denoising diffusion probabilistic models (DDPMs) to the landmark detection task specifically addressing the challenge of limited annotated data in x-ray imaging. Our key innovation lies in leveraging DDPMs for self-supervised pre-training in landmark detection a previously unexplored approach in this domain. This method enables accurate landmark detection with minimal annotated training data (as few as 50 images) surpassing both ImageNet supervised pre-training and traditional self-supervised techniques across three popular x-ray benchmark datasets. To our knowledge this work represents the first application of diffusion models for self-supervised learning in landmark detection which may offer a valuable pre-training approach in few-shot regimes for mitigating data scarcity. To bolster further development and reproducibility we provide open access to our code and pre-trained models for a variety of x-ray related applications: https://github.com/Malga-Vision/DiffusionXray-FewShot-LandmarkDetection.	https://openaccess.thecvf.com//content/WACV2025/html/Di_Via_Self-Supervised_Pre-Training_with_Diffusion_Model_for_Few-Shot_Landmark_Detection_in_WACV_2025_paper.html	Roberto Di Via, Francesca Odone, Vito Paolo Pastore
Semantic Clustering of Image Retrieval Databases used for Visual Localization	Accurate self-localization of unmanned aerial systems (UAS) is needed to reduce their dependency on global navigation satellite systems (GNSS). Image retrieval techniques comparing aerial images with a reference database can be used for visual localization (VL). But the search space may be vast and a full search not feasible on a small UAS. In this work we propose a novel solution that divides the reference database into smaller clusters based on the semantic content of images. To this end we generate and make use of a dataset for semantic segmentation of aerial image captures. By characterizing scenes and objects in images semantically retrieval-based systems are able to differentiate images and scenes efficiently. Using a divide-and-conquer approach images with similar semantics are matched within smaller partial databases. This technique leads to reduced search times and approaches VL as a feasible solution for UAS localization in large-scale outdoor environments.	https://openaccess.thecvf.com//content/WACV2025/html/Holzemann_Semantic_Clustering_of_Image_Retrieval_Databases_used_for_Visual_Localization_WACV_2025_paper.html	Henry HÃ¶lzemann, Torsten Fiolka
Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation	Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each object category. In this way SemPLeS can perform better semantic alignment between object regions and class labels resulting in desired pseudo masks for training segmentation models. The proposed SemPLeS framework achieves competitive performance on standard WSSS benchmarks PASCAL VOC 2012 and MS COCO 2014 and shows compatibility with other WSSS methods. Project page: https://projectdisr.github.io/semples/	https://openaccess.thecvf.com//content/WACV2025/html/Lin_Semantic_Prompt_Learning_for_Weakly-Supervised_Semantic_Segmentation_WACV_2025_paper.html	Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen
Semantic Prompting with Image Token for Continual Learning	Continual learning aims to refine model parameters for new tasks while retaining knowledge from previous tasks. Recently prompt-based learning has emerged to leverage pre-trained models to be prompted to learn subsequent tasks without the reliance on the rehearsal buffer. Although this approach has demonstrated outstanding results existing methods depend on preceding task-selection process to choose appropriate prompts. However imperfectness in task-selection may lead to negative impacts on the performance particularly in the scenarios where the number of tasks is large or task distributions are imbalanced. To address this issue we introduce a novel task-agnostic approach that focuses on the visual semantic information of image tokens eliminating the preceding task prediction. By leveraging the ability of the pre-trained model to discriminate between similar tokens our method not only subdivides the prompt but also eliminates the need for additional forward pass. Consequently we achieve competitive performance on four benchmarks while significantly reducing training time compared to state-of-the-art methods. The code is available at https://github.com/pilsHan/I-Prompt	https://openaccess.thecvf.com//content/WACV2025/html/Han_Semantic_Prompting_with_Image_Token_for_Continual_Learning_WACV_2025_paper.html	Jisu Han, Jaemin Na, Wonjun Hwang
Semantic Segmentation Method for Automated Indoor 3D Reconstruction Based on Architectural-Knowledge-Aware Features	3D point cloud semantic segmentation is an important step for 3D indoors reconstruction. In recent years many outstanding deep learning models have been proposed for semantic segmentation which can achieve remarkable performance. However it is found the index indicating semantic prediction accuracy in terms of structural components (e.g. columns beams etc.) in buildings is far from satisfying lacking sufficient information for subsequent 3D reconstruction. For better segmenting and identifying structural components this work proposes Architectural-Knowledge-Aware features (AKAFs) i.e. F1 and F2 which are strategically incorporated into a developed two-stage training framework wherein outstanding semantic segmentation models are adopted as backbones. By incorporating F1 which formalizes the position distribution pattern of building structural components semantic information can be explored preliminary in the explicit stage (i.e. the first stage). The second stage is the implicit stage where F2 is derived based on the Semantic and Relative Position Fusing Module (SRPFM) which implicitly introduces more relative position information of building components for semantic segmentation. Extensive experiments have been conducted on the S3DIS dataset adopting three outstanding backbones. Results demonstrate that the proposed AKAFs can increase the accuracy of segmentation for structural components by more than 5%. As a result the overall semantic segmentation performance increases by 2%. Even on the current SOTA model (PT-v3) the proposed AKAFs show promising semantic segmentation promotion. Portable and well-compatible with most point-based semantic segmentation models AKAFs can effectively perceive more architectural characteristics hidden in indoor point clouds especially prominent for points of structural components.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_Semantic_Segmentation_Method_for_Automated_Indoor_3D_Reconstruction_Based_on_WACV_2025_paper.html	Yahan Chen, Wenzheng Liu, Xiaowei Luo
Semantically Conditioned Prompts for Visual Recognition under Missing Modality Scenarios	This paper tackles the domain of multimodal prompting for visual recognition specifically when dealing with missing modalities through multimodal Transformers. It presents two main contributions: (i) we introduce a novel prompt learning module which is designed to produce sample-specific prompts and (ii) we show that modality-agnostic prompts can effectively adjust to diverse missing modality scenarios. Our model termed SCP exploits the semantic representation of available modalities to query a learnable memory bank which allows the generation of prompts based on the semantics of the input. Notably SCP distinguishes itself from existing methodologies for its capacity of self-adjusting to both the missing modality scenario and the semantic context of the input without prior knowledge about the specific missing modality and the number of modalities. Through extensive experiments we show the effectiveness of the proposed prompt learning framework and demonstrate enhanced performance and robustness across a spectrum of missing modality cases. Our source code is available at https://github.com/vittoriopipoli/SCP_WACV2025.	https://openaccess.thecvf.com//content/WACV2025/html/Pipoli_Semantically_Conditioned_Prompts_for_Visual_Recognition_under_Missing_Modality_Scenarios_WACV_2025_paper.html	Vittorio Pipoli, Federico Bolelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Costantino Grana, Rita Cucchiara, Elisa Ficarra
Semiotic-Based Construction of a Large Emotional Image Dataset with Neutral Samples	Image Visual Sentiment Analysis (VSA) requires the availability of large annotated datasets whose construction presents many challenges. The necessity of gathering a large amount of labeled images contrasts with the rigorous but lengthy process required for manual annotation based on psychovisual experiments and with the automatic gathering of large amounts of data roughly labeled based on the sentiment analysis of the text accompanying the images like captions tweets and tags. An additional limitation is the scarcity of high-quality datasets with a neutral class which forces the images to be classified into emotions even when the observers show no emotional activation. In this work we present a scalable methodology rooted in semiotics and art theory for the construction of a 3-class (positive negative and neutral) VSA dataset enabling the downloading of a desired quantity of images while maintaining labeling coherence and accuracy. Based on the proposed methodology we introduce and make publicly available a VSA dataset of over 100000 images. To validate the quality of the dataset we used it to train several classifiers and compared their performance with those of classifiers trained on other datasets. The results show that the classifiers trained on the new dataset provide better performance when tested on independent datasets including those commonly used for psychovisual experiments.	https://openaccess.thecvf.com//content/WACV2025/html/Blanchini_Semiotic-Based_Construction_of_a_Large_Emotional_Image_Dataset_with_Neutral_WACV_2025_paper.html	Marco Blanchini, Giovanna Dimitri, Lydia Abady, Benedetta Tondi, Tarcisio Lancioni, Mauro Barni
SenCLIP: Enhancing Zero-Shot Land-Use Mapping for Sentinel-2 with Ground-Level Prompting	"Pre-trained vision-language models (VLMs) such as CLIP demonstrate impressive zero-shot classification capabilities with free-form prompts and even show some generalization in specialized domains. However their performance on satellite imagery is limited due to the under representation of such data in their training sets which predominantly consist of ground-level images. Existing prompting techniques for satellite imagery are often restricted to generic phrases like ""a satellite image of..."" limiting their effectiveness for zero-shot land-use/land-cover (LULC) mapping. To address these challenges we introduce SenCLIP which transfers CLIP's representation to Sentinel-2 imagery by leveraging a large dataset of Sentinel-2 images paired with geotagged ground-level photos from across Europe. We evaluate SenCLIP alongside other state-of-the-art remote sensing VLMs on zero-shot LULC mapping tasks using the EuroSAT and BigEarthNet datasets with both aerial and ground-level prompting styles. Our approach which aligns ground-level representations with satellite imagery demonstrates significant improvements in classification accuracy across both prompt styles opening new possibilities for applying free-form textual descriptions in zero-shot LULC mapping. Code dataset and pretrained models are available at https://github.com/pallavijain-pj/SenCLIP"	https://openaccess.thecvf.com//content/WACV2025/html/Jain_SenCLIP_Enhancing_Zero-Shot_Land-Use_Mapping_for_Sentinel-2_with_Ground-Level_Prompting_WACV_2025_paper.html	Pallavi Jain, Dino Ienco, Roberto Interdonato, Tristan Berchoux, Diego Marcos
SensorFlow: Sensor and Image Fused Video Stabilization	We present SensorFlow a novel image and sensor fusion framework for robust high-quality video stabilization. We start with sensor-based pre-stabilization that smooths out large-scale camera motion. A new angular velocity domain optimization has been introduced to achieve frame rate invariance. We then feed the stabilized optical flows into an occlusion-aware 3D CNN that infers dense warp fields to remove residual translation and jitter. To further avoid distortion we propose a novel masking scheme to determine the disoccluded and dynamic regions in optical flow and inpaint them with spatially smooth flow vectors. Our method is appealing as it shares both the dense warping field's flexibility to correct complex motions and the robustness of sensor data for arbitrarily challenging scenes. We have validated its effectiveness and demonstrated our solution outperforms state-of-the-art alternatives via extensive ablation studies and quantitative comparisons.	https://openaccess.thecvf.com//content/WACV2025/html/Yu_SensorFlow_Sensor_and_Image_Fused_Video_Stabilization_WACV_2025_paper.html	Jiyang Yu, Tianhao Zhang, Fuhao Shi, Lei He, Chia-Kai Liang
Separating Direct and Global Components from Novel Viewpoints	Separating an image of a scene illuminated by a light source into direct components such as specular reflection and diffuse reflection and global components such as inter-reflection and subsurface scattering is important as preprocessing for various computer vision and graphics applications. Conventional methods cannot separate direct and global components from novel viewpoints and have difficulties in robustly separating those components from a small number of images even from known viewpoints. In this paper we propose a method for synthesizing the direct and global components of a scene from novel viewpoints by using a relatively small number of images. Specifically our proposed method uses the multi-view images captured by using a coaxial projector-camera system and then recovers the density and radiance values of each component on the basis of neural radiance fields (NeRF). We conduct a number of experiments using real images captured with a projector-camera system and confirm the effectiveness of our method. In addition we demonstrate that our method is useful for two applications: image-based material editing and 3D shape recovery.	https://openaccess.thecvf.com//content/WACV2025/html/Matsufuji_Separating_Direct_and_Global_Components_from_Novel_Viewpoints_WACV_2025_paper.html	Kengo Matsufuji, Lin Shi, Ryo Kawahara, Takahiro Okabe
Shadow Removal Refinement via Material-Consistent Shadow Edges	Shadow boundaries can be confused with material boundaries as both exhibit sharp changes in luminance or contrast within a scene. However shadows do not modify the intrinsic color or texture of surfaces. Therefore on both sides of shadow edges traversing regions with the same material the original color and texture should be the same if the shadow is removed properly. These shadow/shadow-free pairs are very useful but difficult-to-collect supervision signals. The crucial contribution of this paper is to learn how to identify those shadow edges that traverse material-consistent regions and how to use them as self-supervision for shadow removal refinement during test time. To achieve this we fine-tune SAM an image segmentation foundation model to produce a shadow-invariant segmentation and then extract material-consistent shadow edges by comparing the SAM segmentation with the shadow mask. Utilizing these shadow edges we introduce color- and texture-consistency losses to enhance the shadow removal process. We demonstrate the effectiveness of our method in improving shadow removal results on more challenging in-the-wild images outperforming the state-of-the-art shadow removal methods. Additionally we propose a new metric and an annotated dataset for evaluating the performance of shadow removal methods without the need for paired shadow/shadow-free data. Our code and dataset are available at: https://github.com/cvlab-stonybrook/ShadowRemovalRefine	https://openaccess.thecvf.com//content/WACV2025/html/Hu_Shadow_Removal_Refinement_via_Material-Consistent_Shadow_Edges_WACV_2025_paper.html	Shilin Hu, Hieu Le, ShahRukh Athar, Sagnik Das, Dimitris Samaras
Shape-Biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation	Recent advances in machine learning have greatly benefited object detection and 6D pose estimation. However textureless and metallic objects still pose a significant challenge due to few visual cues and the texture bias of CNNs. To address this issue we propose a strategy for inducing a shape bias to CNN training. In particular by randomizing textures applied to object surfaces during data rendering we create training data without consistent textural cues. This methodology allows for seamless integration into existing data rendering engines and results in negligible computational overhead for data rendering and network training. Our findings demonstrate that the shape bias we induce via randomized texturing improves over existing approaches using style transfer. We evaluate with five detectors and two pose estimators. For three object detectors and for pose estimation in general estimation accuracy improves for textureless and metallic objects. Additionally we show that our approach increases the pose estimation accuracy in the presence of image noise and strong illumination changes. Code available at https://github.com/hoenigpeter/randomized_texturing.	https://openaccess.thecvf.com//content/WACV2025/html/Honig_Shape-Biased_Texture_Agnostic_Representations_for_Improved_Textureless_and_Metallic_Object_WACV_2025_paper.html	Peter HÃ¶nig, Stefan Thalhammer, Jean-Baptiste Weibel, Matthias Hirschmanner, Markus Vincze
ShapeMorph: 3D Shape Completion via Blockwise Discrete Diffusion	We introduce ShapeMorph a diffusion-based method specifically designed for generating precise and diverse 3D shape completions. By integrating an irregular discrete representation with a novel blockwise discrete diffusion model ShapeMorph can produce multiple high-quality shape completions while maintaining fidelity to the input. In particular each 3D shape is encoded into a compact sequence of irregularly distributed discrete variables ensuring an accurate capture of the object's topological details. We then propose a blockwise discrete diffusion model to precisely learn the shape completion distribution based on various incompleteness. We also introduce a Flow transformer into our diffusion process serving as a denoising network to enhance the modeling adaptability and flexibility. ShapeMorph addresses common challenges in existing methods such as poor completion limited diversity and misalignment with the input. Results show that ShapeMorph outperforms state-of-the-art methods and effectively processes a variety of input types and levels of incompleteness.	https://openaccess.thecvf.com//content/WACV2025/html/Li_ShapeMorph_3D_Shape_Completion_via_Blockwise_Discrete_Diffusion_WACV_2025_paper.html	Jiahui Li, Pourya Shamsolmoali, Yue Lu, Masoumeh Zareapoor
Shapley Consensus Deep Learning for Ensemble Pruning	This paper targets a new foundation for designing general-purpose learning systems by establishing a consensus method that facilitates self-adaptation and flexibility to deal with different learning tasks and different data distribution. We present the Shapely Consensus Deep Learning (SCDL) as a consensus method for general-purpose intelligence without the help of a domain expert. SCDL is two-level based learning process. In the first level several deep learning models have been trained for each historical observation. The Shapley Value is determined to compute the contribution of each subset of models in the training. The models are pruned according to their contribution in the learning process. In the second level the loss information of each data distribution is saved in the knowledge base. Both levels are explored to prune the models for each new observation. We present the evaluation of the generality of SCDL using different datasets with different shapes and complexities. The results reveal the effectiveness of SCDL for weakly classification. Concretely SCDL achieved 90% of AUC with less than 86% for the baseline solutions.	https://openaccess.thecvf.com//content/WACV2025/html/Djenouri_Shapley_Consensus_Deep_Learning_for_Ensemble_Pruning_WACV_2025_paper.html	Youcef Djenouri, Ahmed Nabil Belbachir, Asma Belhadi, Nassim Belmecheri, Tomasz Michalak
Shift Equivariant Pose Network	Human pose estimation has been greatly advanced in recent years. However even the best-performing models are not shift equivariant. In particular a small change in input images often results in drastic alterations in output which are problematic especially in video applications. The prevalence of top-down approaches which typically rely on a non-equivariant object detector in the first stage exacerbates this issue. In this paper we first demonstrate that the biased keypoint representation and the non-equivariant network components are the two main obstacles to shift equivariant pose estimation. To address the limitation we propose an unbiased decoding method and redesign the necessary network components (e.g. APS-ResBlock SSP). Extensive experiments show that our method not only produces much more stable results with shifting input but also achieves better metrics with the ability to tolerate inaccurate detector output from the first stage. To our knowledge this is the first work to address the problem of shift equivariance in the field of pose estimation. Our method could be easily applied to existing CNN-based pose estimation networks.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_Shift_Equivariant_Pose_Network_WACV_2025_paper.html	Pengxiao Wang, Tzu-Heng Lin, Chunyu Wang, Yizhou Wang
Sifting through the Haystack - Efficiently Finding Rare Animal Behaviors in Large-Scale Datasets	In the study of animal behavior researchers often record long continuous videos accumulating into large-scale datasets. However the behaviors of interest are often rare compared to routine behaviors. This incurs a heavy cost on manual annotation forcing users to sift through many samples before finding their needles. We propose a pipeline to efficiently sample rare behaviors from large datasets enabling the creation of training datasets for rare behavior classifiers. Our method only needs an unlabeled animal pose or acceleration dataset as input and makes no assumptions regarding the type number or characteristics of the rare behaviors. Our pipeline is based on a recent graph-based anomaly detection model for human behavior which we apply to this new data domain. It leverages anomaly scores to automatically label normal samples while directing human annotation efforts toward anomalies. In research data anomalies may come from many different sources (e.g. signal noise versus true rare instances). Hence the entire labeling budget is focused on the abnormal classes letting the user review and label samples according to their needs. We tested our approach on three datasets of freely-moving animals acquired in the laboratory and the field. We found that graph-based models are particularly useful when studying motion-based behaviors in animals yielding good results while using a small labeling budget. Our method consistently outperformed traditional random sampling offering an average improvement of 70% in performance and creating datasets even when the behavior of interest was only 0.02% of the data. Even when the performance gain was minor (e.g. when the behavior is not rare) our method still reduced the annotation effort by half.	https://openaccess.thecvf.com//content/WACV2025/html/Bar_Sifting_through_the_Haystack_-_Efficiently_Finding_Rare_Animal_Behaviors_WACV_2025_paper.html	Shir Bar, Or Hirschorn, Roi Holzman, Shai Avidan
Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation	Multi-modal semantic segmentation significantly enhances AI agents' perception and scene understanding especially under adverse conditions like low-light or overexposed environments. Leveraging additional modalities (X-modality) like thermal and depth alongside traditional RGB provides complementary information enabling more robust and reliable segmentation. In this work we introduce Sigma a Siamese Mamba network for multi-modal semantic segmentation utilizing the Selective Structured State Space Model Mamba. Unlike conventional methods that rely on CNNs with their limited local receptive fields or Vision Transformers (ViTs) which offer global receptive fields at the cost of quadratic complexity our model achieves global receptive fields coverage with linear complexity. By employing a Siamese encoder and innovating a Mamba fusion mechanism we effectively select essential information from different modalities. A decoder is then developed to enhance the channel-wise modeling ability of the model. Our method Sigma is rigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks demonstrating its superiority and marking the first successful application of State Space Models (SSMs) in multi-modal perception tasks. Code is available at https://github.com/zifuwan/Sigma.	https://openaccess.thecvf.com//content/WACV2025/html/Wan_Sigma_Siamese_Mamba_Network_for_Multi-Modal_Semantic_Segmentation_WACV_2025_paper.html	Zifu Wan, Pingping Zhang, Yuhao Wang, Silong Yong, Simon Stepputtis, Katia Sycara, Yaqi Xie
Sign Language Recognition: A Large-Scale Multi-View Dataset and Comprehensive Evaluation	Vision-based sign language recognition is an extensively researched problem aimed at advancing communication between deaf and hearing individuals. Numerous Sign Language Recognition (SLR) datasets have been introduced to promote research in this field spanning multiple languages vocabulary sizes and signers. However most existing popular datasets focus predominantly on the frontal view of signers neglecting visual information from other perspectives. In practice many sign languages contain words that have similar hand movements and expressions making it challenging to differentiate between them from a single frontal view. Although a few studies have proposed sign language datasets using multi-view data these datasets remain limited in vocabulary size and scale hindering their generalizability and practicality. To address this issue we introduce a new large-scale multi-view sign language recognition dataset spanning 1000 glosses and 30 signers resulting in over 84000 multi-view videos. To the best of our knowledge this is the first multi-view sign language recognition dataset of this scale. In conjunction with offering a comprehensive dataset we perform extensive experiments to assess the performance of state-of-the-art Sign Language Recognition models utilizing on our dataset. The findings indicate that utilizing multi-view data substantially enhances model accuracy across all models with a maximum performance improvement of up to 19.75% compared to models trained on single-view data. Our dataset and baseline models are publicly accessible on GitHub.	https://openaccess.thecvf.com//content/WACV2025/html/Dinh_Sign_Language_Recognition_A_Large-Scale_Multi-View_Dataset_and_Comprehensive_Evaluation_WACV_2025_paper.html	Nguyen Son Dinh, Tuan Dung Nguyen, Duc Tri Tran, Nguyen Dang Huy Pham, Thuan Hieu Tran, Ngoc Anh Tong, Quang Huy Hoang, Phi Le Nguyen
Similarity over Factuality: Are we Making Progress on Multimodal Out-of-Context Misinformation Detection?	"Out-of-context (OOC) misinformation poses a significant challenge in multimodal fact-checking where images are paired with texts that misrepresent their original context to support false narratives. Recent research in evidence-based OOC detection has seen a trend towards increasingly complex architectures incorporating Transformers foundation models and large language models. In this study we introduce a simple yet robust baseline which assesses MUltimodal SimilaritiEs (MUSE) specifically the similarity between image-text pairs and external image and text evidence. Our results demonstrate that MUSE when used with conventional classifiers like Decision Tree Random Forest and Multilayer Perceptron can compete with and even surpass the state-of-the-art on the NewsCLIPpings and VERITE datasets at less than 1% of their computational complexity. Furthermore integrating MUSE in our proposed ""Attentive Intermediate Transformer Representations"" (AITR) significantly improved performance by 3.3% and 7.5% on NewsCLIPpings and VERITE respectively. Nevertheless the success of MUSE relying on surface-level patterns and shortcuts without examining factuality and logical inconsistencies raises critical questions about how we define the task construct datasets collect external evidence and overall how we assess progress in the field. We release our code at: https://github.com/stevejpapad/outcontext-misinfo-progress."	https://openaccess.thecvf.com//content/WACV2025/html/Papadopoulos_Similarity_over_Factuality_Are_we_Making_Progress_on_Multimodal_Out-of-Context_WACV_2025_paper.html	Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, Panagiotis C. Petrantonakis
SimuScope: Realistic Endoscopic Synthetic Dataset Generation through Surgical Simulation and Diffusion Models	Computer-assisted surgical (CAS) systems enhance surgical execution and outcomes by providing advanced support to surgeons. These systems often rely on deep learning models trained on complex challenging-to-annotate data. While synthetic data generation can address these challenges enhancing the realism of such data is crucial. This work introduces a multi-stage pipeline for generating realistic synthetic data featuring a fully-fledged surgical simulator that automatically produces all necessary annotations for modern CAS systems. This simulator generates a wide set of annotations that surpass those available in public synthetic datasets. Additionally it offers a more complex and realistic simulation of surgical interactions including the dynamics between surgical instruments and deformable anatomical environments outperforming existing approaches. To further bridge the visual gap between synthetic and real data we propose a lightweight and flexible image-to-image translation method based on Stable Diffusion (SD) and Low-Rank Adaptation (LoRA). This method leverages a limited amount of annotated data enables efficient training and maintains the integrity of annotations generated by our simulator. The proposed pipeline is experimentally validated and can translate synthetic images into images with real-world characteristics which can generalize to real-world context thereby improving both training and CAS guidance. The code and the dataset are available at https://github.com/SanoScience/SimuScope.	https://openaccess.thecvf.com//content/WACV2025/html/Martyniak_SimuScope_Realistic_Endoscopic_Synthetic_Dataset_Generation_through_Surgical_Simulation_and_WACV_2025_paper.html	Sabina Martyniak, Joanna Kaleta, Diego Dall'Alba, MichaÅ NaskrÄt, Szymon PÅotka, PrzemysÅaw Korzeniowski
Single-Layer Distillation with Fourier Convolutions for Texture Anomaly Detection	In industrial quality control detecting anomalies in visual textures is essential for ensuring product quality and operational efficiency. Early identification of defects prevents faulty items from reaching consumers reduces waste and maintains high standards of production. Numerous unsupervised anomaly detection methods heavily depend on the integration of multiple layers from various pre-trained models a selection often made through empirical means. We propose SingleNet an innovative knowledge distillation approach tailored for fast unsupervised texture anomaly detection using a single layer from a compact pre-trained model. Contrary to the previous knowledge distillation approaches our network leverages fast Fourier convolutions (FFC) to reconstruct a degraded version of the teacher extracted features. At test time we employed a frequency-aware filtering mechanism to reduce reconstruction artifacts caused by discrepancies between teacher and student architectures. Empirical results demonstrate the efficacy of our approach attaining state-of-the-art performance across evaluated datasets coupled with expedited high-speed inference.	https://openaccess.thecvf.com//content/WACV2025/html/Thomine_Single-Layer_Distillation_with_Fourier_Convolutions_for_Texture_Anomaly_Detection_WACV_2025_paper.html	Simon Thomine, Hichem Snoussi
Situational Scene Graph for Structured Human-Centric Situation Understanding	Graph based representation has been widely used in modelling spatio-temporal relationships in video understanding. Although effective existing graph-based approaches focus on capturing the human-object relationships while ignoring fine-grained semantic properties of the action components. These semantic properties are crucial for understanding the current situation such as where does the action takes place what tools are used and functional properties of the objects. In this work we propose a graph-based representation called Situational Scene Graph (SSG) to encode both human-object relationships and the corresponding semantic properties. The semantic details are represented as predefined roles and values inspired by situation frame which is originally designed to represent a single action. Based on our proposed representation we introduce the task of situational scene graph generation and propose a multi-stage pipeline Inter active and Complementary Network (InComNet) to address the task. Given that the existing datasets are not applicable to the task we further introduce a SSG dataset whose annotations consist of semantic role-value frames for human objects and verb predicates of human-object relations. Finally we demonstrate the effectiveness of our proposed SSG representation by testing on different downstream tasks. Experimental results show that the unified representation can not only benefit predicate classification and semantic role-value classification but also benefit reasoning tasks on human centric situation understanding. Code and data are available at https://github.com/LUNAProject22/SSG.	https://openaccess.thecvf.com//content/WACV2025/html/Sugandhika_Situational_Scene_Graph_for_Structured_Human-Centric_Situation_Understanding_WACV_2025_paper.html	Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando
Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects	The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability pose control remains limited to specific objects (e.g. humans) or poses (e.g. frontal view) due to the fact that pose is generally controlled via camera parameters (e.g. rotation angle) or keypoints (e.g. eyes nose). Specifically camera parameters-conditional pose control models generate unrealistic images depending on the object owing to the small size of 3D datasets for training. Also keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g. church) or poses (e.g. back view). To address these limitations we propose depth-based pose control as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses unlike camera parameters and keypoints. However depth-based pose control confronts issues of shape dependency as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue we propose Skip-and-Play (SnP) designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific based on the analysis we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably SnP exhibits the ability to generate images even when the objects in the condition (e.g. a horse) and the prompt (e.g. a hedgehog) differ from each other.	https://openaccess.thecvf.com//content/WACV2025/html/Jo_Skip-and-Play_Depth-Driven_Pose-Preserved_Image_Generation_for_Any_Objects_WACV_2025_paper.html	Kyungmin Jo, Jaegul Choo
Skyeyes: Ground Roaming using Aerial View Images	Integrating aerial imagery-based scene generation into applications like autonomous driving and gaming enhances realism in 3D environments but challenges remain in creating detailed content for occluded areas and ensuring real-time consistent rendering. In this paper we introduce Skyeyes a novel framework that can generate photorealistic sequences of ground view images using only aerial view inputs thereby creating a ground roaming experience. More specifically we combine a 3D representation with a view consistent generation model which ensures coherence between generated images. This method allows for the creation of geometrically consistent ground view images even with large view gaps. The images maintain improved spatial-temporal coherence and realism enhancing scene comprehension and visualization from aerial perspectives. To the best of our knowledge there are no publicly available datasets that contain pairwise geo-aligned aerial and ground view imagery. Therefore we build a large synthetic and geo-aligned dataset using Unreal Engine. Both qualitative and quantitative analyses on this synthetic dataset display superior results compared to other leading synthesis approaches. See the project page for more results: https://chaoren2357.github.io/website-skyeyes/	https://openaccess.thecvf.com//content/WACV2025/html/Gao_Skyeyes_Ground_Roaming_using_Aerial_View_Images_WACV_2025_paper.html	Zhiyuan Gao, Wenbin Teng, Gonglin Chen, Jinsen Wu, Ningli Xu, Rongjun Qin, Andrew Feng, Yajie Zhao
Sli2Vol+: Segmenting 3D Medical Images Based on an Object Estimation Guided Correspondence Flow Network	Deep learning (DL) methods have shown remarkable successes in medical image segmentation often using large amounts of annotated data for model training. However acquiring a large number of diverse labeled 3D medical image datasets is highly difficult and expensive. Recently mask propagation DL methods were developed to reduce the annotation burden on 3D medical images. For example Sli2Vol [59] proposed a self-supervised framework (SSF) to learn correspondences by matching neighboring slices via slice reconstruction in the training stage; the learned correspondences were then used to propagate a labeled slice to other slices in the test stage. But these methods are still prone to error accumulation due to the inter-slice propagation of reconstruction errors. Also they do not handle discontinuities well which can occur between consecutive slices in 3D images as they emphasize exploiting object continuity. To address these challenges in this work we propose a new SSF called Sli2Vol+ for segmenting any anatomical structures in 3D medical images using only a single annotated slice per training and testing volume. Specifically in the training stage we first propagate an annotated 2D slice of a training volume to the other slices generating pseudo-labels (PLs). Then we develop a novel Object Estimation Guided Correspondence Flow Network to learn reliable correspondences between consecutive slices and corresponding PLs in a self-supervised manner. In the test stage such correspondences are utilized to propagate a single annotated slice to the other slices of a test volume. We demonstrate the effectiveness of our method on various medical image segmentation tasks with different datasets showing better generalizability across different organs modalities and modals. Code is available at https://github.com/adlsn/Sli2Volplus	https://openaccess.thecvf.com//content/WACV2025/html/An_Sli2Vol_Segmenting_3D_Medical_Images_Based_on_an_Object_Estimation_WACV_2025_paper.html	Delin An, Pengfei Gu, Milan Sonka, Chaoli Wang, Danny Z. Chen
SmartKC++: Improving Performance of Smartphone-Based Corneal Topographers	Keratoconus an ocular condition marked by progressive corneal thinning and outward bulging presents diagnostic challenges due to the high cost and lack of portability in conventional corneal topographers. These limitations restrict accessibility for many necessitating affordable and mobile alternatives. Innovations like SmartKC offer a low-cost and portable alternative however there still remains some gaps in performance when compared to commercial topographers. In this paper we introduce SmartKC++ a series of innovative methodological improvements to the image processing pipeline of SmartKC aimed at significantly enhancing its diagnostic precision and reliability. Our comprehensive evaluation on a dataset comprising 303 eye images reveals that SmartKC++ boosts the accuracy of automated keratoconus diagnosis by 7.69% relative to SmartKC.	https://openaccess.thecvf.com//content/WACV2025/html/Ganatra_SmartKC_Improving_Performance_of_Smartphone-Based_Corneal_Topographers_WACV_2025_paper.html	Vaibhav Ganatra, Siddhartha Gairola, Pallavi Joshi, Anand Balasubramaniam, Kaushik Murali, Arivunithi Varadharajan, Bellamkonda Mallikarjuna, Nipun Kwatra, Mohit Jain
Social EgoMesh Estimation	Accurately estimating the 3D pose of the camera wearer in egocentric video sequences is crucial to modeling human behavior in virtual and augmented reality applications. The task presents unique challenges due to the limited visibility of the user's body caused by the front-facing camera mounted on their head. Recent research has explored the utilization of the scene and ego-motion but it has overlooked humans' interactive nature. We propose a novel framework for Social Egocentric Estimation of body MEshes (SEE-ME). Our approach is the first to estimate the wearer's mesh using only a latent probabilistic diffusion model which we condition on the scene and for the first time on the social wearer-interactee interactions. Our in depth study sheds light on when social interaction matters most for ego-mesh estimation: it quantifies the impact of interpersonal distance and gaze direction. Overall SEE-ME surpasses the current best technique reducing the pose estimation error (MPJPE) by 53%. The code is available at https://github.com/L-Scofano/SEEME/tree/main.	https://openaccess.thecvf.com//content/WACV2025/html/Scofano_Social_EgoMesh_Estimation_WACV_2025_paper.html	Luca Scofano, Alessio Sampieri, Edoardo De Matteis, Indro Spinelli, Fabio Galasso
Socially-Informed Reconstruction for Pedestrian Trajectory Forecasting	Pedestrian trajectory prediction remains a challenge for autonomous systems particularly due to the intricate dynamics of social interactions. Accurate forecasting requires a comprehensive understanding not only of each pedestrian's previous trajectory but also of their interaction with the surrounding environment an important part of which are other pedestrians moving dynamically in the scene. To learn effective socially-informed representations we propose a model that uses a reconstructor alongside a conditional variational autoencoder-based trajectory forecasting module. This module generates pseudo-trajectories which we use as augmentations throughout the training process. To further guide the model towards social awareness we propose a novel social loss that aids in forecasting of more stable trajectories. We validate our approach through extensive experiments demonstrating strong performances in comparison to state of-the-art methods on the ETH/UCY and SDD benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Damirchi_Socially-Informed_Reconstruction_for_Pedestrian_Trajectory_Forecasting_WACV_2025_paper.html	Haleh Damirchi, Ali Etemad, Michael Greenspan
Solar Multimodal Transformer: Intraday Solar Irradiance Predictor using Public Cameras and Time Series	Accurate intraday solar irradiance forecasting is crucial for optimizing dispatch planning and electricity trading. For this purpose we introduce a novel and effective approach that includes three distinguishing components from the literature: 1) the uncommon use of single-frame public camera imagery; 2) solar irradiance time series scaled with a proposed normalization step which boosts performance; and 3) a lightweight multimodal model called Solar Multimodal Transformer (SMT) that delivers accurate short-term solar irradiance forecasting by combining images and scaled time series. Benchmarking against Solcast a leading solar forecasting service provider our model improved prediction accuracy by 25.95%. Our approach allows for easy adaptation to various camera specifications offering broad applicability for real-world solar forecasting challenges.	https://openaccess.thecvf.com//content/WACV2025/html/Niu_Solar_Multimodal_Transformer_Intraday_Solar_Irradiance_Predictor_using_Public_Cameras_WACV_2025_paper.html	Yanan Niu, Roy Sarkis, Demetri Psaltis, Mario Paolone, Christophe Moser, Luisa Lambertini
SoundLoc3D: Invisible 3D Sound Source Localization and Classification using a Multimodal RGB-D Acoustic Camera	Accurately localizing 3D sound sources and estimating their semantic labels - where the sources may not be visible but are assumed to lie on the physical surface of objects in the scene - have many real applications including detecting gas leak and machinery malfunction. The audio-visual weak- correlation in such setting poses new challenges in deriving innovative methods to answer if or how we can use cross- modal information to solve the task. Towards this end we propose to use an acoustic-camera rig consisting of a pinhole RGB-D camera and a coplanar four-channel microphone array (Mic-Array). By using this rig to record audio-visual signals from multiviews we can use the cross-modal cues to estimate the sound sources 3D locations. Specifically our framework SoundLoc3D treats the task as a set prediction problem each element in the set corresponds to a potential sound source. Given the audio-visual weak-correlation the set representation is initially learned from a single view microphone array signal and then refined by actively incorporating physical surface cues revealed from multiview RGB-D images. We demonstrate the efficiency and superiority of SoundLoc3D on large-scale simulated dataset and further show its robustness to RGB-D measurement inaccuracy and ambient noise interference.	https://openaccess.thecvf.com//content/WACV2025/html/He_SoundLoc3D_Invisible_3D_Sound_Source_Localization_and_Classification_using_a_WACV_2025_paper.html	Yuhang He, Sangyun Shin, Anoop Cherian, Niki Trigoni, Andrew Markham
SoundSil-DS: Deep Denoising and Segmentation of Sound-Field Images with Silhouettes	Development of optical technology has enabled imaging of two-dimensional (2D) sound fields. This acousto-optic sensing enables understanding of the interaction between sound and objects such as reflection and diffraction. Moreover it is expected to be used an advanced measurement technology for sonars in self-driving vehicles and assistive robots. However the low sound-pressure sensitivity of the acousto-optic sensing results in high intensity of noise on images. Therefore denoising is an essential task to visualize and analyze the sound fields. In addition to denoising segmentation of sound and object silhouette is also required to analyze interactions between them. In this paper we propose sound-field-images-with-object-silhouette denoising and segmentation (SoundSil-DS) that jointly perform denoising and segmentation for sound fields and object silhouettes on a visualized image. We developed a new model based on the current state-of-the-art denoising network. We also created a dataset to train and evaluate the proposed method through acoustic simulation. The proposed method was evaluated using both simulated and measured data. We confirmed that our method can applied to experimentally measured data. These results suggest that the proposed method may improve the post-processing for sound fields such as physical model-based three-dimensional reconstruction since it can remove unwanted noise and separate sound fields and other object silhouettes. Our code is available at https://github.com/nttcslab/soundsil-ds.	https://openaccess.thecvf.com//content/WACV2025/html/Tanigawa_SoundSil-DS_Deep_Denoising_and_Segmentation_of_Sound-Field_Images_with_Silhouettes_WACV_2025_paper.html	Risako Tanigawa, Kenji Ishikawa, Noboru Harada, Yasuhiro Oikawa
SpaGBOL: Spatial-Graph-Based Orientated Localisation	Cross-View Geo-Localisation within urban regions is challenging in part due to the lack of geo-spatial structuring within current datasets and techniques. We propose utilising graph representations to model sequences of local observations and the connectivity of the target location. Modelling as a graph enables generating previously unseen sequences by sampling with new parameter configurations. To leverage this newly available information we propose a GNN-based architecture producing spatially strong embeddings and improving discriminability over isolated image embeddings. We outline SpaGBOL introducing three novel contributions. 1) The first graph-structured dataset for Cross-View Geo-Localisation containing multiple streetview images per node to improve generalisation. 2) Introducing GNNs to the problem we develop the first system that exploits the correlation between node proximity and feature similarity. 3) Leveraging the unique properties of the graph representation - we demonstrate a novel retrieval filtering approach based on neighbourhood bearings. SpaGBOL achieves state-of-the-art accuracies on the unseen test graph - with relative Top-1 retrieval improvements on previous techniques of 11% and 50% when filtering with Bearing Vector Matching on the SpaGBOL dataset. Code and dataset available: https://www.github.com/tavisshore/SpaGBOL.	https://openaccess.thecvf.com//content/WACV2025/html/Shore_SpaGBOL_Spatial-Graph-Based_Orientated_Localisation_WACV_2025_paper.html	Tavis Shore, Oscar Mendez, Simon Hadfield
Sparse-View 3D Reconstruction of Clothed Humans via Normal Maps	We present a novel deep learning-based approach to the 3D reconstruction of clothed humans using weak supervision via 2D normal maps. Given a single RGB image or multiview images our network is optimized to infer a person-specific signed distance function (SDF) discretized on a tetrahedral mesh surrounding the body in a rest pose. Subsequently estimated human pose and camera parameters are used to generate a normal map from the SDF. A key aspect of our approach is the direct use of the Marching Tetrahedra algorithm in end-to-end optimization and in order to do so we derive analytical gradients to facilitate straightforward differentiation (and thus backpropagation). Additionally predicted normal maps allow us to leverage pretrained image-to-normal networks in order to minimize a surface error instead of a photometric error. We demonstrate the efficacy of our approach on both labeled and in-the-wild data in the context of existing clothed human reconstruction methods.	https://openaccess.thecvf.com//content/WACV2025/html/Wu_Sparse-View_3D_Reconstruction_of_Clothed_Humans_via_Normal_Maps_WACV_2025_paper.html	Jane Wu, Diego Thomas, Ronald Fedkiw
Spatially-Adaptive Hash Encodings for Neural Surface Reconstruction	"Positional encodings are a common component of neural scene reconstruction methods and provide a way to bias the learning of neural fields towards coarser or finer representations. Current neural surface reconstruction methods use a ""one-size-fits-all"" approach to encoding choosing a fixed set of encoding functions and therefore bias across all scenes. Current state-of-the-art surface reconstruction approaches leverage grid-based multi-resolution hash encoding in order to recover high-detail geometry. We propose a learned approach which allows the network to choose its encoding basis as a function of space by masking the contribution of features stored at separate grid resolutions. The resulting spatially adaptive approach allows the network to fit a wider range of frequencies without introducing noise. We test our approach on standard benchmark surface reconstruction datasets and achieve state-of-the-art performance on two benchmark datasets."	https://openaccess.thecvf.com//content/WACV2025/html/Walker_Spatially-Adaptive_Hash_Encodings_for_Neural_Surface_Reconstruction_WACV_2025_paper.html	Thomas Walker, Octave Mariotti, Amir Vaxman, Hakan Bilen
Spatio-Temporal Context Prompting for Zero-Shot Action Detection	Spatio-temporal action detection encompasses the tasks of localizing and classifying individual actions within a video. Recent works aim to enhance this process by incorporating interaction modeling which captures the relationship between people and their surrounding context. However these approaches have primarily focused on fully-supervised learning and the current limitation lies in the lack of generalization capability to recognize unseen action categories. In this paper we aim to adapt the pretrained image-language models to detect unseen actions. To this end we propose a method which can effectively leverage the rich knowledge of visual-language models to perform Person-Context Interaction. Meanwhile our Context Prompting module will utilize contextual information to prompt labels thereby enhancing the generation of more representative text features. Moreover to address the challenge of recognizing distinct actions by multiple people at the same timestamp we design the Interest Token Spotting mechanism which employs pretrained visual knowledge to find each person's interest context tokens and then these tokens will be used for prompting to generate text features tailored to each individual. To evaluate the ability to detect unseen actions we propose a comprehensive benchmark on J-HMDB UCF101-24 and AVA datasets. The experiments show that our method achieves superior results compared to previous approaches and can be further extended to multi-action videos bringing it closer to real-world applications. The code and data can be found in ST-CLIP.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_Spatio-Temporal_Context_Prompting_for_Zero-Shot_Action_Detection_WACV_2025_paper.html	Wei-Jhe Huang, Min-Hung Chen, Shang-Hong Lai
SpectFormer: Frequency and Attention is What You Need in a Vision Transformer	Vision transformers have been applied successfully for image recognition tasks. There have been either multiheaded self-attention based (ViT [12] DeIT [54]) similar to the original work in textual models or more recently based on spectral layers (Fnet [29] GFNet [46] AFNO [15]). We hypothesize that spectral layers capture high-frequency information such as lines and edges while attention layers capture token interactions. We investigate this hypothesis through this work and observe that indeed mixing spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for vision transformers that has initial spectral and deeper multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance it improves the top-1 accuracy by 2% on ImageNet compared to both GFNet-H and LiT. SpectFormer-H-S reaches 84.25% top-1 accuracy on ImageNet-1K (state of the art for small version). Further Spectformer-H-L achieves 85.7% which is the state of the art for the comparable base version of the transformers. We further validated the SpectFormer performance in other scenarios such as transfer learning on standard datasets such as CIFAR-10 CIFAR-100 Oxford- IIIT-flower and Standford Car datasets. We then investigate its use in downstream tasks such as object detection and instance segmentation on the MS-COCO dataset and observe that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. The source code is available on this website https://github.com/badripatro/SpectFormers.	https://openaccess.thecvf.com//content/WACV2025/html/Patro_SpectFormer_Frequency_and_Attention_is_What_You_Need_in_a_WACV_2025_paper.html	Badri N. Patro, Vinay P. Namboodiri, Vijay S. Agneeswaran
SpiralMLP: A Lightweight Vision MLP Architecture	We present SpiralMLP a novel architecture introduces a Spiral FC layer as a replacement for the conventional Token Mixing approach. Differing from several existing MLP-based models that primarily emphasize axes our Spiral FC layer is designed as a deformable convolution layer with spiral-like offsets. We further adapt Spiral FC into two variants: Self-Spiral FC and Cross-Spiral FC enabling both local and global feature integration seamlessly eliminating the need for additional processing steps. To thoroughly investigate the effectiveness of the spiral-like offsets and validate our design we conduct ablation studies and explore optimal configurations. In empirical tests SpiralMLP reaches state-of-the-art performance similar to Transformers CNNs and other MLPs benchmarking on ImageNet-1k COCO and ADE20K. SpiralMLP still maintains linear computational complexity O(HW) and is compatible with varying input image resolutions. Our study reveals that targeting the full receptive field is not essential for achieving high performance instead adopting a refined approach offers better results.	https://openaccess.thecvf.com//content/WACV2025/html/Mu_SpiralMLP_A_Lightweight_Vision_MLP_Architecture_WACV_2025_paper.html	Haojie Mu, Burhan Ul Tayyab, Nicholas Chua
Spk2ImgMamba: Spiking Camera Image Reconstruction with Multi-Scale State Space Models	As a bio-inspired vision sensor the spiking camera has showcased remarkable capability in high-speed imaging with a sampling rate of 40000 Hz. Reconstructing clear images from continuous spike streams which is obtained by each photosensor continuously detecting photons and firing them asynchronously has garnered significant attention. Despite promising results existing spike-to-image reconstruction methods face challenges in balancing global receptive fields and efficient computation due to the inherent limitations of their backbones. Recently due to powerful long-range modeling and linear complexity the state space model (SSM) has emerged as a competitive alternative to CNNs and Transformers. In this paper we propose a lightweight spike-to-image reconstruction network that harnesses Mamba as the backbone. Our approach sequentially executes three core modules: temporal information integration spatial feature enhancement and progressive image reconstruction. The former accumulates cues across diverse temporal windows to explore both long-term and short-term contexts. Subsequently to model global dependencies while heightening local detail perception we develop a multi-scale SSM block characterized by multi-scale multi-direction scanning which effectively boosts spatial feature representations. Finally intensity images are decoded progressively from the enhanced light-intensity features. Extensive experiments on both synthetic and real-captured data demonstrate that our approach achieves state-of-the-art performance with only 10% of the network parameters and nearly two orders of magnitude less computational effort. The code will be available at https://github.com/interstellarH/Spk2ImgMamba.	https://openaccess.thecvf.com//content/WACV2025/html/Yin_Spk2ImgMamba_Spiking_Camera_Image_Reconstruction_with_Multi-Scale_State_Space_Models_WACV_2025_paper.html	Jiaoyang Yin, Bin Fan, Chao Xu, Tiejun Huang, Boxin Shi
SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface	We present SplatFace a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry. Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure making it possible to reconstruct faces with a limited set of input images. We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process. A novel distance metric splat-to-surface is proposed to improve alignment by considering both the Gaussian position and covariance. The surface information is also utilized to incorporate a world-space densification process resulting in superior reconstruction quality. Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision.	https://openaccess.thecvf.com//content/WACV2025/html/Luo_SplatFace_Gaussian_Splat_Face_Reconstruction_Leveraging_an_Optimizable_Surface_WACV_2025_paper.html	Jiahao Luo, Jing Liu, James Davis
SpotDiffusion: A Fast Approach for Seamless Panorama Generation Over Time	Generating high-resolution images with generative models has recently been made widely accessible by leveraging diffusion models pre-trained on large-scale datasets. Various techniques such as MultiDiffusion and SyncDiffusion have further pushed image generation beyond training resolutions i.e. from square images to panorama by merging multiple overlapping diffusion paths or employing gradient descent to maintain perceptual coherence. However these methods suffer from significant computational inefficiencies due to generating and averaging numerous predictions which is required in practice to produce high-quality and seamless images. This work addresses this limitation and presents a novel approach that eliminates the need to generate and average numerous overlapping denoising predictions. Our method shifts non-overlapping denoising windows over time ensuring that seams in one timestep are corrected in the next. This results in coherent high-resolution images with fewer overall steps. We demonstrate the effectiveness of our approach through qualitative and quantitative evaluations comparing it with MultiDiffusion SyncDiffusion and StitchDiffusion. Our method offers several key benefits including improved computational efficiency and faster inference times while producing comparable or better image quality.	https://openaccess.thecvf.com//content/WACV2025/html/Frolov_SpotDiffusion_A_Fast_Approach_for_Seamless_Panorama_Generation_Over_Time_WACV_2025_paper.html	Stanislav Frolov, Brian B. Moser, Andreas Dengel
Stable Autofocus with Focal Consistency Loss	Autofocus aims to accurately position the camera lens to bring the desired region of interest into focus. Conventional works search for the sharpest frame within the lens movement. However sharpness measure in many real-world settings is ambiguous and may cause a focus hunting problem where the lens continuously moves back and forth to search for the accurate position. To mitigate this problem we introduce a simple yet powerful loss function specifically designed to produce consistent outputs in autofocus systems. The proposed Focal Consistency Loss (FCL) allows autofocus models to better learn the geometric cues relative to each initial position of the lens significantly reducing distracting lens movement and enhancing the user experience when taking a photo. Furthermore we improve autofocus stability by utilizing multiple consecutive frames in a practical way. Experimental results show the effectiveness of FCL in various practical scenarios including multi-frame autofocus for both conventional and dual-pixel images.	https://openaccess.thecvf.com//content/WACV2025/html/Lee_Stable_Autofocus_with_Focal_Consistency_Loss_WACV_2025_paper.html	Sangwon Lee, Myungsub Choi, Nagyeong Lee, Hyong-Euk Lee
Strategic Base Representation Learning via Feature Augmentations for Few-Shot Class Incremental Learning	Few-shot class incremental learning implies the model to learn new classes while retaining knowledge of previously learned classes with a small number of training instances. Existing frameworks typically freeze the parameters of the previously learned classes during the incorporation of new classes. However this approach often results in suboptimal class separation of previously learned classes leading to overlap between old and new classes. Consequently the performance of old classes degrades on new classes. To address these challenges we propose a novel feature augmentation driven contrastive learning framework designed to enhance the separation of previously learned classes to accommodate new classes. Our approach involves augmenting feature vectors and assigning proxy labels to these vectors. This strategy expands the feature space ensuring seamless integration of new classes within the expanded space. Additionally we employ a self-supervised contrastive loss to improve the separation between previous classes. We validate our framework through experiments on three FSCIL benchmark datasets: CIFAR100 miniImageNet and CUB200. The results demonstrate that our Feature Augmentation driven Contrastive Learning framework significantly outperforms other approaches achieving state-of-the-art performance.	https://openaccess.thecvf.com//content/WACV2025/html/Nema_Strategic_Base_Representation_Learning_via_Feature_Augmentations_for_Few-Shot_Class_WACV_2025_paper.html	Parinita Nema, Vinod K Kurmi
Stratified Domain Adaptation: A Progressive Self-Training Approach for Scene Text Recognition	Unsupervised domain adaptation (UDA) has become increasingly prevalent in scene text recognition (STR) especially where training and testing data reside in different domains. The efficacy of existing UDA approaches tends to degrade when there is a large gap between the source and target domains. To deal with this problem gradually shifting or progressively learning to shift from domain to domain is the key issue. In this paper we introduce the Stratified Domain Adaptation (StrDA) approach which examines the gradual escalation of the domain gap for the learning process. The objective is to partition the target data into subsets so that the progressively self-trained model can adapt to gradual changes. We stratify the target data by evaluating the proximity of each data sample to both the source and target domains. We propose a novel method for employing domain discriminators to estimate the out-of-distribution and domain discriminative levels of data samples. Extensive experiments on benchmark scene-text datasets show that our approach significantly improves the performance of baseline (source-trained) STR models. The source code is available at https://github.com/KhaLee2307/StrDA.	https://openaccess.thecvf.com//content/WACV2025/html/Le_Stratified_Domain_Adaptation_A_Progressive_Self-Training_Approach_for_Scene_Text_WACV_2025_paper.html	Kha Nhat Le, Hoang-Tuan Nguyen, Hung Tien Tran, Thanh Duc Ngo
Street TryOn: Learning In-the-Wild Virtual Try-On from Unpaired Person Images	Most virtual try-on research is motivated to serve the fashion business by generating images to demonstrate garments on studio models at a lower cost. However virtual try-on should be a broader application that also allows customers to visualize garments on themselves using their own casual photos known as in-the-wild try-on. Unfortunately the existing methods which achieve plausible results for studio try-on settings perform poorly in the in-the-wild context. This is because these methods often require paired images (garment images paired with images of people wearing the same garment) for training. While such paired data is easy to collect from shopping websites for studio settings it is difficult to obtain for in-the-wild scenes. In this work we fill the gap by (1) introducing a StreetTryOn benchmark to support in-the-wild virtual try-on applications and (2) proposing a novel method to learn virtual try-on from a set of in-the-wild person images directly without requiring paired data. We tackle the unique challenges including warping garments to more diverse human poses and rendering more complex backgrounds faithfully by a novel DensePose warping correction method combined with diffusion-based conditional inpainting. Our experiments show competitive performance for standard studio try-on tasks and SOTA performance for street try-on and cross-domain try-on tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Cui_Street_TryOn_Learning_In-the-Wild_Virtual_Try-On_from_Unpaired_Person_Images_WACV_2025_paper.html	Aiyu Cui, Jay Mahajan, Viraj Shah, Preeti Gomathinayagam, Chang Liu, Svetlana Lazebnik
Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph Network	Given a source portrait the automatic human body reshaping task aims at editing it to an aesthetic body shape. As the technology has been widely used in media several methods have been proposed mainly focusing on generating optical flow to warp the body shape. However those previous works only consider the local transformation of different body parts (arms torso and legs) ignoring the global affinity and limiting the capacity to ensure consistency and quality across the entire body. In this paper we propose a novel Adaptive Affinity-Graph Network (AAGN) which extracts the global affinity between different body parts to enhance the quality of the generated optical flow. Specifically our AAGN primarily introduces the following designs: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages the characteristic of a fully connected graph. AAG represents different body parts as nodes in an adaptive fully connected graph and captures all the affinities between nodes to obtain a global affinity map. The design could better improve the consistency between body parts. (2) Besides for high-frequency details are crucial for photo aesthetics a Body Shape Discriminator (BSD) is designed to extract information from both high-frequency and spatial domains. Particularly an SRM filter is utilized to extract high-frequency details which are combined with spatial features as input to the BSD. With this design BSD guides the Flow Generator (FG) to pay attention to various fine details rather than rigid pixel-level fitting. Extensive experiments conducted on the BR-5K dataset demonstrate that our framework significantly enhances the aesthetic appeal of reshaped photos surpassing all previous work to achieve state-of-the-art in all evaluation metrics. Our code is available at https://github.com/Randle-Github/AGGN.	https://openaccess.thecvf.com//content/WACV2025/html/Deng_Structure-Aware_Human_Body_Reshaping_with_Adaptive_Affinity-Graph_Network_WACV_2025_paper.html	Qiwen Deng, Yangcen Liu
Structured Human Assessment of Text-to-Image Generative Models	Following the great progress in text-conditioned image generation there is a dire need for establishing clear comparison benchmarks. Unfortunately assessing performance of such models is highly subjective and notoriously difficult. Current automatic assessment of generated images quality and their alignment to text are approximate at best while human assessment is subjective poorly calibrated and not very well defined. To address these concerns we propose GenomeBench a new framework for assessing quality of text-to-image generative models. It consists of a prompt dataset richly annotated with semantic components based on a formalized grounding of language and images. On top of it we define a procedure to collect human assessment through a carefully guided question answering process. Finally these assessments are summarized into a novel score built around quality and alignment to text. We show the proposal achieves higher inter-annotator agreement with respect to the baseline human assessment and better correlation between quality and alignment compared to automatic assessment. Finally we use this framework to dissect the performance of recent text-to-image models providing insights on strength and weakness of each.	https://openaccess.thecvf.com//content/WACV2025/html/Corneanu_Structured_Human_Assessment_of_Text-to-Image_Generative_Models_WACV_2025_paper.html	Ciprian A. Corneanu, Qianli Feng, Aleix M. Martinez
Style-Pro: Style-Guided Prompt Learning for Generalizable Vision-Language Models	Pre-trained Vision-language (VL) models such as CLIP have shown significant generalization ability to downstream tasks even with minimal fine-tuning. While prompt learning has emerged as an effective strategy to adapt pre-trained VL models for downstream tasks current approaches frequently encounter severe overfitting to specific downstream data distributions. This overfitting constrains the original behavior of the VL models to generalize to new domains or unseen classes posing a critical challenge in enhancing the adaptability and generalization of VL models. To address this limitation we propose Style-Pro a novel style-guided prompt learning framework that mitigates overfitting and preserves the zero-shot generalization capabilities of CLIP. Style-Pro employs learnable style bases to synthesize diverse distribution shifts guided by two specialized loss functions that ensure style diversity and content integrity. Then to minimize discrepancies between unseen domains and the source domain Style-Pro maps the unseen styles into the known style representation space as a weighted combination of style bases. Moreover to maintain consistency between the style-shifted prompted model and the original frozen CLIP Style-Pro introduces consistency constraints to preserve alignment in the learned embeddings minimizing deviation during adaptation to downstream tasks. Extensive experiments across 11 benchmark datasets demonstrate the effectiveness of Style-Pro consistently surpassing state-of-the-art methods in various settings including base-to-new generalization cross-dataset transfer and domain generalization.	https://openaccess.thecvf.com//content/WACV2025/html/Talemi_Style-Pro_Style-Guided_Prompt_Learning_for_Generalizable_Vision-Language_Models_WACV_2025_paper.html	Niloufar Alipour Talemi, Hossein Kashiani, Fatemeh Afghah
Sun Off Lights On: Photorealistic Monocular Nighttime Simulation for Robust Semantic Perception	Nighttime scenes are hard to semantically perceive with learned models and annotate for humans. Thus realistic synthetic nighttime data become all the more important for learning robust semantic perception at night thanks to their accurate and cheap semantic annotations. However existing data-driven or hand-crafted techniques for generating nighttime images from daytime counterparts suffer from poor realism. The reason is the complex interaction of highly spatially varying nighttime illumination which differs drastically from its daytime counterpart with objects of spatially varying materials in the scene happening in 3D and being very hard to capture with such 2D approaches. The above 3D interaction and illumination shift have proven equally hard to model in the literature as opposed to other conditions such as fog or rain. Our method named Sun Off Lights On (SOLO) is the first to perform nighttime simulation on single images in a photorealistic fashion by operating in 3D. It first explicitly estimates the 3D geometry the materials and the locations of light sources of the scene from the input daytime image and relights the scene by probabilistically instantiating light sources in a way that accounts for their semantics and then running standard ray tracing. Not only is the visual quality and photorealism of our nighttime images superior to competing approaches including diffusion models but the former images are also proven more beneficial for semantic nighttime segmentation in day-to-night adaptation. Code and data are publicly available at https://github.com/ktzevel/SOLO.	https://openaccess.thecvf.com//content/WACV2025/html/Tzevelekakis_Sun_Off_Lights_On_Photorealistic_Monocular_Nighttime_Simulation_for_Robust_WACV_2025_paper.html	Konstantinos Tzevelekakis, Shutong Zhang, Luc Van Gool, Christos Sakaridis
Survival Prediction in Lung Cancer through Multi-Modal Representation Learning	Survival prediction is a crucial task associated with cancer diagnosis and treatment planning. This paper presents a novel approach to survival prediction by harnessing comprehensive information from CT and PET scans along with associated Genomic data. Current methods rely on either a single modality or the integration of multiple modalities for prediction without adequately addressing associations across patients or modalities. We aim to develop a robust predictive model for survival outcomes by integrating multi-modal imaging data with genetic information while accounting for associations across patients and modalities. We learn representations for each modality via a self-supervised module and harness the semantic similarities across the patients to ensure the embeddings are aligned closely. However optimizing solely for global relevance is inadequate as many pairs sharing similar high-level semantics such as tumor type are inadvertently pushed apart in the embedding space. To address this issue we use a cross-patient module (CPM) designed to harness inter-subject correspondences. The CPM module aims to bring together embeddings from patients with similar disease characteristics. Our experimental evaluation of the dataset of Non-Small Cell Lung Cancer (NSCLC) patients demonstrates the effectiveness of our approach in predicting survival outcomes outperforming state-of-the-art methods.	https://openaccess.thecvf.com//content/WACV2025/html/Farooq_Survival_Prediction_in_Lung_Cancer_through_Multi-Modal_Representation_Learning_WACV_2025_paper.html	Aiman Farooq, Deepak Mishra, Santanu Chaudhury
Swap Path Network for Robust Person Search Pre-Training	In person search we detect and rank matches to a query person image within a set of gallery scenes. Most person search models make use of a feature extraction backbone followed by separate heads for detection and re-identification. While pre-training methods for vision backbones are well-established pre-training additional modules for the person search task has not been previously examined. In this work we present the first framework for end-to-end person search pre-training. Our framework splits person search into object-centric and query-centric methodologies and we show that the query-centric framing is robust to label noise and trainable using only weakly-labeled person bounding boxes. Further we provide a novel model dubbed Swap Path Net (SPNet) which implements both query-centric and object-centric training objectives and can swap between the two while using the same weights. Using SPNet we show that query-centric pre-training followed by object-centric fine-tuning achieves state-of-the-art results on the standard PRW and CUHK-SYSU person search benchmarks with 96.4% mAP on CUHK-SYSU and 61.2% mAP on PRW. In addition we show that our method is more effective efficient and robust for person search pre-training than recent backbone-only pre-training alternatives.	https://openaccess.thecvf.com//content/WACV2025/html/Jaffe_Swap_Path_Network_for_Robust_Person_Search_Pre-Training_WACV_2025_paper.html	Lucas Jaffe, Avideh Zakhor
Swin-: Gradient-Based Image Restoration from Image Sequences using Video Swin-Transformers	Most deep-learning models for vision tasks rely on RGB images as their primary input layer assuming the model inherently discovers an optimal representation. In this work we challenge this assumption and show that image gradients offer a straightforward yet robust representation for multi-frame image restoration. We demonstrate that clusters naturally emerge within gradient patches indicating improved estimation of the underlying signal. We develop a Video Swin-Transformer model operating in the gradient domain facilitated by the implementation of two differentiable gradient modules. One module computes image gradients using convolutions with gradient filters while the other reconstructs an RGB image from its gradient representation using deconvolution in the frequency domain. Additionally we employ a composite training loss that measures the error both in the color domain and its gradient counterpart. Applied to a multi-frame image restoration task involving the removal of lighting shadows and occlusions our model consistently outperforms RGB-based counterparts without introducing additional parameters thanks to its gradient regularization. We further apply our framework to various restoration tasks discussing its advantages and limitations. Qualitative results highlight the model's improved generalization to real-world video scenarios demonstrating successful adaptation from synthetic image training to real video data deployment.	https://openaccess.thecvf.com//content/WACV2025/html/Kwiatkowski_Swin-_Gradient-Based_Image_Restoration_from_Image_Sequences_using_Video_Swin-Transformers_WACV_2025_paper.html	Monika Kwiatkowski, Simon Matern, Olaf Hellwich
SwinIA: Self-Supervised Blind-Spot Image Denoising without Convolutions	Self-supervised image denoising implies restoring the signal from a noisy image without access to the ground truth. State-of-the-art solutions for this task rely on predicting masked pixels with a fully-convolutional neural network. This most often requires multiple forward passes information about the noise model or intricate regularization functions. In this paper we propose a Swin Transformer-based Image Autoencoder (SwinIA) the first fully-transformer architecture for self-supervised denoising. The flexibility of the attention mechanism helps to fulfill the blind-spot property that convolutional counterparts normally approximate. SwinIA can be trained end-to-end with a simple mean squared error loss without masking and does not require any prior knowledge about clean data or noise distribution. Simple to use SwinIA establishes the state of the art on several common benchmarks.	https://openaccess.thecvf.com//content/WACV2025/html/Papkov_SwinIA_Self-Supervised_Blind-Spot_Image_Denoising_without_Convolutions_WACV_2025_paper.html	Mikhail Papkov, Pavel Chizhov, Leopold Parts
SynDRA: Synthetic Dataset for Railway Applications	The use of deep learning techniques in railway environments faces significant obstacles especially for computer vision tasks. Such obstacles are mainly due to the inherent safety concerns required for installing the proper equipment on a train and the substantial effort required to precisely annotate large datasets especially for segmentation tasks. Public datasets of real-world images are quite scarce and suffer from severe limitations such as coarse manual annotation or narrow range of scenarios. In addition real-world datasets often do not contain scenes that represent critical situations. To address such limitations this paper introduces SynDRA a synthetic dataset of photo-realistic images generated using a railway simulator built on Unreal Engine 5. SynDRA offers precise pixel-level annotations across diverse scenarios thereby facilitating more effective testing and training of deep learning models for semantic segmentation tasks in railway settings. The advantages of the proposed dataset are validated through a series of experiments that highlight the potential of SynDRA to enhance the performance of deep learning models in scenarios where real-world annotated data is scarce. The dataset is publicly available at the following link: https://syndra.retis.santannapisa.it.	https://openaccess.thecvf.com//content/WACV2025/html/DAmico_SynDRA_Synthetic_Dataset_for_Railway_Applications_WACV_2025_paper.html	Gianluca D'Amico, Federico Nesti, Giulio Rossolini, Mauro Marinoni, Salvatore Sabina, Giorgio Buttazzo
SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection	Developing robust drone detection systems is often constrained by the limited availability of large-scale annotated training data and the high costs associated with real-world data collection. However leveraging synthetic data generated via game engine-based simulations provides a promising and cost-effective solution to overcome this issue. Therefore we present SynDroneVision a synthetic dataset specifically designed for RGB-based drone detection in surveillance applications. Featuring diverse backgrounds lighting conditions and drone models SynDroneVision offers a comprehensive training foundation for deep learning algorithms. To evaluate the dataset's effectiveness we perform a comparative analysis across a selection of recent YOLO detection models. Our findings demonstrate that SynDroneVision is a valuable resource for real-world data enrichment achieving notable enhancements in model performance and robustness while significantly reducing the time and costs of real-world data acquisition. SynDroneVision can be accessed at https://zenodo.org/records/13360116.	https://openaccess.thecvf.com//content/WACV2025/html/Lenhard_SynDroneVision_A_Synthetic_Dataset_for_Image-Based_Drone_Detection_WACV_2025_paper.html	Tamara R. Lenhard, Andreas Weinmann, Kai Franke, Tobias Koch
SyncDiff: Diffusion-Based Talking Head Synthesis with Bottlenecked Temporal Visual Prior for Improved Synchronization	Talking head synthesis also known as speech-to-lip synthesis reconstructs the facial motions that align with the given audio tracks. The synthesized videos are evaluated on mainly two aspects lip-speech synchronization and image fidelity. Recent studies demonstrate that GAN-based and diffusion-based models achieve state-of-the-art (SOTA) performance on this task with diffusion-based models achieving superior image fidelity but experiencing lower synchronization compared to their GAN-based counterparts. To this end we propose SyncDiff a simple yet effective approach to improve diffusion-based models using a temporal pose frame with information bottleneck and facial-informative audio features extracted from AVHuBERT as conditioning input into the diffusion process. We evaluate SyncDiff on two canonical talking head datasets LRS2 and LRS3 for direct comparison with other SOTA models. Experiments on LRS2/LRS3 datasets show that SyncDiff achieves a synchronization score 27.7%/62.3% relatively higher than previous diffusion-based methods while preserving their high-fidelity characteristics.	https://openaccess.thecvf.com//content/WACV2025/html/Fan_SyncDiff_Diffusion-Based_Talking_Head_Synthesis_with_Bottlenecked_Temporal_Visual_Prior_WACV_2025_paper.html	Xulin Fan, Heting Gao, Ziyi Chen, Peng Chang, Mei Han, Mark Hasegawa-Johnson
SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering	Automatically generating realistic musical performance motion can greatly enhance digital media production often involving collaboration between professionals and musicians. However capturing the intricate body hand and finger movements required for accurate musical performances is challenging. Existing methods often fall short due to the complex mapping between audio and motion typically requiring additional inputs like scores or MIDI data. In this work we present SyncViolinist a multi-stage end-to-end framework that generates synchronized violin performance motion solely from audio input. Our method overcomes the challenge of capturing both global and fine-grained performance features through two key modules: a bowing/fingering module and a motion generation module. The bowing/fingering module extracts detailed playing information from the audio which the motion generation module uses to create precise coordinated body motions reflecting the temporal granularity and nature of the violin performance. We demonstrate the effectiveness of SyncViolinist with significantly improved qualitative and quantitative results from unseen violin performance audio outperforming state-of-the-art methods. Extensive subjective evaluations involving professional violinists further validate our approach. The code and dataset are available at https://github.com/Kakanat/SyncViolinist.	https://openaccess.thecvf.com//content/WACV2025/html/Nishizawa_SyncViolinist_Music-Oriented_Violin_Motion_Generation_Based_on_Bowing_and_Fingering_WACV_2025_paper.html	Hiroki Nishizawa, Keitaro Tanaka, Asuka Hirata, Shugo Yamaguchi, Qi Feng, Masatoshi Hamanaka, Shigeo Morishima
TACLE: Task and Class-Aware Exemplar-Free Semi-Supervised Class Incremental Learning	We propose a novel TACLE (TAsk and CLass-awarE) framework for the relatively unexplored and challenging problem of exemplar-free semi-supervised class incremental learning. In this scenario at each new task the model has to learn new classes from both (few) labeled and unlabeled data without access to exemplars from previous classes. In addition to leveraging the capabilities of pre-trained models TACLE proposes a novel task-adaptive threshold thereby maximizing the utilization of the available unlabeled data as incremental learning progresses. Additionally to enhance the performance of the under-represented classes within each task we propose a class-aware weighted cross-entropy loss. We also exploit the unlabeled data for classifier alignment which further enhances the model performance. Extensive experiments on benchmark datasets namely CIFAR10 CIFAR100 and ImageNet-Subset100 demonstrate the effectiveness of the proposed TACLE framework. We further showcase its effectiveness when the unlabeled data is imbalanced and also for the extreme case of one labeled example per class. code: https://github.com/rokmr/TACLE	https://openaccess.thecvf.com//content/WACV2025/html/Kalla_TACLE_Task_and_Class-Aware_Exemplar-Free_Semi-Supervised_Class_Incremental_Learning_WACV_2025_paper.html	Jayateja Kalla, Rohit Kumar, Soma Biswas
TAM-VT: Transformation-Aware Multi-Scale Video Transformer for Segmentation and Tracking	"Video Object Segmentation (VOS) has emerged as an increasingly important problem with availability of larger datasets and more complex and realistic settings which involve long videos with global motion (e.g. in egocentric settings) depicting small objects undergoing both rigid and non-rigid (including state) deformations. While a number of recent approaches have been explored for this task these data characteristics still present challenges. In this work we propose a novel clip-based DETR-style encoder-decoder architecture which focuses on systematically analyzing and addressing aforementioned challenges. Specifically we propose a novel transformation-aware loss that focuses learning on portions of the video where an object undergoes significant deformations - a form of ""soft"" hard examples mining. Further we propose a multiplicative time-coded memory beyond vanilla additive positional encoding which helps propagate context across long videos. Finally we incorporate these in our proposed holistic multi-scale video transformer for tracking via multi-scale memory matching and decoding to ensure sensitivity and accuracy for long videos and small objects. Our model enables on-line inference with long videos in a windowed fashion by breaking the video into clips and propagating context among them. We illustrate that short clip length and longer memory with learned time-coding are important design choices for improved performance. Collectively these technical contributions enable our model to achieve new state-of-the-art (SoTA) performance on two complex egocentric datasets - VISOR [13] and VOST [44] while achieving comparable to SoTA results on the conventional VOS benchmark DAVIS'17 [38]. Detailed ablations vali date our design choices and provide insights into the impor tance of parameter choices and impact on performance."	https://openaccess.thecvf.com//content/WACV2025/html/Goyal_TAM-VT_Transformation-Aware_Multi-Scale_Video_Transformer_for_Segmentation_and_Tracking_WACV_2025_paper.html	Raghav Goyal, Wan-Cyuan Fan, Mennatullah Siam, Leonid Sigal
TFM^2: Training-Free Mask Matching for Open-Vocabulary Semantic Segmentation	The potential of Open-Vocabulary Semantic Segmentation (OVSS) in few-shot scenarios is not fully explored due to the complexity of extending few-shot concepts to semantic segmentation tasks. To address this challenge we propose Training-Free Mask Matching (TFM^2) an efficient mask-based adapter method that enhances OVSS models for the few-shot open vocabulary semantic segmentation task. TFM^2 is a key-value cache that explicitly designed for image masks. We introduce three modules to construct and refine the mask cache subsequently enhancing the OVSS mask classification performance. Comprehensive experiments demonstrate that TFM^2 improves the performance of state-of-the-art OVSS methods by a margin of 1% to 5% across different settings. Moreover TFM^2 is not limited to any specific methods or backbones. This work underscores the importance and potential of few-shot data in OVSS and presents a significant step toward leveraging this potential.	https://openaccess.thecvf.com//content/WACV2025/html/Zhuo_TFM2_Training-Free_Mask_Matching_for_Open-Vocabulary_Semantic_Segmentation_WACV_2025_paper.html	Yaoxin Zhuo, Zachary Bessinger, Lichen Wang, Naji Khosravan, Baoxin Li, Sing Bing Kang
TLDR: Text Based Last-Layer Retraining for Debiasing Image Classifiers	An image classifier may depend on incidental features stemming from a strong correlation between the feature and the classification target in the training dataset. Recently Last Layer Retraining (LLR) with group-balanced datasets is shown to be efficient in mitigating the spurious correlation of classifiers. However the acquisition of image-based group-balanced datasets is costly which hinders the general applicability of the LLR method. In this work we propose to perform LLR based on text datasets built with large language models to debias a general image classifier. To that end we demonstrate that text can generally be a proxy for its corresponding image beyond the image-text joint embedding space which is achieved with a linear projector that ensures orthogonality between its weight and the modality gap of the joint embedding space. In addition we propose a systematic validation procedure that checks whether the generated words are compatible with the embedding space of CLIP and the image classifier which is shown to be effective for improving debiasing performance. We dub these procedures as TLDR (Text-based Last layer retraining for Debiasing image classifieRs) and show our method achieves the performance that is competitive with the LLR methods that require group-balanced image dataset for retraining. Furthermore TLDR outperforms other baselines that involve training the last layer without any group annotated dataset. Codes: https://github.com/beotborry/TLDR	https://openaccess.thecvf.com//content/WACV2025/html/Park_TLDR_Text_Based_Last-Layer_Retraining_for_Debiasing_Image_Classifiers_WACV_2025_paper.html	Juhyeon Park, Seokhyeon Jeong, Taesup Moon
TORE: Token Recycling in Vision Transformers for Efficient Active Visual Exploration	Active Visual Exploration (AVE) optimizes the utilization of robotic resources in real-world scenarios by sequentially selecting the most informative observations. However modern methods require a high computational budget due to processing the same observations multiple times through the autoencoder transformers. As a remedy we introduce a novel approach to AVE called TOken REcycling (TORE). It divides the encoder into extractor and aggregator components. The extractor processes each observation separately enabling the reuse of tokens passed to the aggregator. Moreover to further reduce the computations we decrease the decoder to only one block. Through extensive experiments we demonstrate that TORE outperforms state-of-the-art methods while reducing computational overhead by up to 90%.	https://openaccess.thecvf.com//content/WACV2025/html/Olszewski_TORE_Token_Recycling_in_Vision_Transformers_for_Efficient_Active_Visual_WACV_2025_paper.html	Jan Olszewski, Dawid Damian Rymarczyk, Piotr Wojcik, Mateusz Pach, Bartosz Zielinski
TPD-STR: Text Polygon Detection with Split Transformers	Regressing text in natural scenes with polygonal representations is challenging due to shape prediction difficulties. To address this we introduce Text Polygon Detection with Split Transformers (TPD-STR) which directly regresses polygonal points. TPD-STR incorporates the Decoder Split (DS) architecture to separate polygonal point regression and textness classification and the Positional Information Propagation (PIP) module to enhance classification. Both modules are effective and compatible with existing methods. TPD-STR achieves state-of-the-art (SOTA) performance among regression-based methods surpassing segmentation-based methods on MSRA-TD500 without external data. Adding DS and PIP to existing models further improves performance. Experiments demonstrate the model's ability to detect text instances effectively.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_TPD-STR_Text_Polygon_Detection_with_Split_Transformers_WACV_2025_paper.html	Sangyeon Kim, Sangkuk Lee, Jeesoo Kim, Nojun Kwak
TPP-Gaze: Modelling Gaze Dynamics in Space and Time with Neural Temporal Point Processes	Attention guides our gaze to fixate the proper location of the scene and holds it in that location for the deserved amount of time given current processing demands before shifting to the next one. As such gaze deployment crucially is a temporal process. Existing computational models have made significant strides in predicting spatial aspects of observer's visual scanpaths (where to look) while often putting on the background the temporal facet of attention dynamics (when). In this paper we present TPP-Gaze a novel and principled approach to model scanpath dynamics based on Neural Temporal Point Process (TPP) that jointly learns the temporal dynamics of fixations position and duration integrating deep learning methodologies with point process theory. We conduct extensive experiments across five publicly available datasets. Our results show the overall superior performance of the proposed model compared to state-of-the-art approaches. Source code and trained models are publicly available at: https://github.com/phuselab/tppgaze.	https://openaccess.thecvf.com//content/WACV2025/html/DAmelio_TPP-Gaze_Modelling_Gaze_Dynamics_in_Space_and_Time_with_Neural_WACV_2025_paper.html	Alessandro D'Amelio, Giuseppe Cartella, Vittorio Cuculo, Manuele Lucchi, Marcella Cornia, Rita Cucchiara, Giuseppe Boccignone
TRH2TQA: Table Recognition with Hierarchical Relationships to Table Question-Answering on Business Table Images	Despite advancements in visual question answering challenges persist with documents like financial reports often structured in complicated tabular structures with complex numerical computations. An alternative approach the pipeline-driven methodology includes table recognition (TR) and table question-answering (TQA). Recent advancements in TR support this approach with better accuracy and interpretability. However real-world tables usually represent hierarchical tables. They pose additional challenges due to merged cells and indents necessitating a specific approach for hierarchical relationship extraction. In this paper we propose TRH2TQA (Table Recognition with Hierarchical Relationships to Table Question-Answering) for business table images. It consists of three modules on table images with question-answer pairs. First the TR module extracts structure and textual content from table images into HTML format. Second post-structure extraction is applied to identify header and hierarchical relationships using predicted column span and bounding box. Finally this information is combined with natural language questions in the TQA module to generate the answer through the decoder. In extensive experiments TRH2TQA outperforms in question-answering performance on the VQAonBD 2023 dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Jirachanchaisiri_TRH2TQA_Table_Recognition_with_Hierarchical_Relationships_to_Table_Question-Answering_on_WACV_2025_paper.html	Pongsakorn Jirachanchaisiri, Nam Tuan Ly, Atsuhiro Takasu
TRNeRF: Restoring Blurry Rolling Shutter and Noisy Thermal Images with Neural Radiance Fields	Thermal cameras offer unique detection capabilities in building inspections search and rescue operations and autonomous vehicle perception. Of the different types of thermal cameras uncooled microbolometers are often chosen due to their relative affordability small size and low power consumption. However microbolometers suffer from motion blur rolling shutter distortions and fixed pattern noise which limit the conditions of their use. Nearly all prior methods for microbolometer image restoration account for only one of these degradations and current techniques addressing microbolometer blur and rolling shutter are limited. This paper presents TRNeRF a thermal image restoration method that jointly addresses all three degradations by incorporating the microbolometer image formation model with Neural Radiance Fields (NeRFs). To evaluate TRNeRF this paper introduces a new real-world dataset that is uniquely designed to support two novel quantitative evaluation strategies for thermal image restoration. Experiments demonstrate that TRNeRF is able to recover sharp global shutter and clear thermal images even under extremely aggressive camera motion that causes existing methods to fail. The code and dataset are available at: https://umautobots.github.io/trnerf.	https://openaccess.thecvf.com//content/WACV2025/html/Carmichael_TRNeRF_Restoring_Blurry_Rolling_Shutter_and_Noisy_Thermal_Images_with_WACV_2025_paper.html	Spencer Carmichael, Manohar Bhat, Mani Ramanagopal, Austin Buchan, Ram Vasudevan, Katherine A. Skinner
TRUST: Time-Domain Residual Unsupervised Stability Technique for Improved Heart Rate Estimation	Camera-based estimation of vital signs is a promising method for non-contact health monitoring which analyzes minute changes in video data. However the creation of accurate models for this task is challenging due to the scarcity of datasets that possess synchronized vital sign recordings. Our research enhances an existing non-contrastive unsupervised learning technique for extracting rPPG signals which does not necessitate ground-truth signals during the training process. We have incorporated new time-domain loss functions and added a feature stabilization block to improve the model's stability and accuracy in detecting low-level features. Additionally we have devised a metric to evaluate the feature instability in the model's final layer. Our experiments on four public datasets demonstrate that our method surpasses the performance of current state-of-the-art methods. These advancements make our approach a significant breakthrough in the development of scalable deep-learning models for camera-based heart-rate estimation.	https://openaccess.thecvf.com//content/WACV2025/html/Ahmad_TRUST_Time-Domain_Residual_Unsupervised_Stability_Technique_for_Improved_Heart_Rate_WACV_2025_paper.html	Shahzad Ahmad, Sania Bano, Sukalpa Chanda, Santosh Kumar Vipparthi, Subrahmanyam Murala
TaCOS: Task-Specific Camera Optimization with Simulation	The performance of perception tasks is heavily influenced by imaging systems. However designing cameras with high task performance is costly requiring extensive camera knowledge and experimentation with physical hardware. Additionally cameras and perception tasks are mostly designed in isolation whereas recent methods that jointly design cameras and tasks have shown improved performance. Therefore we present a novel end-to-end optimization approach that co-designs cameras with specific vision tasks. This method combines derivative-free and gradient-based optimizers to support both continuous and discrete camera parameters within manufacturing constraints. We leverage recent computer graphics techniques and physical camera characteristics to simulate the cameras in virtual environments making the design process cost-effective. We validate our simulations against physical cameras and provide a procedurally generated virtual environment. Our experiments demonstrate that our method designs cameras that outperform common off-the-shelf options and more efficiently compared to the state-of-the-art approach requiring only 2 minutes to design a camera on an example experiment compared with 67 minutes for the competing method. Designed to support the development of cameras under manufacturing constraints multiple cameras and unconventional cameras we believe this approach can advance the fully automated design of cameras. Code is available on our project page at https://roboticimaging.org/Projects/TaCOS/.	https://openaccess.thecvf.com//content/WACV2025/html/Yan_TaCOS_Task-Specific_Camera_Optimization_with_Simulation_WACV_2025_paper.html	Chengyang Yan, Donald G. Dansereau
Talking Head Anime 4: Distillation for Real-Time Performance	We study the problem of creating a character model that can be controlled in real time from a single image of an anime character. A solution would greatly reduce the cost of creating avatars computer games and other interactive applications. Talking Head Anime 3 (THA3) is an open source project that attempts to directly address the problem. It takes as input (1) an image of an anime character's upper body and (2) a 45-dimensional pose vector and outputs a new image of the same character taking the specified pose. The range of possible movements is expressive enough for personal avatars and certain types of game characters. THA3's main limitation is its speed. It can achieve interactive frame rates (approximately 20 FPS) only if it is run on a very powerful GPU (Nvidia Titan RTX or better). Based on the insight that avatars and game characters do not need to change their appearance every so often we propose a technique to distill the system into a small student neural network (< 2 MB) specific to a particular character. The student model can generate 512x512 animation frames in real time (no more than 30 FPS) using consumer gaming GPUs while preserving the image quality of the teacher model. For the first time our technique makes the whole system practical for real-time applications.	https://openaccess.thecvf.com//content/WACV2025/html/Khungurn_Talking_Head_Anime_4_Distillation_for_Real-Time_Performance_WACV_2025_paper.html	Pramook Khungurn
Task Configuration Impacts Annotation Quality and Model Training Performance in Crowdsourced Image Segmentation	Many industrial image segmentation systems require training on large annotated datasets but there is little standardization for producing training annotations. Data is often obtained via crowdsourcing but many labeling task configurations are set by convenience and may have unintended effects on data quality. In this work we (1) demonstrate a new software tool for running crowdsourced image segmentation experiments (2) present a dataset capturing variation in segmentation annotations produced under different task configurations and (3) experimentally evaluate the quality of the annotations produced by these different configurations. We show annotation quality can be significantly improved by paying annotators per annotated object rather than per image and by leveraging paintbrush-style drawing tools rather than polygon or curve drag tools. We also show that some task complexity is required to maintain annotator engagement and sufficient task performance. Finally we show that many configuration-related annotation errors degrade model training performance but that models can tolerate error error patterns that are common across crowdsourced annotation schemes.	https://openaccess.thecvf.com//content/WACV2025/html/Bauchwitz_Task_Configuration_Impacts_Annotation_Quality_and_Model_Training_Performance_in_WACV_2025_paper.html	Benjamin R Bauchwitz, Mary Cummings
TaxaBind: A Unified Embedding Space for Ecological Applications	We present TaxaBind a unified embedding space for characterizing any species of interest. TaxaBind is a multimodal embedding space across six modalities: ground-level images of species geographic location satellite image text audio and environmental features useful for solving ecological problems. To learn this joint embedding space we leverage ground-level images of species as a binding modality. We propose multimodal patching a technique for effectively distilling the knowledge from various modalities into the binding modality. We construct two large datasets for pretraining: iSatNat with species images and satellite images and iSoundNat with species images and audio. Additionally we introduce TaxaBench-8k a diverse multimodal dataset with six paired modalities for evaluating deep learning models on ecological tasks. Experiments with TaxaBind demonstrate its strong zero-shot and emergent capabilities on a range of tasks including species classification cross-model retrieval and audio classification. The datasets and models are made available at https://github.com/mvrl/TaxaBind.	https://openaccess.thecvf.com//content/WACV2025/html/Sastry_TaxaBind_A_Unified_Embedding_Space_for_Ecological_Applications_WACV_2025_paper.html	Srikumar Sastry, Subash Khanal, Aayush Dhakal, Adeel Ahmad, Nathan Jacobs
TempA-VLP: Temporal-Aware Vision-Language Pretraining for Longitudinal Exploration in Chest X-ray Image	Longitudinal medical image processing is a significant task to understand the dynamic changes of disease by taking and comparing image series over time providing insights into how conditions evolve and enabling more accurate diagnosis and treatment planning. While recent advancements in biomedical Vision-Language Pre-training (VLP) have enabled label-efficient representation learning with paired medical images and reports existing methods primarily pair a single image with the corresponding textual report limiting their ability to capture temporal relationships. To address this limitation it is essential to learn temporal-aware cross-modal representations from sequential medical images and text reports that highlight the temporal changes occurring between examinations. Specifically we introduce TempA-VLP a temporal-aware vision language pre-training framework with a cross-exam encoder to integrate the information from both prior and current examinations. This approach enables the model to capture dynamic representations that reflect disease progression over time which allows us to (i) achieve state-of-the-art performance in disease progression classification (ii) localize dynamic progression regions across consecutive examinations as demonstrated in our new task dynamic phrase grounding on the Chest-Imagenome Gold dataset and (iii) highlight progression localized regions often relevant to lesion areas which in turn improves disease classification tasks on a single image.	https://openaccess.thecvf.com//content/WACV2025/html/Yang_TempA-VLP_Temporal-Aware_Vision-Language_Pretraining_for_Longitudinal_Exploration_in_Chest_X-ray_WACV_2025_paper.html	Zhuoyi Yang, Liyue Shen
Temporal Dynamics in Visual Data: Analyzing the Impact of Time on Classification Accuracy	"Visual datasets are generally constructed from the samples available at the time of their collection and are not further updated. However these static datasets do not reflect the distribution changes that occur in real data. We analyze how different collection times lead to a shift in class distribution by collecting a set of Flickr images published over 14 years. The proposed ""Visual Classes through Time"" (VCT-107) dataset contains images tagged by their publication date and includes 107 classes covering various topics (human-made objects animals plants food etc.). Images from each class are divided into five collection periods to study the impact of time on classification accuracy. When training different classification models using linear probing we observe an accuracy loss when training on data from one period and testing on other periods. This happens even in the case of a strongly pre-trained model like DinoV2 ViT-B/14. Intuitively the performance loss is generally more significant when the collection periods between the training and test data are further apart. Our analysis reveals that the temporal shift varies between classes with the largest shifts observed for human-made objects and the smallest for natural concepts such as animal species. Our results stress the importance of regularly updating models to adapt to time-induced changes in the distribution of visual classes even when using a strongly pre-trained model. We release the VCT-107 dataset to facilitate research on temporal shifts."	https://openaccess.thecvf.com//content/WACV2025/html/Pegeot_Temporal_Dynamics_in_Visual_Data_Analyzing_the_Impact_of_Time_WACV_2025_paper.html	Tom PÃ©geot, Eva Feillet, Adrian Popescu, Inna Kucher, Bertrand Delezoide
Temporally Grounding Instructional Diagrams in Unconstrained Videos	We study the challenging problem of simultaneously localizing a sequence of queries in the form of instructional diagrams in a video. This requires understanding not only the individual queries but also their interrelationships. However most existing methods focus on grounding one query at a time ignoring the inherent structures among queries such as the general mutual exclusiveness and the temporal order. Consequently the predicted timespans of different step diagrams may overlap considerably or violate the temporal order thus harming the accuracy. In this paper we tackle this issue by simultaneously grounding a sequence of step diagrams. Specifically we propose composite queries constructed by exhaustively pairing up the visual content features of the step diagrams and a fixed number of learnable positional embeddings. Our insight is that self-attention among composite queries carrying different content features suppress each other to reduce timespan overlaps in predictions while the cross-attention corrects the temporal misalignment via content and position joint guidance. We demonstrate the effectiveness of our approach on the IAW dataset for grounding step diagrams and the YouCook2 benchmark for grounding natural language queries significantly outperforming existing methods while simultaneously grounding multiple queries.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_Temporally_Grounding_Instructional_Diagrams_in_Unconstrained_Videos_WACV_2025_paper.html	Jiahao Zhang, Frederic Z. Zhang, Cristian Rodriguez, Yizhak Ben-Shabat, Anoop Cherian, Stephen Gould
Temporally Streaming Audio-Visual Synchronization for Real-World Videos	We introduce RealSync a novel dataset designed to significantly enhance the training and evaluation of models for audio-visual synchronization (AV Sync) tasks. Sourced from high-quality YouTube channels RealSync covers a wide range of content domains providing an improved scale diversity and alignment with broadcast content compared to existing datasets. It features extended-length video samples catering to the critical need for more comprehensive real-world training and evaluation materials. Alongside this dataset we present StreamSync a model tailored for real-world AV Sync applications. StreamSync is designed to be backbone agnostic and incorporates a streaming mechanism that processes consecutive video segments dynamically iteratively refining synchronization predictions. This innovative approach enables StreamSync to outperform existing models offering superior synchronization accuracy with minimal computational cost per iteration. Together our dataset and the StreamSync model establish a new benchmark for AVSync research promising to drive the development of more robust and practical AVSync methods. https://github.com/jvoas655/StreamSync	https://openaccess.thecvf.com//content/WACV2025/html/Voas_Temporally_Streaming_Audio-Visual_Synchronization_for_Real-World_Videos_WACV_2025_paper.html	Jordan G Voas, Wei-Cheng Tseng, Layne Berry, Xixi Hu, Puyuan Peng, James Stuedemann, David Harwath
Test-Time Adaptation in Point Clouds: Leveraging Sampling Variation with Weight Averaging	Test-Time Adaptation (TTA) addresses distribution shifts during testing by adapting a pretrained model without access to source data. In this work we propose a novel TTA approach for 3D point cloud classification combining sampling variation with weight averaging. Our method leverages Farthest Point Sampling (FPS) and K-Nearest Neighbors (KNN) to create multiple point cloud representations adapting the model for each variation using the TENT algorithm. The final model parameters are obtained by averaging the adapted weights leading to improved robustness against distribution shifts. Extensive experiments on ModelNet40-C ShapeNet-C and ScanObjectNN-C datasets with different backbones (Point-MAE PointNet DGCNN) demonstrate that our approach consistently outperforms existing methods while maintaining minimal resource overhead. The proposed method effectively enhances model generalization and stability in challenging real-world conditions. The implementation is available at: https://github.com/AliBahri94/SVWA_TTA.git.	https://openaccess.thecvf.com//content/WACV2025/html/Bahri_Test-Time_Adaptation_in_Point_Clouds_Leveraging_Sampling_Variation_with_Weight_WACV_2025_paper.html	Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani Oghani, Milad Cheraghalikhani, David Osowiechi, Farzad Beizaee, Gustavo A. Vargas Hakim, Ismail Ben Ayed, Christian Desrosiers
Test-Time Adaptation of 3D Point Clouds via Denoising Diffusion Models	Test-time adaptation (TTA) of 3D point clouds is crucial for mitigating discrepancies between training and testing samples in real-world scenarios particularly when handling corrupted point clouds. LiDAR data for instance can be affected by sensor failures or environmental factors causing domain gaps. Adapting models to these distribution shifts online is crucial as training for every possible variation is impractical. Existing methods often focus on fine-tuning pre-trained models based on self-supervised learning or pseudo-labeling which can lead to forgetting valuable source domain knowledge over time and reduce generalization on future tests. In this paper we introduce a novel 3D test-time adaptation method termed 3DD-TTA which stands for 3D Denoising Diffusion Test-Time Adaptation. This method uses a diffusion strategy that adapts input point cloud samples to the source domain while keeping the source model parameters intact. The approach uses a Variational Autoencoder (VAE) to encode the corrupted point cloud into a shape latent and latent points. These latent points are corrupted with Gaussian noise and subjected to a denoising diffusion process. During this process both the shape latent and latent points are updated to preserve fidelity guiding the denoising toward generating consistent samples that align more closely with the source domain. We conduct extensive experiments on the ShapeNet dataset and investigate its generalizability on ModelNet40 and ScanObjectNN achieving state-of-the-art results. The code has been released at https://github.com/hamidreza-dastmalchi/3DD-TTA.	https://openaccess.thecvf.com//content/WACV2025/html/Dastmalchi_Test-Time_Adaptation_of_3D_Point_Clouds_via_Denoising_Diffusion_Models_WACV_2025_paper.html	Hamidreza Dastmalchi, Aijun An, Ali Cheraghian, Shafin Rahman, Sameera Ramasinghe
Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models	The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time involves tuning learnable prompts i.e. test-time prompt tuning. This paper introduces Test-Time Low-rank adaptation (TTL) as an alternative to prompt tuning for zero-shot generalization of large-scale VLMs. Taking inspiration from recent advancements in efficiently fine-tuning large language models TTL offers a test-time parameter-efficient adaptation approach that updates the attention weights of the transformer encoder by maximizing prediction confidence. The self-supervised confidence maximization objective is specified using a weighted entropy loss that enforces consistency among predictions of augmented samples. TTL introduces only a small amount of trainable parameters for low-rank adapters in the model space while keeping the prompts and backbone frozen. Extensive experiments on a variety of natural distribution and cross-domain tasks show that TTL can outperform other techniques for test-time optimization of VLMs in strict zero-shot settings. Specifically TTL outperforms test-time prompt tuning baselines with a significant improvement on average. Our code is available at https://github.com/Razaimam45/TTL-Test-Time-Low-Rank-Adaptation.	https://openaccess.thecvf.com//content/WACV2025/html/Imam_Test-Time_Low_Rank_Adaptation_via_Confidence_Maximization_for_Zero-Shot_Generalization_WACV_2025_paper.html	Raza Imam, Hanan Gani, Muhammad Huzaifa, Karthik Nandakumar
Text Change Detection in Multilingual Documents using Image Comparison	Document comparison typically relies on optical character recognition (OCR) as its core technology. However OCR requires the selection of appropriate language models for each document and the performance of multilingual or hybrid models remains limited. To overcome these challenges we propose text change detection (TCD) using an image comparison model tailored for multilingual documents. Unlike OCR-based approaches our method employs word-level text image-to-image comparison to detect changes. Our model generates bidirectional change segmentation maps between the source and target documents. To enhance performance without requiring explicit text alignment or scaling preprocessing we employ correlations among multi-scale attention features. We also construct a benchmark dataset comprising actual printed and scanned word pairs in various languages to evaluate our model. We validate our approach using our benchmark dataset and public benchmarks Distorted Document Images and the LRDE Document Binarization Dataset. We compare our model against state-of-the-art semantic segmentation and change detection models as well as to conventional OCR-based models.	https://openaccess.thecvf.com//content/WACV2025/html/Park_Text_Change_Detection_in_Multilingual_Documents_using_Image_Comparison_WACV_2025_paper.html	Doyoung Park, Naresh Reddy Yarram, Sunjin Kim, MinKyu Kim, Seongho Joe, Taehee Lee
Text-to-Image Synthesis for Domain Generalization in Face Anti-Spoofing	This paper addresses the challenge of developing robust Face Anti-Spoofing (FAS) models for face recognition systems. Traditional FAS protocols are limited by a lack of diversity in subject identities and environmental conditions restricting generalization to real-world scenarios. Recent advancements in spoof image synthesis have mitigated data scarcity but still fail to capture the full range of facial attributes and environmental variability needed for effective domain generalization. To address this we propose a novel framework capable of generating diverse realistic facial images with text-guided control. We fine-tune Stable Diffusion to extract real facial features and specifically train LoRA layers to capture detailed spoof patterns. Additionally the text-guided control of attributes helps overcome the lack of diversity seen in previous methods. Extensive experiments demonstrate that our text-to-image-based synthetic data generation significantly enhances the robustness of FAS models establishing a new benchmark for domain-independent and reliable anti-spoofing systems.	https://openaccess.thecvf.com//content/WACV2025/html/Ko_Text-to-Image_Synthesis_for_Domain_Generalization_in_Face_Anti-Spoofing_WACV_2025_paper.html	Naeun Ko, Yonghyun Jeong, Jong Chul Ye
Texture Shape and Order Matter: A New Transformer Design for Sequential DeepFake Detection	Sequential DeepFake detection is an emerging task that predicts the manipulation sequence in order. Existing methods typically formulate it as an image-to-sequence problem employing conventional Transformer architectures. However these methods lack dedicated design and consequently result in limited performance. As such this paper describes a new Transformer design called TSOM by exploring three perspectives: Texture Shape and Order of Manipulations. Our method features four major improvements: we describe a new texture-aware branch that effectively captures subtle manipulation traces with a Diversiform Pixel Difference Attention module. Then we introduce a Multi-source Cross-attention module to seek deep correlations among spatial and sequential features enabling effective modeling of complex manipulation traces. To further enhance the cross-attention we describe a Shape-guided Gaussian mapping strategy providing initial priors of the manipulation shape. Finally observing that the subsequent manipulation in a sequence may influence traces left in the preceding one we intriguingly invert the prediction order from forward to backward leading to notable gains as expected. Extensive experimental results demonstrate that our method outperforms others by a large margin highlighting the superiority of our method.	https://openaccess.thecvf.com//content/WACV2025/html/Li_Texture_Shape_and_Order_Matter_A_New_Transformer_Design_for_WACV_2025_paper.html	Yunfei Li, Yuezun Li, Xin Wang, Baoyuan Wu, Jiaran Zhou, Junyu Dong
The FineView Dataset:A 3D Scanned Multi-View Object Dataset of Fine-Grained Category Instances	In the past decade state-of-the-art deep learning models have shown impressive performance in many computer vision tasks by learning from large and diverse image datasets. Most of these datasets consist of web-scraped image collections. This approach however makes it very challenging to obtain desirable data such as multiple views of the same object 3D geometric information or camera parameters for a large-scale image dataset. In this paper we propose a 3D-scanned multi-view 2D image dataset of fine-grained category instances with accurate camera calibration parameters. We describe our bi-directional multi-camera and 3D scanning system and the data collection pipeline. Our target objects are relatively small highly-detailed fine-grained category instances such as insects. We present this dataset as a contribution to fine-grained visual categorization 3D representation learning and for use in other computer vision tasks. The final version of the FineView dataset is available at: https://github.com/byu-vision/fineview	https://openaccess.thecvf.com//content/WACV2025/html/Onda_The_FineView_DatasetA_3D_Scanned_Multi-View_Object_Dataset_of_Fine-Grained_WACV_2025_paper.html	Suguru Onda, Ryan Farrell
Through the Curved Cover: Synthesizing Cover Aberrated Scenes with Refractive Field	Recent extended reality headsets and field robots have adopted covers to protect the front-facing cameras from environmental hazards and falls. The surface irregularities on the cover can lead to optical aberrations like blurring and non-parametric distortions. Novel view synthesis methods like NeRF and 3D Gaussian Splatting are ill-equipped to synthesize from sequences with optical aberrations. To address this challenge we introduce SynthCover to enable novel view synthesis through protective covers for downstream extended reality applications. SynthCover employs a Refractive Field that estimates the cover's geometry enabling precise analytical calculation of refracted rays. Experiments on synthetic and real-world scenes demonstrate our method's ability to accurately model scenes viewed through protective covers achieving a significant improvement in rendering quality compared to prior methods. We also show that the model can adjust well to various cover geometries with synthetic sequences captured with covers of different surface curvatures. To motivate further studies on this problem we provide the benchmarked dataset containing real and synthetic walkable scenes captured with protective cover optical aberrations.	https://openaccess.thecvf.com//content/WACV2025/html/Xie_Through_the_Curved_Cover_Synthesizing_Cover_Aberrated_Scenes_with_Refractive_WACV_2025_paper.html	Liuyue Xie, Jiancong Guo, LÃ¡szlÃ³ A. Jeni, Zhiheng Jia, Mingyang Li, Yunwen Zhou, Chao Guo
TimberVision: A Multi-Task Dataset and Framework for Log-Component Segmentation and Tracking in Autonomous Forestry Operations	Timber represents an increasingly valuable and versatile resource. However forestry operations such as harvesting handling and measuring logs still require substantial human labor in remote environments posing significant safety risks. Progressively automating these tasks has the potential of increasing their efficiency as well as safety but requires an accurate detection of individual logs as well as live trees and their context. Although initial approaches have been proposed for this challenging application domain specialized data and algorithms are still too scarce to develop robust solutions. To mitigate this gap we introduce the TimberVision dataset consisting of more than 2k annotated RGB images containing a total of 51k trunk components including cut and lateral surfaces thereby surpassing any existing dataset in this domain in terms of both quantity and detail by a large margin. Based on this data we conduct a series of ablation experiments for oriented object detection and instance segmentation and evaluate the influence of multiple scene parameters on model performance. We introduce a generic framework to fuse the components detected by our models for both tasks into unified trunk representations. Furthermore we automatically derive geometric properties and apply multi-object tracking to further enhance robustness. Our detection and tracking approach provides highly descriptive and accurate trunk representations solely from RGB image data even under challenging environmental conditions. Our solution is suitable for a wide range of application scenarios and can be readily combined with other sensor modalities.	https://openaccess.thecvf.com//content/WACV2025/html/Steininger_TimberVision_A_Multi-Task_Dataset_and_Framework_for_Log-Component_Segmentation_and_WACV_2025_paper.html	Daniel Steininger, Julia Simon, Andreas Trondl, Markus Murschitz
To Ask or Not to Ask? Detecting Absence of Information in Vision and Language Navigation	"Recent research in Vision Language Navigation (VLN) has overlooked the development of agents' inquisitive abilities which allow them to ask clarifying questions when instructions are incomplete. This paper addresses how agents can recognize ""when"" they lack sufficient information without focusing on ""what"" is missing particularly in VLN tasks with vague instructions. Equipping agents with this ability enhances efficiency by reducing potential digressions and seeking timely assistance. The challenge in identifying such uncertain points is balancing between being overly cautious (high recall) and overly confident (high precision). We propose an attention-based instruction-vagueness estimation module that learns associations between instructions and the agent's trajectory. By leveraging instruction-to-path alignment information during training the module's vagueness estimation performance improves by around 52% in terms of precision-recall balance. In our ablative experiments we also demonstrate the effectiveness of incorporating this additional instruction-to-path attention network alongside the cross-modal attention networks within the navigator module. Our results show that the attention scores from the instruction-to-path attention network serve as better indicators for estimating vagueness."	https://openaccess.thecvf.com//content/WACV2025/html/Abraham_To_Ask_or_Not_to_Ask_Detecting_Absence_of_Information_WACV_2025_paper.html	Savitha Sam Abraham, Sourav Garg, Feras Dayoub
Token Turing Machines are Efficient Vision Models	We propose Vision Token Turing Machines (ViTTM) an efficient low-latency memory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing Machines (NTM) and Token Turing Machines (TTM) which were applied to NLP and sequential visual understanding tasks. ViTTMs are designed for non-sequential computer vision tasks such as image classification and segmentation. Our model creates two sets of tokens: process tokens and memory tokens; process tokens pass through encoder blocks and read-write from memory tokens at each encoder block in the network allowing them to store and retrieve information from memory. By ensuring that there are fewer process tokens we are able to reduce the inference time of the network while maintaining its accuracy. On ImageNet-1K the state-of-the-art ViT-B has median latency of 529.5ms and 81.0% accuracy while our ViTTM-B is 56% faster (234.1ms) with 2.4x fewer FLOPs with an accuracy of 82.9%. On ADE20K semantic segmentation ViT-B achieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model acheives a 45.17 mIoU with 26.8 FPS (+94%).	https://openaccess.thecvf.com//content/WACV2025/html/Jajal_Token_Turing_Machines_are_Efficient_Vision_Models_WACV_2025_paper.html	Purvish Jajal, Nick Eliopoulous, Benjamin Shiue-Hal Chou, George K. Thiravathukal, James C. Davis, Yung-Hsiang Lu
TokenBinder: Text-Video Retrieval with One-to-Many Alignment Paradigm	Text-Video Retrieval (TVR) methods typically match query-candidate pairs by aligning text and video features in coarse-grained fine-grained or combined (coarse-to-fine) manners. However these frameworks predominantly employ a one(query)-to-one(candidate) alignment paradigm which struggles to discern nuanced differences among candidates leading to frequent mismatches. Inspired by Comparative Judgement in human cognitive science where decisions are made by directly comparing items rather than evaluating them independently we propose TokenBinder. This innovative two-stage TVR framework introduces a novel one-to-many coarse-to-fine alignment paradigm imitating the human cognitive process of identifying specific items within a large collection. Our method employs a Focused-view Fusion Network with a sophisticated cross-attention mechanism dynamically aligning and comparing features across multiple videos to capture finer nuances and contextual variations. Extensive experiments on six benchmark datasets confirm that TokenBinder substantially outperforms existing state-of-the-art methods. These results demonstrate its robustness and the effectiveness of its fine-grained alignment in bridging intra- and inter-modality information gaps in TVR tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_TokenBinder_Text-Video_Retrieval_with_One-to-Many_Alignment_Paradigm_WACV_2025_paper.html	Bingqing Zhang, Zhuo Cao, Heming Du, Xin Yu, Xue Li, Jiajun Liu, Sen Wang
Towards Accurate Unified Anomaly Segmentation	Unsupervised anomaly detection (UAD) from images strives to model normal data distributions creating discriminative representations to distinguish and precisely localize anomalies. Despite recent advancements in the efficient and unified one-for-all scheme challenges persist in accurately segmenting anomalies for further monitoring. Moreover this problem is obscured by the widely-used AUROC metric under imbalanced UAD settings. This motivates us to emphasize the significance of precise segmentation of anomaly pixels using pAP and DSC as metrics. To address the unsolved segmentation task we introduce the Unified Anomaly Segmentation (UniAS). UniAS presents a multi-level hybrid pipeline that progressively enhances normal information from coarse to fine incorporating a novel multi-granularity gated CNN (MGG-CNN) into Transformer layers to explicitly aggregate local details from different granularities. UniAS achieves state-of-the-art anomaly segmentation performance attaining 65.12/59.33 and 40.06/32.50 in pAP/DSC on the MVTec-AD and VisA datasets respectively surpassing previous methods significantly. The codes are shared at https://github.com/Mwxinnn/UniAS.	https://openaccess.thecvf.com//content/WACV2025/html/Ma_Towards_Accurate_Unified_Anomaly_Segmentation_WACV_2025_paper.html	Wenxin Ma, Qingsong Yao, Xiang Zhang, Zhelong Huang, Zihang Jiang, S.Kevin Zhou
Towards Generalized Face Anti-Spoofing from a Frequency Shortcut View	The generalization capability of a face anti-spoofing (FAS) model is critical to its practicality in the real world. Recent studies have theoretically and empirically uncovered that neural networks tend to exploit easy-to-learn frequency sets for decisions. These simplicity-biased representations depending on what best simplifies the training objective may hamper generalization. This paper thus focuses on mitigating the frequency shortcut learning of prior FAS models for improved generalization. Specifically we introduce a frequency-aware autoencoder to retain more frequency details in intermediate features via reconstruction facilitating comprehensive judgment of FAS. Based on the encoder output we propose a dynamic frequency masking mechanism to select and suppress the probable shortcut bands during training enabling broader horizons on under-explored frequencies. Moreover we employ a style inhibited modulation to weaken stylized information in frequency space to reduce the reliance on spurious style features. Experiment results on generalized FAS benchmarks verify the superiority of our framework over existing methods. Our code has been integrated into this project: https://github.com/VISION-SJTU/UniDefense.	https://openaccess.thecvf.com//content/WACV2025/html/Cao_Towards_Generalized_Face_Anti-Spoofing_from_a_Frequency_Shortcut_View_WACV_2025_paper.html	Junyi Cao, Chao Ma
Towards High-Fidelity Head Blending with Chroma Keying for Industrial Applications	We introduce an industrial Head Blending pipeline for the task of seamlessly integrating an actor's head onto a target body in digital content creation. The key challenge stems from discrepancies in head shape and hair structure which lead to unnatural boundaries and blending artifacts. Existing methods treat foreground and background as a single task resulting in suboptimal blending quality. To address this problem we propose CHANGER a novel pipeline that decouples background integration from foreground blending. By utilizing chroma keying for artifact-free background generation and introducing Head shape and long Hair augmentation (H2 augmentation) to simulate a wide range of head shapes and hair styles CHANGER improves generalization on innumerable various real-world cases. Furthermore our Foreground Predictive Attention Transformer (FPAT) module enhances foreground blending by predicting and focusing on key head and body regions. Quantitative and qualitative evaluations on benchmark datasets demonstrate that our CHANGER outperforms state-of-the-art methods delivering high-fidelity industrial-grade results.	https://openaccess.thecvf.com//content/WACV2025/html/Lew_Towards_High-Fidelity_Head_Blending_with_Chroma_Keying_for_Industrial_Applications_WACV_2025_paper.html	Hah Min Lew, Sahng-Min Yoo, Hyunwoo Kang, Gyeong-Moon Park
Towards On-the-Fly Novel Category Discovery in Dynamic Long-Tailed Distributions	As the diversity of real-world object categories increases the need for sophisticated classification methods also grows. However Novel Category Discovery (NCD) which aims to predict unseen categories often falls short in scenarios where new categories are constantly updated and data distributions are potentially biased. In addition existing dynamic NCD approaches assume that incremental stages introduce a fixed number of new classes and often overlook distributional biases in real-world classes. To address these limitations we propose a novel framework Novel Category Discovery for Dynamic Long-Tailed distribution (NCD-DLT) which deals with the more realistic and challenging scenario where imbalanced unlabeled data are introduced incrementally and sporadically over time. Unlike conventional methods requiring k-means clustering on all test samples our approach identifies novel categories on-the-fly predicting categories for individual data points as they arrive. We propose an advanced hash-based clustering technique leveraging a double-hashing strategy to mitigate collisions and incorporating a greedy hash regularization loss for sparse representations to enhance clustering capabilities. Furthermore we implement distillation losses during training to preserve the model's discriminative power across stages without forgetting prior knowledge. Finally we introduce a novel graph merging algorithm based on the Hash Hamming Graph revealing the dataset's clustering structure. It serves as a mechanism for pseudo-labeling in training and acts as a post-processing tool reallocating less confident samples to more appropriate clusters. Our comprehensive approach addresses the limitations of existing NCD methods in the dynamic scenario of novel category discovery in long-tailed distributions demonstrating improved accuracy for both uniform and long-tailed scenarios.	https://openaccess.thecvf.com//content/WACV2025/html/Jung_Towards_On-the-Fly_Novel_Category_Discovery_in_Dynamic_Long-Tailed_Distributions_WACV_2025_paper.html	Hoin Jung, Xiaoqian Wang
Towards Privacy-Preserving Split Learning for ControlNet	With the emerging trend of large generative models ControlNet is introduced to enable users to fine-tune pre-trained models with their own data for various use cases. A natural question arises: how can we train ControlNet models while ensuring users' data privacy across distributed devices? We first propose a new distributed learning structure that eliminates the need for the server to send gradients based on split learning. We discover that in the context of fine-tuning ControlNet with split learning most existing attacks are ineffective except for two mentioned in previous literature. To counter these threats we leverage the properties of diffusion models and design a new timestep sampling policy during forward processes. We also propose a privacy-preserving activation function and a method to prevent private text prompts from leaving clients tailored for image generation with diffusion models. Our experimental results demonstrate that our algorithms and systems greatly enhance the efficiency of distributed fine-tuning for ControlNet while ensuring users' data privacy without compromising image generation quality.	https://openaccess.thecvf.com//content/WACV2025/html/Yao_Towards_Privacy-Preserving_Split_Learning_for_ControlNet_WACV_2025_paper.html	Dixi Yao
Towards Real-Time Open-Vocabulary Video Instance Segmentation	In this paper we address the challenge of performing open-vocabulary video instance segmentation (OV-VIS) in real-time. We analyze the computational bottlenecks of state-of-the-art foundation models that performs OV-VIS and propose a new method TROY-VIS that significantly improves processing speed while maintaining high accuracy. We introduce three key techniques: (1) Decoupled Attention Feature Enhancer to speed up information interaction between different modalities and scales; (2) Flash Embedding Memory for obtaining fast text embeddings of object categories; and (3) Kernel Interpolation for exploiting the temporal continuity in videos. Our experiments demonstrate that TROY-VIS achieves the best trade-off between accuracy and speed on two large-scale OV-VIS benchmarks BURST and LV-VIS running 20x faster than GLEE-Lite (25 FPS v.s. 1.25 FPS) with comparable or even better accuracy. These results demonstrate TROY-VIS's potential for real-time applications in dynamic environments such as mobile robotics and augmented reality. Code and model will be released at https://github.com/google-research/troyvis.	https://openaccess.thecvf.com//content/WACV2025/html/Yan_Towards_Real-Time_Open-Vocabulary_Video_Instance_Segmentation_WACV_2025_paper.html	Bin Yan, Martin Sundermeyer, David Joseph Tan, Huchuan Lu, Federico Tombari
Towards Robust Training via Gradient-Diversified Backpropagation	Neural networks are prone to be vulnerable to adversarial attacks and domain shifts. Adversarial-driven methods including adversarial training and adversarial augmentation have been frequently proposed to improve the model's robustness against adversarial attacks and distribution-shifted samples. Nonetheless recent research on adversarial attacks has cast a spotlight on the robustness lacuna against attacks targeted at deep semantic layers. Our analysis reveals that previous adversarial-driven methods tend to generate overpowering perturbations in deep semantic layers leading to distortion of the training for these layers. This can be primarily attributed to the exclusive utilization of loss functions on the output layer for adversarial gradient generation. This inherent practice projects an excessive adversarial impact on the deep semantic layers elevating the difficulty of training such layers. Therefore from the standing point of relaxing the excessive perturbations in the deep semantic layer and diversifying the adversarial gradients to ensure robust training for deep semantic layers this paper proposes a novel Stochastic Loss Integration Method (SLIM) which can be instantiated into the existing adversarial-driven methods in a plug-and-play manner. Experimental results across diverse tasks including classification and segmentation as well as various areas such as adversarial robustness and domain generalization validate the effectiveness of our proposed method. Furthermore we provide an in-depth analysis to offer a comprehensive understanding of layer-wise training involving various loss terms.	https://openaccess.thecvf.com//content/WACV2025/html/He_Towards_Robust_Training_via_Gradient-Diversified_Backpropagation_WACV_2025_paper.html	Xilin He, Cheng Luo, Qinliang Lin, Weicheng Xie, Muhammad Haris Khan, Siyang Song, Linlin Shen
Towards Secure and Usable 3D Assets: A Novel Framework for Automatic Visible Watermarking	3D models particularly AI-generated ones have witnessed a recent surge across various industries such as entertainment. Hence there is an alarming need to protect the intellectual property and avoid the misuse of these valuable assets. As a viable solution to address these concerns we rigorously define the novel task of automated 3D visible watermarking in terms of two competing aspects: watermark quality and asset utility. Moreover we propose a method of embedding visible watermarks that automatically determines the right location orientation and number of watermarks to be placed on arbitrary 3D assets for high watermark quality and asset utility. Our method is based on a novel rigid-body optimization that uses back-propagation to automatically learn transforms for ideal watermark placement. In addition we propose a novel curvature-matching method for fusing the watermark into the 3D model that further improves readability and security. Finally we provide a detailed experimental analysis on two benchmark 3D datasets validating the superior performance of our approach in comparison to baselines. Code and demo are available here.	https://openaccess.thecvf.com//content/WACV2025/html/Singh_Towards_Secure_and_Usable_3D_Assets_A_Novel_Framework_for_WACV_2025_paper.html	Gursimran Singh, Tianxi Hu, Mohammad Akbari, Qiang Tang, Yong Zhang
Towards Unbiased Continual Learning: Avoiding Forgetting in the Presence of Spurious Correlations	Continual Learning (CL) has emerged as a paramount area in Artificial Intelligence (AI) because of its ability to learn multiple tasks sequentially without significant performance degradation. Despite the growing interest in CL frameworks a critical aspect must be addressed: the inherent biases within training data. In this work we show that if overlooked these biases can significantly impair the efficacy of continual learning models by inducing reliance on suboptimal shortcuts during data stream and memory retention exacerbating catastrophic forgetting. In response we present Learning without Shortcuts (LwS) which sets forth two primary objectives: (i) to identify and mitigate the exploitation of spurious correlations within the data stream and (ii) to develop a novel mechanism that constructs a fair memory buffer used in replay-based CL strategies. Our buffer construction strategy exploits the model confidence in a given example to balance the portion of samples per class hence their contribution when replay activates. Unlike existing methods LwS is agnostic to protected attributes and results highlight that the proposed solution is indeed resilient to spurious correlations in CL settings. Code is available at https://github.com/aimagelab/mammoth	https://openaccess.thecvf.com//content/WACV2025/html/Capitani_Towards_Unbiased_Continual_Learning_Avoiding_Forgetting_in_the_Presence_of_WACV_2025_paper.html	Giacomo Capitani, Lorenzo Bonicelli, Angelo Porrello, Federico Bolelli, Simone Calderara, Elisa Ficarra
Towards Unsupervised Blind Face Restoration using Diffusion Prior	Blind face restoration methods have shown remarkable performance particularly when trained on large-scale synthetic datasets with supervised learning. These datasets are often generated by simulating low-quality face images with a handcrafted image degradation pipeline. The models trained on such synthetic degradations however cannot deal with inputs of unseen degradations. In this paper we address this issue by using only a set of input images with unknown degradations and without ground truth targets to fine-tune a restoration model that learns to map them to clean and contextually consistent outputs. We utilize a pre-trained diffusion model as a generative prior through which we generate high quality images from the natural image distribution while maintaining the input image content through consistency constraints. These generated images are then used as pseudo targets to fine-tune a pre-trained restoration model. Unlike many recent approaches that employ diffusion models at test time we only do so during training and thus maintain an efficient inference-time performance. Extensive experiments show that the proposed approach can consistently improve the perceptual quality of pre-trained blind face restoration models while maintaining great consistency with the input contents. Our best model also achieves the state-of-the-art results on both synthetic and real-world datasets.	https://openaccess.thecvf.com//content/WACV2025/html/Kuai_Towards_Unsupervised_Blind_Face_Restoration_using_Diffusion_Prior_WACV_2025_paper.html	Tianshu Kuai, Sina Honari, Igor Gilitschenski, Alex Levinshtein
Towards Utilising a Range of Neural Activations for Comprehending Representational Associations	Recent efforts to understand intermediate representations in deep neural networks have commonly attempted to label individual neurons and combinations of neurons that make up linear directions in the latent space by examining extremal neuron activations and the highest direction projections. In this paper we show that this approach although yielding a good approximation for many purposes fails to capture valuable information about the behaviour of a representation. Neural network activations are generally dense and so a more complex but realistic scenario is that linear directions encode information at various levels of stimulation. We hypothesise that non-extremal level activations contain complex information worth investigating such as statistical associations and thus may be used to locate confounding human interpretable concepts. We explore the value of studying a range of neuron activations by taking the case of mid-level output neuron activations and demonstrate on a synthetic dataset how they can inform us about aspects of representations in the penultimate layer not evident through analysing maximal activations alone. We use our findings to develop a method to curate data from mid-range logit samples for retraining to mitigate spurious correlations or confounding concepts in the penultimate layer on real benchmark datasets. The success of our method exemplifies the utility of inspecting non-maximal activations to extract complex relationships learned by models.	https://openaccess.thecvf.com//content/WACV2025/html/OMahony_Towards_Utilising_a_Range_of_Neural_Activations_for_Comprehending_Representational_WACV_2025_paper.html	Laura O'Mahony, Nikola S. Nikolov, David JP O'Sullivan
Towards Zero-Shot 3D Anomaly Localization	3D anomaly detection and localization is of great significance for industrial inspection. Prior 3D anomaly detection and localization methods focus on the setting that the testing data share the same category as the training data which is normal. However in real-world applications the normal training data for the target 3D objects can be unavailable due to issues like data privacy or export control regulation. To tackle these challenges we identify a new task - zero-shot 3D anomaly detection and localization where the training and testing classes do not overlap. To this end we design 3DzAL a novel patch-level contrastive learning framework based on pseudo anomalies generated using the inductive bias from task-irrelevant 3D xyz data to learn more representative feature representations. Furthermore we train a normalcy classifier network to classify the normal patches and pseudo anomalies and utilize the classification result jointly with feature distance to design anomaly scores. Instead of directly using the patch point clouds we introduce adversarial perturbations to the input patch xyz data before feeding into the 3D normalcy classifier for the classification-based anomaly score. We show that 3DzAL outperforms the state-of-the-art anomaly detection and localization performance.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_Towards_Zero-Shot_3D_Anomaly_Localization_WACV_2025_paper.html	Yizhou Wang, Kuan-Chuan Peng, Yun Fu
Towards a Training Free Approach for 3D Scene Editing	Text driven diffusion models have shown remarkable capabilities in editing images. However when editing 3D scenes existing works mostly rely on training a NeRF for 3D editing. Recent NeRF editing methods leverages edit operations by deploying 2D diffusion models and project these edits into 3D space. They require strong positional priors alongside text prompt to identify the edit location. These methods are operational on small 3D scenes and are more generalized to particular scene. They require training for each specific edit and cannot be exploited in real-time edits. To address these limitations we propose a novel method FreeEdit to make edits in training free manner using mesh representations as a substitute for NeRF. Training-free methods are now a possibility because of the advances in foundation model's space. We leverage these models to bring a training-free alternative and introduce solutions for insertion replacement and deletion. We consider insertion replacement and deletion as basic blocks for performing intricate edits with certain combinations of these operations. Given a text prompt and a 3D scene our model is capable of identifying what object should be inserted/replaced or deleted and location where edit should be performed. We also introduce a novel algorithm as part of FreeEdit to find the optimal location on grounding object for placement. We evaluate our model by comparing it with baseline models on a wide range of scenes using quantitative and qualitative metrics and showcase the merits of our method with respect to others. Project page: https://vivekmadhavaram.github.io/FreeEdit_page/1.	https://openaccess.thecvf.com//content/WACV2025/html/Madhavaram_Towards_a_Training_Free_Approach_for_3D_Scene_Editing_WACV_2025_paper.html	Vivek Madhavaram, Shivangana Rawat, Chaitanya Devaguptapu, Charu Sharma, Manohar Kaul
TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion Models	Despite remarkable achievements in video synthesis achieving granular control over complex dynamics such as nuanced movement among multiple interacting objects still presents a significant hurdle for dynamic world modeling compounded by the necessity to manage appearance and disappearance drastic scale changes and ensure consistency for instances across frames. These challenges hinder the development of video generation that can faithfully mimic real-world complexity limiting utility for applications requiring high-level realism and controllability including advanced scene simulation and training of perception systems. To address that we propose TrackDiffusion a novel video generation framework affording fine-grained trajectory-conditioned motion control via diffusion models which facilitates the precise manipulation of the object trajectories and interactions overcoming the prevalent limitation of scale and continuity disruptions. A pivotal component of TrackDiffusion is the instance enhancer which explicitly ensures inter-frame consistency of multiple objects a critical factor overlooked in the current literature. Moreover we demonstrate that generated video sequences by our TrackDiffusion can be used as training data for visual perception models. To the best of our knowledge this is the first work to apply video diffusion models with tracklet conditions and demonstrate that generated frames can be beneficial for improving the performance of object trackers.	https://openaccess.thecvf.com//content/WACV2025/html/Li_TrackDiffusion_Tracklet-Conditioned_Video_Generation_via_Diffusion_Models_WACV_2025_paper.html	Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Dit-Yan Yeung, Huchuan Lu, Xu Jia
Training-Free Medical Image Inverses via Bi-Level Guided Diffusion Models	In medical imaging inverse problems aim to infer high-fidelity images from incomplete noisy measurements minimizing expenses and risks to patients in clinical settings. Diffusion models have recently emerged as a promising solution to such practical challenges proving particularly useful for the training-free inference of images from partially acquired measurements in Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). A central challenge however is how to guide an unconditional denoised estimate to conform to the measurement information. Existing methods often employ either deficient projection or inefficient posterior approximation leading to suboptimal performance. In this paper we propose Bi-level Guided Diffusion Models (BGDM) a zero-shot imaging framework that efficiently steers the image generation process through a bi-level guidance strategy. Specifically BGDM first approximates an inner-level conditional posterior mean to establish an initial measurement-consistent prediction and then solves an outer-level proximal optimization objective to reinforce the measurement consistency. Our experimental findings leveraging publicly available MRI and CT datasets indicate that BGDM is more effective and efficient compared to baseline methods consistently generating high-fidelity medical images and significantly reducing hallucinatory artifacts in cases of sparse measurements. Code https://github.com/hosseinaskari-cs/BGDM.	https://openaccess.thecvf.com//content/WACV2025/html/Askari_Training-Free_Medical_Image_Inverses_via_Bi-Level_Guided_Diffusion_Models_WACV_2025_paper.html	Hossein Askari, Fred Roosta, Hongfu Sun
Transferable-Guided Attention is All You Need for Video Domain Adaptation	Unsupervised domain adaptation (UDA) in videos is a challenging task that remains not well explored compared to image-based UDA techniques. Although vision transformers (ViT) achieve state-of-the-art performance in many computer vision tasks their use in video UDA has been little explored. Our key idea is to use transformer layers as a feature encoder and incorporate spatial and temporal transferability relationships into the attention mechanism. A Transferable-guided Attention (TransferAttn) framework is then developed to exploit the capacity of the transformer to adapt cross-domain knowledge across different backbones. To improve the transferability of ViT we introduce a novel and effective module named Domain Transferable-guided Attention Block (DTAB). DTAB compels ViT to focus on the spatio-temporal transferability relationship among video frames by changing the self-attention mechanism to a transferability attention mechanism. Extensive experiments were conducted on UCF-HMDB Kinetics-Gameplay and Kinetics-NEC Drone datasets with different backbones like ResNet101 I3D and STAM to verify the effectiveness of TransferAttn compared with state-of-the-art approaches. Also we demonstrate that DTAB yields performance gains when applied to other state-of-the-art transformer-based UDA methods from both video and image domains. Our code is available at https://github.com/Andre-Sacilotti/transferattn-project-code.	https://openaccess.thecvf.com//content/WACV2025/html/Sacilotti_Transferable-Guided_Attention_is_All_You_Need_for_Video_Domain_Adaptation_WACV_2025_paper.html	AndrÃ© Sacilotti, Samuel Felipe dos Santos, Nicu Sebe, Jurandy Almeida
Transferring Foundation Models for Generalizable Robotic Manipulation	Improving the generalization capabilities of general-purpose robotic manipulation in real world has long been a significant challenge. Existing approaches often rely on collecting large-scale robotic data which is costly and time-consuming. However due to insufficient diversity of data they typically suffer from limiting their capability in open-domain scenarios with new objects and diverse environments. In this paper we propose a novel paradigm that effectively leverages language-reasoning segmentation mask generated by internet-scale foundation models to condition robot manipulation tasks. By integrating the mask modality which incorporates semantic geometric and temporal correlation priors derived from vision foundation models into the end-to-end policy model our approach can effectively and robustly perceive object pose and enable sample-efficient generalization learning including new object instances semantic categories and unseen backgrounds. We first introduce a series of foundation models to ground natural language demands across multiple tasks. Secondly we develop a two-stream 2D policy model based on imitation learning which processes raw images and object masks to predict robot actions with a local-global perception manner. Extensive real-world experiments conducted on a Franka Emika robot and a low-cost dual-arm robot demonstrate the effectiveness of our proposed paradigm and policy. Demos can be found in link1 or link2 and our code will be released at https://github.com/MCG-NJU/TPM.	https://openaccess.thecvf.com//content/WACV2025/html/Yang_Transferring_Foundation_Models_for_Generalizable_Robotic_Manipulation_WACV_2025_paper.html	Jiange Yang, Wenhui Tan, Chuhao Jin, Keling Yao, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang
Transientangelo: Few-Viewpoint Surface Reconstruction using Single-Photon Lidar	We consider the problem of few-viewpoint 3D surface reconstruction using raw measurements from a lidar system. Lidar captures 3D scene geometry by emitting pulses of light to a target and recording the speed-of-light time delay of the reflected light. However conventional lidar systems do not output the raw captured waveforms of backscattered light; instead they preprocess these data into a 3D point cloud. Since this procedure typically does not accurately model the noise statistics of the system exploit spatial priors or incorporate information about downstream tasks it ultimately discards useful information that is encoded in raw measurements of backscattered light. Here we propose to leverage raw measurements captured with a single-photon lidar system from multiple viewpoints to optimize a neural surface representation of a scene. The measurements consist of time-resolved photon count histograms or transients which capture information about backscattered light at picosecond time scales. Additionally we develop new regularization strategies that improve robustness to photon noise enabling accurate surface reconstruction with as few as 10 photons per pixel. Our method outperforms other techniques for few-viewpoint 3D reconstruction based on depth maps point clouds or conventional lidar as demonstrated in simulation and with captured data.	https://openaccess.thecvf.com//content/WACV2025/html/Luo_Transientangelo_Few-Viewpoint_Surface_Reconstruction_using_Single-Photon_Lidar_WACV_2025_paper.html	Weihan Luo, Anagh Malik, David B Lindell
Treading Towards Privacy-Preserving Table Structure Recognition	We present TabGuard a privacy-preserving framework for an end-to-end secure Table Structure Recognition. TabGuard masks all the contents of the table locally and utilizes the masked table image for structure recognition. Our method is simple yet effective for detecting table cells while preserving the inherent table alignment characteristics to reconstruct tables. Our approach benefits from inductive bias expressed through an approximated table grid which helps alleviate challenges in the detection of cells that are small or have extreme aspect ratios. Experimental results demonstrate that our solution not only establishes a new state-of-the-art on several benchmark datasets but also effectively addresses long-standing challenges associated with dense tables having complex layouts. We make our code publically available at https://github.com/sachinraja13/TabGuard.	https://openaccess.thecvf.com//content/WACV2025/html/Raja_Treading_Towards_Privacy-Preserving_Table_Structure_Recognition_WACV_2025_paper.html	Sachin Raja, Ajoy Mondal, C.V. Jawahar
TreeFormer: Single-View Plant Skeleton Estimation via Tree-Constrained Graph Generation	Accurate estimation of plant skeletal structure (e.g. branching structure) from images is essential for smart agriculture and plant science. Unlike human skeletons with fixed topology plant skeleton estimation presents a unique challenge i.e. estimating arbitrary tree graphs from images. While recent graph generation methods successfully infer thin structures from images it is challenging to constrain the output graph strictly to a tree structure. To this problem we present TreeFormer a plant skeleton estimator via tree-constrained graph generation. Our approach combines learning-based graph generation with traditional graph algorithms to impose the constraints during the training loop. Specifically our method projects an unconstrained graph onto a minimum spanning tree (MST) during the training loop and incorporates this prior knowledge into the gradient descent optimization by suppressing unwanted feature values. Experiments show that our method accurately estimates target plant skeletal structures for multiple domains: Synthetic tree patterns real botanical roots and grapevine branches. Our implementations are available at https://github.com/huntorochi/TreeFormer/.	https://openaccess.thecvf.com//content/WACV2025/html/Liu_TreeFormer_Single-View_Plant_Skeleton_Estimation_via_Tree-Constrained_Graph_Generation_WACV_2025_paper.html	Xinpeng Liu, Hiroaki Santo, Yosuke Toda, Fumio Okura
Tumor Synthesis Conditioned on Radiomics	Due to privacy concerns obtaining large datasets is challenging in medical image analysis especially with 3D modalities like Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing generative models developed to address this issue often face limitations in output diversity and thus cannot accurately represent 3D medical images. We propose a tumor-generation model that utilizes radiomics features as generative conditions. Radiomics features are high-dimensional handcrafted semantic features that are biologically well-grounded and thus are good candidates for conditioning. Our model employs a GAN-based model to generate tumor masks and a diffusion-based approach to generate tumor texture conditioned on radiomics features. Our method allows the user to generate tumor images according to user-specified radiomics features such as size shape and texture at an arbitrary location. This enables the physicians to easily visualize tumor images to better understand tumors according to changing radiomics features. Our approach allows for the removal manipulation and repositioning of tumors generating various tumor types in different scenarios. The model has been tested on tumors in four different organs (kidney lung breast and brain) across CT and MRI. The synthesized images are shown to effectively aid in training for downstream tasks and their authenticity was also evaluated through expert evaluations. Our method has potential usage in treatment planning with diverse synthesized tumors. Our code is available at github.com/jongdory/TS-Radiomics.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_Tumor_Synthesis_Conditioned_on_Radiomics_WACV_2025_paper.html	Jonghun Kim, Inye Na, Eun Sook Ko, Hyunjin Park
Tuned Contrastive Learning	In recent times contrastive learning based loss functions have become increasingly popular for visual self-supervised representation learning owing to their state-of-the-art (SOTA) performance. Most of the modern contrastive learning methods generalize only to one positive and multiple negatives per anchor in a batch. A recent state-of-the-art contrastive loss called supervised contrastive (SupCon) loss extends self-supervised contrastive learning to supervised setting by generalizing to multiple positives and negatives in a batch and improves upon the cross-entropy loss. In this paper we propose a novel contrastive loss function -- Tuned Contrastive Learning (TCL) loss that generalizes to multiple positives and negatives in a batch and offers parameters to tune and improve the gradient responses from hard positives and hard negatives. We provide theoretical analysis of our loss function's gradient response and show mathematically how it is better than that of SupCon loss. We empirically compare our loss function with SupCon loss and cross-entropy loss in supervised setting on multiple classification-task datasets to show its effectiveness. We also show the stability of our loss function to a range of hyper-parameter settings. Unlike SupCon loss which is only applied to supervised setting we show how to extend TCL to self-supervised setting and empirically compare it with various SOTA self-supervised learning methods. Hence we show that TCL loss achieves performance on par with SOTA methods in both supervised and self-supervised settings.	https://openaccess.thecvf.com//content/WACV2025/html/Animesh_Tuned_Contrastive_Learning_WACV_2025_paper.html	Chaitanya Animesh, Manmohan Chandraker
U-MixFormer: UNet-Like Transformer with Mix-Attention for Efficient Semantic Segmentation	Semantic segmentation has witnessed remarkable advancements with the adaptation of the Transformer architecture. Parallel to the strides made by the Transformer CNN-based U-Net has seen significant progress especially in high-resolution medical imaging and remote sensing. This dual success inspired us to merge both strengths leading to the inception of a U-Net-based vision transformer decoder tailored for efficient contextual encoding. Here we propose a novel transformer decoder U-MixFormer built upon the U-Net structure designed for efficient semantic segmentation. Our approach distinguishes itself from the previous transformer methods by leveraging lateral connections between the encoder and decoder stages as feature queries for the attention modules apart from the traditional reliance on skip connections. Moreover we innovatively mix hierarchical feature maps from various encoder and decoder stages to form a unified representation for keys and values giving rise to our unique mix-attention module. Our approach demonstrates state-of-the-art performance across various configurations. Extensive experiments show that U-MixFormer outperforms SegFormer FeedFormer and SegNeXt by a large margin. For example U-MixFormer-B0 surpasses SegFormer-B0 and FeedFormer-B0 with 3.8% and 2.0% higher mIoU and 27.3% and 21.8% less computation and outperforms SegNext with 3.3% higher mIoU with MSCAN-T encoder on ADE20K. Code available at https://github.com/julian-klitzing/u-mixformer.	https://openaccess.thecvf.com//content/WACV2025/html/Yeom_U-MixFormer_UNet-Like_Transformer_with_Mix-Attention_for_Efficient_Semantic_Segmentation_WACV_2025_paper.html	Seul-Ki Yeom, Julian von Klitzing
UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark	Localizing unusual activities in videos such as abnormal behaviors or traffic incidents holds practical significance. However pretrained foundation models struggle with localizing diverse unusual events likely because of their insufficient representation in the models' pretraining datasets. To explore foundation models' capability in localizing unusual activities we introduce UAL-Bench a comprehensive benchmark for unusual activity localization featuring three video datasets (UAG-OOPS UAG-SSBD and UAG-FunQA) and an instruction-tuning dataset (OOPS-UAG-Instruct) to improve model capabilities. We also introduce a new metric R@1 TD <= p as an auxiliary metric to reasonably consider detections as true positive if their starting and ending timestamps are within a threshold. On UAL-Bench we evaluate three approaches: Video-Language Models (Vid-LLMs) instruction-tuned Vid-LLMs and a novel integration of Vision-Language Models and Large Language Models (VLM-LLM). Our results show the VLM-LLM approach excels in localizing short-span unusual events and predicting their onset (start time) more accurately than Vid-LLMs. Our findings highlight the challenges posed by long-duration videos particularly in autism diagnosis scenarios and the need for further advancements in localization techniques. Our work not only provides a benchmark for unusual activity localization but also outlines the key challenges for existing foundation models suggesting future research directions on this important task.	https://openaccess.thecvf.com//content/WACV2025/html/Abdullah_UAL-Bench_The_First_Comprehensive_Unusual_Activity_Localization_Benchmark_WACV_2025_paper.html	Hasnat Md Abdullah, Tian Liu, Kangda Wei, Shu Kong, Ruihong Huang
UCDR-Adapter: Exploring Adaptation of Pre-Trained Vision-Language Models for Universal Cross-Domain Retrieval	Universal Cross-Domain Retrieval (UCDR) retrieves relevant images from unseen domains and classes without semantic labels ensuring robust generalization. Existing methods commonly employ prompt tuning with pre-trained vision-language models but are inherently limited by static prompts reducing adaptability. We propose UCDR-Adapter which enhances pre-trained models with adapters and dynamic prompt generation through a two-phase training strategy. First Source Adapter Learning integrates class semantics with domain-specific visual knowledge using a Learnable Textual Semantic Template and optimizes Class and Domain Prompts via momentum updates and dual loss functions for robust alignment. Second Target Prompt Generation creates dynamic prompts by attending to masked source prompts enabling seamless adaptation to unseen domains and classes. Unlike prior approaches UCDR-Adapter dynamically adapts to evolving data distributions enhancing both flexibility and generalization. During inference only the image branch and generated prompts are used eliminating reliance on textual inputs for highly efficient retrieval. Extensive benchmark experiments show that UCDR-Adapter consistently outperforms ProS in most cases and other state-of-the-art methods on UCDR U^cCDR and U^dCDR settings.	https://openaccess.thecvf.com//content/WACV2025/html/Jiang_UCDR-Adapter_Exploring_Adaptation_of_Pre-Trained_Vision-Language_Models_for_Universal_Cross-Domain_WACV_2025_paper.html	Haoyu Jiang, Zhi-Qi Cheng, Gabriel Moreira, Jiawen Zhu, Jingdong Sun, Bukun Ren, Jun-Yan He, Qi Dai, Xian-Sheng Hua
USWformer: Efficient Sparse Wavelet Transformer for Underwater Image Enhancement	Transformer-based methods have shown great promise in underwater image enhancement (UIE) tasks due to their capability to model long-range dependencies which are vital for reconstructing clear images. While numerous effective attention mechanisms have been devised to handle the computational requirements of transformers they frequently incorporate redundant information and noisy interactions from irrelevant regions. Additionally the current methods focusing solely on the raw pixel space constrains the exploration of the underwater image frequency dynamics thus hindering the models from fully leveraging their potential for producing high-quality images. To address these challenges we propose USWformer an efficient UIE Sparse Wavelet Transformer Network (1.19 M parameters) to eliminate the redundant features in both the spatial and frequency domains. The USWformer consists of two fundamental components: a Sparse Wavelet Self-Attention (SWSA) block and a Multi-scale Wavelet Feed-Forward Network (MWFN). The SWSA block selectively preserves essential attention scores from the keys corresponding to each query adjusting the feature details. MWFN further diminishes the feature redundancy in the aggregated features thereby improving the enhancement of the underwater images. We assess the efficacy of our approach across benchmark datasets comprising synthetic and real-world underwater images showcasing its superiority via thorough ablation studies and comparative analyses.	https://openaccess.thecvf.com//content/WACV2025/html/Mishra_USWformer_Efficient_Sparse_Wavelet_Transformer_for_Underwater_Image_Enhancement_WACV_2025_paper.html	Priyanka Mishra, Nancy Mehta, Santosh Kumar Vipparthi, Subrahmanyam Murala
UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction	3D Gaussian splatting (3DGS) offers the capability to achieve real-time high quality 3D scene rendering. However 3DGS assumes that the scene is in a clear medium environment and struggles to generate satisfactory representations in underwater scenes where light absorption and scattering are prevalent and moving objects are involved. To overcome these we introduce a novel Gaussian Splatting-based method UW-GS designed specifically for underwater applications. It introduces a color appearance that models distance-dependent color variation employs a new physics-based density control strategy to enhance clarity for distant objects and uses a binary motion mask to handle dynamic content. Optimized with a well-designed loss function supporting for scattering media and strengthened by pseudo-depth maps UW-GS outperforms existing methods with PSNR gains up to 1.26dB. To fully verify the effectiveness of the model we also developed a new underwater dataset S-UW with dynamic object masks. The code of UW-GS and S-UW will be available at https://github.com/WangHaoran16/UW-GS.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_UW-GS_Distractor-Aware_3D_Gaussian_Splatting_for_Enhanced_Underwater_Scene_Reconstruction_WACV_2025_paper.html	Haoran Wang, Nantheera Anantrasirichai, Fan Zhang, David Bull
UnDIVE: Generalized Underwater Video Enhancement using Generative Priors	With the rise of marine exploration underwater imaging has gained significant attention as a research topic. Underwater video enhancement has become crucial for real-time computer vision tasks in marine exploration. However most existing methods focus on enhancing individual frames and neglect video temporal dynamics leading to visually poor enhancements. Furthermore the lack of ground-truth references limits the use of abundant available underwater video data in many applications. To address these issues we propose a two-stage framework for enhancing underwater videos. The first stage uses a denoising diffusion probabilistic model to learn a generative prior from unlabeled data capturing robust and descriptive feature representations. In the second stage this prior is incorporated into a physics-based image formulation for spatial enhancement while also enforcing temporal consistency between video frames. Our method enables real-time and computationally-efficient processing of high-resolution underwater videos at lower resolutions and offers efficient enhancement in the presence of diverse water-types. Extensive experiments on four datasets show that our approach generalizes well and outperforms existing enhancement methods. Our code is available at github.com/suhas-srinath/undive.	https://openaccess.thecvf.com//content/WACV2025/html/Srinath_UnDIVE_Generalized_Underwater_Video_Enhancement_using_Generative_Priors_WACV_2025_paper.html	Suhas Srinath, Aditya Chandrasekar, Hemang Jamadagni, Rajiv Soundararajan, Prathosh A P
Uncertainty Aware Interest Point Detection and Description	Interest point detection and description play an important role in many visual tasks including image registration pose estimation 3D reconstruction and more. State-of-the-art interest point detection techniques are based on deep neural networks (NNs) which are prone to produce overconfident predictions. However calibrated and robust uncertainty measurement is crucial when deploying deep NN models in safety critical applications. In this work we propose a novel Uncertainty-Aware interest Point (UAPoint) detection method to address this problem. Our method leverages evidential learning to learn both aleatoric and epistemic uncertainty. We further propose a constrained sampling scheme to construct more efficient training pairs for the descriptor decoder. We evaluate our method on a wide range of benchmarks and show that our method achieves state-of-the-art performance. Code will be released upon publication. Code will be released in https://github.com/JingboZeng/ UAPoint.	https://openaccess.thecvf.com//content/WACV2025/html/Zeng_Uncertainty_Aware_Interest_Point_Detection_and_Description_WACV_2025_paper.html	Jingbo Zeng, Zaiwang Gu, Weide Liu, Lile Cai, Jun Cheng
Uncertainty Awareness Enables Efficient Labeling for Cancer Subtyping in Digital Pathology	Machine-learning-assisted cancer subtyping is a promising avenue in digital pathology. Cancer subtyping models however require careful training using expert annotations so that they can be inferred with a degree of known certainty (or uncertainty). To this end we introduce the concept of uncertainty awareness into a self-supervised contrastive learning model. This is achieved by computing an evidence vector at every epoch which assesses the model's confidence in its predictions. The derived uncertainty score is then utilized as a metric to selectively label the most crucial images that require further annotation thus iteratively refining the training process. With just 1-10% of strategically selected annotations we attain state-of-the-art performance in cancer subtyping on benchmark datasets. Our method not only strategically guides the annotation process to minimize the need for extensive labeled datasets but also improve the precision and efficiency of classifications. This development is particularly beneficial in settings where the availability of labeled data is limited offering a promising direction for future research and application in digital pathology. Our code is available at https://github.com/Nirhoshan/AI-for-histopathology	https://openaccess.thecvf.com//content/WACV2025/html/Sivaroopan_Uncertainty_Awareness_Enables_Efficient_Labeling_for_Cancer_Subtyping_in_Digital_WACV_2025_paper.html	Nirhoshan Sivaroopan, Chamuditha Jayanga Galappaththige, Chalani Ekanayake, Hasindri Watawana, Ranga Rodrigo, Chamira U.S. Edussooriya, Dushan N. Wadduwage
Uncertainty and Energy Based Loss Guided Semi-Supervised Semantic Segmentation	Semi-supervised (SS) semantic segmentation exploits both labeled and unlabeled images to overcome tedious and costly pixel-level annotation problems. Pseudolabel supervision is one of the core approaches of training networks with both pseudo labels and ground-truth labels. This work uses aleatoric or data uncertainty and energy based modeling in intersection-union pseudo supervised network. The aleatoric uncertainty is modeling the inherent noise variations of the data in a network with two predictive branches. The per-pixel variance parameter obtained from the network gives a quantitative idea about the data uncertainty. Moreover energy-based loss realizes the potential of generative modeling on the downstream SS segmentation task. The aleatoric and energy loss are applied in conjunction with pseudo-intersection labels pseudo-union labels and ground-truth on the respective network branch. The comparative analysis with state-of-the-art methods has shown improvement in performance metrics. The code is availaible at https://visdomlab.github.io/DUEB/.	https://openaccess.thecvf.com//content/WACV2025/html/Thakur_Uncertainty_and_Energy_Based_Loss_Guided_Semi-Supervised_Semantic_Segmentation_WACV_2025_paper.html	Rini Smita Thakur, Vinod K Kurmi
Uncertainty-Aware Online Extrinsic Calibration: A Conformal Prediction Approach	Accurate sensor calibration is crucial for autonomous systems yet its uncertainty quantification remains underexplored. We present the first approach to integrate uncertainty awareness into online extrinsic calibration combining Monte Carlo Dropout with Conformal Prediction to generate prediction intervals with a guaranteed level of coverage. Our method proposes a framework to enhance existing calibration models with uncertainty quantification compatible with various network architectures. Validated on KITTI (RGB Camera-LiDAR) and DSEC (Event Camera-LiDAR) datasets we demonstrate effectiveness across different visual sensor types measuring performance with adapted metrics to evaluate the efficiency and reliability of the intervals. By providing calibration parameters with quantifiable confidence measures we offer insights into the reliability of calibration estimates which can greatly improve the robustness of sensor fusion in dynamic environments and usefully serve the Computer Vision community.	https://openaccess.thecvf.com//content/WACV2025/html/Cocheteux_Uncertainty-Aware_Online_Extrinsic_Calibration_A_Conformal_Prediction_Approach_WACV_2025_paper.html	Mathieu Cocheteux, Julien Moreau, Franck Davoine
Uncertainty-Aware Regularization for Image-to-Image Translation	The importance of quantifying uncertainty in deep networks has become paramount for reliable real-world applications. In this paper we propose a method to improve uncertainty estimation in medical Image-to-Image (I2I) translation. Our model integrates aleatoric uncertainty and employs Uncertainty-Aware Regularization (UAR) inspired by simple priors to refine uncertainty estimates and enhance reconstruction quality. We show that by leveraging simple priors on parameters our approach captures more robust uncertainty maps effectively refining them to indicate precisely where the network encounters difficulties while being less affected by noise. Our experiments demonstrate that UAR not only improves translation performance but also provides better uncertainty estimations particularly in the presence of noise and artifacts. We validate our approach using two medical imaging datasets showcasing its effectiveness in maintaining high confidence in familiar regions while accurately identifying areas of uncertainty in novel/ambiguous scenarios.	https://openaccess.thecvf.com//content/WACV2025/html/Vats_Uncertainty-Aware_Regularization_for_Image-to-Image_Translation_WACV_2025_paper.html	Anuja Vats, Ivar Farup, Marius Pedersen, Kiran Raja
Uncertainty-Based Data-Wise Label Smoothing for Calibrating Multiple Instance Learning in Histopathology Image Classification	Deep neural networks (DNNs) have transformed biomedical image analysis particularly in histopathology with Whole Slide Images (WSIs) classification. However training DNNs requires large annotated datasets which is challenging due to the high heterogeneity and high resolution of WSIs. Multiple Instance Learning (MIL) has become a popular method for weakly supervised classification in this context training with only slide-level labels. Despite the advancements ensuring the reliability of model performance is crucial in safety-critical domains including healthcare. Deep learning models in real-world decision-making systems must accurately predict probability estimates to reflect the true likelihood of correctness known as confidence calibration. This study introduces a novel calibration framework UDLS which uses data-wise label smoothing based on predictive uncertainty to improve the calibration of MIL frameworks. This approach involves augmenting WSIs with PatchFeatureDropout computing predictive uncertainty estimates for original data and applying these estimates to each sample for label smoothing during model training. Experimental results on benchmark histopathology datasets show noticeable improvements in both calibration and classification performance highlighting UDLS's potential for enhancing the reliability of predictions from deep learning models in clinical settings.	https://openaccess.thecvf.com//content/WACV2025/html/Park_Uncertainty-Based_Data-Wise_Label_Smoothing_for_Calibrating_Multiple_Instance_Learning_in_WACV_2025_paper.html	Hyeongmin Park, Sungrae Hong, Chanjae Song, Jongwoo Kim, Mun Yong Yi
Uncertainty-Guided Cross Attention Ensemble Mean Teacher for Semi-Supervised Medical Image Segmentation	This work proposes a novel framework Uncertainty-Guided Cross Attention Ensemble Mean Teacher (UG-CEMT) for achieving state-of-the-art performance in semi-supervised medical image segmentation. UG-CEMT leverages the strengths of co-training and knowledge distillation by combining a Cross-attention Ensemble Mean Teacher framework (CEMT) inspired by Vision Transformers (ViT) with uncertainty-guided consistency regularization and Sharpness-Aware Minimization emphasizing uncertainty. UG-CEMT improves semi-supervised performance while maintaining a consistent network architecture and task setting by fostering high disparity between sub-networks. Experiments demonstrate significant advantages over existing methods like Mean Teacher and Cross-pseudo Supervision in terms of disparity domain generalization and medical image segmentation performance. UG-CEMT achieves state-of-the-art results on multi-center prostate MRI and cardiac MRI datasets where object segmentation is particularly challenging. Our results show that using only 10% labeled data UG-CEMT approaches the performance of fully supervised methods demonstrating its effectiveness in exploiting unlabeled data for robust medical image segmentation. The code is publicly available at https://github.com/Meghnak13/UG-CEMT	https://openaccess.thecvf.com//content/WACV2025/html/Karri_Uncertainty-Guided_Cross_Attention_Ensemble_Mean_Teacher_for_Semi-Supervised_Medical_Image_WACV_2025_paper.html	Meghana Karri, Amit Soni Arya, Koushik Biswas, Nicolo Gennaro, Vedat Cicek, Gorkem Durak, Yury S. Velichko, Ulas Bagci
Uncertainty-Guided Metric Learning without Labels	Unsupervised metric learning aims to learn the discriminative representations by grouping similar examples in the absence of labels. Many unsupervised metric learning algorithms combine clustering-based pseudo-label generation with embedding fine-tuning. However pseudo-labels can be unreliable and noisy. This could affect metric learning and degrade the quality of the learned representations. In this work we propose an approach to reduce the negative effect of label noise on learning discriminative embeddings by using context and prediction uncertainty. In particular we refine the pseudo-labels by aggregating information from neighbors. We propose a function to weigh the pairs leveraging their prediction confidence and uncertainty. We modify the metric learning loss function to incorporate this weight. Experimental results demonstrate the effectiveness of our proposed method on standard datasets for metric learning.	https://openaccess.thecvf.com//content/WACV2025/html/Devalraju_Uncertainty-Guided_Metric_Learning_without_Labels_WACV_2025_paper.html	Dhanunjaya Varma Devalraju, C Chandra Sekhar
Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion	"We introduce NOVIC an innovative real-time uNconstrained Open Vocabulary Image Classifier that uses an autoregressive transformer to generatively output classification labels as language. Leveraging the extensive knowledge of CLIP models NOVIC harnesses the embedding space to enable zero-shot transfer from pure text to images. Traditional CLIP models despite their ability for open vocabulary classification require an exhaustive prompt of potential class labels restricting their application to images of known content or context. To address this we propose an ""object decoder"" model that is trained on a large-scale 92M-target dataset of templated object noun sets and LLM-generated captions to always output the object noun in question. This effectively inverts the CLIP text encoder and allows textual object labels from essentially the entire English language to be generated directly from image-derived embedding vectors without requiring any a priori knowledge of the potential content of an image and without any label biases. The trained decoders are tested on a mix of manually and web-curated datasets as well as standard image classification benchmarks and achieve fine-grained prompt-free prediction scores of up to 87.5% a strong result considering the model must work for any conceivable image and without any contextual clues."	https://openaccess.thecvf.com//content/WACV2025/html/Allgeuer_Unconstrained_Open_Vocabulary_Image_Classification_Zero-Shot_Transfer_from_Text_to_WACV_2025_paper.html	Philipp Allgeuer, Kyra Ahrens, Stefan Wermter
Uni-SLAM: Uncertainty-Aware Neural Implicit SLAM for Real-Time Dense Indoor Scene Reconstruction	Neural implicit fields have recently emerged as a powerful representation method for multi-view surface reconstruction due to their simplicity and state-of-the-art performance. However reconstructing thin structures of indoor scenes while ensuring real-time performance remains a challenge for dense visual SLAM systems. Previous methods do not consider varying quality of input RGB-D data and employ fixed-frequency mapping process to reconstruct the scene which could result in the loss of valuable information in some frames. In this paper we propose Uni-SLAM a decoupled 3D spatial representation based on hash grids for indoor reconstruction. We introduce a novel defined predictive uncertainty to reweight the loss function along with strategic local-to-global bundle adjustment. Experiments on synthetic and real-world datasets demonstrate that our system achieves state-of-the-art tracking and mapping accuracy while maintaining real-time performance. It significantly improves over current methods with a 25% reduction in depth L1 error and a 66.86% completion rate within 1 cm on the Replica dataset reflecting a more accurate reconstruction of thin structures. Project page: https://shaoxiang777.github.io/project/uni-slam/	https://openaccess.thecvf.com//content/WACV2025/html/Wang_Uni-SLAM_Uncertainty-Aware_Neural_Implicit_SLAM_for_Real-Time_Dense_Indoor_Scene_WACV_2025_paper.html	Shaoxiang Wang, Yaxu Xie, Chun-Peng Chang, Christen Millerdurai, Alain Pagani, Didier Stricker
UniTMGE: Uniform Text-Motion Generation and Editing Model via Diffusion	Current methods have shown promising results in applying diffusion models to motion generation given text input. However these methods are limited to unimodal inputs and outputs restricted to motion generation alone and lacking multimodal control capabilities. To address these issues we introduce TMMGE a text-motion multimodal generation and editing framework based on diffusion. TMMGE overcomes single-modality limitations enabling exceptional performance and strong generalization across multiple tasks like text-driven motion generation motion captioning motion completion and multi-modal motion editing. TMMGE comprises three components: UTMV for mapping text and motion into a shared latent space using contrastive learning a controllable diffusion model customized for the UTMV space and MCRE for unifying multimodal conditions into CLIP representations enabling precise multimodal control and flexible motion editing through simple linear operations. We conducted both closed-world experiments and open-world experiments using the Motion-X dataset with detailed text descriptions with results demonstrating our model's effectiveness and generalizability across multiple tasks.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_UniTMGE_Uniform_Text-Motion_Generation_and_Editing_Model_via_Diffusion_WACV_2025_paper.html	Ruoyu Wang, Yangfan He, Tengjiao Sun, Xiang Li, Tianyu Shi
Unified Framework for Open-World Compositional Zero-Shot Learning	Open-World Compositional Zero-Shot Learning (OW-CZSL) addresses the challenge of recognizing novel compositions of known primitives and entities. Even though prior works utilize language knowledge for recognition such approaches exhibit limited interactions between language-image modalities. Our approach primarily focuses on enhancing the inter-modality interactions through fostering richer interactions between image and textual data. Additionally we introduce a novel module aimed at alleviating the computational burden associated with exhaustive exploration of all possible compositions during the inference stage. While previous methods exclusively learn compositions jointly or independently we introduce an advanced hybrid procedure that leverages both learning mechanisms to generate final predictions. Our proposed model achieves state-of-the-art in OW-CZSL in three datasets while surpassing Large Vision Language Models (LLVM) in two datasets. Our code is available at https://github.com/hirunima/OWCZSL	https://openaccess.thecvf.com//content/WACV2025/html/Jayasekara_Unified_Framework_for_Open-World_Compositional_Zero-Shot_Learning_WACV_2025_paper.html	Hirunima Jayasekara, Khoi Pham, Nirat Saini, Abhinav Shrivastava
Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing	Text-guided image generation and editing using diffusion models have achieved remarkable advancements. Among these tuning-free methods have gained attention for their ability to perform edits without extensive model adjustments offering simplicity and efficiency. However existing tuning-free approaches often struggle with balancing fidelity and editing precision. Reconstruction errors in DDIM Inversion are partly attributed to the cross-attention mechanism in U-Net which introduces misalignments during the inversion and reconstruction process. To address this we analyze reconstruction from a structural perspective and propose a novel approach that replaces traditional cross-attention with uniform attention maps significantly enhancing image reconstruction fidelity. Our method effectively minimizes distortions caused by varying text conditions during noise prediction. To complement this improvement we introduce an adaptive mask-guided editing technique that integrates seamlessly with our reconstruction approach ensuring consistency and accuracy in editing tasks. Experimental results demonstrate that our approach not only excels in achieving high-fidelity image reconstruction but also performs robustly in real image composition and editing scenarios. This study underscores the potential of uniform attention maps to enhance the fidelity and versatility of diffusion-based image processing methods. Code is available at https://github.com/Mowenyii/Uniform-Attention-Maps.	https://openaccess.thecvf.com//content/WACV2025/html/Mo_Uniform_Attention_Maps_Boosting_Image_Fidelity_in_Reconstruction_and_Editing_WACV_2025_paper.html	Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen
Unifying Low-Resolution and High-Resolution Alignment by Event Cameras for Space-Time Video Super-Resolution	Event cameras deliver asynchronous pixel intensity changes which result in sparse event data that offers the advantages of high temporal resolution. These high temporal characteristics make researchers naturally incorporate event cameras into video frame interpolation (VFI) and video super-resolution (VSR). In this paper we make the first attempt to solve the space-time video super-resolution (STVSR) task effectively addressing both VFI and VSR simultaneously by leveraging temporally dense events. STVSR aims to generate intermediate high-resolution (HR) videos between consecutive low-resolution (LR) frames. To fully exploit the high temporal frequency of events for STVSR we focus on temporal alignment in two stages at low-resolution and after up-sampling in high-resolution. In temporal alignment at low-resolution to upsample spatial dimensions effectively we leverage high temporal features to preserve spatial context. On the other hand for temporal alignment at the high-resolution stage we employ a deformable sampling process from events to achieve accurate alignment with forward and backward directions. In addition we provide the SuperREST dataset which features high-frequency details and complex motion in an RGB-Event setup. Experimental results on several datasets demonstrate that our method achieves a significant performance gain on STVSR tasks with low computational cost. Our codes and datasets are available at https://github.com/Chohoonhee/ESTNet.	https://openaccess.thecvf.com//content/WACV2025/html/Cho_Unifying_Low-Resolution_and_High-Resolution_Alignment_by_Event_Cameras_for_Space-Time_WACV_2025_paper.html	Hoonhee Cho, Jae-Young Kang, Taewoo Kim, Yuhwan Jeong, Kuk-Jin Yoon
Unleashing Potentials of Vision-Language Models for Zero-Shot HOI Detection	Human-Object Interaction (HOI) detection aims to localize human-object pairs and recognize their interactions as triplets. Recent advancements in pre-trained vision-language model (VLM) have improved zero-shot HOI detection enabling identification of unseen triplets. However existing methods leverage the VLM as an additional encoder only for interaction prediction not for human/object detection. This limitation hinders their ability to detect unseen objects. Furthermore the additional encoder increases both model size and computational cost. This paper proposes a novel HOI detection framework ECI-HOI which unleashes potentials of the pre-trained VLM for the zero-shot HOI detection by leveraging it for both of the sub-tasks. We first employ CLIP as a single image encoder reducing redundancy in the network architecture. In addition we propose an instance selector and a HO pair decoder to effectively harmonize the human/object detection and the interaction prediction in zero-shot manner. We evaluate our model under various settings on HICO-DET and our two new testsets: out-of-distribution image testset and novel object testset. Our model outperforms the state-of-the-art models while reducing the model size by more than 50% especially achieving a +10.01 mAP improvement under the unseen object setting on HICO-DET. The results on the proposed datasets highlight the zero-shot performance of our model on more challenging settings.	https://openaccess.thecvf.com//content/WACV2025/html/Yamada_Unleashing_Potentials_of_Vision-Language_Models_for_Zero-Shot_HOI_Detection_WACV_2025_paper.html	Moyuru Yamada, Nimish Dharamshi, Ayushi Kohli, Prasad Kasu, Ainulla Khan, Manu Ghulyani
Unsupervised Denoising for Signal-Dependent and Row-Correlated Imaging Noise	Accurate analysis of microscopy images is hindered by the presence of noise. This noise is usually signal-dependent and often additionally correlated along rows or columns of pixels. Current self- and unsupervised denoisers can address signal-dependent noise but none can reliably remove noise that is also row- or column-correlated. Here we present the first fully unsupervised deep learning-based denoiser capable of handling imaging noise that is row-correlated as well as signal-dependent. Our approach uses a Variational Autoencoder (VAE) with a specially designed autoregressive decoder. This decoder is capable of modeling row-correlated and signal-dependent noise but is incapable of independently modeling underlying clean signal. The VAE therefore produces latent variables containing only clean signal information and these are mapped back into image space using a proposed second decoder network. Our method does not require a pre-trained noise model and can be trained from scratch using unpaired noisy data. We benchmark our approach on microscopy datatsets from a range of imaging modalities and sensor types each with row- or column-correlated signal-dependent noise and show that it outperforms existing self- and unsupervised denoisers.	https://openaccess.thecvf.com//content/WACV2025/html/Salmon_Unsupervised_Denoising_for_Signal-Dependent_and_Row-Correlated_Imaging_Noise_WACV_2025_paper.html	Benjamin Salmon, Alexander Krull
Unsupervised Domain Adaptive Visual Question Answering in the Era of Multi-Modal Large Language Models	Unsupervised domain adaptation (UDA) for visual question answering (VQA) has attracted research interest. However with Multi-modal Large Language Models (MLLMs) showing great performance on VQA datasets UDA for VQA based on MLLMs remains unexplored. To fill this gap we propose the first systematic approach to Unsupervised Domain Adaptation VQA based on MLLMs (UDAM). First we introduce semantic context feature alignment and domain query feature alignment which utilize a single token embedding for each modality to capture contextual domain information from unimodal inputs and conduct coarse-grained feature alignment on it thus alleviating domain shifts in the unimodal feature space. Second we propose the novel semantics-guided query feature alignment which differentiates important domain-specific queries from learnable query outputs and conducts fine-grained feature alignment controlled by a semantics-guided weight map to reduce domain shifts in the cross-modal feature space. Third we devise a pair-wise domain-aware prompt strategy which aids UDA by prompting MLLMs to discern the commonality of tasks and the distinctiveness of domains in multi-modal inputs. Extensive experiments demonstrate UDAM's effectiveness in adapting MLLMs to unlabeled new domains.	https://openaccess.thecvf.com//content/WACV2025/html/Weng_Unsupervised_Domain_Adaptive_Visual_Question_Answering_in_the_Era_of_WACV_2025_paper.html	Weixi Weng, Rui Zhang, Xiaojun Meng, Jieming Zhu, Qun Liu, Chun Yuan
Unsupervised Single-Image Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training	Unsupervised intrinsic image decomposition (IID) is the task of separating a natural image into albedo and shade without ground truth during training. Although a recent model employing light detection and ranging (LiDAR) intensity demonstrated impressive performance the necessity of LiDAR intensity during inference restricts its practicality. To expand the usage scenario while maintaining the IID quality achieved by using both an image and its corresponding LiDAR intensity we propose a novel approach that utilizes an image without LiDAR intensity during inference while utilizing both an image and LiDAR intensity during training. Specifically our proposed model processes an image and LiDAR intensity individually using distinct encoder paths during training but utilizes only an image-encoder path during inference. Additionally we introduce an albedo-alignment loss aligning the gray-scale albedo from an image to that from its corresponding LiDAR intensity. LiDAR intensity is not affected by illumination effects including cast shadows thus albedo-alignment loss transfers the illumination-invariant property of LiDAR intensity to the image-encoder path. Furthermore we also propose image-LiDAR conversion (ILC) paths that mutually translates the style of an image and LiDAR intensity. IID models translate an image into albedo and shade styles while keeping the image contents thus it is important to separate the image into contents and style. Trained with pairs of an image and its corresponding LiDAR intensity which share contents but differ in style the mutual translation in ILC paths improve the accuracy of the separation. Consequently our model achieves comparable IID quality to the existing model with LiDAR intensity while utilizing only an image without LiDAR intensity during inference.	https://openaccess.thecvf.com//content/WACV2025/html/Sato_Unsupervised_Single-Image_Intrinsic_Image_Decomposition_with_LiDAR_Intensity_Enhanced_Training_WACV_2025_paper.html	Shogo Sato, Takuhiro Kaneko, Kazuhiko Murasaki, Taiga Yoshida, Ryuichi Tanida, Akisato Kimura
Unsupervised Video Highlight Detection by Learning from Audio and Visual Recurrence	With the exponential growth of video content the need for automated video highlight detection to extract key moments or highlights from lengthy videos has become increasingly pressing. This technology has the potential to enhance user experiences by allowing quick access to relevant content across diverse domains. Existing methods typically rely either on expensive manually labeled frame-level annotations or on a large external dataset of videos for weak supervision through category information. To overcome this we focus on unsupervised video highlight detection eliminating the need for manual annotations. We propose a novel unsupervised approach which capitalizes on the premise that significant moments tend to recur across multiple videos of the similar category in both audio and visual modalities. Surprisingly audio remains under-explored especially in unsupervised algorithms despite its potential to detect key moments. Through a clustering technique we identify pseudo-categories of videos and compute audio pseudo-highlight scores for each video by measuring the similarities of audio features among audio clips of all the videos within each pseudo-category. Similarly we also compute visual pseudo-highlight scores for each video using visual features. Then we combine audio and visual pseudo-highlights to create the audio-visual pseudo ground-truth highlight of each video for training an audio-visual highlight detection network. Extensive experiments and ablation studies on three benchmarks showcase the superior performance of our method over prior work.	https://openaccess.thecvf.com//content/WACV2025/html/Islam_Unsupervised_Video_Highlight_Detection_by_Learning_from_Audio_and_Visual_WACV_2025_paper.html	Zahidul Islam, Sujoy Paul, Mrigank Rochan
User-in-the-Loop Evaluation of Multimodal LLMs for Activity Assistance	Our research investigates the capability of modern multimodal reasoning models powered by Large Language Models (LLMs) to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors e.g. camera 2) forecast future actions for accomplishing the activity and 3) replan based on the user in the loop. To evaluate the first two capabilities grounding visual history and forecasting in short and long horizons we conduct benchmarking of two prominent classes of multimodal LLM approaches - Socratic Models and Vision Conditioned Language Models (VCLMs) on video-based action anticipation tasks using offline datasets. These offline benchmarks however do not allow us to close the loop with the user which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end we conduct a first-of-its-kind user study with 18 participants performing 3 different multi-step cooking activities while wearing an egocentric observation device called Aria and following assistance from multimodal LLMs. We find that the Socratic approach outperforms VCLMs in both offline and online settings. We further highlight how grounding long visual history common in activity assistance remains challenging in current models especially for VCLMs and demonstrate that offline metrics do not indicate online performance.	https://openaccess.thecvf.com//content/WACV2025/html/Verghese_User-in-the-Loop_Evaluation_of_Multimodal_LLMs_for_Activity_Assistance_WACV_2025_paper.html	Mrinal Verghese, Brian Chen, Hamid Eghbalzadeh, Tushar Nagarajan, Ruta P Desai
Utilizing Uncertainty in 2D Pose Detectors for Probabilistic 3D Human Mesh Recovery	Monocular 3D human pose and shape estimation is an inherently ill-posed problem due to depth ambiguities occlusions and truncations. Recent probabilistic approaches learn a distribution over plausible 3D human meshes by maximizing the likelihood of the ground-truth pose given an image. We show that this objective function alone is not sufficient to best capture the full distributions. Instead we propose to additionally supervise the learned distributions by minimizing the distance to distributions encoded in heatmaps of a 2D pose detector. Moreover we reveal that current methods often generate incorrect hypotheses for invisible joints which is not detected by the evaluation protocols. We demonstrate that person segmentation masks can be utilized during training to significantly decrease the number of invalid samples and introduce two metrics to evaluate it. Our normalizing flow-based approach predicts plausible 3D human mesh hypotheses that are consistent with the image evidence while maintaining high diversity for ambiguous body parts. Experiments on 3DPW and EMDB show that we outperform other state-of-the-art probabilistic methods. Code is available for research purposes at https://github.com/twehrbein/humr.	https://openaccess.thecvf.com//content/WACV2025/html/Wehrbein_Utilizing_Uncertainty_in_2D_Pose_Detectors_for_Probabilistic_3D_Human_WACV_2025_paper.html	Tom Wehrbein, Marco Rudolph, Bodo Rosenhahn, Bastian Wandt
V-MIND: Building Versatile Monocular Indoor 3D Detector with Diverse 2D Annotations	The field of indoor monocular 3D object detection is gaining significant attention fueled by the increasing demand in VR/AR and robotic applications. However its advancement is impeded by the limited availability and diversity of 3D training data owing to the labor-intensive nature of 3D data collection and annotation processes. In this paper we present V-MIND (Versatile Monocular INdoor Detector) which enhances the performance of indoor 3D detectors across a diverse set of object classes by harnessing publicly available large-scale 2D datasets. By leveraging well-established monocular depth estimation techniques and camera intrinsic predictors we can generate 3D training data by converting large-scale 2D images into 3D point clouds and subsequently deriving pseudo 3D bounding boxes. To mitigate distance errors inherent in the converted point clouds we introduce a novel 3D self-calibration loss for refining the pseudo 3D bounding boxes during training. Additionally we propose a novel ambiguity loss to address the ambiguity that arises when introducing new classes from 2D datasets. Finally through joint training with existing 3D datasets and pseudo 3D bounding boxes derived from 2D datasets V-MIND achieves state-of-the-art object detection performance across a wide range of classes on the Omni3D indoor dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Jhang_V-MIND_Building_Versatile_Monocular_Indoor_3D_Detector_with_Diverse_2D_WACV_2025_paper.html	Jin-Cheng Jhang, Tao Tu, Fu-En Wang, Ke Zhang, Min Sun, Cheng-Hao Kuo
VADet: Multi-Frame LiDAR 3D Object Detection using Variable Aggregation	Input aggregation is a simple technique used by state-of-the-art LiDAR 3D object detectors to improve detection. However increasing aggregation is known to have diminishing returns and even performance degradation due to objects responding differently to the number of aggregated frames. To address this limitation we propose an efficient adaptive method which we call Variable Aggregation Detection (VADet). Instead of aggregating the entire scene using a fixed number of frames VADet performs aggregation per object with the number of frames determined by an object's observed properties such as speed and point density. VADet thus reduces the inherent trade-offs of fixed aggregation and is not architecture specific. To demonstrate its benefits we apply VADet to three popular single-stage detectors and achieve state-of-the-art performance on the Waymo dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_VADet_Multi-Frame_LiDAR_3D_Object_Detection_using_Variable_Aggregation_WACV_2025_paper.html	Chengjie Huang, Vahdat Abdelzad, Sean Sedwards, Krzysztof Czarnecki
VG-SSL: Benchmarking Self-Supervised Representation Learning Approaches for Visual Geo-Localization	Visual Geo-localization (VG) is a critical research area for identifying geo-locations from visual inputs particularly in autonomous navigation for robotics and vehicles. Current VG methods often learn feature extractors from geo-labeled images to create dense geographically relevant representations. Recent advances in Self-Supervised Learning (SSL) have demonstrated its capability to achieve performance on par with supervised techniques with unlabeled images. This study presents a novel VG-SSL framework designed for versatile integration and benchmarking of diverse SSL methods for representation learning in VG featuring a unique geo-related pair strategy GeoPair. Through extensive performance analysis we adapt SSL techniques to improve VG on datasets from hand-held and car-mounted cameras used in robotics and autonomous vehicles. Our results show that contrastive learning and information maximization methods yield superior geo-specific representation quality matching or surpassing the performance of state-of-the-art VG techniques. To our knowledge This is the first benchmarking study of SSL in VG highlighting its potential in enhancing geo-specific visual representations for robotics and autonomous vehicles. The code is publicly available at https://github.com/arplaboratory/VG-SSL.	https://openaccess.thecvf.com//content/WACV2025/html/Xiao_VG-SSL_Benchmarking_Self-Supervised_Representation_Learning_Approaches_for_Visual_Geo-Localization_WACV_2025_paper.html	Jiuhong Xiao, Gao Zhu, Giuseppe Loianno
VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors	We present a stereo-matching method for depth estimation from high-resolution images using visual hulls as priors and a memory-efficient technique for the correlation computation. Our method uses object masks extracted from supplementary views of the scene to guide the disparity estimation effectively reducing the search space for matches. This approach is specifically tailored to stereo rigs in volumetric capture systems where an accurate depth plays a key role in the downstream reconstruction task. To enable training and regression at high resolutions targeted by recent systems our approach extends a sparse correlation computation into a hybrid sparse-dense scheme suitable for application in leading recurrent network architectures. We evaluate the performance-efficiency trade-off of our method compared to state-of-the-art approaches and demonstrate the efficacy of the visual hull guidance. In addition we propose a training scheme for a further reduction of memory requirements during optimization facilitating training on high-resolution data.	https://openaccess.thecvf.com//content/WACV2025/html/Plack_VHS_High-Resolution_Iterative_Stereo_Matching_with_Visual_Hull_Priors_WACV_2025_paper.html	Markus Plack, Hannah DrÃ¶ge, Leif Van Holland, Matthias B. Hullin
VIIS: Visible and Infrared Information Synthesis for Severe Low-Light Image Enhancement	Images captured in severe low-light circumstances often suffer from significant information absence. Existing singular modality image enhancement methods struggle to restore image regions lacking valid information. By leveraging light-impervious infrared images visible and infrared image fusion methods have the potential to reveal information hidden in darkness. However they primarily emphasize inter-modal complementation but neglect intra-modal enhancement limiting the perceptual quality of output images. To address these limitations we propose a novel task dubbed visible and infrared information synthesis (VIIS) which aims to achieve both information enhancement and fusion of the two modalities. Given the difficulty in obtaining ground truth in the VIIS task we design an information synthesis pretext task (ISPT) based on image augmentation. We employ a diffusion model as the framework and design a sparse attention-based dual-modalities residual (SADMR) conditioning mechanism to enhance information interaction between the two modalities. This mechanism enables features with prior knowledge from both modalities to adaptively and iteratively attend to each modality's information during the denoising process. Our extensive experiments demonstrate that our model qualitatively and quantitatively outperforms not only the state-of-the-art methods in relevant fields but also the newly designed baselines capable of both information enhancement and fusion. The code is available at https://github.com/Chenz418/VIIS.	https://openaccess.thecvf.com//content/WACV2025/html/Zhao_VIIS_Visible_and_Infrared_Information_Synthesis_for_Severe_Low-Light_Image_WACV_2025_paper.html	Chen Zhao, Mengyuan Yu, Fan Yang, Peiguang Jing
VILLS : Video-Image Learning to Learn Semantics for Person Re-Identification	Person Re-identification is a research area with significant real world applications. Despite recent progress existing methods face challenges in robust re-identification in the wild e.g. by focusing only on a particular modality and on unreliable patterns such as clothing. A generalized method is highly desired but remains elusive to achieve due to issues such as the trade-off between spatial and temporal resolution and inaccurate feature extraction. We propose VILLS (Video-Image Learning to Learn Semantics) a self-supervised method that jointly learns spatial and temporal features from images and videos. VILLS first designs a local semantic extraction module that adaptively extracts semantically consistent and robust spatial features. Then VILLS designs a unified feature learning and adaptation module to represent image and video modalities in a consistent feature space. By Leveraging self-supervised large-scale pre-training VILLS establishes a new State-of-The-Art that significantly outperforms existing image and video-based methods.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_VILLS__Video-Image_Learning_to_Learn_Semantics_for_Person_Re-Identification_WACV_2025_paper.html	Siyuan Huang, Ram Prabhakar Kathirvel, Yuxiang Guo, Rama Chellappa, Cheng Peng
VISIONARY: Novel Spatial-Spectral Attention Mechanism for Hyperspectral Image Denoising	Image denoising mitigates noise from the captured images and thereby enhances the efficacy of high-demand vision applications such as classification and segmentation. Hyperspectral Images (HSIs) with their multiple spectral bands provide valuable information and make them highly applicable in real-world applications. Current Deep Learning methods mainly employ Transformers to denoise HSIs spatially and spectrally through self-attention (SA). However SA focuses on individual samples and overlooks potential correlations within the images indicating room for improvement. Moreover existing Transformer-based denoising methods often fail to appropriately balance the importance of spatial and spectral features. This paper presents a novel method VISIONARY to address these issues by obtaining better HSI feature representation. To this end it introduces the Spatial-Spectral-Cubic Transformer (SSCformer) block to address the shortcomings of Transformers in HSI denoising particularly their inability to capture correlations within images of the same type by introducing Global Feature Attention (GFA). Additionally the SSCformer independently determines the optimal weightage for spatial and spectral features using attention mechanisms leading to more effective denoising. Our method VISIONARY is based on the integration of Transformer U-Net and CNN architecture. Experimental results demonstrate that VISIONARY outperforms well-known methods on publicly available datasets and our SSCformer block can be easily integrated with existing Transformer-based HSI denoising methods to improve their efficacy.	https://openaccess.thecvf.com//content/WACV2025/html/Dixit_VISIONARY_Novel_Spatial-Spectral_Attention_Mechanism_for_Hyperspectral_Image_Denoising_WACV_2025_paper.html	Aditya Dixit, Nischit Hosamani, Puneet Gupta, Ankur Garg
VLTP: Vision-Language Guided Token Pruning for Task-Oriented Segmentation	Vision Transformers (ViTs) have emerged as the backbone of many segmentation models consistently achieving state-of-the-art (SOTA) performance. However their success comes at a significant computational cost. Image token pruning is one of the most effective strategies to address this complexity. However previous approaches fall short when applied to more complex task-oriented segmentation (TOS) where the class of each image patch is not predefined but dependent on the specific input task. This work introduces the Vision Language Guided Token Pruning (VLTP) a novel token pruning mechanism that can accelerate ViT-based segmentation models particularly for TOS guided by multi-modal large language model (MLLM). We argue that ViT does not need to process every image token through all of its layers--only the tokens related to reasoning tasks are necessary. We design a new pruning decoder to take both image tokens and vision-language guidance as input to predict the relevance of each image token to the task. Only image tokens with high relevance are passed to deeper layers of the ViT. Experiments show that the VLTP framework reduces the computational costs of ViT by approximately 25% without performance degradation and by around 40% with only a 1% performance drop. The code associated with this study can be found at this URL.	https://openaccess.thecvf.com//content/WACV2025/html/Chen_VLTP_Vision-Language_Guided_Token_Pruning_for_Task-Oriented_Segmentation_WACV_2025_paper.html	Hanning Chen, Yang Ni, Wenjun Huang, Yezi Liu, SungHeon Jeong, Fei Wen, Nathaniel Bastian, Hugo Latapie, Mohsen Imani
VM-Gait: Multi-Modal 3D Representation Based on Virtual Marker for Gait Recognition	Gait recognition plays a vital role in biometric applications by analyzing the unique characteristics of an individual's walking pattern. Methods based on 2D representations such as silhouettes and skeletons are increasingly being developed to learn the shape features and joint dynamic movements. Nevertheless the effectiveness of 2D representation-based methods is impeded by factors such as changes in viewpoint partial occlusion and noisy environments. 3D representation-based methods can complement 2D representation-based approaches by providing more precise dynamic body shapes and motion information along with increased robustness against changes in viewpoint and partial occlusion. However the complexity of acquiring accurate 3D representations and the challenges associated with extracting dynamic topological features from sequences of 3D representations hinder the development of 3D representations-based methods. In this paper we present VM-Gait a novel multi-modal gait recognition framework that harnesses the advantages of integrating both 2D and 3D representations. Furthermore we introduce a new 3D representation Virtual Marker into gait recognition to efficiently learn topological features from 3D representations avoiding the computational complexities inherent in directly learning from 3D representations like 3D meshes or 3D point clouds. Extensive experiments demonstrate that the proposed framework effectively learns and fuses discriminative information from different gait modalities enhancing gait recognition performance.	https://openaccess.thecvf.com//content/WACV2025/html/Wang_VM-Gait_Multi-Modal_3D_Representation_Based_on_Virtual_Marker_for_Gait_WACV_2025_paper.html	Zhao-Yang Wang, Jiang Liu, Jieneng Chen, Rama Chellappa
VMAs: Video-to-Music Generation via Semantic Alignment in Web Music Videos	We present a framework for learning to generate background music from video inputs. Unlike existing works that rely on symbolic musical annotations which are limited in quantity and diversity our method leverages large-scale web videos accompanied by background music. This enables our model to learn to generate realistic and diverse music. To accomplish this goal we develop a generative video-music Transformer with a novel semantic video-music alignment scheme. Our model uses a joint autoregressive and contrastive learning objective which encourages the generation of music aligned with high-level video content. We also introduce a novel video-beat alignment scheme to match the generated music beats with the low-level motions in the video. Lastly to capture fine-grained visual cues in a video needed for realistic background music generation we introduce a new temporal video encoder architecture allowing us to efficiently process videos consisting of many densely sampled frames. We train our framework on our newly curated DISCO-MV dataset consisting of 2.2M video-music samples which is orders of magnitude larger than any prior datasets used for video music generation. Our method outperforms existing approaches on the DISCO-MV and MusicCaps datasets according to various music generation evaluation metrics including human evaluation. Results are available at https://genjib.github.io/project_page/VMAs/index.html	https://openaccess.thecvf.com//content/WACV2025/html/Lin_VMAs_Video-to-Music_Generation_via_Semantic_Alignment_in_Web_Music_Videos_WACV_2025_paper.html	Yan-Bo Lin, Yu Tian, Linjie Yang, Gedas Bertasius, Heng Wang
VaLID: Variable-Length Input Diffusion for Novel View Synthesis	Novel View Synthesis (NVS) which tries to produce a realistic image at the target view given source view images and their corresponding poses is a fundamental problem in 3D Vision. As this task is heavily under-constrained some recent work like Zero123 [18] tries to solve this problem with generative modeling specifically using pre-trained diffusion models. Although this strategy generalizes well to new scenes compared to neural radiance field-based methods it offers low levels of flexibility. For example it can only accept a single-view image as input despite realistic applications often offering multiple input images. This is because the source-view images and corresponding poses are processed separately and injected into the model at different stages. Thus it is not trivial to generalize the model into multi-view source images once they are available. To solve this issue we try to process each pose image pair separately and then fuse them as a unified visual representation which will be injected into the model to guide image synthesis at the target-views. However inconsistency and computation costs increase as the number of input source-view images increases. To solve these issues the Multi-view Cross Former module is proposed which maps variable-length input data to fix-size output data. A two-stage training strategy is introduced to further improve the efficiency during training time. Qualitative and quantitative evaluation over multiple datasets demonstrates the effectiveness of the proposed method against previous approaches. The code will be released according to the acceptance.	https://openaccess.thecvf.com//content/WACV2025/html/Li_VaLID_Variable-Length_Input_Diffusion_for_Novel_View_Synthesis_WACV_2025_paper.html	Shijie Li, Farhad G. Zanjani, Haitam Ben Yahia, Yuki Asano, Juergen Gall, Amirhossein Habibian
VerA: Versatile Anonymization Applicable to Clinical Facial Photographs	The demand for privacy in facial image dissemination is gaining ground internationally echoed by the proliferation of regulations such as GDPR DPDPA CCPA PIPL and APPI. While recent advances in anonymization surpass pixelation or blur methods additional constraints to the task pose challenges. Largely unaddressed by current anonymization methods are clinical images and pairs of before-and-after clinical images illustrating facial medical interventions e.g. facial surgeries or dental procedures. We present VerA the first Versatile Anonymization framework that solves two challenges in clinical applications: A) it preserves selected semantic areas (e.g. mouth region) to show medical intervention results that is anonymization is only applied to the areas outside the preserved area; and B) it produces anonymized images with consistent personal identity across multiple photographs which is crucial for anonymizing photographs of the same person taken before and after a clinical intervention. We validate our results on both single and paired anonymization of clinical images through extensive quantitative and qualitative evaluation. We also demonstrate that VerA reaches the state of the art on established anonymization tasks in terms of photorealism and de-identification.	https://openaccess.thecvf.com//content/WACV2025/html/Helou_VerA_Versatile_Anonymization_Applicable_to_Clinical_Facial_Photographs_WACV_2025_paper.html	Majed El Helou, Doruk Cetin, Petar Stamenkovic, Niko Benjamin Huber, Fabio ZÃ¼nd
VideoGameBunny: Towards Vision Assistants for Video Games	Large multimodal models known as LMMs hold substantial promise across various domains from personal assistance in daily tasks to sophisticated applications like medical diagnostics. However their capabilities have limitations in the video game domain including challenges with scene understanding hallucinations and inaccurate descriptions of video game content especially in open-source models. This paper describes the development of VideoGameBunny a LLaVA-style model based on Bunny specifically tailored for understanding images from video games. We release intermediate checkpoints training logs and an extensive dataset comprising 185259 video game images from 413 titles along with 389565 image-instruction pairs that include image captions question-answer pairs and a JSON representation of 16 elements of 136974 images. Our experiments show that our high-quality game-related data has the potential to make a relatively small model outperform the much larger state-of-the-art model LLaVa-1.6-34b which has more than four times the number of parameters. Our study paves the way for future research in video game understanding on tasks such as playing commentary and debugging. Code and data are available at videogamebunny.github.io	https://openaccess.thecvf.com//content/WACV2025/html/Taesiri_VideoGameBunny_Towards_Vision_Assistants_for_Video_Games_WACV_2025_paper.html	Mohammad Reza Taesiri, Cor-Paul Bezemer
VioPose: Violin Performance 4D Pose Estimation by Hierarchical Audiovisual Inference	Musicians delicately control their bodies to generate music. Sometimes their motions are too subtle to be captured by the human eye. To analyze how they move to produce the music we need to estimate precise 4D human pose (3D pose over time). However current state-of-the-art (SoTA) visual pose estimation algorithms struggle to produce accurate monocular 4D poses because of occlusions partial views and human-object interactions. They are limited by the viewing angle pixel density and sampling rate of the cameras and fail to estimate fast and subtle movements such as in the musical effect of vibrato. We leverage the direct causal relationship between the music produced and the human motions creating them to address these challenges. We propose VioPose: a novel multimodal network that hierarchically estimates dynamics. High-level features are cascaded to low-level features and integrated into Bayesian updates. Our architecture is shown to produce accurate pose sequences facilitating precise motion analysis and outperforms SoTA. As part of this work we collected the largest and the most diverse calibrated violin-playing dataset including video sound and 3D motion capture poses. Code and dataset can be found in our project page https://sj-yoo.info/viopose/.	https://openaccess.thecvf.com//content/WACV2025/html/Yoo_VioPose_Violin_Performance_4D_Pose_Estimation_by_Hierarchical_Audiovisual_Inference_WACV_2025_paper.html	Seong Jong Yoo, Snehesh Shrestha, Irina Muresanu, Cornelia Fermuller
VipDiff: Towards Coherent and Diverse Video Inpainting via Training-Free Denoising Diffusion Models	Recent video inpainting methods have achieved encouraging improvements by leveraging optical flow to guide pixel propagation from reference frames either in the image space or feature space. However they would produce severe artifacts when the masked area is too large and no pixel correspondences could be found. Recently denoising diffusion models have demonstrated impressive performance in generating diverse and high-quality images and have been exploited in a number of works for image inpainting. These methods however cannot be applied directly to videos to produce temporal-coherent inpainting results. In this paper we propose a training-free framework named VipDiff for conditioning diffusion model on the reverse diffusion process to produce temporal-coherent inpainting results without requiring any training data or fine-tuning the pre-trained models. VipDiff takes optical flow as guidance to extract valid pixels from reference frames to serve as constraints in optimizing the randomly sampled Gaussian noise and uses the generated results for further pixel propagation and conditional generation. VipDiff also allows for generating diverse video inpainting results over different sampled noise. Experiments demonstrate that our VipDiff outperforms state-of-the-art methods in terms of both spatial-temporal coherence and fidelity.	https://openaccess.thecvf.com//content/WACV2025/html/Xie_VipDiff_Towards_Coherent_and_Diverse_Video_Inpainting_via_Training-Free_Denoising_WACV_2025_paper.html	Chaohao Xie, Kai Han, Kwan-Yee K. Wong
Vision-Aware Text Features in Referring Image Segmentation: From Object Understanding to Context Understanding	Referring image segmentation is a challenging task that involves generating pixel-wise segmentation masks based on natural language descriptions. The complexity of this task increases with the intricacy of the sentences provided. Existing methods have relied mostly on visual features to generate the segmentation masks while treating text features as supporting components. However this under-utilization of text understanding limits the model's capability to fully comprehend the given expressions. In this work we propose a novel framework that specifically emphasizes object and context comprehension inspired by human cognitive processes through Vision-Aware Text Features. Firstly we introduce a CLIP Prior module to localize the main object of interest and embed the object heatmap into the query initialization process. Secondly we propose a combination of two components: Contextual Multimodal Decoder and Meaning Consistency Constraint to further enhance the coherent and consistent interpretation of language cues with the contextual understanding obtained from the image. Our method achieves significant performance improvements on three benchmark datasets RefCOCO RefCOCO+ and G-Ref. Project page: https://vatex.hkustvgd.com.	https://openaccess.thecvf.com//content/WACV2025/html/Nguyen-Truong_Vision-Aware_Text_Features_in_Referring_Image_Segmentation_From_Object_Understanding_WACV_2025_paper.html	Hai Nguyen-Truong, E-Ro Nguyen, Tuan-Anh Vu, Minh-Triet Tran, Binh-Son Hua, Sai-Kit Yeung
Vision-Based Landing Guidance through Tracking and Orientation Estimation	Fixed-wing aerial vehicles are equipped with functionalities such as ILS (instrument landing system) PAR (precision approach radar) and DGPS (differential global positioning system) enabling fully automated landings. However these systems impose significant costs on airport operations due to high installation and maintenance requirements. Moreover since these navigation parameters come from ground or satellite signals they are vulnerable to interference. A more cost-effective and independent alternative for guiding landing is a vision-based system that detects the runway and aligns the aircraft reducing the pilot's cognitive load. This paper proposes a novel framework that addresses three key challenges in developing autonomous vision-based landing systems. Firstly to overcome the lack of aerial front-view video data we created high-quality videos simulating landing approaches through the generator code available in the LARD (landing approach runway detection dataset) repository. Secondly in contrast to former studies focusing on object detection for finding the runway we chose the state-of-the-art model LoRAT to track runways within bounding boxes in each video frame. Thirdly to align the aircraft with the designated landing runway we extract runway keypoints from the resulting LoRAT frames and estimate the camera relative pose via the Perspective-n-Point algorithm. Our experimental results over a dataset of generated videos and original images from the LARD dataset consistently demonstrate the proposed framework's highly accurate tracking and alignment capabilities. Our approach source code and the LoRAT model pre-trained with LARD videos are available at https://github.com/jpklock2/vision-based-landing-guidance	https://openaccess.thecvf.com//content/WACV2025/html/Ferreira_Vision-Based_Landing_Guidance_through_Tracking_and_Orientation_Estimation_WACV_2025_paper.html	JoÃ£o P. K. Ferreira, JoÃ£o P. Pinto, JÃºlia Moura, Yi Li, Cristiano L. Castro, Plamen Angelov
Visual Robustness Benchmark for Visual Question Answering (VQA)	Can Visual Question Answering (VQA) systems maintain their performance when deployed in the real world? Or are they susceptible to realistic corruption effects e.g. image blur which can be detrimental in sensitive applications such as medical VQA? While linguistic robustness has been thoroughly explored within the VQA literature there has yet to be any significant work on visual robustness. In this work we present the first large-scale benchmark to evaluate the visual robustness of VQA models including multimodal large language models and zero-shot evaluation and assess the strength of the realistic corruption effects. Additionally we have designed several robustness evaluation metrics that quantify an aspect of robustness. These sub-metrics can be aggregated into a unified metric and tailored to fit a variety of use cases. The experiments reveal important insights into the relationships between model size accuracy and robustness against the visual corruptions. Our benchmark highlights the need for a balanced approach in model development that considers model performance without compromising robustness.	https://openaccess.thecvf.com//content/WACV2025/html/Ishmam_Visual_Robustness_Benchmark_for_Visual_Question_Answering_VQA_WACV_2025_paper.html	Farhan Ishmam, Ishmam Tashdeed, Talukder Asir Saadat, Hamjajul Ashmafee, Abu Raihan Mostofa Kamal, Azam Hossain
VisualFusion: Enhancing Blog Content with Advanced Infographic Pipeline	Infographics represent a key component of any blog or article facilitating effective communication of ideas while fostering reader engagement. However many content creators possess limited expertise in crafting visually striking infographics. This gap is effectively addressed by our proposed pipeline designed to aid writers in generating compelling infographics tailored to their written content. Our pipeline uses textual content and tabular data from the blog to generate anchor plots. Leveraging LLM for prompt generation the pipeline integrates the generated prompts with these anchor plots through an Image to Image (I2I) generation model. We observe that the majority of the resulting images generated using this approach align with the article's narrative and effectively represent the underlying tabular data. Additionally we introduce our proposed AADaT (Aesthetical Adherence to Data and Text) Score adept at comprehensively assessing aesthetics textual alignment data fidelity and overall image quality concurrently. In comparative evaluations our pipeline has demonstrated around 15% superior performance relative to state-of-the-art models such as DALLE and Stable Diffusion Large by showcasing much better data adherence and aesthetics. While state-of-the-art models excel in some metrics but falter in others our pipeline demonstrates a balanced performance across all metrics.	https://openaccess.thecvf.com//content/WACV2025/html/Deo_VisualFusion_Enhancing_Blog_Content_with_Advanced_Infographic_Pipeline_WACV_2025_paper.html	Anurag Deo, Savita Bhat, Shirish Karande
Volumetric Conditioning Module to Control Pretrained Diffusion Models for 3D Medical Images	Spatial control methods using additional modules on pretrained diffusion models have gained attention for enabling conditional generation in natural images. These methods guide the generation process with new conditions while leveraging the capabilities of large models. They could be beneficial as training strategies in the context of 3D medical imaging where training a diffusion model from scratch is challenging due to high computational costs and data scarcity. However the potential application of spatial control methods with additional modules to 3D medical images has not yet been explored. In this paper we present a tailored spatial control method for 3D medical images with a novel lightweight module Volumetric Conditioning Module (VCM). Our VCM employs an asymmetric U-Net architecture to effectively encode complex information from various levels of 3D conditions providing detailed guidance in image synthesis. To examine the applicability of spatial control methods and the effectiveness of VCM for 3D medical data we conduct experiments under single and multimodal conditions scenarios across a wide range of dataset sizes from extremely small datasets with 10 samples to large datasets with 500 samples. The experimental results show that the VCM is effective for conditional generation and efficient in terms of requiring less training data and computational resources. We further investigate the potential applications for our spatial control method through axial super-resolution for medical images. Our code is available at https://github.com/SSTDVProject/ VCM.git	https://openaccess.thecvf.com//content/WACV2025/html/Ahn_Volumetric_Conditioning_Module_to_Control_Pretrained_Diffusion_Models_for_3D_WACV_2025_paper.html	Suhyun Ahn, Wonjung Park, Jihoon Cho, Jinah Park
VortSDF: 3D Modeling with Centroidal Voronoi Tesselation on Signed Distance Field	Volumetric shape representations have become ubiquitous in multi-view reconstruction tasks. They often build on regular voxel grids as discrete representations of 3D shape functions such as SDF or radiance fields either as the full shape model or as sampled instantiations of continuous representations as with neural networks. Despite their proven efficiency voxel representations come with the precision versus complexity trade-off. This inherent limitation can significantly impact performance when moving away from simple and uncluttered scenes. In this paper we investigate an alternative discretization strategy with the Centroidal Voronoi Tessellation (CVT). CVTs allow to better partition the observation space with respect to shape occupancy and to focus the discretization around shape surfaces. To leverage this discretization strategy for multi-view reconstruction we introduce a volumetric optimization framework that combines explicit SDF fields with a shallow color network in order to estimate 3D shape properties over tetrahedral grids. Experimental results with Chamfer statistics validate this approach with unprecedented reconstruction quality on various scenarios such as objects open scenes or human.	https://openaccess.thecvf.com//content/WACV2025/html/Thomas_VortSDF_3D_Modeling_with_Centroidal_Voronoi_Tesselation_on_Signed_Distance_WACV_2025_paper.html	Diego Thomas, Briac Toussaint, Jean-Sebastien Franco, Edmond Boyer
WAFFLE: Multimodal Floorplan Understanding in the Wild	Buildings are a central feature of human culture and are increasingly being analyzed with computational methods. However recent works on computational building understanding have largely focused on natural imagery of buildings neglecting the fundamental element defining a building's structure - its floorplan. Conversely existing works on floorplan understanding are extremely limited in scope often focusing on floorplans of a single semantic category and region (e.g. floorplans of apartments from a single country). In this work we introduce WAFFLE a novel multimodal floorplan understanding dataset of nearly 20K floorplan images and metadata curated from Internet data spanning diverse building types locations and data formats. By using a large language model and multimodal foundation models we curate and extract semantic information from these images and their accompanying noisy metadata. We show that WAFFLE enables progress on new building understanding tasks both discriminative and generative which were not feasible using prior datasets. We will publicly release WAFFLE along with our code and trained models providing the research community with a new foundation for learning the semantics of buildings.	https://openaccess.thecvf.com//content/WACV2025/html/Ganon_WAFFLE_Multimodal_Floorplan_Understanding_in_the_Wild_WACV_2025_paper.html	Keren Ganon, Morris Alper, Rachel Mikulinsky, Hadar Averbuch-Elor
WARLearn: Weather-Adaptive Representation Learning	This paper introduces WARLearn a novel framework designed for adaptive representation learning in challenging and adversarial weather conditions. Leveraging the in-variance principal used in Barlow Twins we demonstrate the capability to port the existing models initially trained on clear weather data to effectively handle adverse weather conditions. With minimal additional training our method exhibits remarkable performance gains in scenarios characterized by fog and low-light conditions. This adaptive framework extends its applicability beyond adverse weather settings offering a versatile solution for domains exhibiting variations in data distributions. Furthermore WARLearn is invaluable in scenarios where data distributions undergo significant shifts over time enabling models to remain updated and accurate. Our experimental findings reveal a remarkable performance with a mean average precision (mAP) of 52.6% on unseen real-world foggy dataset (RTTS). Similarly in low light conditions our framework achieves a mAP of 55.7% on unseen real-world low light dataset (ExDark). Notably WARLearn surpasses the performance of state-of-the-art frameworks including FeatEnHancer Image Adaptive YOLO DENet C2PNet PairLIE and ZeroDCE by a substantial margin in adverse weather improving the baseline performance in both foggy and low light conditions. The WARLearn code is available at https://github.com/ShubhamAgarwal12/WARLearn	https://openaccess.thecvf.com//content/WACV2025/html/Agarwal_WARLearn_Weather-Adaptive_Representation_Learning_WACV_2025_paper.html	Shubham Agarwal, Raz Birman, Ofer Hadar
WINE : Wavelet-Guided GAN Inversion and Editing for High-Fidelity Refinement	Recent advanced GAN inversion models aim to convey high-fidelity information from original images to generators through methods using generator tuning or high-dimensional feature learning. Despite these efforts accurately reconstructing image-specific details remains as a challenge due to the inherent limitations both in terms of training and structural aspects leading to a bias towards low-frequency information. In this paper we look into the widely used pixel loss in GAN inversion revealing its predominant focus on the reconstruction of low-frequency features. We then propose WINE a Wavelet-guided GAN inversion and editing model which transfers the high-frequency information through wavelet coefficients via newly proposed wavelet loss and wavelet fusion scheme. Notably WINE is the first attempt to interpret GAN inversion in the frequency domain. Our experimental results showcase the precision of WINE in preserving high-frequency details and enhancing image quality. Even in editing scenarios WINE outperforms existing state-of-the-art GAN inversion models with a fine balance between editability and reconstruction quality. Pre-trained model and codes will be publicized after the review process.	https://openaccess.thecvf.com//content/WACV2025/html/Kim_WINE__Wavelet-Guided_GAN_Inversion_and_Editing_for_High-Fidelity_Refinement_WACV_2025_paper.html	Chaewon Kim, Seung Jun Moon, Gyeong-Moon Park
Wavelength- and Depth-Aware Deep Image Prior for Blind Hyperspectral Imagery Deblurring with Coarse Depth Guidance	Hyperspectral imagery (HSI) provides detailed spectral information enabling precise analysis of materials. However HSI imaging suffers from blurring degradation which results in the loss of fine details and hinders subsequent applications. The degree of blurriness is highly related to wavelength and depth existing deblurring methods either lack the utilization of spectral correlation or ignore the depth variation since paired HSI and depth data are difficult to acquire and less discussed leading to degraded performance when encountering wide-range HSIs of non-planar scenes. To address these challenges in both data acquisition and algorithm design we propose a novel approach that simultaneously collects both modalities and integrates depth refinement into a blind HSI deblurring model with wavelength- and depth-aware deep image prior. Specifically we capture blurred HSI and coarse depth map with separate devices followed by registration. Our method performs depth-guided deblurring through depth-variant multi-channel kernel estimation and soft-weight map-based layer composition while simultaneously refining the depth. The proposed approach effectively restores fine details with fewer artifacts showing superior performance for both simulated blurred HSIs and real captured HSIs.	https://openaccess.thecvf.com//content/WACV2025/html/Li_Wavelength-_and_Depth-Aware_Deep_Image_Prior_for_Blind_Hyperspectral_Imagery_WACV_2025_paper.html	Jiahuan Li, Xiaoyu Dong, Wei He, Naoto Yokoya
WeedsGalore: A Multispectral and Multitemporal UAV-Based Dataset for Crop and Weed Segmentation in Agricultural Maize Fields	Weeds are one of the major reasons for crop yield loss but current weeding practices fail to manage weeds in an efficient and targeted manner. Effective weed management is especially important for crops with high worldwide production such as maize to maximize crop yield for meeting increasing global demands. Advances in near-sensing and computer vision enable the development of new tools for weed management. Specifically state-of-the-art segmentation models coupled with novel sensing technologies can facilitate timely and accurate weeding and monitoring systems. However learning-based approaches require annotated data and show a lack of generalization to aerial imaging for different crops. We present a novel dataset for semantic and instance segmentation of crops and weeds in agricultural maize fields. The multispectral UAV-based dataset contains images with RGB red-edge and near-infrared bands a large number of plant instances dense annotations for maize and four weed classes and is multitemporal. We provide extensive baseline results for both tasks including probabilistic methods to quantify prediction uncertainty improve model calibration and demonstrate the approach's applicability to out-of-distribution data. The results show the effectiveness of the two additional bands compared to RGB only and better performance in our target domain than models trained on existing datasets. We hope our dataset advances research on methods and operational systems for fine-grained weed identification enhancing the robustness and applicability of UAV-based weed management. The dataset and code are available at https://github.com/GFZ/weedsgalore.	https://openaccess.thecvf.com//content/WACV2025/html/Celikkan_WeedsGalore_A_Multispectral_and_Multitemporal_UAV-Based_Dataset_for_Crop_and_WACV_2025_paper.html	Ekin Celikkan, Timo Kunzmann, Yertay Yeskaliyev, Sibylle Itzerott, Nadja Klein, Martin Herold
Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers	Few-shot knowledge distillation recently emerged as a viable approach to harness the knowledge of large-scale pre-trained models using limited data and computational resources. In this paper we propose a novel few-shot feature distillation approach for vision transformers. Our approach is based on two key steps. Leveraging the fact that vision transformers have a consistent depth-wise structure we first copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students) where the intermittence factor controls the complexity of the student transformer with respect to its teacher. Next we employ an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge into the student in a few-shot scenario aiming to recover the information processing carried out by the skipped teacher layers. We present comprehensive experiments with supervised and self-supervised transformers as teachers on six data sets from various domains (natural medical and satellite images) and tasks (classification and segmentation). The empirical results confirm the superiority of our approach over state-of-the-art competitors. Moreover the ablation results demonstrate the usefulness of each component of the proposed pipeline. We release our code at https://github.com/dianagrigore/WeCoLoRA.	https://openaccess.thecvf.com//content/WACV2025/html/Grigore_Weight_Copy_and_Low-Rank_Adaptation_for_Few-Shot_Distillation_of_Vision_WACV_2025_paper.html	Diana-Nicoleta Grigore, Mariana-Iuliana Georgescu, Jon Alvarez Justo, Tor Johansen, Andreea Iuliana Ionescu, Radu Tudor Ionescu
When Cars Meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather	In Federated Learning (FL) multiple clients collaboratively train a global model without sharing private data. In semantic segmentation the Federated source Free Domain Adaptation (FFREEDA) setting is of particular interest where clients undergo unsupervised training after supervised pretraining at the server side. While few recent works address FL for autonomous vehicles intrinsic real-world challenges such as the presence of adverse weather conditions and the existence of different autonomous agents are still unexplored. To bridge this gap we address both problems and introduce a new federated semantic segmentation setting where both car and drone clients co-exist and collaborate. Specifically we propose a novel approach for this setting which exploits a batch-norm weather-aware strategy to dynamically adapt the model to the different weather conditions while hyperbolic space prototypes are used to align the heterogeneous client representations. Finally we introduce FLYAWARE the first semantic segmentation dataset with adverse weather data for aerial vehicles.	https://openaccess.thecvf.com//content/WACV2025/html/Rizzoli_When_Cars_Meet_Drones_Hyperbolic_Federated_Learning_for_Source-Free_Domain_WACV_2025_paper.html	Giulia Rizzoli, Matteo Caligiuri, Donald Shenaj, Francesco Barbato, Pietro Zanuttigh
When Visual State Space Model Meets Backdoor Attacks	The recently proposed Visual State Space Model (VMamba) operating on the principle of state space mechanisms (SSM) processes images as a sequence of patches and outperforms Vision Transformers (ViT) in several computer vision tasks. Given their substantial design differences from CNNs and ViT it is crucial to investigate their vulnerability to backdoor attacks and the impact of various advanced backdoor attacks on their robustness. Backdoor attacks involve embedding a specific trigger into a small subset of training images which remains dormant until activated later. While the model performs well on clean test images an attacker can manipulate its decisions by presenting the trigger in one of the test images. This work examines state-of-the-art (SOTA) vision architectures (ResNet ViT MLP-mixer VMamba) focusing on their susceptibility to backdoor attacks and the effect of different backdoor attacks on their robustness. The well-known Visual State Space Model (VMamba) is the least susceptible to backdoor attacks among these architectures. To address this in this paper we propose two novel QR decomposition-based backdoor attacks that are visually imperceptible and achieve a high attack success rate (ASR) against the VMamba. We also present a qualitative analysis of the proposed backdoor attacks explaining the reasons behind their success or failure against the VMamba model. Experiments and results conducted on two popular image datasets (CIFAR-10 and ImageNet-1K) demonstrate that the proposed backdoor attacks exceed the performance of SOTA backdoor attacks and effectively fool the recently proposed VMamba model.	https://openaccess.thecvf.com//content/WACV2025/html/Nagaonkar_When_Visual_State_Space_Model_Meets_Backdoor_Attacks_WACV_2025_paper.html	Sankalp Nagaonkar, Achyut Mani Tripathi, Ashish Mishra
Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers	Self-attention in Transformers comes with a high computational cost because of their quadratic computational complexity but their effectiveness in addressing problems in language and vision has sparked extensive research aimed at enhancing their efficiency. However diverse experimental conditions spanning multiple input domains prevent a fair comparison based solely on reported results posing challenges for model selection. To address this gap in comparability we perform a large-scale benchmark of more than 45 models for image classification evaluating key efficiency aspects including accuracy speed and memory usage. Our benchmark provides a standardized baseline for efficiency-oriented transformers. We analyze the results based on the Pareto front - the boundary of optimal models. Surprisingly despite claims of other models being more efficient ViT remains Pareto optimal across multiple metrics. We observe that hybrid attention-CNN models exhibit remarkable inference memory- and parameter-efficiency. Moreover our benchmark shows that using a larger model in general is more efficient than using higher resolution images. Thanks to our holistic evaluation we provide a centralized resource for practitioners and researchers facilitating informed decisions when selecting or developing efficient transformers.	https://openaccess.thecvf.com//content/WACV2025/html/Nauen_Which_Transformer_to_Favor_A_Comparative_Analysis_of_Efficiency_in_WACV_2025_paper.html	Tobias Christian Nauen, Sebastian Palacio, Federico Raue, Andreas Dengel
Who Brings the Frisbee: Probing Hidden Hallucination Factors in Large Vision-Language Model via Causality Analysis	Recent advancements in large vision-language models (LVLM) have significantly enhanced their ability to comprehend visual inputs alongside natural language. However a major challenge in their real-world application is hallucination where LVLMs generate non-existent visual elements eroding user trust. The underlying mechanism driving this multimodal hallucination is poorly understood. Minimal research has illuminated whether contexts such as sky tree or grass field involve the LVLM in hallucinating a frisbee. We hypothesize that hidden factors such as objects contexts and semantic foreground-background structures induce hallucination. This study proposes a novel causal approach: a hallucination probing system to identify these hidden factors. By analyzing the causality between images text prompts and network saliency we systematically explore interventions to block these factors. Our experimental findings show that a straightforward technique based on our analysis can significantly reduce hallucinations. Additionally our analyses indicate the potential to edit network internals to minimize hallucinated outputs.	https://openaccess.thecvf.com//content/WACV2025/html/Huang_Who_Brings_the_Frisbee_Probing_Hidden_Hallucination_Factors_in_Large_WACV_2025_paper.html	Po-Hsuan Huang, Jeng-Lin Li, Chin-Po Chen, Ming-Ching Chang, Wei-Chao Chen
WiGNet: Windowed Vision Graph Neural Network	In recent years Graph Neural Networks (GNNs) have demonstrated strong adaptability to various real-world challenges with architectures such as Vision GNN (ViG) achieving state-of-the-art performance in several computer vision tasks. However their practical applicability is hindered by the computational complexity of constructing the graph which scales quadratically with the image size. In this paper we introduce a novel Windowed vision Graph neural Network (WiGNet) model for efficient image processing. WiGNet explores a different strategy from previous works by partitioning the image into windows and constructing a graph within each window. Therefore our model uses graph convolutions instead of the typical 2D convolution or self-attention mechanism. WiGNet effectively manages computational and memory complexity for large image sizes. We evaluate our method in the ImageNet-1k benchmark dataset and test the adaptability of WiGNet using the CelebA-HQ dataset as a downstream task with higher-resolution images. In both of these scenarios our method achieves competitive results compared to previous vision GNNs while keeping memory and computational complexity at bay. WiGNet offers a promising solution toward the deployment of vision GNNs in real-world applications. We publicly released the code and pre-trained models at https://github.com/EIDOSLAB/WiGNet.	https://openaccess.thecvf.com//content/WACV2025/html/Spadaro_WiGNet_Windowed_Vision_Graph_Neural_Network_WACV_2025_paper.html	Gabriele Spadaro, Marco Grangetto, Attilio Fiandrotti, Enzo Tartaglione, Jhony H. Giraldo
XPose: Towards Extreme Low Light Hand Pose Estimation	Recent advances in deep learning have enabled considerable strides in hand pose estimation in well-lit conditions. However to the best of our knowledge there is no existing method for hand pose estimation from RGB images captured in low light conditions. This task is highly challenging due to the overwhelming amount of noise which plague image capture in low-light conditions (<1 lux). In this paper we propose XPose the first method for extreme low light hand pose estimation from RGB images. We also introduce the first dataset for low light hand pose estimation consisting of 120k images along with accurate hand pose labels. Our dataset consists of images captured in low light and well-lit conditions from multiple viewpoints. We propose an innovative deep learning based methodology for monocular low-light hand pose estimation using guidance from well-lit and multi-view images available in our dataset during training time. We show that our method using the proposed LLPose dataset significantly outperforms existing methods for hand pose estimation both qualitatively and quantitatively in low light conditions.	https://openaccess.thecvf.com//content/WACV2025/html/Rosh_XPose_Towards_Extreme_Low_Light_Hand_Pose_Estimation_WACV_2025_paper.html	Green Rosh, Meghana Shankar, Prateek Kukreja, Anmol Namdev, Pawan Prasad B H
XR-MBT: Multi-Modal Full Body Tracking for XR through Self-Supervision with Learned Depth Point Cloud Registration	Tracking the full body motions of users in XR (AR/VR) devices is a fundamental challenge to bring a sense of authentic social presence. Due to the absence of dedicated leg sensors currently available body tracking methods adopt a synthesis approach to generate plausible motions given a 3-point signal from the head and controller tracking. In order to enable mixed reality features modern XR devices are capable of estimating depth information of the headset surroundings using available sensors combined with dedicated machine learning models. Such egocentric depth sensing cannot drive the body directly as it is not registered and is incomplete due to limited field-of-view and body self-occlusions. For the first time we propose to leverage the available depth sensing signal combined with self-supervision to learn a multi-modal pose estimation model capable of tracking full body motions in real time on XR devices. We demonstrate how current 3-point motion synthesis models can be extended to point cloud modalities using a semantic point cloud encoder network combined with a residual network for multi-modal pose estimation. These modules are trained jointly in a self-supervised way leveraging a combination of real unregistered point clouds and simulated data obtained from motion capture. We compare our approach against several state-of-the-art systems for XR body tracking and show that our method accurately tracks a diverse range of body motions. XR-MBT tracks legs in XR for the first time whereas traditional synthesis approaches based on partial body tracking are blind.	https://openaccess.thecvf.com//content/WACV2025/html/Rozumnyi_XR-MBT_Multi-Modal_Full_Body_Tracking_for_XR_through_Self-Supervision_with_WACV_2025_paper.html	Denys Rozumnyi, Nadine Bertsch, Othman Sbai, Filippo Arcadu, Yuhua Chen, Artsiom Sanakoyeu, Manoj Kumar, Catherine Herold, Robin Kips
ZAHA: Introducing the Level of Facade Generalization and the Large-Scale Point Cloud Facade Semantic Segmentation Benchmark Dataset	Facade semantic segmentation is a long-standing challenge in photogrammetry and computer vision. Although the last decades have witnessed the influx of facade segmentation methods there is a lack of comprehensive facade classes and data covering the architectural variability. In ZAHA we introduce Level of Facade Generalization (LoFG) novel hierarchical facade classes designed based on international urban modeling standards ensuring compatibility with real-world challenging classes and uniform methods' comparison. Realizing the LoFG we present to date the largest semantic 3D facade segmentation dataset providing 601 million annotated points at five and 15 classes of LoFG2 and LoFG3 respectively. Moreover we analyze the performance of baseline semantic segmentation methods on our introduced LoFG classes and data complementing it with a discussion on the unresolved challenges for facade segmentation. We firmly believe that ZAHA shall facilitate further development of 3D facade semantic segmentation methods enabling robust segmentation indispensable in creating urban digital twins.	https://openaccess.thecvf.com//content/WACV2025/html/Wysocki_ZAHA_Introducing_the_Level_of_Facade_Generalization_and_the_Large-Scale_WACV_2025_paper.html	Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst
Zero-Shot Class Unlearning in CLIP with Synthetic Samples	Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals' right to be forgotten under rigorous regulations such as GDPR. In this work we focus on unlearning within CLIP a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally importantly we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.	https://openaccess.thecvf.com//content/WACV2025/html/Kravets_Zero-Shot_Class_Unlearning_in_CLIP_with_Synthetic_Samples_WACV_2025_paper.html	Alexey Kravets, Vinay Namboodiri
Zero-Shot Detection of Out-of-Context Objects using Foundation Models	We address the problem of detecting out-of-context (OOC) objects in a scene. Given an image we aim to detect whether the image has objects that are not present in their usual context and localize such OOC objects. Existing approaches for OOC detection rely on defining the common context in terms of the manually constructed features such as the co-occurrence of objects spatial relations between objects and shape and size of the objects and then learning such context for a given dataset. But context is often nuanced ranging from very common to very surprising. Further learned context from specific datasets may not be generalized as datasets may not truly represent the human notion of what is in context. Motivated by the success of large language models and more generally foundation models (FMs) in common sense reasoning we investigate the FM's ability to capture a more generalized notion of context. We find that a pre-trained FM such as GPT-4 provides a more nuanced notion of OOC and enables zero-shot OOC detection when coupled with other pre-trained FMs for caption generation such as BLIP-2 and image in-painting with Stable Diffusion 2.0. Our approach does not need any dataset-specific training. We demonstrate the efficacy of our approach on two OOC object detection datasets achieving 90.8% zero-shot accuracy on the MIT-OOC dataset and 87.26% on the IJCAI22-COCO-OOC dataset.	https://openaccess.thecvf.com//content/WACV2025/html/Roy_Zero-Shot_Detection_of_Out-of-Context_Objects_using_Foundation_Models_WACV_2025_paper.html	Anirban Roy, Adam Cobb, Ramneet Kaur, Sumit Jha, Nathaniel Bastian, Alexander Berenbeim, Robert Thomson, Iain Cruickshank, Alvaro Velasquez, Susmit Jha
ZeroComp: Zero-Shot Object Compositing from Image Intrinsics via Diffusion	We present ZeroComp an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with a Stable Diffusion model to utilize its scene priors together operating as an effective rendering engine. During training ZeroComp uses intrinsic images based on geometry albedo and masked shading all without the need for paired images of scenes with and without composite objects. Once trained it seamlessly integrates virtual 3D objects into scenes adjusting shading to create realistic composites. We develop a high-quality evaluation dataset and demonstrate that ZeroComp outperforms methods using explicit lighting estimations and generative techniques in quantitative and human perception benchmarks. Additionally ZeroComp extends to real and outdoor image compositing even when trained solely on synthetic indoor data showcasing its effectiveness in image compositing.	https://openaccess.thecvf.com//content/WACV2025/html/Zhang_ZeroComp_Zero-Shot_Object_Compositing_from_Image_Intrinsics_via_Diffusion_WACV_2025_paper.html	Zitian Zhang, FrÃ©dÃ©ric Fortier-Chouinard, Mathieu Garon, Anand Bhattad, Jean-FranÃ§ois Lalonde
eLIR-Net: An Efficient AI Solution for Image Retouching	Picture quality serves as a primary differentiator for prominent display panel manufacturers. AI-based solutions have made remarkable progress in delivering expert-level image color remastering operations. However their demand on intensive computation resources heavily impedes the on-device usage in industries where speed and scale are crucial. In this paper we propose an extremely lightweight image-retouching network (eLIR-Net) that can be deployed on resource-restricted hardware like mobile terminals and embedded devices. The eLIR-Net takes in easily computable and intuitive color distributions to represent the macro view of an image which is encoded by a well-designed condition network to guide the generative base network to converge in the desired direction thus generating visually captivating outputs. Both quantitative and qualitative results show the proposed eLIR-Net can achieve equivalent or superior performance compared to the benchmark models at an affordable cost 6.8K parameters which is 24.1% the size of the smallest state-of-the-art network to the best of our knowledge. This work showcases the possibility that compact models deliver competitive performance compared to large models with affordable cost that enables the benefits of AI to be shared across a wider range of industrial applications.	https://openaccess.thecvf.com//content/WACV2025/html/Zhao_eLIR-Net_An_Efficient_AI_Solution_for_Image_Retouching_WACV_2025_paper.html	Tingting Zhao, Chenguang Liu, Kamal Jnawali, Chang Su
uLayout: Unified Room Layout Estimation for Perspective and Panoramic Images	We present uLayout a unified model for estimating room layout geometries from both perspective and panoramic images whereas traditional solutions require different model designs for each image type. The key idea of our solution is to unify both domains into the equirectangular projection particularly allocating perspective images into the most suitable latitude coordinate to effectively exploit both domains seamlessly. To address the Field-of-View (FoV) difference between the input domains we design uLayout with a shared feature extractor with an extra 1D-Convolution layer to condition each domain input differently. This conditioning allows us to efficiently formulate a column-wise feature regression problem regardless of the FoV input. This simple yet effective approach achieves competitive performance with current state-of-the-art solutions and shows for the first time a single end-to-end model for both domains. Extensive experiments in the real-world datasets LSUN Matterport3D PanoContext and Stanford 2D-3D evidence the contribution of our approach. Code is available at https://github.com/JonathanLee112/uLayout	https://openaccess.thecvf.com//content/WACV2025/html/Lee_uLayout_Unified_Room_Layout_Estimation_for_Perspective_and_Panoramic_Images_WACV_2025_paper.html	Jonathan Lee, Bolivar E Solarte, Chin-Hsuan Wu, Jin-Cheng Jhang, Fu-En Wang, Yi-Hsuan Tsai, Min Sun
